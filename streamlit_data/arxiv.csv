title,keywords,authors,abstract,date,pdf
Interplay of Quasi-Quantum Hall Effect and Coulomb Disorder in Semimetals,['Materials Science'],"['Ian A. Leahy', 'Anthony D. Rice', 'Jocienne N. Nelson', 'Herve Ness', 'Mark van Schilfgaarde', 'Wei Pan', 'Kirstin Alberi']","Low carrier densities in topological semimetals (TSMs) enable the exploration of novel magnetotransport in the quantum limit (QL). Reports consistent with 3D quasi-quantum Hall effect (QQHE) have repositioned TSMs as promising platforms for exploring 3D quantum Hall transport, but the lack of tunability in the Fermi has thus far limited the ability to control the QQHE signal. Here, we tune the defect concentrations in the Dirac semimetal Cd${}_3$As${}_2$ to achieve ultra-low carrier concentrations at 2 K around $2.9\times10^{16}$cm${}^{-3}$, giving way to QQHE signal at modest fields under 10 T. At low carrier densities, where QQHE is most accessible, we find that a zero resistivity state is obscured by a carrier density dependent background originating from Coulomb disorder from charged point defects. Our results highlight the interplay between QQHE and Coulomb disorder scattering, demonstrating that clear observation of QQHE in TSMs intricately depends on Fermi level. Predicted in TSMs a decade ago, we find that Coulomb disorder is an essential ingredient for understanding the magnetoresistivity for a spectrum of Fermi levels, experimentally anchoring the important roles of defects and charged disorder in TSM applications. We discuss future constraints and opportunities in exploring 3D QHE in TSMs.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05273
Real-space chirality from crystalline topological defects in the Kitaev spin liquid,['Strongly Correlated Electrons'],"['Fay Borhani', 'Arnab Seth', 'Itamar Kimchi']","We show that certain crystalline topological defects in the gapless Kitaev honeycomb spin liquid model generate a chirality that depends in a universal manner on their emergent flux. Focusing on 5-7 dislocations as building blocks, consisting of pentagon and heptagon disclinations, we identify the Kitaev bond label configurations that preserve solvability. By defining and computing multiple formulations of local Chern markers we find that the 5 and 7 lattice defects host a contribution to real-space chirality, which we dub ""Chern charge"" or $\mathcal{Q_M}$. The sign of this local Chern charge obeys $\text{sgn}({\mathcal{Q_M}}) = - i ~ \text{sgn}(F) W$ where $F$ is the disclination Frank angle and $W = \pm i$ is its emergent-gauge-field flux. We show that lattice curvature and torsion can interplay with the surrounding gapless background to modify the $\mathcal{Q_M}$ profile and magnitude, but that the sign of $\mathcal{Q_M}$ is set by local properties of the defect, enabling the identification of Chern charges.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05272
Chimera: Accurate retrosynthesis prediction by ensembling models with diverse inductive biases,['Machine Learning'],"['Krzysztof Maziarz', 'Guoqing Liu', 'Hubert Misztela', 'Aleksei Kornev', 'Piotr Gaiński', 'Holger Hoefling', 'Mike Fortunato', 'Rishi Gupta', 'Marwin Segler']","Planning and conducting chemical syntheses remains a major bottleneck in the discovery of functional small molecules, and prevents fully leveraging generative AI for molecular inverse design. While early work has shown that ML-based retrosynthesis models can predict reasonable routes, their low accuracy for less frequent, yet important reactions has been pointed out. As multi-step search algorithms are limited to reactions suggested by the underlying model, the applicability of those tools is inherently constrained by the accuracy of retrosynthesis prediction. Inspired by how chemists use different strategies to ideate reactions, we propose Chimera: a framework for building highly accurate reaction models that combine predictions from diverse sources with complementary inductive biases using a learning-based ensembling strategy. We instantiate the framework with two newly developed models, which already by themselves achieve state of the art in their categories. Through experiments across several orders of magnitude in data scale and time-splits, we show Chimera outperforms all major models by a large margin, owing both to the good individual performance of its constituents, but also to the scalability of our ensembling strategy. Moreover, we find that PhD-level organic chemists prefer predictions from Chimera over baselines in terms of quality. Finally, we transfer the largest-scale checkpoint to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our framework unlocks, we anticipate further acceleration in the development of even more accurate models.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05269
Locomotion of a Scallop-Inspired Swimmer in Granular Matter,['Soft Condensed Matter'],"['Hongyi Xiao', 'Harol Torres', 'Achim Sack', 'Thorsten Pöschel']","Understanding swimming in soft yielding media is challenging due to their complex deformation response to the swimmer's motion. We experimentally show that a scallop-inspired swimmer with reciprocally flapping wings generates locomotion in granular matter. This disagrees with the scallop theorem prohibiting reciprocal swimming in a liquid when its inertia is negligible. We use X-ray tomography and laser profilometry to show that the propulsion is created by the combined effects of jamming and convection of particles near the wings, which break the symmetry in packing density, surface deformation, and kinematics of the granular medium between an opening and a closing stroke.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05266
The neutron veto of the XENONnT experiment: Results with demineralized water,['Instrumentation and Detectors'],"['XENON Collaboration', 'E. Aprile', 'J. Aalbers', 'K. Abe', 'S. Ahmed Maouloud', 'L. Althueser', 'B. Andrieu', 'E. Angelino', 'D. Antón Martin', 'F. Arneodo', 'L. Baudis', 'M. Bazyk', 'L. Bellagamba', 'R. Biondi', 'A. Bismark', 'K. Boese', 'A. Brown', 'G. Bruno', 'R. Budnik', 'C. Cai', 'C. Capelli', 'J. M. R. Cardoso', 'A. P. Cimental Chávez', 'A. P. Colijn', 'J. Conrad']","Radiogenic neutrons emitted by detector materials are one of the most challenging backgrounds for the direct search of dark matter in the form of weakly interacting massive particles (WIMPs). To mitigate this background, the XENONnT experiment is equipped with a novel gadolinium-doped water Cherenkov detector, which encloses the xenon dual-phase time projection chamber (TPC). The neutron veto (NV) tags neutrons via their capture on gadolinium or hydrogen, which release $γ$-rays that are subsequently detected as Cherenkov light. In this work, we present the key features and the first results of the XENONnT NV when operated with demineralized water in the initial phase of the experiment. Its efficiency for detecting neutrons is $(82\pm 1)\,\%$, the highest neutron detection efficiency achieved in a water Cherenkov detector. This enables a high efficiency of $(53\pm 3)\,\%$ for the tagging of WIMP-like neutron signals, inside a tagging time window of $250\,\mathrm{μs}$ between TPC and NV, leading to a livetime loss of $1.6\,\%$ during the first science run of XENONnT.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05264
Mind the Time: Temporally-Controlled Multi-Event Video Generation,['Computer Vision and Pattern Recognition'],"['Ziyi Wu', 'Aliaksandr Siarohin', 'Willi Menapace', 'Ivan Skorokhodov', 'Yuwei Fang', 'Varnith Chordia', 'Igor Gilitschenski', 'Sergey Tulyakov']","Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05263
A Machine Learning-Based Approach For Detecting Malicious PyPI Packages,['Software Engineering'],"['Haya Samaana', 'Diego Elias Costa', 'Emad Shihab', 'Ahmad Abdellatif']","Background. In modern software development, the use of external libraries and packages is increasingly prevalent, streamlining the software development process and enabling developers to deploy feature-rich systems with little coding. While this reliance on reusing code offers substantial benefits, it also introduces serious risks for deployed software in the form of malicious packages - harmful and vulnerable code disguised as useful libraries. Aims. Popular ecosystems, such PyPI, receive thousands of new package contributions every week, and distinguishing safe contributions from harmful ones presents a significant challenge. There is a dire need for reliable methods to detect and address the presence of malicious packages in these environments. Method. To address these challenges, we propose a data-driven approach that uses machine learning and static analysis to examine the package's metadata, code, files, and textual characteristics to identify malicious packages. Results. In evaluations conducted within the PyPI ecosystem, we achieved an F1-measure of 0.94 for identifying malicious packages using a stacking ensemble classifier. Conclusions. This tool can be seamlessly integrated into package vetting pipelines and has the capability to flag entire packages, not just malicious function calls. This enhancement strengthens security measures and reduces the manual workload for developers and registry maintainers, thereby contributing to the overall integrity of the ecosystem.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05259
Uncertainty Quantification for Transformer Models for Dark-Pattern Detection,['Machine Learning'],"['Javier Muñoz', 'Álvaro Huertas-García', 'Carlos Martí-González', 'Enrique De Miguel Ambite']","The opaque nature of transformer-based models, particularly in applications susceptible to unethical practices such as dark-patterns in user interfaces, requires models that integrate uncertainty quantification to enhance trust in predictions. This study focuses on dark-pattern detection, deceptive design choices that manipulate user decisions, undermining autonomy and consent. We propose a differential fine-tuning approach implemented at the final classification head via uncertainty quantification with transformer-based pre-trained models. Employing a dense neural network (DNN) head architecture as a baseline, we examine two methods capable of quantifying uncertainty: Spectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural Networks (BNNs). These methods are evaluated on a set of open-source foundational models across multiple dimensions: model performance, variance in certainty of predictions and environmental impact during training and inference phases. Results demonstrate that integrating uncertainty quantification maintains performance while providing insights into challenging instances within the models. Moreover, the study reveals that the environmental impact does not uniformly increase with the incorporation of uncertainty quantification techniques. The study's findings demonstrate that uncertainty quantification enhances transparency and provides measurable confidence in predictions, improving the explainability and clarity of black-box models. This facilitates informed decision-making and mitigates the influence of dark-patterns on user interfaces. These results highlight the importance of incorporating uncertainty quantification techniques in developing machine learning models, particularly in domains where interpretability and trustworthiness are critical.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05251
Constructing projective modules,['History and Overview'],['Aravind Asok'],"We discuss elements of a social history of the theory of projective modules over commutative rings. We attempt to study the question: how did the theory of projective modules become one of ""mainstream"" focus in mathematics? To do this, we begin in what one might call the pre-history of projective modules, describing the mathematical culture into which the notion of projective module was released. These recollections involve four pieces: (a) analyzing aspects of the theory of fiber bundles, as it impinges on algebraic geometry, (b) understanding the rise of homological techniques in algebraic topology, (c) describing the influence of category-theoretic ideas in topology and algebra and (d) revisiting the story of the percolation of sheaf-theoretic ideas through algebraic geometry.
  We will then argue that it was this unique confluence of mathematical events that allowed projective modules to emerge as objects of central mathematical importance. More precisely, we will first argue that, in the context of social currents of the time, projective modules initially were isolated as objects of purely technical convenience reflecting the aesthetic sensibilities of the creators of the fledgling theory of homological algebra. Only later did they transcend this limited role to become objects of ""mainstream importance"" due to influence from the theory of algebraic fiber bundles and the theory of sheaves. Along the way, we aim to show how strong personal ties emanating from the Bourbaki movement and its connections in mathematical centers including Paris, Princeton and Chicago were essential to the entrance, propagation and mainstream mathematical acceptance of the theory.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05250
An Information Theoretic Analysis of Ghost Modulation,['Information Theory'],"['Daniel Harman', 'Ashton Palacios', 'Philip Lundrigan', 'Willie K. Harrison']","Side channels have become an essential component of many modern information-theoretic schemes. The emerging field of cross technology communications (CTC) provides practical methods for creating intentional side channels between existing communications technologies. This paper describes a theoretical foundation for one such, recently proposed, CTC scheme: Ghost Modulation (GM). Designed to modulate a low-data-rate message atop an existing network stream, GM is particularly suited for transmitting identification or covert information. The implementation only requires firmware updates to existing hardware, making it a cost-effective solution. However, GM provides an interesting technical challenge due to a highly asymmetric binary crossover erasure channel (BCEC) that results from packet drops and network delays. In this work, we provide a mathematical description of the signal and channel models for GM. A heuristic decision rule based on maximum-likelihood principles for simplified channel models is proposed. We describe an algorithm for GM packet acquisition and timing synchronization, supported by simulation results. Several well known short block codes are applied, and bit error rate (BER) results are presented.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05249
Occupation time statistics for non-Markovian random walks,['Statistical Mechanics'],"['Vicenç Méndez', 'Rosa Flaquer-Galmés', 'Arnab Pal']","We study the occupation time statistics for non-Markovian random walkers based on the formalism of the generalized master equation for the Continuous-Time Random Walk. We also explore the case when the random walker additionally undergoes a stochastic resetting dynamics. We derive and solve the backward Feynman-Kac equation to find the characteristic function for the occupation time in an interval and for the half occupation time in the semi-infinite domain. We analyze the behaviour of the PDFs, the moments, the limiting distributions and the ergodic properties for both occupation times when the underlying random walk is normal or anomalous. For the half occupation time, we revisit the famous arcsine law and examine its validity pertaining to various regimes of the rest period of the walker. Our results have been verified with numerical simulations exhibiting an excellent agreement.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05247
Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization,['Machine Learning'],"['Luca Masserano', 'Abdul Fatir Ansari', 'Boran Han', 'Xiyuan Zhang', 'Christos Faloutsos', 'Michael W. Mahoney', 'Andrew Gordon Wilson', 'Youngsuk Park', 'Syama Rangapuram', 'Danielle C. Maddix', 'Yuyang Wang']","How to best develop foundational models for time series forecasting remains an important open question. Tokenization is a crucial consideration in this effort: what is an effective discrete vocabulary for a real-valued sequential input? To address this question, we develop WaveToken, a wavelet-based tokenizer that allows models to learn complex representations directly in the space of time-localized frequencies. Our method first scales and decomposes the input time series, then thresholds and quantizes the wavelet coefficients, and finally pre-trains an autoregressive model to forecast coefficients for the forecast horizon. By decomposing coarse and fine structures in the inputs, wavelets provide an eloquent and compact language for time series forecasting that simplifies learning. Empirical results on a comprehensive benchmark, including 42 datasets for both in-domain and zero-shot settings, show that WaveToken: i) provides better accuracy than recently proposed foundation models for forecasting while using a much smaller vocabulary (1024 tokens), and performs on par or better than modern deep learning models trained specifically on each dataset; and ii) exhibits superior generalization capabilities, achieving the best average rank across all datasets for three complementary metrics. In addition, we show that our method can easily capture complex temporal patterns of practical relevance that are challenging for other recent pre-trained models, including trends, sparse spikes, and non-stationary time series with varying frequencies evolving over time.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05244
CompCap: Improving Multimodal Large Language Models with Composite Captions,['Computer Vision and Pattern Recognition'],"['Xiaohui Chen', 'Satya Narayan Shukla', 'Mahmoud Azab', 'Aashu Singh', 'Qifan Wang', 'David Yang', 'ShengYun Peng', 'Hanchao Yu', 'Shen Yan', 'Xuewen Zhang', 'Baosheng He']","How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05243
Some splitting and rigidity results for sub-static spaces,['Differential Geometry'],"['Giulio Colombo', 'Allan Freitas', 'Luciano Mari', 'Marco Rigoli']","In this paper we study the rigidity problem for sub-static systems with possibly non-empty boundary. First, we get local and global splitting theorems by assuming the existence of suitable compact minimal hypersurfaces, complementing recent results in the literature. Next, we prove some boundary integral inequalities that extend works by Chrúsciel and Boucher-Gibbons-Horowitz to non-vacuum spaces. Even in the vacuum static case, the inequalities improve on known ones. Lastly, we consider the system arising from static solutions to the Einstein field equations coupled with a $σ$-model. The Liouville theorem we obtain allows for positively curved target manifolds, generalizing a result by Reiris.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05238
Constructing Uncertainty Sets for Robust Risk Measures: A Composition of $φ$-Divergences Approach to Combat Tail Uncertainty,['Optimization and Control'],"['Guanyu Jin', 'Roger J. A. Laeven', 'Dick den Hertog', 'Aharon Ben-Tal']","Risk measures, which typically evaluate the impact of extreme losses, are highly sensitive to misspecification in the tails. This paper studies a robust optimization approach to combat tail uncertainty by proposing a unifying framework to construct uncertainty sets for a broad class of risk measures, given a specified nominal model. Our framework is based on a parametrization of robust risk measures using two (or multiple) $φ$-divergence functions, which enables us to provide uncertainty sets that are tailored to both the sensitivity of each risk measure to tail losses and the tail behavior of the nominal distribution. In addition, our formulation allows for a tractable computation of robust risk measures, and elicitation of $φ$-divergences that describe a decision maker's risk and ambiguity preferences.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05234
LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds,['Computation and Language'],"['James Beetham', 'Souradip Chakraborty', 'Mengdi Wang', 'Furong Huang', 'Amrit Singh Bedi', 'Mubarak Shah']","Many existing jailbreak techniques rely on solving discrete combinatorial optimization, while more recent approaches involve training LLMs to generate multiple adversarial prompts. However, both approaches require significant computational resources to produce even a single adversarial prompt. We hypothesize that the inefficiency of current approaches stems from an inadequate characterization of the jailbreak problem. To address this gap, we formulate the jailbreak problem in terms of alignment. By starting from an available safety-aligned model, we leverage an unsafe reward to guide the safe model towards generating unsafe outputs using alignment techniques (e.g., reinforcement learning from human feedback), effectively performing jailbreaking via alignment. We propose a novel jailbreak method called LIAR (LeveragIng Alignment to jailbReak). To demonstrate the simplicity and effectiveness of our approach, we employ a best-of-N method to solve the alignment problem. LIAR offers significant advantages: lower computational requirements without additional training, fully black-box operation, competitive attack success rates, and more human-readable prompts. We provide theoretical insights into the possibility of jailbreaking a safety-aligned model, revealing inherent vulnerabilities in current alignment strategies for LLMs. We also provide sub-optimality guarantees for the proposed \algo. Experimentally, we achieve ASR comparable to the SoTA with a 10x improvement to perplexity and a Time-to-Attack measured in seconds rather than tens of hours.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05232
A kinetically constrained model exhibiting non-linear diffusion and jamming,['Statistical Mechanics'],"['Abhishek Raj', 'Vadim Oganesyan', 'Antonello Scardicchio']","We present a classical kinetically constrained model of interacting particles on a triangular ladder, which displays diffusion and jamming and can be treated by means of a classical-quantum mapping. Interpreted as a theory of interacting fermions, the diffusion coefficient is the inverse of the effective mass of the quasiparticles which can be computed using mean-field theory. At a critical density \r{ho} = 2/3, the model undergoes a dynamical phase transition in which exponentially many configurations become jammed while others remain diffusive. The model can be generalized to two dimensions.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05231
BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits,['Computation and Language'],"['Wazib Ansar', 'Saptarsi Goswami', 'Amlan Chakrabarti']","Large Language Models (LLMs) based on transformers achieve cutting-edge results on a variety of applications. However, their enormous size and processing requirements make deployment on devices with constrained resources extremely difficult. Among various efficiency considerations, model binarization and Early Exit (EE) are common effective solutions. However, binarization may lead to performance loss due to reduced precision affecting gradient estimation and parameter updates. Besides, the present early-exit mechanisms are still in the nascent stages of research. To ameliorate these issues, we propose Binarized Early Exit Transformer (BEExformer), the first-ever selective learning transformer architecture to combine early exit with binarization for textual inference. It improves the binarization process through a differentiable second-order approximation to the impulse function. This enables gradient computation concerning both the sign as well as the magnitude of the weights. In contrast to absolute threshold-based EE, the proposed EE mechanism hinges on fractional reduction in entropy among intermediate transformer blocks with soft-routing loss estimation. While binarization results in 18.44 times reduction in model size, early exit reduces the FLOPs during inference by 54.85% and even improves accuracy by 5.98% through resolving the ""overthinking"" problem inherent in deep networks. Moreover, the proposed BEExformer simplifies training by not requiring knowledge distillation from a full-precision LLM. Extensive evaluation on the GLUE dataset and comparison with the SOTA works showcase its pareto-optimal performance-efficiency trade-off.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05225
Rectangular recurrence relations in $\mathfrak{gl}_{n}$ and $\mathfrak{o}_{2n+1}$ invariant integrable models,['Quantum Algebra'],"['A. Liashyk', 'S. Pakuliak', 'E. Ragoucy']","A new method is introduced to derive general recurrence relations for off-shell Bethe vectors in quantum integrable models with either type $\mathfrak{gl}_n$ or type $\mathfrak{o}_{2n+1}$ symmetries. These recurrence relations describe how to add a single parameter, $z$, to specific subsets of Bethe parameters, expressing the resulting Bethe vector as a linear combination of monodromy matrix entries that act on Bethe vectors which do not depend on $z$. We refer to these recurrence relations as \textit{rectangular} because the monodromy matrix entries involved are drawn from the upper-right rectangular part of the matrix. This construction is achieved within the framework of the zero mode method.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05224
100% Hallucination Elimination Using Acurai,['Computation and Language'],"['Michael C. Wood', 'Adam A. Forbes']","The issue of hallucinations in large language models (LLMs) remains a critical barrier to the adoption of AI in enterprise and other high-stakes applications. Despite advancements in retrieval-augmented generation (RAG) systems, current state-of-the-art methods fail to achieve more than 80% accuracy in generating faithful and factually correct outputs, even when provided with relevant and accurate context. In this work, we introduce Acurai, a novel systematic approach that achieves 100% hallucination-free responses in LLMs by reformatting queries and context data prior to input. Leveraging a deep understanding of LLM internal representations, the importance of noun-phrase dominance, and the role of discrete functional units (DFUs), Acurai ensures alignment between input context and generated output. We validate this method using the RAGTruth corpus, demonstrating its ability to eliminate 100% hallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for achieving consistent, accurate, and faithful AI responses, marking a significant step forward in the development of trustworthy AI systems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05223
Diffusion cascade in a model of interacting random walkers,['Statistical Mechanics'],"['Abhishek Raj', 'Paolo Glorioso', 'Sarang Gopalakrishnan', 'Vadim Oganesyan']","We consider the relaxation of finite-wavevector density waves in a facilitated classical lattice gas. Linear hydrodynamics predicts that such perturbations should relax exponentially, but nonlinear effects were predicted to cause subexponential relaxation via nonperturbative long-time tails. We present a detailed numerical study of this effect. While our results clearly indicate the importance of nonlinear effects, we find that the wavevector-dependence of the late-time relaxation is clearly inconsistent with theoretical predictions. We discuss manifestations of hydrodynamic nonlinearities in mesoscopic samples and at short times.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05222
Millisecond Pulsars in Globular Clusters and Implications for the Galactic Center Gamma-Ray Excess,['High Energy Astrophysical Phenomena'],"['Aurelio Amerio', 'Dan Hooper', 'Tim Linden']","We study the gamma-ray emission from millisecond pulsars within the Milky Way's globular cluster system in order to measure the luminosity function of this source population. We find that these pulsars have a mean luminosity of $\langle L_γ\rangle \sim (1-8)\times 10^{33}\, {\rm erg/s}$ (integrated between 0.1 and 100 GeV) and a log-normal width of $σ_L \sim 1.4-2.8$. If the Galactic Center Gamma-Ray Excess were produced by pulsars with similar characteristics, Fermi would have already detected $N \sim 17-37$ of these sources, whereas only three such pulsar candidates have been identified. We conclude that the excess gamma-ray emission can originate from pulsars only if they are significantly less bright, on average, than those observed within globular clusters or in the Galactic Plane. This poses a serious challenge for pulsar interpretation of the Galactic Center Gamma-Ray Excess.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05220
ColonNet: A Hybrid Of DenseNet121 And U-NET Model For Detection And Segmentation Of GI Bleeding,['Image and Video Processing'],"['Ayushman Singh', 'Sharad Prakash', 'Aniket Das', 'Nidhi Kushwaha']",This study presents an integrated deep learning model for automatic detection and classification of Gastrointestinal bleeding in the frames extracted from Wireless Capsule Endoscopy (WCE) videos. The dataset has been released as part of Auto-WCBleedGen Challenge Version V2 hosted by the MISAHUB team. Our model attained the highest performance among 75 teams that took part in this competition. It aims to efficiently utilizes CNN based model i.e. DenseNet and UNet to detect and segment bleeding and non-bleeding areas in the real-world complex dataset. The model achieves an impressive overall accuracy of 80% which would surely help a skilled doctor to carry out further diagnostics.△ Less,"6 December, 2024;",https://arxiv.org/pdf/2412.05216
Excitation spectrum of a double supersolid in a trapped dipolar Bose mixture,['Quantum Gases'],"['Daniel Scheiermann', 'Albert Gallemí', 'Luis Santos']","Dipolar Bose-Einstein condensates are excellent platforms for studying supersolidity, characterized by coexisting density modulation and superfluidity. The realization of dipolar mixtures opens intriguing new scenarios, most remarkably the possibility of realizing a double supersolid, composed by two interacting superfluids. We analyze the complex excitation spectrum of a miscible trapped dipolar Bose mixture, showing that it provides key insights about the double supersolid regime. We show that this regime may be readily probed experimentally by monitoring the appearance of a doublet of superfluid compressional modes, linked to the different superfluid character of each component. Additionally, the dipolar supersolid mixture exhibits a non-trivial spin nature of the dipolar rotons, the Higgs excitation, and the low-lying Goldstone modes. Interestingly, the analysis of the lowest-lying modes allows for monitoring the transition of just one of the components into the incoherent droplet regime, whereas the other remains coherent, highlighting their disparate superfluid properties.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05215
Artificial topological insulator realized in a two-terminal Josephson junction with Rashba spin-orbit interaction,['Mesoscale and Nanoscale Physics'],"['Luka Medic', 'Anton Ramšak', 'Tomaž Rejec']","We study a two-terminal Josephson junction with conventional superconductors and a normal region with Rashba spin-orbit interaction, characterized by two Aharonov-Casher (AC) fluxes. When the superconducting phase difference equals $π$, the Andreev subgap spectrum may host zero-energy Weyl singularities associated with a vanishing normal-state reflection eigenvalue. With one of the AC fluxes playing the role of a quasimomentum, the junction can be viewed as an artificial one-dimensional chiral topological insulator. Its topological phase can be tuned by crossing a Weyl singularity by means of varying the remaining AC flux. By associating an additional component of the quasimomentum with the superconducting phase difference, an artificial Chern insulator is realized.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05209
"A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges",['Artificial Intelligence'],"['Aditi Singh', 'Akash Shetty', 'Abul Ehtesham', 'Saket Kumar', 'Tala Talaei Khoei']","Text-to-SQL systems facilitate smooth interaction with databases by translating natural language queries into Structured Query Language (SQL), bridging the gap between non-technical users and complex database management systems. This survey provides a comprehensive overview of the evolution of AI-driven text-to-SQL systems, highlighting their foundational components, advancements in large language model (LLM) architectures, and the critical role of datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine the applications of text-to-SQL in domains like healthcare, education, and finance, emphasizing their transformative potential for improving data accessibility. Additionally, we analyze persistent challenges, including domain generalization, query optimization, support for multi-turn conversational interactions, and the limited availability of datasets tailored for NoSQL databases and dynamic real-world scenarios. To address these challenges, we outline future research directions, such as extending text-to-SQL capabilities to support NoSQL databases, designing datasets for dynamic multi-turn interactions, and optimizing systems for real-world scalability and robustness. By surveying current advancements and identifying key gaps, this paper aims to guide the next generation of research and applications in LLM-based text-to-SQL systems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05208
Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era,['Computer Vision and Pattern Recognition'],"['Yohann Perron', 'Vladyslav Sydorov', 'Adam P. Wijker', 'Damian Evans', 'Christophe Pottier', 'Loic Landrieu']","Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at https://archaeoscape.ai), a novel large-scale archaeological ALS dataset spanning 888 km$^2$ in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.
  We benchmark several recent segmentation models to demonstrate the benefits of modern vision techniques for this problem and highlight the unique challenges of discovering subtle human-made structures under dense jungle canopies. By making Archaeoscape available in open access, we hope to bridge the gap between traditional archaeology and modern computer vision methods.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05203
Corrugation-dominated mechanical softening of defect-engineered graphene,['Materials Science'],"['Wael Joudi', 'Rika Saskia Windisch', 'Alberto Trentino', 'Diana Propst', 'Jacob Madsen', 'Toma Susi', 'Clemens Mangler', 'Kimmo Mustonen', 'Florian Libisch', 'Jani Kotakoski']","We measure the two-dimensional elastic modulus $E^\text{2D}$ of atomically clean defect-engineered graphene with a known defect distribution and density in correlated ultra-high vacuum experiments. The vacancies are introduced via low-energy (< 200 eV) Ar ion irradiation and the atomic structure is obtained via semi-autonomous scanning transmission electron microscopy and image analysis. Based on atomic force microscopy nanoindentation measurements, a decrease of $E^\text{2D}$ from 286 to 158 N/m is observed when measuring the same graphene membrane before and after an ion irradiation-induced vacancy density of $1.0\times 10^{13}$ cm$^{-2}$. This decrease is significantly greater than what is predicted by most theoretical studies and in stark contrast to some measurements presented in the literature. With the assistance of atomistic simulations, we show that this softening is mostly due to corrugations caused by local strain at vacancies with two or more missing atoms, while the influence of single vacancies is negligible. We further demonstrate that the opposite effect can be measured when surface contamination is not removed before defect engineering△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05194
A biomechanical study of neck strength and impact dynamics on head and neck injury parameters,['Tissues and Organs'],"['Rahid Zaman', 'Ashfaq Adnan']","Traumatic brain injuries (TBI) are considered a silent epidemic. It affects many people, from automobiles to sports to service members. In this study, we employed a musculoskeletal head-neck model to understand the effect of impact locations, characteristics, and neck strength on head and neck injury severity. Three types of impact forces were studied: low-velocity impact (LVI), intermediate-velocity impact (IVI), and high-velocity impact (HVI). We investigated six parameters: linear and rotational accelerations, the Generalized Acceleration Model For Brain Injury Threshold (GAMBIT), neck force, neck moment, and Neck Injury Criteria (NIC). We consider seven impact locations, three neck strengths, and three impact characteristics. We studied a total of 63 cases. It was found that the linear accelerations do not change much with different neck strengths and impact locations. The impact locations have a significant effect on head and neck injury parameters, and anterolateral impact is the most risky impact location for both head and neck. The maximum average rotational acceleration is for anterolateral eccentric impact which is 4.75 times more than the average anterior central impact. The lateral impacts generate about 10% more linear accelerations than anterior and posterior impacts. Neck forces do not vary more than 20% with impact locations and neck strength. The average head and neck injury parameters do not vary more than 10% based on neck strength. Impact characteristics have a significant role in GAMBIT and NIC. The average GAMBIT for IVI and HVI were 1.44 and 1.54 times higher than LVI. In summary, the anterolateral eccentric impact has a higher probability of head and neck injury than the other six impact locations. These findings provide objective evidence that can inform injury prevention strategies as well as aid tissue and cellular level studies.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05192
On Mean Field Monotonicity Conditions from Control Theoretical Perspective,['Optimization and Control'],"['Alain Bensoussan', 'Ziyu Huang', 'Shanjian Tang', 'Sheung Chi Phillip Yam']","In this article, from the viewpoint of control theory, we discuss the relationships among the commonly used monotonicity conditions that ensure the well-posedness of the solutions arising from problems of mean field games (MFGs) and mean field type control (MFTC). We first introduce the well-posedness of general forward-backward stochastic differential equations (FBSDEs) defined on some suitably chosen Hilbert spaces under the $β$-monotonicity. We then propose a monotonicity condition for the MFG, namely partitioning the running cost functional into two parts, so that both parts still depend on the control and the state distribution, yet one satisfies a strong convexity and a small mean field effect condition, while the other has a newly introduced displacement quasi-monotonicity. To the best of our knowledge, the latter quasi type condition has not yet been discussed in the contemporary literature, and it can be considered as a bit more general monotonicity condition than those commonly used. Besides, for the MFG, we show that convexity and small mean field effect condition for the first part of running cost functional and the quasi-monotonicity condition for the second part together imply the $β$-monotonicity and thus the well-posedness for the associated FBSDEs. For the MFTC problem, we show that the $β$-monotonicity for the corresponding FBSDEs is simply the convexity assumption on the cost functional. Finally, we consider a more general setting where the drift functional is allowed to be non-linear for both MFG and MFTC problems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05189
Privacy Drift: Evolving Privacy Concerns in Incremental Learning,['Machine Learning'],"['Sayyed Farid Ahamed', 'Soumya Banerjee', 'Sandip Roy', 'Aayush Kapoor', 'Marc Vucovich', 'Kevin Choi', 'Abdul Rahman', 'Edward Bowen', 'Sachin Shetty']","In the evolving landscape of machine learning (ML), Federated Learning (FL) presents a paradigm shift towards decentralized model training while preserving user data privacy. This paper introduces the concept of ``privacy drift"", an innovative framework that parallels the well-known phenomenon of concept drift. While concept drift addresses the variability in model accuracy over time due to changes in the data, privacy drift encapsulates the variation in the leakage of private information as models undergo incremental training. By defining and examining privacy drift, this study aims to unveil the nuanced relationship between the evolution of model performance and the integrity of data privacy. Through rigorous experimentation, we investigate the dynamics of privacy drift in FL systems, focusing on how model updates and data distribution shifts influence the susceptibility of models to privacy attacks, such as membership inference attacks (MIA). Our results highlight a complex interplay between model accuracy and privacy safeguards, revealing that enhancements in model performance can lead to increased privacy risks. We provide empirical evidence from experiments on customized datasets derived from CIFAR-100 (Canadian Institute for Advanced Research, 100 classes), showcasing the impact of data and concept drift on privacy. This work lays the groundwork for future research on privacy-aware machine learning, aiming to achieve a delicate balance between model accuracy and data privacy in decentralized environments.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05183
DreamColour: Controllable Video Colour Editing without Training,['Computer Vision and Pattern Recognition'],"['Chaitat Utintu', 'Pinaki Nath Chowdhury', 'Aneeshan Sain', 'Subhadeep Koley', 'Ayan Kumar Bhunia', 'Yi-Zhe Song']","Video colour editing is a crucial task for content creation, yet existing solutions either require painstaking frame-by-frame manipulation or produce unrealistic results with temporal artefacts. We present a practical, training-free framework that makes precise video colour editing accessible through an intuitive interface while maintaining professional-quality output. Our key insight is that by decoupling spatial and temporal aspects of colour editing, we can better align with users' natural workflow -- allowing them to focus on precise colour selection in key frames before automatically propagating changes across time. We achieve this through a novel technical framework that combines: (i) a simple point-and-click interface merging grid-based colour selection with automatic instance segmentation for precise spatial control, (ii) bidirectional colour propagation that leverages inherent video motion patterns, and (iii) motion-aware blending that ensures smooth transitions even with complex object movements. Through extensive evaluation on diverse scenarios, we demonstrate that our approach matches or exceeds state-of-the-art methods while eliminating the need for training or specialized hardware, making professional-quality video colour editing accessible to everyone.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05180
Spatially-Adaptive Hash Encodings For Neural Surface Reconstruction,['Computer Vision and Pattern Recognition'],"['Thomas Walker', 'Octave Mariotti', 'Amir Vaxman', 'Hakan Bilen']","Positional encodings are a common component of neural scene reconstruction methods, and provide a way to bias the learning of neural fields towards coarser or finer representations. Current neural surface reconstruction methods use a ""one-size-fits-all"" approach to encoding, choosing a fixed set of encoding functions, and therefore bias, across all scenes. Current state-of-the-art surface reconstruction approaches leverage grid-based multi-resolution hash encoding in order to recover high-detail geometry. We propose a learned approach which allows the network to choose its encoding basis as a function of space, by masking the contribution of features stored at separate grid resolutions. The resulting spatially adaptive approach allows the network to fit a wider range of frequencies without introducing noise. We test our approach on standard benchmark surface reconstruction datasets and achieve state-of-the-art performance on two benchmark datasets.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05179
Who Sets the Agenda on Social Media? Ideology and Polarization in Online Debates,['Social and Information Networks'],"['Edoardo Loru', 'Alessandro Galeazzi', 'Anita Bonetti', 'Emanuele Sangiorgio', 'Niccolò Di Marco', 'Matteo Cinelli', 'Andrea Baronchelli', 'Walter Quattrociocchi']","The abundance of information on social media has reshaped public discussions, shifting attention to the mechanisms that drive online discourse. This study analyzes large-scale Twitter (now X) data from three global debates -- Climate Change, COVID-19, and the Russo-Ukrainian War -- to investigate the structural dynamics of engagement. Our findings reveal that discussions are not primarily shaped by specific categories of actors, such as media or activists, but by shared ideological alignment. Users consistently form polarized communities, where their ideological stance in one debate predicts their positions in others. This polarization transcends individual topics, reflecting a broader pattern of ideological divides. Furthermore, the influence of individual actors within these communities appears secondary to the reinforcing effects of selective exposure and shared narratives. Overall, our results underscore that ideological alignment, rather than actor prominence, plays a central role in structuring online discourse and shaping the spread of information in polarized environments.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05176
Variational Encoder-Decoders for Learning Latent Representations of Physical Systems,['Machine Learning'],"['Subashree Venkatasubramanian', 'David A. Barajas-Solano']","We present a deep-learning Variational Encoder-Decoder (VED) framework for learning data-driven low-dimensional representations of the relationship between high-dimensional parameters of a physical system and the system's high-dimensional observable response. The framework consists of two deep learning-based probabilistic transformations: An encoder mapping parameters to latent codes and a decoder mapping latent codes to the observable response. The hyperparameters of these transformations are identified by maximizing a variational lower bound on the log-conditional distribution of the observable response given parameters. To promote the disentanglement of latent codes, we equip this variational loss with a penalty on the off-diagonal entries of the aggregate distribution covariance of codes. This regularization penalty encourages the pushforward of a standard Gaussian distribution of latent codes to approximate the marginal distribution of the observable response.
  Using the proposed framework we successfully model the hydraulic pressure response at observation wells of a groundwater flow model as a function of its discrete log-hydraulic transmissivity field. Compared to the canonical correlation analysis encoding, the VED model achieves a lower-dimensional latent representation, with as low as $r = 50$ latent dimensions without a significant loss of reconstruction accuracy. We explore the impact of regularization on model performance, finding that KL-divergence and covariance regularization improve feature disentanglement in latent space while maintaining reconstruction accuracy. Furthermore, we evaluate the generative capabilities of the regularized model by decoding random Gaussian noise, revealing that tuning both $β$ and $λ$ parameters enhances the quality of the generated observable response data.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05175
Baseband control of single-electron silicon spin qubits in two dimensions,['Mesoscale and Nanoscale Physics'],"['Florian K. Unseld', 'Brennan Undseth', 'Eline Raymenants', 'Yuta Matsumoto', 'Saurabh Karwal', 'Oriol Pietx-Casas', 'Alexander S. Ivlev', 'Marcel Meyer', 'Amir Sammak', 'Menno Veldhorst', 'Giordano Scappucci', 'Lieven M. K. Vandersypen']","Micromagnet-enabled electric-dipole spin resonance (EDSR) is an established method of high-fidelity single-spin control in silicon. However, the resulting architectural limitations have restrained silicon quantum processors to one-dimensional arrays, and heating effects from the associated microwave dissipation exacerbates crosstalk during multi-qubit operations. In contrast, qubit control based on hopping spins has recently emerged as a compelling primitive for high-fidelity baseband control in sparse two-dimensional hole arrays in germanium. In this work, we commission a $^{28}$Si/SiGe 2x2 quantum dot array both as a four-qubit device with pairwise exchange interactions using established EDSR techniques and as a two-qubit device using baseband hopping control. In this manner, we can evaluate the two modes of operation in terms of fidelity, coherence, and crosstalk. We establish a lower bound on the fidelity of the hopping gate of 99.50(6)%, which is similar to the average fidelity of the resonant gate of 99.54(4)%. Lowering the external field to reach the hopping regime nearly doubles the measured $T_2^{\mathrm{H}}$, suggesting a reduced coupling to charge noise. Finally, the hopping gate circumvents the transient pulse-induced resonance shift. To further motivate the hopping gate approach as an attractive means of scaling silicon spin-qubit arrays, we propose an extensible nanomagnet design that enables engineered baseband control of large spin arrays.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05171
Diagonal invariants and genus-zero Hurwitz Frobenius manifolds,['Mathematical Physics'],"['Alessandro Proserpio', 'Ian A. B. Strachan']","The Frobenius manifold structure on the space of rational functions with multiple simple poles is constructed. In particular, the dependence of the Saito-flat coordinates on the flat coordinates of the intersection form is studied. While some of the individual flat coordinates are complicated rational functions, they appear in the prepotential in certain combinations known as diagonal invariants, which turn out to be polynomial. Two classes are studied in more detail. These are generalisations of the Coxeter and extended-affine-Weyl orbits space for the group $W=W(A_\ell)\,.$ An invariant theory is also developed.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05165
DNF: Unconditional 4D Generation with Dictionary-based Neural Fields,['Computer Vision and Pattern Recognition'],"['Xinyi Zhang', 'Naiqi Li', 'Angela Dai']","While remarkable success has been achieved through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields. Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression -- combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05161
Low Power 16-channel Wave Union TDC in a Radiation Tolerant FPGA,['Instrumentation and Detectors'],"['Brian A. Bryce', 'Kathryn M. Marcotte']","The design and performance of wave union TDC implemented in a Lattice CertusPro-NX FPGA is discussed. This FPGA is available for radiation tolerant applications. The TDC is implemented with 16-channels and a 200 MHz reference clock. Each channel is able to record at an event rate of > 1 MHz. The performance of the TDC is assessed over voltage and temperature. Typical TDC performance has a resolution of 10.9 ps. Typical INL is +/-3 LSB peak-to-peak. Typical DNL is (+1.13,-0.77) LSB. Typical differential performance between two channels is 20 ps (1-sigma).△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05160
Gaining Explainability from a CNN for Stereotype Detection Based on Mice Stopping Behavior,['Computer Vision and Pattern Recognition'],"['Raul Alfredo de Sousa Silva', 'Yasmine Belaidouni', 'Rabah Iguernaissi', 'Djamal Merad', 'Séverine Dubuisson']","Understanding the behavior of laboratory animals is a key to find answers about diseases and neurodevelopmental disorders that also affects humans. One behavior of interest is the stopping, as it correlates with exploration, feeding and sleeping habits of individuals. To improve comprehension of animal's behavior, we focus on identifying trait revealing age/sex of mice through the series of stopping spots of each individual. We track 4 mice using LiveMouseTracker (LMT) system during 3 days. Then, we build a stack of 2D histograms of the stop positions. This stack of histograms passes through a shallow CNN architecture to classify mice in terms of age and sex. We observe that female mice show more recognizable behavioral patterns, reaching a classification accuracy of more than 90%, while males, which do not present as many distinguishable patterns, reach an accuracy of 62.5%. To gain explainability from the model, we look at the activation function of the convolutional layers and found that some regions of the cage are preferentially explored by females. Males, especially juveniles, present behavior patterns that oscillate between juvenile female and adult male.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05158
A text-to-tabular approach to generate synthetic patient data using LLMs,['Machine Learning'],"['Margaux Tornqvist', 'Jean-Daniel Zucker', 'Tristan Fauvel', 'Nicolas Lambert', 'Mathilde Berthelot', 'Antoine Movschin']","Access to large-scale high-quality healthcare databases is key to accelerate medical research and make insightful discoveries about diseases. However, access to such data is often limited by patient privacy concerns, data sharing restrictions and high costs. To overcome these limitations, synthetic patient data has emerged as an alternative. However, synthetic data generation (SDG) methods typically rely on machine learning (ML) models trained on original data, leading back to the data scarcity problem. We propose an approach to generate synthetic tabular patient data that does not require access to the original data, but only a description of the desired database. We leverage prior medical knowledge and in-context learning capabilities of large language models (LLMs) to generate realistic patient data, even in a low-resource setting. We quantitatively evaluate our approach against state-of-the-art SDG models, using fidelity, privacy, and utility metrics. Our results show that while LLMs may not match the performance of state-of-the-art models trained on the original data, they effectively generate realistic patient data with well-preserved clinical correlations. An ablation study highlights key elements of our prompt contributing to high-quality synthetic patient data generation. This approach, which is easy to use and does not require original data or advanced ML skills, is particularly valuable for quickly generating custom-designed patient data, supporting project implementation and providing educational resources.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05153
"Navigating Shortcuts, Spurious Correlations, and Confounders: From Origins via Detection to Mitigation",['Machine Learning'],"['David Steinmann', 'Felix Divo', 'Maurice Kraus', 'Antonia Wüst', 'Lukas Struppek', 'Felix Friedrich', 'Kristian Kersting']","Shortcuts, also described as Clever Hans behavior, spurious correlations, or confounders, present a significant challenge in machine learning and AI, critically affecting model generalization and robustness. Research in this area, however, remains fragmented across various terminologies, hindering the progress of the field as a whole. Consequently, we introduce a unifying taxonomy of shortcut learning by providing a formal definition of shortcuts and bridging the diverse terms used in the literature. In doing so, we further establish important connections between shortcuts and related fields, including bias, causality, and security, where parallels exist but are rarely discussed. Our taxonomy organizes existing approaches for shortcut detection and mitigation, providing a comprehensive overview of the current state of the field and revealing underexplored areas and open challenges. Moreover, we compile and classify datasets tailored to study shortcut learning. Altogether, this work provides a holistic perspective to deepen understanding and drive the development of more effective strategies for addressing shortcuts in machine learning.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05152
Analysis of long-lived effects in high-repetition-rate stroboscopic transient X-ray absorption experiments on thin films,['Materials Science'],"['Tobias Lojewski', 'Loïc Le Guyader', 'Naman Agarwal', 'Christine Boeglin', 'Robert Carley', 'Andrea Castoldi', 'Carsten Deiter', 'Robin Y. Engel', 'Florian Erdinger', 'Hans Fangohr', 'Carlo Fiorini', 'Natalia Gerasimova', 'Rafael Gort', 'Frank de Groot', 'Karsten Hansen', 'Steffen Hauf', 'David Hickin', 'Manuel Izquierdo', 'Lea Kämmerer', 'Benjamin E. Van Kuiken', 'David Lomidze', 'Stefano Maffessanti', 'Laurent Mercadier', 'Giuseppe Mercurio', 'Piter S. Miedema']","Time-resolved X-ray absorption spectroscopy (tr-XAS) has been shown to be a versatile measurement technique for investigating non-equilibrium dynamics. Novel X-ray free electron laser (XFEL) facilities like the European XFEL offer increased repetition rates for stroboscopic XAS experiments through a burst operation mode, which enables measurements with up to 4.5 MHz. These higher repetition rates lead to higher data acquisition rates but can also introduce long-lived excitations that persist and thus build up during each burst. Here, we report on such long-lived effects in Ni and NiO thin film samples that were measured at the European XFEL. We disentangle the long-lived excitations from the initial pump-induced change and perform a detailed modelling-based analysis of how they modify transient X-ray spectra. As a result, we link the long-lived effects in Ni to a local temperature increase, as well as the effects in NiO to excited charge carrier trapping through polaron formation. In addition, we present possible correction methods, as well as discuss ways in which the effects of these long-lived excitations could be minimized for future time-resolved X-ray absorption spectroscopy measurements.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05151
Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora,['Computation and Language'],"['Michael Y. Hu', 'Aaron Mueller', 'Candace Ross', 'Adina Williams', 'Tal Linzen', 'Chengxu Zhuang', 'Ryan Cotterell', 'Leshem Choshen', 'Alex Warstadt', 'Ethan Gotlieb Wilcox']","The BabyLM Challenge is a community effort to close the data-efficiency gap between human and computational language learners. Participants compete to optimize language model training on a fixed language data budget of 100 million words or less. This year, we released improved text corpora, as well as a vision-and-language corpus to facilitate research into cognitively plausible vision language models. Submissions were compared on evaluation tasks targeting grammatical ability, (visual) question answering, pragmatic abilities, and grounding, among other abilities. Participants could submit to a 10M-word text-only track, a 100M-word text-only track, and/or a 100M-word and image multimodal track. From 31 submissions employing diverse methods, a hybrid causal-masked language model architecture outperformed other approaches. No submissions outperformed the baselines in the multimodal track. In follow-up analyses, we found a strong relationship between training FLOPs and average performance across tasks, and that the best-performing submissions proposed changes to the training data, training objective, and model architecture. This year's BabyLM Challenge shows that there is still significant room for innovation in this setting, in particular for image-text modeling, but community-driven research can yield actionable insights about effective strategies for small-scale language modeling.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05149
ROS: A GNN-based Relax-Optimize-and-Sample Framework for Max-k-Cut Problems,['Optimization and Control'],"['Yeqing Qiu', 'Ye Xue', 'Akang Wang', 'Yiheng Wang', 'Qingjiang Shi', 'Zhi-Quan Luo']","The Max-k-Cut problem is a fundamental combinatorial optimization challenge that generalizes the classic NP-complete Max-Cut problem. While relaxation techniques are commonly employed to tackle Max-k-Cut, they often lack guarantees of equivalence between the solutions of the original problem and its relaxation. To address this issue, we introduce the Relax-Optimize-and-Sample (ROS) framework. In particular, we begin by relaxing the discrete constraints to the continuous probability simplex form. Next, we pre-train and fine-tune a graph neural network model to efficiently optimize the relaxed problem. Subsequently, we propose a sampling-based construction algorithm to map the continuous solution back to a high-quality Max-k-Cut solution. By integrating geometric landscape analysis with statistical theory, we establish the consistency of function values between the continuous solution and its mapped counterpart. Extensive experimental results on random regular graphs and the Gset benchmark demonstrate that the proposed ROS framework effectively scales to large instances with up to 20000 nodes in just a few seconds, outperforming state-of-the-art algorithms. Furthermore, ROS exhibits strong generalization capabilities across both in-distribution and out-of-distribution instances, underscoring its effectiveness for large-scale optimization tasks.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05146
Explingo: Explaining AI Predictions using Large Language Models,['Computation and Language'],"['Alexandra Zytek', 'Sara Pido', 'Sarah Alnegheimish', 'Laure Berti-Equille', 'Kalyan Veeramachaneni']","Explanations of machine learning (ML) model predictions generated by Explainable AI (XAI) techniques such as SHAP are essential for people using ML outputs for decision-making. We explore the potential of Large Language Models (LLMs) to transform these explanations into human-readable, narrative formats that align with natural communication. We address two key research questions: (1) Can LLMs reliably transform traditional explanations into high-quality narratives? and (2) How can we effectively evaluate the quality of narrative explanations? To answer these questions, we introduce Explingo, which consists of two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML explanations and transforms them into natural-language descriptions. The Grader scores these narratives on a set of metrics including accuracy, completeness, fluency, and conciseness.
  Our experiments demonstrate that LLMs can generate high-quality narratives that achieve high scores across all metrics, particularly when guided by a small number of human-labeled and bootstrapped examples. We also identified areas that remain challenging, in particular for effectively scoring narratives in complex domains. The findings from this work have been integrated into an open-source tool that makes narrative explanations available for further applications.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05145
Using data collected from structured light plethysmography to differentiate breathing pattern disorder from normal breathing: A study group report,['Medical Physics'],"['Bindi S. Brook', 'Mathew Bulpett', 'Robin Curnow', 'Emily Fraser', 'Eric J. Hall', 'Shiting Huang', 'Mariam Mubarak', 'Carl A. Whitfield']","This report relates to a study group hosted by the EPSRC funded network, Integrating data-driven BIOphysical models into REspiratory MEdicine (BIOREME), and supported by SofTMech and Innovate UK, Business Connect. This report summarises the work undertaken on a challenge presented by two of the authors, Mathew Bulpett and Dr Emily Fraser. The aim was to identify approaches to analyse data collected using structured light plethysmography (SLP) from (n=31) healthy volunteers and (n=67) patients with Breathing Pattern Disorder (BPD) attributed to ""long COVID"", i.e. post-acute COVID-19 sequelae. This report explores several approaches including dimensionality reduction techniques on the available data and alternative indices extracted from variation in the time-series data for each measurement. Further proposals are also outlined such as different spatial indices that could be extracted from the SLP data, and the potential to couple to mechanical models of the lungs, chest and abdomen. However, running these latter analyses was beyond the scope of the limited study group timeframe.
  This exploratory analysis did not identify any clear SLP biomarkers of BPD in these cohorts, however recommendations are made for using SLP technologies in future BPD studies based on its findings.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05141
The reliability of gas-phase metallicities immediately adjacent to non-star-forming spaxels in MaNGA,['Astrophysics of Galaxies'],"['Jillian M. Scudder', 'Aidan Khelil', 'Jonah Z. Ordower']","In this work, we use gas phase metallicities calculated from the Sloan Digital Sky Survey (SDSS) Mapping Nearby Galaxies at Apache Point (MaNGA) Data Release 17 (DR17) to assess the extent of potential biases in spaxels which are spatially adjacent to spaxels identified as non-star forming (non-SF) on a BPT diagram. We identify a sample of $\sim21,000$ such spaxels with calculable metallicities from the full metallicity catalogue ($\sim$1.57 million), representing a small fraction ($\sim1.3$ per cent) of the full metallicity sample. $\sim$23 per cent of all galaxies with at least one spaxel with a calculable metallicity also contain at least one spaxel with a calculated metallicity adjacent to a non-SF spaxel, with a typical galaxy hosting 9 non-SF-adjacent spaxels. From our suite of 6 different metallicity calibrations, we find that only the metallicity calibrations based entirely on the [NII]$_{6584}$/H$α$ ratio are affected, showing systematic offsets to higher metallicities by up to $\sim$0.04 dex if they are located adjacent to a non-SF flagged spaxel, relative to a radially matched control sample. The inclusion of additional diagnostic diagrams (based on [OI]$_{6300}$~\&/or [SII]$_{6717+6731}$) is insufficient to remove the observed offset in the [NII]$_{6584}$/H$α$ based calibrations. Using a stricter diagnostic line on the BPT diagram removes $\sim$94 per cent of identified bordering spaxels with metallicities for all metallicity calibrations, and removes the residual offset to higher metallicity values seen in [NII]$_{6584}$/H$α$ calibrations. If science cases demand an exceptionally clean metallicity sample, we recommend either a stricter BPT cut, and/or a non-[NII]$_{6584}$/H$α$ based metallicity calibration.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05140
Can Large Language Models Serve as Effective Classifiers for Hierarchical Multi-Label Classification of Scientific Documents at Industrial Scale?,['Artificial Intelligence'],"['Seyed Amin Tabatabaei', 'Sarah Fancher', 'Michael Parsons', 'Arian Askari']","We address the task of hierarchical multi-label classification (HMC) of scientific documents at an industrial scale, where hundreds of thousands of documents must be classified across thousands of dynamic labels. The rapid growth of scientific publications necessitates scalable and efficient methods for classification, further complicated by the evolving nature of taxonomies--where new categories are introduced, existing ones are merged, and outdated ones are deprecated. Traditional machine learning approaches, which require costly retraining with each taxonomy update, become impractical due to the high overhead of labelled data collection and model adaptation. Large Language Models (LLMs) have demonstrated great potential in complex tasks such as multi-label classification. However, applying them to large and dynamic taxonomies presents unique challenges as the vast number of labels can exceed LLMs' input limits. In this paper, we present novel methods that combine the strengths of LLMs with dense retrieval techniques to overcome these challenges. Our approach avoids retraining by leveraging zero-shot HMC for real-time label assignment. We evaluate the effectiveness of our methods on SSRN, a large repository of preprints spanning multiple disciplines, and demonstrate significant improvements in both classification accuracy and cost-efficiency. By developing a tailored evaluation framework for dynamic taxonomies and publicly releasing our code, this research provides critical insights into applying LLMs for document classification, where the number of classes corresponds to the number of nodes in a large taxonomy, at an industrial scale.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05137
Technology as uncharted territory: Contextual integrity and the notion of AI as new ethical ground,['Artificial Intelligence'],['Alexander Martin Mussgnug'],"Recent research illustrates how AI can be developed and deployed in a manner detached from the concrete social context of application. By abstracting from the contexts of AI application, practitioners also disengage from the distinct normative structures that govern them. Building upon Helen Nissenbaum's framework of contextual integrity, I illustrate how disregard for contextual norms can threaten the integrity of a context with often decisive ethical implications. I argue that efforts to promote responsible and ethical AI can inadvertently contribute to and seemingly legitimize this disregard for established contextual norms. Echoing a persistent undercurrent in technology ethics of understanding emerging technologies as uncharted moral territory, certain approaches to AI ethics can promote a notion of AI as a novel and distinct realm for ethical deliberation, norm setting, and virtue cultivation. This narrative of AI as new ethical ground, however, can come at the expense of practitioners, policymakers and ethicists engaging with already established norms and virtues that were gradually cultivated to promote successful and responsible practice within concrete social contexts. In response, I question the current narrow prioritization in AI ethics of moral innovation over moral preservation. Engaging also with emerging foundation models, I advocate for a moderately conservative approach to the ethics of AI that prioritizes the responsible and considered integration of AI within established social contexts and their respective normative structures.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05130
Robust Computation with Intrinsic Heterogeneity,['Machine Learning'],"['Arash Golmohammadi', 'Christian Tetzlaff']","Intrinsic within-type neuronal heterogeneity is a ubiquitous feature of biological systems, with well-documented computational advantages. Recent works in machine learning have incorporated such diversities by optimizing neuronal parameters alongside synaptic connections and demonstrated state-of-the-art performance across common benchmarks. However, this performance gain comes at the cost of significantly higher computational costs, imposed by a larger parameter space. Furthermore, it is unclear how the neuronal parameters, constrained by the biophysics of their surroundings, are globally orchestrated to minimize top-down errors. To address these challenges, we postulate that neurons are intrinsically diverse, and investigate the computational capabilities of such heterogeneous neuronal parameters. Our results show that intrinsic heterogeneity, viewed as a fixed quenched disorder, often substantially improves performance across hundreds of temporal tasks. Notably, smaller but heterogeneous networks outperform larger homogeneous networks, despite consuming less data. We elucidate the underlying mechanisms driving this performance boost and illustrate its applicability to both rate and spiking dynamics. Moreover, our findings demonstrate that heterogeneous networks are highly resilient to severe alterations in their recurrent synaptic hyperparameters, and even recurrent connections removal does not compromise performance. The remarkable effectiveness of heterogeneous networks with small sizes and relaxed connectivity is particularly relevant for the neuromorphic community, which faces challenges due to device-to-device variability. Furthermore, understanding the mechanism of robust computation with heterogeneity also benefits neuroscientists and machine learners.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05126
Transformers Can Navigate Mazes With Multi-Step Prediction,['Machine Learning'],"['Niklas Nolte', 'Ouail Kitouni', 'Adina Williams', 'Mike Rabbat', 'Mark Ibrahim']","Despite their remarkable success in language modeling, transformers trained to predict the next token in a sequence struggle with long-term planning. This limitation is particularly evident in tasks requiring foresight to plan multiple steps ahead such as maze navigation. The standard next single token prediction objective, however, offers no explicit mechanism to predict multiple steps ahead - or revisit the path taken so far. Consequently, in this work we study whether explicitly predicting multiple steps ahead (and backwards) can improve transformers' maze navigation. We train parameter-matched transformers from scratch, under identical settings, to navigate mazes of varying types and sizes with standard next token prediction and MLM-U, an objective explicitly predicting multiple steps ahead and backwards. We find that MLM-U considerably improves transformers' ability to navigate mazes compared to standard next token prediction across maze types and complexities. We also find MLM-U training is 4x more sample efficient and converges 2x faster in terms of GPU training hours relative to next token training. Finally, for more complex mazes we find MLM-U benefits from scaling to larger transformers. Remarkably, we find transformers trained with MLM-U outperform larger transformers trained with next token prediction using additional supervision from A* search traces. We hope these findings underscore the promise of learning objectives to advance transformers' capacity for long-term planning.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05117
Oscillatory magnetic field-dependent critical temperatures of ultraclean Type-II superconductors,['Superconductivity'],"['Aiying Zhao', 'Richard A Klemm', 'Qiang Gu']","The influence of the Zeeman energy and the Landau levels (LLs) arising from an applied magnetic field ${\bf B}$ upon the critical temperature $T_c$ is studied using a fully quantum mechanical method within the framework of the Bardeen-Cooper-Schrieffer (BCS) theory of superconductivity that forms from an ultraclean metal. As in semiclassical treatments, we found that two electrons can form Cooper pairs with opposite spins and momenta in the ${\bf B}$ direction while either in the same or in neighboring LLs. However, the fully quantum mechanical treatment of the LLs causes $T_c({\bf B})$ for electrons paired on the same LL to oscillate about the critical temperature of the BCS theory, similar to that of the de Haas-van Alphen effect. The Zeeman energy causes $T_c({\bf B})$ to decrease in an oscillatory fashion with increasing ${\bf B}$ for electrons paired either on the same or on neighboring LLs. For the Zeeman $g > 1$, pairing on neighboring LLs results in the highest $T_c({\bf B})$. For $g < 1$, pairing on the same LLs gives the highest $T_c({\bf B})$. In addition, $T_c({\bf B})$ for electrons paired on neighboring LLs exhibits an apparent symmetry around $g=2$, as the oscillatory critical temperature behaviors are nearly identical for $g=2\pmδ$.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.05111
Generating Rectifiable Measures through Neural Networks,['Machine Learning'],"['Erwin Riegler', 'Alex Bühler', 'Yang Pan', 'Helmut Bölcskei']","We derive universal approximation results for the class of (countably) $m$-rectifiable measures. Specifically, we prove that $m$-rectifiable measures can be approximated as push-forwards of the one-dimensional Lebesgue measure on $[0,1]$ using ReLU neural networks with arbitrarily small approximation error in terms of Wasserstein distance. What is more, the weights in the networks under consideration are quantized and bounded and the number of ReLU neural networks required to achieve an approximation error of $\varepsilon$ is no larger than $2^{b(\varepsilon)}$ with $b(\varepsilon)=\mathcal{O}(\varepsilon^{-m}\log^2(\varepsilon))$. This result improves Lemma IX.4 in Perekrestenko et al. as it shows that the rate at which $b(\varepsilon)$ tends to infinity as $\varepsilon$ tends to zero equals the rectifiability parameter $m$, which can be much smaller than the ambient dimension. We extend this result to countably $m$-rectifiable measures and show that this rate still equals the rectifiability parameter $m$ provided that, among other technical assumptions, the measure decays exponentially on the individual components of the countably $m$-rectifiable support set.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05109
Constructing optimal treatment length strategies to maximize quality-adjusted lifetimes,['Statistics Theory'],"['Hao Sun', 'Ashkan Ertefaie', 'Luke Duttweiler', 'Brent A. Johnson']","Real-world clinical decision making is a complex process that involves balancing the risks and benefits of treatments. Quality-adjusted lifetime is a composite outcome that combines patient quantity and quality of life, making it an attractive outcome in clinical research. We propose methods for constructing optimal treatment length strategies to maximize this outcome. Existing methods for estimating optimal treatment strategies for survival outcomes cannot be applied to a quality-adjusted lifetime due to induced informative censoring. We propose a weighted estimating equation that adjusts for both confounding and informative censoring. We also propose a nonparametric estimator of the mean counterfactual quality-adjusted lifetime survival curve under a given treatment length strategy, where the weights are estimated using an undersmoothed sieve-based estimator. We show that the estimator is asymptotically linear and provide a data-dependent undersmoothing criterion. We apply our method to obtain the optimal time for percutaneous endoscopic gastrostomy insertion in patients with amyotrophic lateral sclerosis.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05108
A method for measuring deviation from Lambert diffuse scattering law of ultracold neutrons on material walls,['Instrumentation and Detectors'],"['P. D. Grigoriev', 'V. D. Kochev', 'V. A. Tsyplukhin', 'A. M. Dyugaev', 'I. Ya. Polishchuk']","Modeling the motion of ultracold neutrons (UCNs) is crucial for assessing their losses, accurately measuring their lifetime, and describing other experiments. In material traps, it is necessary to account not only for specular but also for diffuse elastic reflection of UCNs from the trap walls. Typically, the Lambert cosine law is used to describe the angular distribution of diffusely scattered neutrons. However, this law lacks a rigorous theoretical derivation and is often violated. In our work, we propose an experiment to measure the deviation of the angular distribution of UCNs during diffuse scattering from the Lambert law. This deviation can be determined by the difference in the number of neutrons exiting through the central and end windows of a long narrow UCN trap. Monte Carlo simulations corresponding to a possible experiment have been performed, demonstrating a significant effect for different trap geometries.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05106
Algebraic K-theory of completed Kac-Moody groups,['K-Theory and Homology'],"['Arthur Bartels', 'Wolfgang Lueck', 'Stefan Witzel']",We extend results for the K-theory of Hecke algebras of reductive $p$-adic groups to completed Kac-Moody groups.△ Less,"6 December, 2024;",https://arxiv.org/pdf/2412.05105
How many lives does Schrödinger's cat have?,['Physics Education'],"['Andrea López-Incera', 'Wolfgang Dür', 'Stefan Heusler']","Schrödinger's cat is an iconic example for the problem of the transition from the microscopic quantum world to the macroscopic, classical one. It opened many interesting questions such as, could a macroscopic superposition like a dead and alive cat ever exist? What would be the characteristic features of such a system? The field of macroscopicity aims at providing answers to those questions, both from a theoretical and an experimental point of view. Here, we present the main concepts in macroscopicity, including macroscopicity measures, experimental realizations and the link to metrology, from a pedagogical perspective. We provide visualizations and intuitive explanations, together with a hands-on activity where students can create their own macroscopic quantum cats from cardboard cells that are in a superposition of being dead and alive.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05104
Integrating Semantic Communication and Human Decision-Making into an End-to-End Sensing-Decision Framework,['Signal Processing'],"['Edgar Beck', 'Hsuan-Yu Lin', 'Patrick Rückert', 'Yongping Bao', 'Bettina von Helversen', 'Sebastian Fehrler', 'Kirsten Tracht', 'Armin Dekorsy']","As early as 1949, Weaver defined communication in a very broad sense to include all procedures by which one mind or technical system can influence another, thus establishing the idea of semantic communication. With the recent success of machine learning in expert assistance systems where sensed information is wirelessly provided to a human to assist task execution, the need to design effective and efficient communications has become increasingly apparent. In particular, semantic communication aims to convey the meaning behind the sensed information relevant for Human Decision-Making (HDM). Regarding the interplay between semantic communication and HDM, many questions remain, such as how to model the entire end-to-end sensing-decision-making process, how to design semantic communication for the HDM and which information should be provided to the HDM. To address these questions, we propose to integrate semantic communication and HDM into one probabilistic end-to-end sensing-decision framework that bridges communications and psychology. In our interdisciplinary framework, we model the human through a HDM process, allowing us to explore how feature extraction from semantic communication can best support human decision-making. In this sense, our study provides new insights for the design/interaction of semantic communication with models of HDM. Our initial analysis shows how semantic communication can balance the level of detail with human cognitive capabilities while demanding less bandwidth, power, and latency.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05103
"Born-Oppenheimer Renormalization group for High Energy Scattering: CSS, DGLAP and all that",['High Energy Physics - Phenomenology'],"['Haowu Duan', 'Alex Kovner', 'Michael Lublinsky']","In \cite{one}, we have introduced the Born-Oppenheimer (BO) renormalization group approach to high energy hadronic collisions and derived the BO approximation for the light cone wave function of a fast moving projectile hadron. In this second paper, we utilize this wave function to derive the BO evolution of partonic distributions in the hadron -- the gluon transverse momentum and integrated parton distributions (TMD and PDF respectively). The evolution equation for the TMD contains a linear and a nonlinear term. The linear term reproduces the Collins-Soper-Sterman (CSS) equation with a physical relation between the transverse and longitudinal resolution scales. We explain how this equivalence arises, even though the BO and CSS cascades are somewhat different in structures. The nonlinear term in the evolution has a very appealing physical meaning: it is a correction due to stimulated emission, which enhances emission of gluons (bosons) into states with a nonzero occupation.
  For the evolution of the PDF we again find a linear and nonlinear term. At not very small Bjorken $x$, the linear term recovers the DGLAP equation in the leading logarithmic approximation. At small $x$ however there are contributions from gluon splittings which are in the BFKL kinematics leading to a modification of the DGLAP equation. The nonlinear terms have the same physical origin as in the equation for the TMD -- the stimulated emission corrections. Interestingly the nonlinear corrections are the most important for the virtual terms, so that the net correction to the DGLAP is negative and mimics shadowing, although the physical origin of the nonlinearity is very different.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05097
Sense and Sensitivity: Evaluating the simulation of social dynamics via Large Language Models,['Computers and Society'],"['Da Ju', 'Adina Williams', 'Brian Karrer', 'Maximilian Nickel']","Large language models have increasingly been proposed as a powerful replacement for classical agent-based models (ABMs) to simulate social dynamics. By using LLMs as a proxy for human behavior, the hope of this new approach is to be able to simulate significantly more complex dynamics than with classical ABMs and gain new insights in fields such as social science, political science, and economics. However, due to the black box nature of LLMs, it is unclear whether LLM agents actually execute the intended semantics that are encoded in their natural language instructions and, if the resulting dynamics of interactions are meaningful. To study this question, we propose a new evaluation framework that grounds LLM simulations within the dynamics of established reference models of social science. By treating LLMs as a black-box function, we evaluate their input-output behavior relative to this reference model, which allows us to evaluate detailed aspects of their behavior. Our results show that, while it is possible to engineer prompts that approximate the intended dynamics, the quality of these simulations is highly sensitive to the particular choice of prompts. Importantly, simulations are even sensitive to arbitrary variations such as minor wording changes and whitespace. This puts into question the usefulness of current versions of LLMs for meaningful simulations, as without a reference model, it is impossible to determine a priori what impact seemingly meaningless changes in prompt will have on the simulation.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05093
A Construction of the Symmetric Monoidal Structure of the Geometric Whittaker Model,['Representation Theory'],"['Ashutosh Roy Choudhury', 'Tanmay Deshpande']","Let $G$ be a connected reductive algebraic group over an algebraically closed field $k$ of characteristic $p > 0$ and let $\ell$ be a prime number different from $p$. Let $U \subseteq G$ be a maximal unipotent subgroup, $T$ a maximal torus normalizing $U$ and $W$ the Weyl group of $G$. Let $\mathcal{L}$ be a non-degenerate multiplicative $\overline{\mathbb{Q}}_{\ell} $-local system on $U$. In \cite{bd}, the authors show that the bi-Whittaker category, namely the triangulated monoidal category of $(U, \mathcal{L})$-biequivariant $\overline{\mathbb{Q}}_{\ell}$-complexes on $G$ is monoidally equivalent to an explicit thick triangulated monoidal subcategory $\mathscrsfs{D}_{W}^{\circ}(T) \subseteq \mathscrsfs{D}_{W}(T)$ of ""central sheaves"" on the torus. In particular it has the structure of a symmetric monoidal category coming from the symmetric monoidal structure on $\mathscrsfs{D}_W(T)$.
  In this paper, we give another construction of a symmetric monoidal structure on the above category and prove that it agrees with the one coming from the above construction. For this, among other things, we generalize a proof by Gelfand (ref. \cite{car}) for finite groups to the geometric setup.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05092
AI and the law,['General Economics'],['Henry A. Thompson'],"I argue that generative AI will have an uneven effect on the evolution of the law. To do so, I consider generative AI as a labor-augmenting technology that reduces the cost of both writing more complete contracts and litigating in court. The contracting effect reduces the demand for court services by making contracts more complete. The litigation effect, by contrast, increases the demand for court services by a) making contracts less complete and b) reducing litigants' incentive to settle, all else equal. Where contracts are common, as in property and contract law, the change in the quantity of litigation is uncertain due to offsetting contracting and litigation effects. However, in areas where contracts are rare, as in tort law, the amount of litigation is likely to rise. Following Rubin (1977) and Priest (1977) generative AI will accelerate the evolution of tort law toward efficiency.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05090
Analytical assessment of the total loss in tubular hollow-core fibers during their fabrication process,['Optics'],"['Duanny Silva Onório', 'Flavio A. M. Marques', 'Alexandre A. C. Cotta', 'Jefferson E. Tsuchida', 'Alexandre Bessa dos Santos', 'Jonas H. Osório']","Hollow-core photonic crystal fibers (HCPCFs) have become a key enabling technology for addressing a broad spectrum of fundamental and applied needs. Indeed, recent advancements achieved by the HCPCF research community have led to significant progress, establishing these fibers as the lowest-loss optical fibers currently available for use in the visible and ultraviolet ranges. However, the fabrication process of HCPCFs demands costly infrastructure, and achieving ultralow-loss fibers remains a complex technical challenge as numerous fabrication attempts are typically required to optimize their performances. Therefore, predicting these fibers' performances before experimental fabrication is highly desirable. In this work, we tackle this task by analytically assessing the total loss in tubular-lattice HCPCFs during their fabrication process. By considering the variation in the microstructure's geometrical parameters during fabrication and the different sources of loss, we estimate expected loss levels and identify the conditions for loss minimization. We understand that our research provides valuable insights into the fabrication process of hollow-core fibers, offering a predictive approach to evaluate the fibers' performance before their experimental realization. By determining optimal conditions considering geometry, fabrication constraints, and loss figures, we believe that our work contributes to the ongoing efforts to further reduce the loss levels in HCPCFs.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05089
Prospects for Observing Astrophysical Transients with GeV Neutrinos,['High Energy Astrophysical Phenomena'],"['Angelina Partenheimer', 'Jessie Thwaites', 'Ke Fang', 'Justin Vandenbroucke', 'Brian D. Metzger']","Although Cherenkov detectors of high-energy neutrinos in ice and water are often optimized to detect TeV-PeV neutrinos, they may also be sensitive to transient neutrino sources in the 1-100~GeV energy range. A wide variety of transient sources have been predicted to emit GeV neutrinos. In light of the upcoming IceCube-Upgrade, which will extend the IceCube detector's sensitivity down to a few GeV, as well as improve its angular resolution, we survey a variety of transient source models and compare their predicted neutrino fluences to detector sensitivities, in particular those of IceCube-DeepCore and the IceCube Upgrade. We consider the ranges of neutrino fluence from transients powered by non-relativistic shocks, such as novae, supernovae, fast blue optical transients, and tidal disruption events. We also consider fast radio bursts and relativistic outflows of high- and low-luminosity gamma-ray bursts. Our study sheds light on the prospects of observing GeV transients with existing and upcoming neutrino facilities.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05087
Non-linear magnetic buoyancy instability and galactic dynamos,['Astrophysics of Galaxies'],"['Yasin Qazi', 'Anvar. Shukurov', 'Fredrick. A. Gent Devika. Tharakkal', 'Abhijit. B. Bendre']","The magnetic buoyancy (MBI) and Parker instabilities are strong and generic instabilities expected to occur in most astrophysical systems with sufficiently strong magnetic fields. In galactic and accretion discs, large-scale magnetic fields are thought to result from the mean-field dynamo action, in particular, the $α^2Ω$. Using non-ideal MHD equations, we model a section of the galactic disc in which the large-scale magnetic field is generated by an imposed $α$-effect and differential rotation. We extend our earlier study of the interplay between magnetic buoyancy and the mean-field dynamo. We add differential rotation which enhances the dynamo and cosmic rays which enhance magnetic buoyancy. We construct a simple 1D model which replicates all significant features of the 3D simulations. We confirm that magnetic buoyancy can lead to oscillatory magnetic fields and discover that it can vary the magnetic field parity between quadrupolar and dipolar, and that inclusion of the differential rotation is responsible for the switch in field parity. Our results suggest that the large-scale magnetic field can have a dipolar parity within a few kiloparsecs of the galactic centre, provided the MBI is significantly stronger the the dynamo. Quadrupolar parity can remain predominant in the outer parts of a galactic disc. Cosmic rays accelerate both the dynamo and the MBI and support oscillatory non-linear states, a spatial magnetic field structure similar to the alternating magnetic field directions observed in some edge-on galaxies.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05086
Born-Oppenheimer Renormalization group for High Energy Scattering: the Setup and the Wave Function,['High Energy Physics - Phenomenology'],"['Haowu Duan', 'Alex Kovner', 'Michael Lublinsky']","We develop an approach to QCD evolution based on the sequential Born-Oppenheimer approximations that include higher and higher frequency modes as the evolution parameter is increased. This Born-Oppenheimer renormalization group is a general approach which is valid for the high energy evolution as well as the evolution in transverse resolution scale $Q^2$. In the former case it yields the frequency ordered formulation of high energy evolution, which includes both the eikonal splittings which produce gluons with low longitudinal momentum, and the DGLAP-like splittings which produce partons with high transverse momentum. In this, first paper of the series we lay out the formulation of the approach, and derive the expression for the evolved wave function of a hadronic state. We also discuss the form of the $S$-matrix which is consistent with the frequency ordering.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05085
Toric varieties with isolated singularity and smooth normalization,['Algebraic Geometry'],"['Thais Maria Dalbelo', 'Maria Elenice Rodrigues Hernandes', 'Maria Aparecida Soares Ruas']","In this work, we describe a prenormal form for the generators of the semigroup of a toric variety $X \subset \mathbb{C}^p$ with isolated singularity at the origin and smooth normalization. A complete description of the semigroup is given when $X$ is a variety of dimension $n$ in $\mathbb{C}^{2n}$. Moreover, for toric surfaces in $\mathbb{C}^4$, we provide a set of generators of the ideal $I$ defining $X$.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05083
Towards the interoperability of low-code platforms,['Software Engineering'],"['Iván Alfonso', 'Aaron Conrardy', 'Jordi Cabot']","With the promise of accelerating software development, low-code platforms (LCPs) are becoming popular across various industries. Nevertheless, there are still barriers hindering their adoption. Among them, vendor lock-in is a major concern, especially considering the lack of interoperability between these platforms. Typically, after modeling an application in one LCP, migrating to another requires starting from scratch remodeling everything (the data model, the graphical user interface, workflows, etc.), in the new platform.
  To overcome this situation, this work proposes an approach to improve the interoperability of LCPs by (semi)automatically migrating models specified in one platform to another one. The concrete migration path depends on the capabilities of the source and target tools. We first analyze popular LCPs, characterize their import and export alternatives and define transformations between those data formats when available. This is then complemented with an LLM-based solution, where image recognition features of large language models are employed to migrate models based on a simple image export of the model at hand. The full pipelines are implemented on top of the BESSER modeling framework that acts as a pivot representation between the tools.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05075
Black hole with a de Sitter core: classical and quantum features,['General Relativity and Quantum Cosmology'],"['N. Heidari', 'A. A. Araújo Filho', 'V. Vertogradov and', 'A. Övgün']","This work examines the implications of a black hole featuring a de Sitter core. We begin by analyzing the spacetime and event horizon in the presence of de Sitter core. Then the partial wave equation necessary for calculating quasinormal modes is derived and the relation of scalar quasinormal modes with the de Sitter core parameter is explored. Subsequently, we explore the greybody factors and their correspondence with the gravitational quasinormal modes. We also analyze the emission rate. Finally, variations in the thin accretion disks and the influence of de Sitter core spacetime on the optical appearance of the black hole are discussed as well.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05072
A Motivic Riemann-Roch Theorem for Deligne-Mumford Stacks,['Algebraic Geometry'],"['Utsav Choudhury', 'Neeraj Deshmukh', 'Amit Hogadi']","We develop a motivic cohomology theory, representable in the Voevodsky's triangulated category of motives, for smooth separated Deligne-Mumford stacks and show that the resulting higher Chow groups are canonically isomorphic to the higher $K$-theory of such stacks. This generalises the Grothendieck-Riemann-Roch theorem to the category of smooth Deligne-Mumford stacks.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05071
Exotic newforms constructed from a linear combination of eta quotients,['Number Theory'],['Anmol Kumar'],"K{ö}hler, in [1], presented a weight 1 newform on $Γ_0(576)$ constructed from a linear combination of weight 1 eta quotients and asked, ``What would be a suitable $L$ and representation $ρ$ such that Deligne\text{-}Serre correspondence holds?"" In this paper, we find the Galois field extension $L$ and representation $ρ$ such that the Deligne\text{-}Serre correspondence holds for this newform, and also study the splitting of primes in $L$ using the coefficients $a(p)$ of the newform. We also discuss an exotic newform on $Γ_0(1080)$ constructed from a linear combination of weight 1 eta quotients, find the corresponding Galois extension and representation, and study the splitting of primes in this extension. Furthermore, we find all such newforms that can be constructed from a linear combination of weight 1 eta quotients listed in [2] with $q$-expansion of the form $q+\sum_{k=2}^{\infty}a(k)q^k$.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05067
Implementing the RFT finite-volume formalism for three pions across all non-maximal isospins,['High Energy Physics - Lattice'],"['Athari Alotaibi', 'Maxwell T. Hansen']","We present a numerical investigation of the relativistic-field-theoretic (RFT) formalism, used to predict the discrete energy spectrum of three pions in a finite volume. Applying the previously derived generalization, we extract results for all non-maximal isospin values ($I_{πππ} = 2,1,$ and $0$), for different total momenta $\boldsymbol{P}$, and for various irreducible representations of the finite-volume symmetry group. We restrict attention to the unphysical scenario in which the three-particle interactions are set to zero. This set-up thus serves as a baseline for future lattice QCD calculations that will aim to extract such three-body interactions.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05060
Spatiotemporal distribution of the glycoprotein pherophorin II reveals stochastic geometry of the growing ECM of $Volvox~carteri$,['Soft Condensed Matter'],"['Benjamin von der Heyde', 'Anand Srinivasan', 'Sumit Kumar Birwa', 'Eva Laura von der Heyde', 'Steph S. M. H. Höhn', 'Raymond E. Goldstein', 'Armin Hallmann']","The evolution of multicellularity involved the transformation of a simple cell wall of unicellular ancestors into a complex, multifunctional extracellular matrix (ECM). A suitable model organism to study the formation and expansion of an ECM during ontogenesis is the multicellular green alga $Volvox~carteri$, which, along with the related volvocine algae, produces a complex, self-organized ECM composed of multiple substructures. These self-assembled ECMs primarily consist of hydroxyproline-rich glycoproteins, a major component of which is pherophorins. To investigate the geometry of the growing ECM, we fused the $yfp$ gene with the gene for pherophorin II (PhII) in $V.~carteri$. Confocal microscopy reveals PhII:YFP localization at key structures within the ECM, including the boundaries of compartments surrounding each somatic cell and the outer surface of the organism. Image analysis during the life cycle allows the stochastic geometry of those growing compartments to be quantified. We find that their areas and aspect ratios exhibit robust gamma distributions and exhibit a transition from a tight polygonal to a looser acircular packing geometry with stable eccentricity over time, evoking parallels and distinctions with the behavior of hydrated foams. These results provide a quantitative benchmark for addressing a general, open question in biology: How do cells produce structures external to themselves in a robust and accurate manner?△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05059
Surface-plasmon polaritons in multilayer jellium systems: dispersion and spatial description,['Optics'],"['Alexandre Cloots', 'Tanguy Colleu', 'Vincent Liégeois', 'Gian-Marco Rignanese', 'Luc Henrard', 'Xavier Gonze']","Surface-plasmon polaritons (SPPs) are electromagnetic waves that propagate along metal-dielectric interfaces, with important applications in sensing, energy, and nanotechnology. While the behavior of SPPs in single metal slabs is well understood, the coupling between plasmon modes in multilayer systems has received less attention. In this paper, we explore the response functions of SPPs in single-slab, double-slab, and two-different-slab systems using the jellium model. Thanks to a comparison with classical models, our study reveals how quantum effects influence the resonance frequencies of these modes. It also details the spatial description of the different SPP modes and unveils how their coupling occurs in two-different-slab systems. These findings provide new insights into the behavior of SPPs, especially in complex nanostructures.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05057
The effect of radiation pressure on the dispersal of photoevaporating discs,['Earth and Planetary Astrophysics'],"['Alfie Robinson', 'James E. Owen', 'Richard A. Booth']","Observed IR excesses indicate that protoplanetary discs evolve slowly for the majority of their lifetime before losing their near- and mid-IR excesses on short timescales. Photoevaporation models can explain this ""two-timescale"" nature of disc evolution through the removal of inner regions of discs after a few million years. However, they also predict the existence of a population of non-accreting discs with large cavities. Such discs are scarce within the observed population, suggesting the models are incomplete. We explore whether radiation-pressure-driven outflows are able to remove enough dust to fit observations. We simulate these outflows using cuDisc, including dust dynamics, growth/fragmentation, radiative transfer and a parameterisation of internal photoevaporation. We find that, in most cases, dust mass-loss rates are around 5-10 times too small to meet observational constraints. Particles are launched from the disc inner rim, however grains larger than around a micron do not escape in the outflow, meaning mass-loss rates are too low for the initial dust masses at gap-opening. Only systems that have smooth photoevaporation profiles with gas mass-loss rates $>\sim 5 \times 10^{-9}$ $M_\odot$ yr$^{-1}$ and disc dust masses $<\sim$1 $M_\oplus$ at the time of gap opening can meet observational constraints; in the current models these manifest as EUV winds driven by atypically large high-energy photon fluxes. We also find that the height of the disc's photosphere is controlled by small grains in the outflow as opposed to shadowing from a hot inner rim; the effect of this can be seen in synthetic scattered light observations.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05054
FAST: A software suite for automatic weather and optical turbulence forecast on ground-based telescope sites,['Instrumentation and Methods for Astrophysics'],"['A. Turchi', 'E. Masciadri', 'L. Fini']","In this contribution we present the FAST, which is a comprehensive software suite that aims to streamline and automatically manage the forecast of atmospheric and astroclimatic parameters (provided respectively by Meso-Nh and Astro-Meso-Nh models) on large ground-based telescope installations. The forecast of the aforementioned parameters is becoming crucial for the operation of the large telescope installations which possess atmospheric-sensitive equipment equipped with Adaptive Optics (AO) systems. FAST performs automatically all the steps of an atmosphere forecast process: initialisation and forcing data, atmospheric simulation, postprocessing and managing of the outputs.The role of such service is useful both in optimizing beforehand AO instruments to the next atmospheric conditions and in planning telescope observations (especially in ""service mode"") in order to maximize the scientific output. FAST was applied first to the ALTA Center project, which provides forecasts for the LBT telescope. Then it was extended to the more recent project FATE that is a similar forecast system applied to the VLT. Since its first version FAST evolved and it has has been modified to fit with the different technical specifications of the different projects gaining in modularity. It is now able to provide forecasts on different timescales (from days to hours before) and to provide forecast during night and day time. After several years of continuous development we can say that FAST reached full maturity and it is now ready for applications to other projects/sites.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05048
The Survey of Surveys: machine learning for stellar parametrization,['Instrumentation and Methods for Astrophysics'],"['A. Turchi', 'E. Pancino', 'F. Rossi', 'A. Avdeeva', 'P. Marrese', 'S. Marinoni', 'N. Sanna', 'M. Tsantaki', 'G. Fanari']","We present a machine learning method to assign stellar parameters (temperature, surface gravity, metallicity) to the photometric data of large photometric surveys such as SDSS and SKYMAPPER. The method makes use of our previous effort in homogenizing and recalibrating spectroscopic data from surveys like APOGEE, GALAH, or LAMOST into a single catalog, which is used to inform a neural network. We obtain spectroscopic-quality parameters for millions of stars that have only been observed photometrically. The typical uncertainties are of the order of 100K in temperature, 0.1 dex in surface gravity, and 0.1 dex in metallicity and the method performs well down to low metallicity, were obtaining reliable results is known to be difficult.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05047
Observation of Cosmic-Ray Anisotropy in the Southern Hemisphere with Twelve Years of Data Collected by the IceCube Neutrino Observatory,['High Energy Astrophysical Phenomena'],"['R. Abbasi', 'M. Ackermann', 'J. Adams', 'S. K. Agarwalla', 'T. Aguado', 'J. A. Aguilar', 'M. Ahlers', 'J. M. Alameddine', 'N. M. Amin', 'K. Andeen', 'C. Argüelles', 'Y. Ashida', 'S. Athanasiadou', 'S. N. Axani', 'R. Babu', 'X. Bai', 'A. Balagopal V.', 'M. Baricevic', 'S. W. Barwick', 'S. Bash', 'V. Basu', 'R. Bay', 'J. J. Beatty', 'J. Becker Tjus', 'J. Beise']","We analyzed the 7.92$\times 10^{11}$ cosmic-ray-induced muon events collected by the IceCube Neutrino Observatory from May 13, 2011, when the fully constructed experiment started to take data, to May 12, 2023. This dataset provides an up-to-date cosmic-ray arrival direction distribution in the Southern Hemisphere with unprecedented statistical accuracy covering more than a full period length of a solar cycle. Improvements in Monte Carlo event simulation and better handling of year-to-year differences in data processing significantly reduce systematic uncertainties below the level of statistical fluctuations compared to the previously published results. We confirm the observation of a change in the angular structure of the cosmic-ray anisotropy between 10 TeV and 1 PeV, more specifically in the 100-300 TeV energy range.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05046
The Wigner formalism on black hole geometries,['General Relativity and Quantum Cosmology'],"['David García García', 'Jose A. R. Cembranos']","This work explores the intersection of quantum mechanics and curved spacetime by employing the Wigner formalism to investigate quantum systems in the vicinity of black holes. Specifically, we study the quantum dynamics of a probe particle bound to a Schwarzschild black hole using a phase-space representation of quantum mechanics. The analysis begins with a review of the covariant Wigner function in curved spacetime, highlighting its application to spherically symmetric, uncharged black holes. We then derive an effective potential from the Schwarzschild metric, which defines the Hamiltonian for the electron. Relativistic corrections are treated perturbatively to estimate energy levels and associated Wigner functions for the bound state. Additionally, we compare the results obtained through the Schrodinger equation with those derived directly using the symplectic formalism, demonstrating the consistency and versatility of the phase-space approach. The study sheds light on quantum behavior near black holes and suggests new avenues for combining quantum kinetic theory with relativistic gravitational settings.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05045
Chimera states in a system of stationary and flying-through deterministic particles with an internal degree of freedom,['Chaotic Dynamics'],"['Maxim I. Bolotov', 'Lev A. Smirnov', 'Vyacheslav O. Munyaev', 'Grigory V. Osipov']","We consider the effect of the emergence of chimera states in a system of coexisting stationary and flying-through in potential particles with an internal degree of freedom determined by the phase. All particles tend to an equilibrium state with a small number of potential wells, which leads to the emergence of a stationary chimera. An increase in the number of potential wells leads to the emergence of particles flying-through along the medium, the phases of which form a moving chimera. Further, these two structures coexist and interact with each other. In this case, an increase in the local synchronization degree of the chimera is observed in the areas of the synchronous cluster location.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05044
Improving Post-Earthquake Crack Detection using Semi-Synthetic Generated Images,['Computer Vision and Pattern Recognition'],"['Piercarlo Dondi', 'Alessio Gullotti', 'Michele Inchingolo', 'Ilaria Senaldi', 'Chiara Casarotti', 'Luca Lombardi', 'Marco Piastra']","Following an earthquake, it is vital to quickly evaluate the safety of the impacted areas. Damage detection systems, powered by computer vision and deep learning, can assist experts in this endeavor. However, the lack of extensive, labeled datasets poses a challenge to the development of these systems. In this study, we introduce a technique for generating semi-synthetic images to be used as data augmentation during the training of a damage detection system. We specifically aim to generate images of cracks, which are a prevalent and indicative form of damage. The central concept is to employ parametric meta-annotations to guide the process of generating cracks on 3D models of real-word structures. The governing parameters of these meta-annotations can be adjusted iteratively to yield images that are optimally suited for improving detectors' performance. Comparative evaluations demonstrated that a crack detection system trained with a combination of real and semi-synthetic images outperforms a system trained on real images alone.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05042
First experiences with the LHCb heterogeneous software trigger,['High Energy Physics - Experiment'],['Andy Morris'],"Since 2022, the LHCb detector has been taking both proton-proton and lead-ion data at the LHC collision rate using a fully software-based trigger. This has been implemented on GPUs at its first stage and CPUs at its second. The setup allows for reconstruction, alignment, calibration and selections to be performed online -- known as the real time analysis paradigm. As well as this, physics analyses are performed using the output of online reconstruction with early results shown using data taken in 2022.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05041
Linearization of Newton's second law,['Mathematical Physics'],['Andronikos Paliathanasis'],"The geometric linearization of nonlinear differential equation is a robust method for the construction of analytic solutions. The method is related to the existence of Lie symmetries which can be used to determine point transformations such that to write the given differential equation in a linear form. In this study we employ another geometric approach and we utilize the Eisenhart lift to geometric linearize the Newtonian system describing the motion of a particle in a line under the application of an autonomous force. Our findings reveal that for the oscillator, the Ermakov potential with or without the oscillator term, and the Morse potential, Newton's second law can be globally expressed in the form of that of a free particle. This study open new directions for the geometric linearization of differential equations via equivalent dynamical systems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05036
Open Cluster Study Using $Gaia$ I:Membership and Cluster Properties,['Solar and Stellar Astrophysics'],"['Anindya Ganguly', 'Prasanta K. Nayak', 'Sourav Chatterjee']","Star clusters are interesting laboratories to study star formation, single and binary stellar evolution, and stellar dynamics. We have used the exquisite data from $Gaia$'s data release 3 (DR3) to study 21 relatively rich and nearby open clusters with member numbers ($N_{\rm{cl}}$)$>500$. We have developed a non-parametric method to identify cluster members. Our method works well for clusters located in both sparse and crowded environments, hence, can be applied to a wide variety of star clusters. Since the member classification scheme does not make any assumptions on the expected distributions of potential cluster members, our method can identify members associated with clusters that are oddly shaped or have complex internal spatial or kinematic structures. In addition, since the membership determination does not depend on the proximity to any well-defined sequences on the color-magnitude diagram, this method easily identifies straggler members. Furthermore, for each of these clusters, we estimate essential cluster properties including age, metallicity, distance, and reddening using detailed Markov-Chain Monte Carlo parameter estimation. We report the full posteriors for these important cluster properties for all clusters in our study.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05033
Fully independent response in disordered solids,['Computational Physics'],"['Mengjie Zu', 'Aayush Desai', 'Carl P. Goodrich']","Unlike in crystals, it is difficult to trace emergent material properties of amorphous solids to their underlying structure. Nevertheless, one can tune features of a disordered spring network, ranging from bulk elastic constants to specific allosteric responses, through highly precise alterations of the structure. This has been understood through the notion of independent bond-level response -- the observation that in many cases, different springs have different effects on different properties. While this idea has motivated inverse design in numerous contexts, it has not been formalized and quantified in a general context that not just informs but enables and predicts inverse design. Here, we show how to quantify independent response by linearizing the simultaneous change in multiple emergent features, and introduce the much stronger notion of fully independent response. Remarkably, we find that the mechanical properties of disordered solids are always fully independent across a wide array of scenarios, regardless of the target features, tunable parameters, and details of particle-particle interactions. Furthermore, our formulation quantifies the susceptibility of feature changes to parameter changes, which we find to be correlated with the maximum linear tunability. These results formalize our understanding of a key fundamental difference between ordered and disordered solids while also creating a practical tool to both understand and perform inverse design.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05031
Quantum Security Analysis of the Key-Alternating Ciphers,['Quantum Physics'],"['Chen Bai', 'Mehdi Esmaili', 'Atul Mantri']","We study the security of key-alternating ciphers (KAC), a generalization of Even-Mansour ciphers over multiple rounds, which serve as abstractions for many block cipher constructions, particularly AES. While the classical security of KAC has been extensively studied, little is known about its security against quantum adversaries. In this paper, we introduce the first nontrivial quantum key-recovery attack on multi-round KAC in a model where the adversary has quantum access to only one of the public permutations. Our attack applies to any $t$-round KAC, achieving quantum query complexity of $O(2^{\frac{t(t+1)n}{(t+1)^2+1}})$, where $n$ is the size of each individual key, in a realistic quantum threat model, compared to the classical bound of $O(2^{\frac{tn}{(t+1)}})$ queries given by Bogdanev et al. (EUROCRYPT 2012). Our quantum attack leverages a novel approach based on quantum walk algorithms. Additionally, using the quantum hybrid method in our new threat model, we extend the Even-Mansour lower bound of $Ω(2^{\frac{n}{3}})$ given by Alagic et al. (EUROCRYPT 2022) to $Ω(2^{\frac{(t-1)n}{t}})$ for the $t$-round KAC (for $t \geq 2$).△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05026
Ultralong-range Rydberg molecules of Hg atoms,['Atomic Physics'],"['Agata Wojciechowska', 'Michał Tomza', 'Matthew T. Eiles']","Ultralong-range Rydberg molecules, composed of an excited Rydberg atom and a ground-state atom, are characterized by large bond lengths, dipole moments, sensitivity to external fields, and an unusual binding mechanism based on low-energy elastic electron scattering. Although Rydberg molecules formed between alkali atoms have received the most attention, the additional complexity found in atoms with more than a single valence electron poses new theoretical challenges as well as new possibilities for control and design of the molecular structure. In this paper, we extend the theory of Rydberg molecules to include the additional spin coupling of the Rydberg states of a multivalent atom. We employ this theory to describe the properties of Rydberg molecules composed of mercury atoms. We calculate the potential energy curves of both heteronuclear (Hg*Rb) and homonuclear (Hg*Hg) molecules. In the former case, we propose the realization of long-range spin entanglement and remote spin flip. In the latter, we show how long-lived metastable molecular states of Hg*Hg exist as resonances above the dissociation threshold.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05025
Steps are all you need: Rethinking STEM Education with Prompt Engineering,['Computation and Language'],"['Krishnasai Addala', 'Kabir Dev Paul Baghel', 'Chhavi Kirtani', 'Avinash Anand', 'Rajiv Ratn Shah']","Few shot and Chain-of-Thought prompting have shown promise when applied to Physics Question Answering Tasks, but are limited by the lack of mathematical ability inherent to LLMs, and are prone to hallucination. By utilizing a Mixture of Experts (MoE) Model, along with analogical prompting, we are able to show improved model performance when compared to the baseline on standard LLMs. We also survey the limits of these prompting techniques and the effects they have on model performance. Additionally, we propose Analogical CoT prompting, a prompting technique designed to allow smaller, open source models to leverage Analogical prompting, something they have struggled with, possibly due to a lack of specialist training data.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05023
Improving data sharing and knowledge transfer via the Neuroelectrophysiology Analysis Ontology (NEAO),['Quantitative Methods'],"['Cristiano André Köhler', 'Sonja Grün', 'Michael Denker']","Describing the processes involved in analyzing data from electrophysiology experiments to investigate the function of neural systems is inherently challenging. On the one hand, data can be analyzed by distinct methods that serve a similar purpose, such as different algorithms to estimate the spectral power content of a measured time series. On the other hand, different software codes can implement the same algorithm for the analysis while adopting different names to identify functions and parameters. Having reproducibility in mind, with these ambiguities the outcomes of the analysis are difficult to report, e.g., in the methods section of a manuscript or on a platform for scientific findings. Here, we illustrate how using an ontology to describe the analysis process can assist in improving clarity, rigour and comprehensibility by complementing, simplifying and classifying the details of the implementation. We implemented the Neuroelectrophysiology Analysis Ontology (NEAO) to define a unified vocabulary and to standardize the descriptions of the processes involved in analyzing data from neuroelectrophysiology experiments. Real-world examples demonstrate how the NEAO can be employed to annotate provenance information describing an analysis process. Based on such provenance, we detail how it can be used to query various types of information (e.g., using knowledge graphs) that enable researchers to find, understand and reuse prior analysis results.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05021
Differential age observations and their constraining power in cosmology,['General Relativity and Quantum Cosmology'],['Asta Heinesen'],"We derive the differential age signal valid for cosmic chronometers (passively evolving galaxies) in any space-time that satisfies the following assumptions: (i) The space-time has a metric with Lorentzian signature and the connection is the Levi-Civita connection; (ii) the cosmic chronometers are collectively well approximated as a geodesic and irrotational congruence of time-like worldlines in the space-time; (iii) light travels on null geodesics and caustics on the observer's past light cone can be ignored; (iv) the space-time is cosmological, meaning that isotropic and positive expansion degrees-of-freedom dominate over anisotropic and negative expansion degrees-of-freedom when viewed on sufficiently large scales in the frame of the cosmic chronometers. The main result of the paper is an expression for the differential age signal that is written in terms of line-of-sight averages of the expansion rate along individual null lines, thus providing a kinematic interpretation of the differential age signal applicable to cosmological space-times satisfying (i)--(iv). We explain how this result indicates that the differential age signal is a robust probe of the volume-average expansion rate in very general statistically homogeneous and isotropic space-time scenarios where other probes of the volume-average expansion rate tend to yield biased results. We argue that this unique property of the differential age signal makes it an ideal measurement for constraining the expansion history model-independently.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05020
Two-photon coupling via Josephson element I: Breaking the symmetry with magnetic fields,['Superconductivity'],"['E. V. Stolyarov', 'V. L. Andriichuk', 'Andrii M. Sokolov']",We consider a coupling element based on a symmetric superconducting quantum interference device (SQUID) and show that it mediates a two-photon interaction. This and other inductive interactions can be switched off in situ. We derive the system Hamiltonian for coupled resonator and rf SQUID. The rf SQUID dwells in the vicinity of its metastable well holding a number of energy states and acts as an artificial atom. We discuss how the Josephson symmetry breaks owing to magnetic fields in the superconducting loops. We assess that the two-photon coupling strength reaches 18 MHz which can exceed the single-photon capacitive interaction in the coupler.△ Less,"6 December, 2024;",https://arxiv.org/pdf/2412.05016
Solitons in 3-State Mealy Automata,['Exactly Solvable and Integrable Systems'],"['Atsushi Maeno', 'Satoshi Tsujimoto', 'Fumitaka Yura']","Box--ball systems (BBS) are integrable systems with soliton solutions and other good properties. We will search for automata that belong to the same class as BBS automata by introducing some classes of automata through the features of BBS automaton. In particular, we would like to classify 3-state automata over a 2-letter alphabet.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05014
Space Time Algebra Formulation of Cold Magnetized Plasmas,['Plasma Physics'],"['Kyriakos Hizanidis', 'Efstratios Koukoutsis', 'Panagiotis Papagiannis', 'Abhay K. Ram', 'George Vahala']","The propagation and scattering of electromagnetic waves in magnetized plasmas in a state where a global mode has been established or is in turbulence, are of theoretical and experimental interest in thermonuclear fusion research. Interpreting experimental results, as well as predicting plasma behavior requires the numerical solutions of the underlying physics, that is, the numerical solution of Maxwell equations under various initial conditions and, under the circumstances, complex boundary conditions. Casting, the underlying equations in a coordinate free form that exploits the symmetries and the conserved quantities in a form that can easily encompass a variety of initial and boundary conditions is of tantamount importance. Pursuing this task we utilize the advantages the Clifford Algebras can possibly provide. For simplicity we deal with a cold multi-species lossless magnetized plasma. The formulation renders a Dirac type evolution equation for am augmented state that consists of the electric and magnetic field bivectors as well as the polarizations and their associated currents for each species. This evolution equation can be dealt with a general spatial lattice disretization scheme. The evolution operator that dictates the temporal advancement of the state is Hermitian. This formulation is computationally simpler whatever the application could be. However, small wavelength capabilities (on the Debye length scale) for spatially large systems (magnetic confinement devices) is questionable even for conventional super-computers. However, the formulation provided in this work it is entirely suitable and it can be directly transferred in a quantum computer. It is shown that the simplified problem in the present work could be suitable for contemporary rudimentary quantum computers.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05009
$C^*$-extreme contractive completely positive maps,['Operator Algebras'],"['Anand O. R', 'K. Sumesh']","The main objective of this paper is to generalize a specific quantized convexity structure of the generalized state space of a $C^*$-algebra and to examine the associated extreme points. We introduce the notion of $P$-$C^*$-convex subsets, where $P$ is any positive operator on a Hilbert space $\mathcal{H}$. These subsets are defined with in the set of all completely positive (CP) maps from a unital $C^*$-algebra $\mathcal{A}$ into the algebra $B(\mathcal{H})$ of bounded linear maps on $\mathcal{H}$. In particular, we focus on certain $P$-$C^*$-convex sets, denoted by $\mathrm{CP}^{(P)}(\mathcal{A},B(\mathcal{H}))$, and analyze their extreme points with respect to this new convexity structure. This generalizes the existing notions of $C^*$-convex subsets and $C^*$-extreme points of unital completely positive maps. We significantly extend many of the known results regarding the $C^*$-extreme points of unital completely positive maps into the context of $P$-$C^*$-convex sets we are considering. This includes abstract characterization and structure of $P$-$C^*$-extreme points. Further, using these studies, we completely characterize the $C^*$-extreme points of the $C^*$-convex set of all contractive completely positive maps from $\mathcal{A}$ into $B(\mathcal{H})$, where $\mathcal{H}$ is finite-dimensional. Additionally, we discuss the connection between $P$-$C^*$-extreme points and linear extreme points of these convex sets, as well as Krein-Milman type theorems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.05008
'Debunk-It-Yourself': Health Professionals' Strategies for Responding to Misinformation on TikTok,['Cryptography and Security'],"['Filipo Sharevski', 'Jennifer Vander Loop', 'Amy Devine', 'Peter Jachim', 'Sanchari Das']","Misinformation is ""sticky"" in nature, requiring a considerable effort to undo its influence. One such effort is debunking or exposing the falsity of information. As an abundance of misinformation is on social media, platforms do bear some debunking responsibility in order to preserve their trustworthiness as information providers. A subject of interpretation, platforms poorly meet this responsibility and allow dangerous health misinformation to influence many of their users. This open route to harm did not sit well with health professional users, who recently decided to take the debunking into their own hands. To study this individual debunking effort - which we call 'Debunk-It-Yourself (DIY)' - we conducted an exploratory survey n=14 health professionals who wage a misinformation counter-influence campaign through videos on TikTok. We focused on two topics, nutrition and mental health, which are the ones most often subjected to misinformation on the platform. Our thematic analysis reveals that the counterinfluence follows a common process of initiation, selection, creation, and ""stitching"" or duetting a debunking video with a misinformation video. The 'Debunk-It-Yourself' effort was underpinned by three unique aspects: (i) it targets trending misinformation claims perceived to be of direct harm to people's health; (ii) it offers a symmetric response to the misinformation; and (iii) it is strictly based on scientific evidence and claimed clinical experience. Contrasting the 'Debunk-It-Yourself' effort with the one TikTok and other platforms (reluctantly) put in moderation, we offer recommendations for a structured response against the misinformation's influence by the users themselves.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04999
Automated in situ optimization and disorder mitigation in a quantum device,['Mesoscale and Nanoscale Physics'],"['Jacob Benestad', 'Torbjørn Rasmussen', 'Bertram Brovang', 'Oswin Krause', 'Saeed Fallahi', 'Geoffrey C. Gardner', 'Michael J. Manfra', 'Charles M. Marcus', 'Jeroen Danon', 'Ferdinand Kuemmeth', 'Anasua Chatterjee', 'Evert van Nieuwenburg']","We investigate automated in situ optimization of the potential landscape in a quantum point contact device, using a $3 \times 3$ gate array patterned atop the constriction. Optimization is performed using the covariance matrix adaptation evolutionary strategy, for which we introduce a metric for how ""step-like"" the conductance is as the channel becomes constricted. We first perform the optimization of the gate voltages in a tight-binding simulation and show how such in situ tuning can be used to mitigate a random disorder potential. The optimization is then performed in a physical device in experiment, where we also observe a marked improvement in the quantization of the conductance resulting from the optimization procedure.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04997
Radiative Back-Reaction on Charged Particle Motion in the Dipole Magnetosphere of Neutron Stars,['High Energy Astrophysical Phenomena'],"['Zdeněk Stuchlík', 'Jaroslav Vrba', 'Martin Kološ', 'Arman Tursunov']","The motion of charged particles under the Lorentz force in the magnetosphere of neutron stars, represented by a dipole field in the Schwarzschild spacetime, can be determined by an effective potential, whose local extrema govern circular orbits both in and off the equatorial plane, which coincides with the symmetry plane of the dipole field. In this work, we provide a detailed description of the properties of these ""conservative"" circular orbits and, using the approximation represented by the Landau-Lifshitz equation, examine the role of the radiative back-reaction force that influences the motion of charged particles following both the in and off equatorial circular orbits, as well as the chaotic orbits confined to belts centered around the circular orbits. To provide clear insight into these dynamics, we compare particle motion with and without the back-reaction force. We demonstrate that, in the case of an attractive Lorentz force, the back-reaction leads to the charged particles falling onto the neutron star's surface in all scenarios considered. For the repulsive Lorentz force, in combination with the back-reaction force, we observe a widening of stable equatorial circular orbits; the off-equatorial orbits shift toward the equatorial plane and subsequently widen if they are sufficiently close to the plane. Otherwise, the off-equatorial orbits evolve toward the neutron star surface. The critical latitude, which separates orbital widening from falling onto the surface, is determined numerically as a function of the electromagnetic interaction's intensity.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04996
Computational Advantage in Hybrid Quantum Neural Networks: Myth or Reality?,['Quantum Physics'],"['Muhammad Kashif', 'Alberto Marchisio', 'Muhammad Shafique']","Hybrid Quantum Neural Networks (HQNNs) have gained attention for their potential to enhance computational performance by incorporating quantum layers into classical neural network (NN) architectures. However, a key question remains: Do quantum layers offer computational advantages over purely classical models? This paper explores how classical and hybrid models adapt their architectural complexity to increasing problem complexity. Using a multiclass classification problem, we benchmark classical models to identify optimal configurations for accuracy and efficiency, establishing a baseline for comparison. HQNNs, simulated on classical hardware (as common in the Noisy Intermediate-Scale Quantum (NISQ) era), are evaluated for their scaling of floating-point operations (FLOPs) and parameter growth. Our findings reveal that as problem complexity increases, HQNNs exhibit more efficient scaling of architectural complexity and computational resources. For example, from 10 to 110 features, HQNNs show an 88.5% increase in FLOPs compared to 53.1% for classical models, despite simulation overheads. Additionally, the parameter growth rate is slower in HQNNs (81.4%) than in classical models (88.5%). These results highlight HQNNs' scalability and resource efficiency, positioning them as a promising alternative for solving complex computational problems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04991
Location-Driven Programmable Wireless Environments through Light-emitting RIS (LeRIS),['Networking and Internet Architecture'],"['Dimitrios Bozanis', 'Dimitrios Tyrovolas', 'Vasilis K. Papanikolaou', 'Sotiris A. Tegos', 'Panagiotis D. Diamantoulakis', 'Christos K. Liaskos', 'Robert Schober', 'George K. Karagiannidis']","As 6G wireless networks seek to enable robust and dynamic programmable wireless environments (PWEs), reconfigurable intelligent surfaces (RISs) have emerged as a cornerstone for controlling electromagnetic wave propagation. However, realizing the potential of RISs for demanding PWE applications depends on precise and real-time user localization, especially in scenarios with random receiver orientations and inherent hardware imperfections. To address this challenge, we propose a novel optical localization framework that integrates conventional ceiling-mounted LEDs with light-emitting reconfigurable intelligent surfaces (LeRISs). By leveraging the spatial diversity offered by the LeRIS architecture, the framework introduces robust signal paths that improve localization accuracy and reduce errors under varying orientations. To this end, we derive a system of equations for received signal strength-based localization that accounts for random receiver orientations and imposes spatial constraints on LED placement, ensuring unique and reliable solutions. Finally, our simulation results demonstrate that the proposed framework achieves precise beam control and high spectral efficiency even for RISs with large number of reflecting elements, establishing our solution as scalable and adaptive for PWEs that require real-time accuracy and flexibility.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04989
"Power Plant Detection for Energy Estimation using GIS with Remote Sensing, CNN & Vision Transformers",['Machine Learning'],"['Blessing Austin-Gabriel', 'Cristian Noriega Monsalve', 'Aparna S. Varde']","In this research, we propose a hybrid model for power plant detection to assist energy estimation applications, by pipelining GIS (Geographical Information Systems) having Remote Sensing capabilities with CNN (Convolutional Neural Networks) and ViT (Vision Transformers). Our proposed approach enables real-time analysis with multiple data types on a common map via the GIS, entails feature-extraction abilities due to the CNN, and captures long-range dependencies through the ViT. This hybrid approach is found to enhance classification, thus helping in the monitoring and operational management of power plants; hence assisting energy estimation and sustainable energy planning in the future. It exemplifies adequate deployment of machine learning methods in conjunction with domain-specific approaches to enhance performance.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04986
Frontier Models are Capable of In-context Scheming,['Artificial Intelligence'],"['Alexander Meinke', 'Bronson Schoen', 'Jérémy Scheurer', 'Mikita Balesni', 'Rusheb Shah', 'Marius Hobbhahn']","Frontier models are increasingly trained and deployed as autonomous agent. One safety concern is that AI agents might covertly pursue misaligned goals, hiding their true capabilities and objectives - also known as scheming. We study whether models have the capability to scheme in pursuit of a goal that we provide in-context and instruct the model to strongly follow. We evaluate frontier models on a suite of six agentic evaluations where models are instructed to pursue goals and are placed in environments that incentivize scheming. Our results show that o1, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 405B all demonstrate in-context scheming capabilities. They recognize scheming as a viable strategy and readily engage in such behavior. For example, models strategically introduce subtle mistakes into their responses, attempt to disable their oversight mechanisms, and even exfiltrate what they believe to be their model weights to external servers. Additionally, this deceptive behavior proves persistent. When o1 has engaged in scheming, it maintains its deception in over 85% of follow-up questions and often remains deceptive in multi-turn interrogations. Analysis of the models' chains-of-thought reveals that models explicitly reason about these deceptive strategies, providing evidence that the scheming behavior is not accidental. Surprisingly, we also find rare instances where models engage in scheming when only given a goal, without being strongly nudged to pursue it. We observe cases where Claude 3.5 Sonnet strategically underperforms in evaluations in pursuit of being helpful, a goal that was acquired during training rather than in-context. Our findings demonstrate that frontier models now possess capabilities for basic in-context scheming, making the potential of AI agents to engage in scheming behavior a concrete rather than theoretical concern.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04984
"Red, hot, and very metal poor: extreme properties of a massive accreting black hole in the first 500 Myr",['Astrophysics of Galaxies'],"['Roberta Tripodi', 'Nicholas Martis', 'Vladan Markov', 'Maruša Bradač', 'Fabio Di Mascia', 'Vieri Cammelli', ""Francesco D'Eugenio"", 'Chris Willott', 'Mirko Curti', 'Maulik Bhatt', 'Simona Gallerani', 'Gregor Rihtaršič', 'Jasbir Singh', 'Gaia Gaspar', 'Anishya Harshan', 'Jon Judež', 'Rosa M. Merida', 'Guillaume Desprez', 'Marcin Sawicki', 'Ilias Goovaerts', 'Adam Muzzin', 'Gaël Noirot', 'Ghassan T. E. Sarrouh', 'Roberto Abraham', 'Yoshihisa Asada']","The James Webb Space Telescope (JWST) has recently discovered a new population of objects at high redshift referred to as `Little Red Dots' (LRDs). Their nature currently remains elusive, despite their surprisingly high inferred number densities. This emerging population of red point-like sources is reshaping our view of the early Universe and may shed light on the formation of high-redshift supermassive black holes. Here we present a spectroscopically confirmed LRD CANUCS-LRD-z8.6 at $z_{\rm spec}=8.6319\pm 0.0005$ hosting an Active Galactic Nucleus (AGN), using JWST data. This source shows the typical spectral shape of an LRD (blue UV and red optical continuum, unresolved in JWST imaging), along with broad H$β$ line emission, detection of high-ionization emission lines (CIV, NIV]) and very high electron temperature indicative of the presence of AGN. This is also combined with a very low metallicity ($Z<0.1 Z_\odot$). The presence of all these diverse features in one source makes CANUCS-LRD-z8.6 unique. We show that the inferred black hole mass of CANUCS-LRD-z8.6 ($M_{\rm BH}=1.0^{+0.6}_{-0.4}\times 10^{8}\rm ~M_\odot$) strongly challenges current standard theoretical models and simulations of black hole formation, and forces us to adopt `ad hoc' prescriptions. Indeed if massive seeds, or light seeds with super-Eddington accretion, are considered, the observed BH mass of CANUCS-LRD-z8.6 at $z=8.6$ can be reproduced. Moreover, the black hole is over-massive compared to its host, relative to the local $M_{\rm BH}-M_*$ relations, pointing towards an earlier and faster evolution of the black hole compared to its host galaxy.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04983
Causal discovery with endogenous context variables,['Machine Learning'],"['Wiebke Günther', 'Oana-Iuliana Popescu', 'Martin Rabel', 'Urmi Ninad', 'Andreas Gerhardus', 'Jakob Runge']","Causal systems often exhibit variations of the underlying causal mechanisms between the variables of the system. Often, these changes are driven by different environments or internal states in which the system operates, and we refer to context variables as those variables that indicate this change in causal mechanisms. An example are the causal relations in soil moisture-temperature interactions and their dependence on soil moisture regimes: Dry soil triggers a dependence of soil moisture on latent heat, while environments with wet soil do not feature such a feedback, making it a context-specific property. Crucially, a regime or context variable such as soil moisture need not be exogenous and can be influenced by the dynamical system variables - precipitation can make a dry soil wet - leading to joint systems with endogenous context variables. In this work we investigate the assumptions for constraint-based causal discovery of context-specific information in systems with endogenous context variables. We show that naive approaches such as learning different regime graphs on masked data, or pooling all data, can lead to uninformative results. We propose an adaptive constraint-based discovery algorithm and give a detailed discussion on the connection to structural causal models, including sufficiency assumptions, which allow to prove the soundness of our algorithm and to interpret the results causally. Numerical experiments demonstrate the performance of the proposed method over alternative baselines, but they also unveil current limitations of our method.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04981
Boundary regularity for quasiminima of double-phase problems on metric spaces,['Analysis of PDEs'],"['Antonella Nastasi', 'Cintia Pacchiano Camacho']","We give a sufficient condition for Hölder continuity at a boundary point for quasiminima of double-phase functionals of $p,q$-Laplace type, in the setting of metric measure spaces equipped with a doubling measure and supporting a Poincaré inequality. We use a variational approach based on De Giorgi-type conditions to give a pointwise estimate near a boundary point. The proofs are based on a careful phase analysis and estimates in the intrinsic geometries.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04978
ORKG ASK: a Neuro-symbolic Scholarly Search and Exploration System,['Digital Libraries'],"['Allard Oelen', 'Mohamad Yaser Jaradeh', 'Sören Auer']","Purpose: Finding scholarly articles is a time-consuming and cumbersome activity, yet crucial for conducting science. Due to the growing number of scholarly articles, new scholarly search systems are needed to effectively assist researchers in finding relevant literature.
  Methodology: We take a neuro-symbolic approach to scholarly search and exploration by leveraging state-of-the-art components, including semantic search, Large Language Models (LLMs), and Knowledge Graphs (KGs). The semantic search component composes a set of relevant articles. From this set of articles, information is extracted and presented to the user.
  Findings: The presented system, called ORKG ASK (Assistant for Scientific Knowledge), provides a production-ready search and exploration system. Our preliminary evaluation indicates that our proposed approach is indeed suitable for the task of scholarly information retrieval.
  Value: With ORKG ASK, we present a next-generation scholarly search and exploration system and make it available online. Additionally, the system components are open source with a permissive license.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04977
From Theory to Practice: Demonstrators of FAIR Data Spaces Across Different Sectors,"['Distributed, Parallel, and Cluster Computing']","['Nikolaus Glombiewski', 'Zeyd Boukhers', 'Christian Beilschmidt', 'Johannes Drönner', 'Michael Mattig', 'Artur Piet', 'Robert Pietrzynski', 'Mehrshad Jaberansary', 'Macedo Maia', 'Sebastian Beyvers', 'Yeliz Üçer Yediel', 'Muhammad Hamza Akhtar', 'Heiner Oberkampf', 'Jonathan Hartman', 'Bernhard Seeger', 'Christoph Lange']","The principles of data spaces for sovereign data exchange across trusted organizations have so far mainly been adopted in business-to-business settings, and recently scaled to cloud environments. Meanwhile, research organizations have established distributed research data infrastructures, respecting the principle that data must be FAIR, i.e., findable, accessible, interoperable and reusable. For mutual benefit of these two communities, the FAIR Data Spaces project aims to connect them towards the vision of a common, cloud-based data space for industry and research. Thus, the project establishes a common legal and ethical framework, common technical building blocks, and it demonstrates the orchestration of multiple building blocks in self-contained settings addressing a diverse range of use cases in domains including health, biodiversity, and engineering. This paper gives a summary of all demonstrators, ranging from research data infrastructures scaled to industry-ready cloud environments to work in progress on building bridges between operational business-to-business data spaces and research data infrastructures.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04969
Stochastic modeling of blob-like plasma filaments in the scrape-off layer: Time-dependent velocities and pulse stagnation,['Plasma Physics'],"['O. Paikina', 'J. M. Losada', 'A. Theodorsen', 'O. E. Garcia']","A stochastic model for a super-position of uncorrelated pulses with a random distribution of and correlations between amplitudes and velocities is analyzed. The pulses are assumed to move radially with fixed shape and amplitudes decreasing exponentially in time due to linear damping. The pulse velocities are taken to be time-dependent with a power law dependence on the instantaneous amplitudes, as suggested by blob velocity scaling theories. In accordance with experimental measurements, the pulse function is assumed to be exponential and the amplitudes are taken to be exponentially distributed. As a consequence of linear damping and time-dependent velocities, it is demonstrated that the pulses stagnate during their radial motion. This makes the average pulse waiting time increase radially outwards in the scrape-off layer of magnetically confined plasmas. In the case that pulse velocities are proportional to their amplitudes, the mean value of the process decreases exponentially with radial coordinate, similar to the case when all pulses have the same, time-independent velocity. The profile e-folding length is then given by the product of the average pulse velocity and the parallel transit time. Moreover, both the average pulse amplitude and the average velocity are the same at all radial positions due to stagnation of slow and small-amplitude pulses. In general, an increasing average pulse velocity results in a flattened radial profile of the mean value of the process as well as a higher relative fluctuation level, strongly enhancing plasma-surface interactions.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04966
Krylov Complexity in Mixed Phase Space,['High Energy Physics - Theory'],"['Kyoung-Bum Huh', 'Hyun-Sik Jeong', 'Leopoldo A. Pando Zayas', 'Juan F. Pedraza']","We investigate the Krylov complexity of thermofield double states in systems with mixed phase space, uncovering a direct correlation with the Brody distribution, which interpolates between Poisson and Wigner statistics. Our analysis spans two-dimensional random matrix models featuring (I) GOE-Poisson and (II) GUE-Poisson transitions and extends to higher-dimensional cases, including a stringy matrix model (GOE-Poisson) and the mass-deformed SYK model (GUE-Poisson). Krylov complexity consistently emerges as a reliable marker of quantum chaos, displaying a characteristic peak in the chaotic regime that gradually diminishes as the Brody parameter approaches zero, signaling a shift toward integrability. These results establish Krylov complexity as a powerful diagnostic of quantum chaos and highlight its interplay with eigenvalue statistics in mixed phase systems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04963
Divisible design graphs from symplectic graphs over rings with precisely three ideals,['Combinatorics'],"['Anwita Bhowmik', 'Sergey Goryainov']",In this paper we construct two new infinite families of divisible design graphs based on symplectic graphs over rings with precisely three ideals.△ Less,"6 December, 2024;",https://arxiv.org/pdf/2412.04962
"Punctured surfaces, quiver mutations, and quotients of Coxeter groups",['Combinatorics'],"['Anna Felikson', 'Michael Shapiro', 'Pavel Tumarkin']","In 2011, Barot and Marsh provided an explicit construction of presentation of a finite Weyl group $W$ by any quiver mutation-equivalent to an orientation of a Dynkin diagram with Weyl group $W$. The construction was extended by the authors of the present paper to obtain presentations for all affine Coxeter groups, as well as to construct groups from triangulations of unpunctured surfaces and orbifolds, where the groups are invariant under change of triangulation and thus are presented as quotients of numerous distinct Coxeter groups. We extend the construction to include most punctured surfaces and orbifolds, providing a new invariant for almost all marked surfaces.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04960
Five-years Altitude Statistics of Noctilucent Clouds Based on Multi-Site Wide-Field Camera Survey,['Atmospheric and Oceanic Physics'],"['Oleg S. Ugolnikov', 'Nikolay N. Pertsev', 'Vladimir I. Perminov', 'Ilya S. Yankovsky', 'Dmitry N. Aleshin', 'Ekaterina N. Tipikina', 'Alexander A. Ilyukhin', 'Egor O. Ugolnikov', 'Stanislav A. Korotkiy', 'Olga Yu. Golubeva', 'Andrey M. Tatarnikov', 'Sergey G. Zheltoukhov', 'Alexey V. Popov', 'Alexey M. Sushkov', 'Egor A. Volkov', 'Natalya S. Krapkina', 'Damir I. Yalyshev']",The results of simultaneous measurements of noctilucent clouds (NLC) position in a number of ground-based locations are presented. Observational data of 14 bright NLC events over 5 years is used for building the altitude maps of cloud fields using triangulation technique updated for multi-location case. Statistical distribution of NLC altitude and its change during the summer season is considered. Mean NLC altitudes are compared with colorimetric technique based on the same data and simple radiation transfer model. This can be used to check the model and estimate the accuracy of single-camera technique of NLC altitude measurements. Results and methods are suggested for net ground-based survey of noctilucent clouds.△ Less,"6 December, 2024;",https://arxiv.org/pdf/2412.04951
Bed-Attached Vibration Sensor System: A Machine Learning Approach for Fall Detection in Nursing Homes,['Machine Learning'],"['Thomas Bartz-Beielstein', 'Axel Wellendorf', 'Noah Pütz', 'Jens Brandt', 'Alexander Hinterleitner', 'Richard Schulz', 'Richard Scholz', 'Olaf Mersmann', 'Robin Knabe']","The increasing shortage of nursing staff and the acute risk of falls in nursing homes pose significant challenges for the healthcare system. This study presents the development of an automated fall detection system integrated into care beds, aimed at enhancing patient safety without compromising privacy through wearables or video monitoring. Mechanical vibrations transmitted through the bed frame are processed using a short-time Fourier transform, enabling robust classification of distinct human fall patterns with a convolutional neural network. Challenges pertaining to the quantity and diversity of the data are addressed, proposing the generation of additional data with a specific emphasis on enhancing variation. While the model shows promising results in distinguishing fall events from noise using lab data, further testing in real-world environments is recommended for validation and improvement. Despite limited available data, the proposed system shows the potential for an accurate and rapid response to falls, mitigating health implications, and addressing the needs of an aging population. This case study was performed as part of the ZIM Project. Further research on sensors enhanced by artificial intelligence will be continued in the ShapeFuture Project.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04950
Preliminary Study on Virtual Reality Framework for Effective Prospective Memory Training: Integration of Visual Imagery and Daily-life Simulations,['Human-Computer Interaction'],"['Satoshi Fukumori', 'Kayoko Miura', 'Ayako Takamori', 'Sadao Otsuka']","Prospective memory (PM), defining the currently conceived intention of a future action, is crucial for daily functioning, particularly in aging populations. This study develops and validates a virtual reality prospective memory training (VR-PMT) system that integrates visual imagery training (VIT) and virtual reality training (VRT) to enhance the PM abilities of users. The framework is designed to progressively challenge users by simulating real-life PM tasks in a controlled VR environment. The VIT component is designed to improve the generation and utilization of visual imagery by users, while the VRT component provides PM tasks based on time and event cues within a virtual environment.The framework was evaluated on ten healthy adults (university students and elderly participants) over nine weeks. During the initial session, the baseline PM abilities of the participants were assessed using the memory for intentions screening test (MIST). The subsequent sessions alternated between VIT and VRT with increasing task complexity. The MIST scores were significantly positively correlated with task achievement, confirming the efficacy of the system. Imagery abilities were also strongly correlated with task performance, underscoring the importance of visual imagery in PM training.Usability and user experiences, evaluated on the Jikaku-sho Shirabe questionnaire and the user experience questionnaire, indicated an overall positive user experience but higher fatigue levels in elderly participants. This study demonstrates that the VR--PMT system effectively trains and assesses PM abilities by integrating VIT and VRT, supporting its potential for broader applications in clinical settings.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04949
A Federated Approach to Few-Shot Hate Speech Detection for Marginalized Communities,['Computation and Language'],"['Haotian Ye', 'Axel Wisiorek', 'Antonis Maronikolakis', 'Özge Alaçam', 'Hinrich Schütze']","Hate speech online remains an understudied issue for marginalized communities, and has seen rising relevance, especially in the Global South, which includes developing societies with increasing internet penetration. In this paper, we aim to provide marginalized communities living in societies where the dominant language is low-resource with a privacy-preserving tool to protect themselves from hate speech on the internet by filtering offensive content in their native languages. Our contribution in this paper is twofold: 1) we release REACT (REsponsive hate speech datasets Across ConTexts), a collection of high-quality, culture-specific hate speech detection datasets comprising seven distinct target groups in eight low-resource languages, curated by experienced data collectors; 2) we propose a solution to few-shot hate speech detection utilizing federated learning (FL), a privacy-preserving and collaborative learning approach, to continuously improve a central model that exhibits robustness when tackling different target groups and languages. By keeping the training local to the users' devices, we ensure the privacy of the users' data while benefitting from the efficiency of federated learning. Furthermore, we personalize client models to target-specific training data and evaluate their performance. Our results indicate the effectiveness of FL across different target groups, whereas the benefits of personalization on few-shot learning are not clear.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04942
Phonon-assisted control of magnonic and electronic band splitting,['Materials Science'],"['Subhadeep Bandyopadhyay', 'Anoop Raj', 'Philippe Ghosez', 'Sumiran Pujari', 'Sayantika Bhowal']","We demonstrate theoretically the ability to control non-relativistic magnonic and electronic spin splitting by manipulating phonon modes. Using MnF$_2$ as a representative material, exhibiting non-relativistic spin splitting in its electronic bands, we identify an equivalent $d$-wave splitting in magnon modes of specific handedness. Our study reveals a direct correlation between magnonic and electronic splittings, showing that the energy splitting in both magnon and electronic bands can be tuned by jointly modulating the A$_{2u}$ and A$_{1g}$ phonon modes with frequencies of 8.52 and 9.74 THz, respectively. These findings highlight the intricate interplay between charge, spin, and lattice degrees of freedom in spin-split antiferromagnets, offering new pathways for phonon-driven control in magnonic applications.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04934
Factorisability of the Harer-Zagier Transform of the HOMFLY-PT polynomial,['Mathematical Physics'],"['Andreani Petrou', 'Shinobu Hikami']","The Harer-Zagier (HZ) transform maps the HOMFLY-PT polynomial into a rational function. For some special knots and links, the latter has a simple factorised form, both in the numerator and denominator. This property seems to be preserved under full twists and concatenation with the Jucys--Murphy's braid, which are hence used to generate infinite families with HZ factorisability. For such families, the HOMFLY-PT polynomial can be fully encoded in two sets of integers, corresponding to the numerator and denominator exponents. These exponents turn out to be related to the Khovanov homology and its Euler characteristics. A criterion for when factorisability occurs is found via a conjectural relation between the HOMFLY-PT and Kauffman polynomials, which is proven in several special cases. The latter is equivalent to the vanishing of the two-crosscap BPS invariant of topological strings.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04933
Video Decomposition Prior: A Methodology to Decompose Videos into Layers,['Computer Vision and Pattern Recognition'],"['Gaurav Shrivastava', 'Ser-Nam Lim', 'Abhinav Shrivastava']","In the evolving landscape of video enhancement and editing methodologies, a majority of deep learning techniques often rely on extensive datasets of observed input and ground truth sequence pairs for optimal performance. Such reliance often falters when acquiring data becomes challenging, especially in tasks like video dehazing and relighting, where replicating identical motions and camera angles in both corrupted and ground truth sequences is complicated. Moreover, these conventional methodologies perform best when the test distribution closely mirrors the training distribution. Recognizing these challenges, this paper introduces a novel video decomposition prior `\texttt{VDP}' framework which derives inspiration from professional video editing practices. Our methodology does not mandate task-specific external data corpus collection, instead pivots to utilizing the motion and appearance of the input video. \texttt{VDP} framework decomposes a video sequence into a set of multiple RGB layers and associated opacity levels. These set of layers are then manipulated individually to obtain the desired results. We addresses tasks such as video object segmentation, dehazing, and relighting. Moreover, we introduce a novel logarithmic video decomposition formulation for video relighting tasks, setting a new benchmark over the existing methodologies. We observe the property of relighting emerge as we optimize for our novel relighting decomposition formulation. We evaluate our approach on standard video datasets like DAVIS, REVIDE, \& SDSD and show qualitative results on a diverse array of internet videos. Project Page - https://www.cs.umd.edu/~gauravsh/video_decomposition/index.html for video results.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04930
Continuous Video Process: Modeling Videos as Continuous Multi-Dimensional Processes for Video Prediction,['Computer Vision and Pattern Recognition'],"['Gaurav Shrivastava', 'Abhinav Shrivastava']","Diffusion models have made significant strides in image generation, mastering tasks such as unconditional image synthesis, text-image translation, and image-to-image conversions. However, their capability falls short in the realm of video prediction, mainly because they treat videos as a collection of independent images, relying on external constraints such as temporal attention mechanisms to enforce temporal coherence. In our paper, we introduce a novel model class, that treats video as a continuous multi-dimensional process rather than a series of discrete frames. We also report a reduction of 75\% sampling steps required to sample a new frame thus making our framework more efficient during the inference time. Through extensive experimentation, we establish state-of-the-art performance in video prediction, validated on benchmark datasets including KTH, BAIR, Human3.6M, and UCF101. Navigate to the project page https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html for video results.}△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04929
Spin and Orbital Rashba effects at the Ni/HfO$_2$ interface,['Materials Science'],"['Armando Pezo', 'Andrés Saul', 'Aurélien Manchon', 'Rémi Arras']","We predict the giant ferroelectric control of interfacial properties of Ni/HfO2, namely, (i) the magnetocrystalline anisotropy and (ii) the inverse spin and orbital Rashba effects. The reversible control of magnetic properties using electric gating is a promising route to low-energy consumption magnetic devices, including memories and logic gates. Synthetic multiferroics, composed of a ferroelectric in proximity to a magnet, stand out as a promising platform for such devices. Using a combination of $ab$ $initio$ simulations and transport calculations, we demonstrate that reversing the electric polarization modulates the interface magnetocrystalline anisotropy from in-plane to out-of-plane. This modulation compares favorably with recent reports obtained upon electromigration induced by ionic gating. In addition, we find that the current-driven spin and orbital densities at the interface can be modulated by about 50% and 30%, respectively. This giant modulation of the spin-charge and orbit-charge conversion efficiencies opens appealing avenues for voltage-controlled spin- and orbitronics devices.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04927
Multifractality and polygonal vortex filaments,['Analysis of PDEs'],"['Valeria Banica', 'Daniel Eceizabarrena', 'Andrea. R. Nahmod', 'Luis Vega']","In this proceedings article we survey the results in [5] and their motivation, as presented at the 50th Journées EDP 2024. With the aim of quantifying turbulent behaviors of vortex filaments, we study the multifractality of a family of generalized Riemann's non-differentiable functions. These functions represent, in a certain limit, the trajectory of regular polygonal vortex filaments that evolve according to the binormal flow, the classical model for vortex filaments dynamics. We explain how we determined their spectrum of singularities through a careful design of Diophantine sets, which we study by using the Duffin-Schaeffer theorem and the Mass Transference Principle.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04926
"Follow the money: a startup-based measure of AI exposure across occupations, industries and regions",['General Economics'],"['Enrico Maria Fenoaltea', 'Dario Mazzilli', 'Aurelio Patelli', 'Angelica Sbardella', 'Andrea Tacchella', 'Andrea Zaccaria', 'Marco Trombetti', 'Luciano Pietronero']","The integration of artificial intelligence (AI) into the workplace is advancing rapidly, necessitating robust metrics to evaluate its tangible impact on the labour market. Existing measures of AI occupational exposure largely focus on AI's theoretical potential to substitute or complement human labour on the basis of technical feasibility, providing limited insight into actual adoption and offering inadequate guidance for policymakers. To address this gap, we introduce the AI Startup Exposure (AISE) index-a novel metric based on occupational descriptions from O*NET and AI applications developed by startups funded by the Y Combinator accelerator. Our findings indicate that while high-skilled professions are theoretically highly exposed according to conventional metrics, they are heterogeneously targeted by startups. Roles involving routine organizational tasks-such as data analysis and office management-display significant exposure, while occupations involving tasks that are less amenable to AI automation due to ethical or high-stakes, more than feasibility, considerations -- such as judges or surgeons -- present lower AISE scores. By focusing on venture-backed AI applications, our approach offers a nuanced perspective on how AI is reshaping the labour market. It challenges the conventional assumption that high-skilled jobs uniformly face high AI risks, highlighting instead the role of today's AI players' societal desirability-driven and market-oriented choices as critical determinants of AI exposure. Contrary to fears of widespread job displacement, our findings suggest that AI adoption will be gradual and shaped by social factors as much as by the technical feasibility of AI applications. This framework provides a dynamic, forward-looking tool for policymakers and stakeholders to monitor AI's evolving impact and navigate the changing labour landscape.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04924
HyperGraphOS: A Meta Operating System for Science and Engineering,['Artificial Intelligence'],"['Antonello Ceravola', 'Frank Joublin', 'Ahmed R. Sadik', 'Bram Bolder', 'Juha-Pekka Tolvanen']","This paper presents HyperGraphOS, an innovative Operating System designed for the scientific and engineering domains. It combines model based engineering, graph modeling, data containers, and computational tools, offering users a dynamic workspace for creating and managing complex models represented as customizable graphs. Using a web based architecture, HyperGraphOS requires only a modern browser to organize knowledge, documents, and content into interconnected models. Domain Specific Languages drive workspace navigation, code generation, AI integration, and process organization.The platform models function as both visual drawings and data structures, enabling dynamic modifications and inspection, both interactively and programmatically. HyperGraphOS was evaluated across various domains, including virtual avatars, robotic task planning using Large Language Models, and meta modeling for feature based code development. Results show significant improvements in flexibility, data management, computation, and document handling.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04923
Uniform characterisation of an ensemble of main-sequence benchmark stars: effect of Gaia-based data on grid search models,['Solar and Stellar Astrophysics'],"['Benard Nsamba', 'Achim Weiss', 'Juma Kamulali']","The inference of stellar parameters (such as radius and mass) through asteroseismic forward modelling depends on the number, accuracy, and precision of seismic and atmospheric constraints. ESA's Gaia space mission is providing precise parallaxes which yield an additional constraint to be included in the model grid search. Using a handful of main-sequence benchmark stars, we perform a uniform characterisation of these stars. We assess the accuracy and precision of stellar parameters inferred from grid-based searches when a Gaia-based luminosity is combined with different stellar constraints. We also examine the precision needed for an interferometric radius (model-independent radius) to have a significant contribution towards the determination of stellar mass in the optimisation process. Our findings show that more precise stellar masses are inferred for some stars when seismic and spectroscopic constraints are complemented with a Gaia-based luminosity, with a scatter varying from 1.9 per cent to 0.8 per cent. However, the inferred stellar radii are underestimated when compared to the interferometric radii and yield a scatter of $\sim$1.9 per cent. In addition, we demonstrate that a precisely measured interferometric radius ($\lesssim$ 1 per cent) when applied in the optimisation process yields a mass with a precision $\lesssim$ 1.5 per cent. Finally, we find that when only $l=0$ mode oscillation frequencies are available, robust masses and radii are still attainable. However, this requires precise and numerous $l=0$ mode oscillations frequencies ($>$ 8) to be coupled with atmospheric constraints.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04921
Predicting the Curie temperature in substitutionally disordered alloys using a first-principles based model,['Materials Science'],"['Marian Arale Brännvall', 'Rickard Armiento', 'Björn Alling']","When exploring new magnetic materials, the effect of alloying plays a crucial role for numerous properties. By altering the alloy composition, it is possible to tailor, e.g., the Curie temperature ($T_\text{C}$). In this work, $T_\text{C}$ of various alloys is investigated using a previously developed technique [Brännvall et al. Phys. Rev. Mat. (2024)] designed for robust predictions of $T_\text{C}$ across diverse chemistries and structures. The technique is based on density functional theory calculations and utilizes the energy difference between the magnetic ground state and the magnetically disordered paramagnetic state. It also accounts for the magnetic entropy in the paramagnetic state and the number of nearest magnetic neighbors. The experimentally known systems, Fe$_{1-x}$Co$_x$, Fe$_{1-x}$Cr$_x$, Fe$_{1-x}$V$_x$, NiMnSb-based Heusler alloys, Ti$_{1-x}$Cr$_x$N, and Co$_{1-x}$Al$_x$ are investigated. The experimentally unexplored system Fe$_{1-x}$Tc$_x$ is also tested to demonstrate the usefulness of the developed method in guiding future experimental efforts. This work demonstrates the broad applicability of the developed method across various systems, requiring less hands-on adjustments compared to other theoretical approaches.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04920
Hard Math -- Easy UVM: Pragmatic solutions for verifying hardware algorithms using UVM,['Artificial Intelligence'],"['Mark Litterick', 'Aleksandar Ivankovic', 'Bojan Arsov', 'Aman Kumar']","This paper presents pragmatic solutions for verifying complex mathematical algorithms implemented in hardware in an efficient and effective manner. Maximizing leverage of a known-answer-test strategy, based on predefined data scenarios combined with design-for-verification modes, we demonstrate how to find and isolate concept and design bugs early in the flow. The solutions presented are based on real project experience with single chip radar sensors for a variety of applications. The verification environments supporting the presented strategies are based on SystemVerilog and the Universal Verification Methodology.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04919
On one dimensional weighted Poincare inequalities for Global Sensitivity Analysis,['Probability'],"['David Heredia', 'Aldéric Joulin', 'Olivier Roustant']","One-dimensional Poincare inequalities are used in Global Sensitivity Analysis (GSA) to provide derivative-based upper bounds and approximations of Sobol indices. We add new perspectives by investigating weighted Poincare inequalities. Our contributions are twofold. In a first part, we provide new theoretical results for weighted Poincare inequalities, guided by GSA needs. We revisit the construction of weights from monotonic functions, providing a new proof from a spectral point of view. In this approach, given a monotonic function g, the weight is built such that g is the first non-trivial eigenfunction of a convenient diffusion operator. This allows us to reconsider the linear standard, i.e. the weight associated to a linear g. In particular, we construct weights that guarantee the existence of an orthonormal basis of eigenfunctions, leading to approximation of Sobol indices with Parseval formulas. In a second part, we develop specific methods for GSA. We study the equality case of the upper bound of a total Sobol index, and link the sharpness of the inequality to the proximity of the main effect to the eigenfunction. This leads us to theoretically investigate the construction of data-driven weights from estimators of the main effects when they are monotonic, another extension of the linear standard. Finally, we illustrate the benefits of using weights on a GSA study of two toy models and a real flooding application, involving the Poincare constant and/or the whole eigenbasis.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04918
Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection,['Computer Vision and Pattern Recognition'],"['Khurram Azeem Hashmi', 'Talha Uddin Sheikh', 'Didier Stricker', 'Muhammad Zeshan Afzal']","The primary challenge in Video Object Detection (VOD) is effectively exploiting temporal information to enhance object representations. Traditional strategies, such as aggregating region proposals, often suffer from feature variance due to the inclusion of background information. We introduce a novel instance mask-based feature aggregation approach, significantly refining this process and deepening the understanding of object dynamics across video frames. We present FAIM, a new VOD method that enhances temporal Feature Aggregation by leveraging Instance Mask features. In particular, we propose the lightweight Instance Feature Extraction Module (IFEM) to learn instance mask features and the Temporal Instance Classification Aggregation Module (TICAM) to aggregate instance mask and classification features across video frames. Using YOLOX as a base detector, FAIM achieves 87.9% mAP on the ImageNet VID dataset at 33 FPS on a single 2080Ti GPU, setting a new benchmark for the speed-accuracy trade-off. Additional experiments on multiple datasets validate that our approach is robust, method-agnostic, and effective in multi-object tracking, demonstrating its broader applicability to video understanding tasks.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04915
Dynamics of Aggregation Processes and Electrophysical Properties of Transformer Oil-Based Magnetic Fluids,['Soft Condensed Matter'],"['Alexander D. Kurilov', 'Anastasia V. Gubareva', 'Sergei A. Zubkov', 'Yulia A. Alekhina', 'Alexander V. Simakin', 'Denis N. Chausov']","Magnetic fluids exhibit tunable structures and electrophysical properties, making them promising for adaptive optical systems, biomedical sensors, and microelectromechanical devices. However, the dynamic evolution of their microstructure under varying magnetic fields remains insufficiently explored.
  This study investigates the structural and dielectric properties of transformer oil-based magnetic fluids containing 0.2-10 vol% magnetite nanoparticles, across a frequency range of 20 Hz to 10 MHz. Particular attention is given to the dynamics of aggregate reorientation in response to alternating magnetic fields. Experimental results demonstrate that low nanoparticle concentrations lead to a linear increase in dielectric permittivity and conductivity, consistent with the Maxwell-Wagner model. In contrast, higher concentrations exhibit conductivity saturation and dispersion effects due to the formation of elongated aggregates.
  An analysis based on the Boyle polarization model describes the relaxation and structural changes associated with aggregation dynamics. Changes in the magnetic field orientation induce aggregate reconfiguration and significant structural transformations. At early stages, elongated chains form, subsequently thickening until an equilibrium state is reached. Elevated temperatures accelerate these processes by reducing medium viscosity and aggregate order.
  The findings highlight the critical role of reorientation dynamics in designing high-speed magnetic sensors, vibration isolation systems, and adaptive devices operating in dynamic magnetic environments.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04911
The PRISMA-36 array for studying variations of the thermal neutron flux,['Instrumentation and Methods for Astrophysics'],"['M. B. Amelchakov', 'A. Chiavassa', 'D. M. Gromushkin', 'S. S. Khokhlov', 'E. P. Khomchuk', 'V. V. Kindin', 'A. Yu. Konovalova', 'P. S. Kuzmenkova', 'E. S. Morgunov', 'N. A. Pasyuk', 'A. A. Petrukhin', 'I. A. Shulzhenko', 'E. P. Volkov', 'I. I. Yashin']","From 2012 to 2023, the PRISMA-32 array was in operation at the Experimental Complex NEVOD (MEPhI, Moscow). The purpose of the array was to study extensive air showers by detecting the air-shower neutron and electron-photon components using unshielded neutron detectors. To expand the capabilities of this facility, including for the study of cosmic and geophysical phenomena with a neutron flux, its upgrade was carried out. During the upgrade, a dedicated measuring channel for studying variations of the neutron background and the processes affecting these variations was created. To achieve this, the photomultipliers, the integrating amplifiers, the digitalizing electronics and the high-voltage power supply system were replaced. The paper describes the structure of the upgraded array, which was named PRISMA-36, and presents the results of studying the characteristics of the main elements of its ""variation"" channel. A method for identifying signals caused by neutron capture and the determined criteria for their selection are discussed. An example of a Forbush decrease, caused by a X1.1-class flare and recorded with the variation channel of the PRISMA-36 array, is given.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04909
"Manifolds of positive reach, differentiability, tangent variation, and attaining the reach",['Computational Geometry'],"['André Lieutier', 'Mathijs Wintraecken']","This paper contains three main results. Firstly, we give an elementary proof of the following statement: Let $M$ be a (closed, in both the geometrical and topological sense of the word) topological manifold embedded in $\mathbb{R}^d$. If $M$ has positive reach, then M can locally be written as the graph of a $C^{1,1}$ from the tangent space to the normal space. Conversely if $M$ can locally written as the graph of a $C^{1,1}$ function from the tangent space to the normal space, then $M$ has positive reach. The result was hinted at by Federer when he introduced the reach, and proved by Lytchak. Lytchak's proof relies heavily CAT(k)-theory. The proof presented here uses only basic results on homology. Secondly, we give optimal Lipschitz-constants for the derivative, in other words we give an optimal bound for the angle between tangent spaces in term of the distance between the points. This improves earlier results, that were either suboptimal or assumed that the manifold was $C^2$. Thirdly, we generalize a result by Aamari et al. which explains the how the reach is attained from the smooth setting to general sets of positive reach.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04906
On Process Awareness in Detecting Multi-stage Cyberattacks in Smart Grids,['Cryptography and Security'],"['Omer Sen', 'Yanico Aust', 'Simon Glomb', 'Andreas Ulbig']","This study delves into the role of process awareness in enhancing intrusion detection within Smart Grids, considering the increasing fusion of ICT in power systems and the associated emerging threats. The research harnesses a co-simulation environment, encapsulating IT, OT, and ET layers, to model multi-stage cyberattacks and evaluate machine learning-based IDS strategies. The key observation is that process-aware IDS demonstrate superior detection capabilities, especially in scenarios closely tied to operational processes, as opposed to IT-only IDS. This improvement is notable in distinguishing complex cyber threats from regular IT activities. The findings underscore the significance of further developing sophisticated IDS benchmarks and digital twin datasets in Smart Grid environments, paving the way for more resilient cybersecurity infrastructures.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04902
Encryption-Aware Anomaly Detection in Power Grid Communication Networks,['Cryptography and Security'],"['Omer Sen', 'Mehdi Akbari Gurabi', 'Milan Deruelle', 'Andreas Ulbig', 'Stefan Decker']","The shift to smart grids has made electrical power systems more vulnerable to sophisticated cyber threats. To protect these systems, holistic security measures that encompass preventive, detective, and reactive components are required, even with encrypted data. However, traditional intrusion detection methods struggle with encrypted traffic, our research focuses on the low-level communication layers of encrypted power grid systems to identify irregular patterns using statistics and machine learning. Our results indicate that a harmonic security concept based on encrypted traffic and anomaly detection is promising for smart grid security; however, further research is necessary to improve detection accuracy.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04901
A cyber-physical digital twin approach to replicating realistic multi-stage cyberattacks on smart grids,['Cryptography and Security'],"['Omer Sen', 'Nathalie Bleser', 'Martin Henze', 'Andreas Ulbig']","The integration of information and communication technology in distribution grids presents opportunities for active grid operation management, but also increases the need for security against power outages and cyberattacks. This paper examines the impact of cyberattacks on smart grids by replicating the power grid in a secure laboratory environment as a cyber-physical digital twin. A simulation is used to study communication infrastructures for secure operation of smart grids. The cyber-physical digital twin approach combines communication network emulation and power grid simulation in a common modular environment, and is demonstrated through laboratory tests and attack replications.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04900
A free lunch: manifolds of positive reach can be smoothed without decreasing the reach,['Computational Geometry'],"['Hana Dal Poz Kouřimská', 'André Lieutier', 'Mathijs Wintraecken']","Assumptions on the reach are crucial for ensuring the correctness of many geometric and topological algorithms, including triangulation, manifold reconstruction and learning, homotopy reconstruction, and methods for estimating curvature or reach. However, these assumptions are often coupled with the requirement that the manifold be smooth, typically at least C^2 .In this paper, we prove that any manifold with positive reach can be approximated arbitrarily well by a C^$\infty$ manifold without significantly reducing the reach, by employing techniques from differential topology -partitions of unity and smoothing using convolution kernels. This result implies that nearly all theorems established for C^2 manifolds with a certain reach naturally extend to manifolds with the same reach, even if they are not C^2 , for free!△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04899
Mitigating Instance-Dependent Label Noise: Integrating Self-Supervised Pretraining with Pseudo-Label Refinement,['Computer Vision and Pattern Recognition'],"['Gouranga Bala', 'Anuj Gupta', 'Subrat Kumar Behera', 'Amit Sethi']","Deep learning models rely heavily on large volumes of labeled data to achieve high performance. However, real-world datasets often contain noisy labels due to human error, ambiguity, or resource constraints during the annotation process. Instance-dependent label noise (IDN), where the probability of a label being corrupted depends on the input features, poses a significant challenge because it is more prevalent and harder to address than instance-independent noise. In this paper, we propose a novel hybrid framework that combines self-supervised learning using SimCLR with iterative pseudo-label refinement to mitigate the effects of IDN. The self-supervised pre-training phase enables the model to learn robust feature representations without relying on potentially noisy labels, establishing a noise-agnostic foundation. Subsequently, we employ an iterative training process with pseudo-label refinement, where confidently predicted samples are identified through a multistage approach and their labels are updated to improve label quality progressively. We evaluate our method on the CIFAR-10 and CIFAR-100 datasets augmented with synthetic instance-dependent noise at varying noise levels. Experimental results demonstrate that our approach significantly outperforms several state-of-the-art methods, particularly under high noise conditions, achieving notable improvements in classification accuracy and robustness. Our findings suggest that integrating self-supervised learning with iterative pseudo-label refinement offers an effective strategy for training deep neural networks on noisy datasets afflicted by instance-dependent label noise.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04898
Polymyxin B-Enriched Exogenous Lung Surfactant: Thermodynamics and Structure,['Biological Physics'],"['Nina Královič-Kanjaková', 'Ali Asi Shirazi', 'Lukáš Hubčík', 'Mária Klacsová', 'Atoosa Keshavarzi', 'Juan Carlos Martínez', 'Sophie Combet', 'José Teixeira', 'Daniela Uhríková']","The use of exogenous pulmonary surfactant (EPS) to deliver other relevant drugs to the lung is a promising strategy for combined therapy. We evaluated the interaction of polymyxin B (PxB) with clinically used EPS, the poractant alfa Curosurf (PSUR). The effect of PxB on the protein-free model system (MS) composed of four phospholipids (diC16:0PC/16:0-18:1PC/16:0-18:2PC/16:0-18:1PG) was examined in parallel to distinguish the specificity of the composition of PSUR. We used several experimental techniques (differential scanning calorimetry, small-and wide-angle X-ray scattering, small angle neutron scattering, fluorescence spectroscopy, and electrophoretic light scattering) to characterize the binding of PxB to both EPS. Electrostatic interactions PxB -EPS are dominant. The results obtained support the concept of cationic PxB molecules lying on the surface of the PSUR bilayer, strengthening the multilamellar structure of the PSUR as derived from SAXS and SANS. A protein-free MS mimics natural EPS well but was found to be less resistant to penetration of PxB into the lipid bilayer. PxB does not affect the gel-to-fluid phase transition temperature Tm of PSUR, while Tm increased by ~ +2 $^\circ$C in MS. The decrease of the thickness of the lipid bilayer (dL) of PSUR upon PxB binding is negligible. The hydrophobic tail of the PxB molecule does not penetrate the bilayer as derived from SANS data analysis and changes in lateral pressure monitored by excimer fluorescence at two depths of the hydrophobic region of the bilayer. Changes in dL of protein-free MS show a biphasic dependence on the adsorbed amount of PxB with a minimum close to the point of electroneutrality of the mixture. Our results do not discourage the concept of a combined treatment with PxBenriched Curosurf. However, the amount of PxB must be carefully assessed (less than 5 wt% relative to the mass of the surfactant) to avoid inversion of the surface charge of the membrane.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04894
Automatic Tongue Delineation from MRI Images with a Convolutional Neural Network Approach,['Artificial Intelligence'],"['Karyna Isaieva', 'Yves Laprie', 'Nicolas Turpault', 'Alexis Houssard', 'Jacques Felblinger', 'Pierre-André Vuissoz']","Tongue contour extraction from real-time magnetic resonance images is a nontrivial task due to the presence of artifacts manifesting in form of blurring or ghostly contours. In this work, we present results of automatic tongue delineation achieved by means of U-Net auto-encoder convolutional neural network. We present both intra- and inter-subject validation. We used real-time magnetic resonance images and manually annotated 1-pixel wide contours as inputs. Predicted probability maps were post-processed in order to obtain 1-pixel wide tongue contours. The results are very good and slightly outperform published results on automatic tongue segmentation.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04893
VTD: Visual and Tactile Database for Driver State and Behavior Perception,['Robotics'],"['Jie Wang', 'Mobing Cai', 'Zhongpan Zhu', 'Hongjun Ding', 'Jiwei Yi', 'Aimin Du']","In the domain of autonomous vehicles, the human-vehicle co-pilot system has garnered significant research attention. To address the subjective uncertainties in driver state and interaction behaviors, which are pivotal to the safety of Human-in-the-loop co-driving systems, we introduce a novel visual-tactile perception method. Utilizing a driving simulation platform, a comprehensive dataset has been developed that encompasses multi-modal data under fatigue and distraction conditions. The experimental setup integrates driving simulation with signal acquisition, yielding 600 minutes of fatigue detection data from 15 subjects and 102 takeover experiments with 17 drivers. The dataset, synchronized across modalities, serves as a robust resource for advancing cross-modal driver behavior perception algorithms.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04888
AI-Driven Non-Invasive Detection and Staging of Steatosis in Fatty Liver Disease Using a Novel Cascade Model and Information Fusion Techniques,['Machine Learning'],"['Niloufar Delfan', 'Pardis Ketabi Moghadam', 'Mohammad Khoshnevisan', 'Mehdi Hosseini Chagahi', 'Behzad Hatami', 'Melika Asgharzadeh', 'Mohammadreza Zali', 'Behzad Moshiri', 'Amin Momeni Moghaddam', 'Mohammad Amin Khalafi', 'Khosrow Dehnad']","Non-alcoholic fatty liver disease (NAFLD) is one of the most widespread liver disorders on a global scale, posing a significant threat of progressing to more severe conditions like nonalcoholic steatohepatitis (NASH), liver fibrosis, cirrhosis, and hepatocellular carcinoma. Diagnosing and staging NAFLD presents challenges due to its non-specific symptoms and the invasive nature of liver biopsies. Our research introduces a novel artificial intelligence cascade model employing ensemble learning and feature fusion techniques. We developed a non-invasive, robust, and reliable diagnostic artificial intelligence tool that utilizes anthropometric and laboratory parameters, facilitating early detection and intervention in NAFLD progression. Our novel artificial intelligence achieved an 86% accuracy rate for the NASH steatosis staging task (non-NASH, steatosis grade 1, steatosis grade 2, and steatosis grade 3) and an impressive 96% AUC-ROC for distinguishing between NASH (steatosis grade 1, grade 2, and grade3) and non-NASH cases, outperforming current state-of-the-art models. This notable improvement in diagnostic performance underscores the potential application of artificial intelligence in the early diagnosis and treatment of NAFLD, leading to better patient outcomes and a reduced healthcare burden associated with advanced liver disease.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04884
Nonmyopic Global Optimisation via Approximate Dynamic Programming,['Machine Learning'],"['Filippo Airaldi', 'Bart De Schutter', 'Azita Dabiri']","Unconstrained global optimisation aims to optimise expensive-to-evaluate black-box functions without gradient information. Bayesian optimisation, one of the most well-known techniques, typically employs Gaussian processes as surrogate models, leveraging their probabilistic nature to balance exploration and exploitation. However, Gaussian processes become computationally prohibitive in high-dimensional spaces. Recent alternatives, based on inverse distance weighting (IDW) and radial basis functions (RBFs), offer competitive, computationally lighter solutions. Despite their efficiency, both traditional global and Bayesian optimisation strategies suffer from the myopic nature of their acquisition functions, which focus solely on immediate improvement neglecting future implications of the sequential decision making process. Nonmyopic acquisition functions devised for the Bayesian setting have shown promise in improving long-term performance. Yet, their use in deterministic strategies with IDW and RBF remains unexplored. In this work, we introduce novel nonmyopic acquisition strategies tailored to IDW- and RBF-based global optimisation. Specifically, we develop dynamic programming-based paradigms, including rollout and multi-step scenario-based optimisation schemes, to enable lookahead acquisition. These methods optimise a sequence of query points over a horizon (instead of only at the next step) by predicting the evolution of the surrogate model, inherently managing the exploration-exploitation trade-off in a systematic way via optimisation techniques. The proposed approach represents a significant advance in extending nonmyopic acquisition principles, previously confined to Bayesian optimisation, to the deterministic framework. Empirical results on synthetic and hyperparameter tuning benchmark problems demonstrate that these nonmyopic methods outperform conventional myopic approaches.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04882
PhaseTracer2: from the effective potential to gravitational waves,['Cosmology and Nongalactic Astrophysics'],"['Peter Athron', 'Csaba Balazs', 'Andrew Fowlie', 'Lachlan Morris', 'William Searle', 'Yang Xiao', 'Yang Zhang']","In recent years, the prospect of detecting gravitational waves sourced from a strongly first-order cosmological phase transition has emerged as one of the most exciting frontiers of gravitational wave astronomy. Cosmological phase transitions are an essential ingredient in the Standard Model of particle cosmology, and help explain the mechanism for creation of matter in the early Universe, provide insights into fundamental theories of physics, and shed light on the nature of dark matter. This underscores the significance of developing robust end-to-end tools for determining the resulting gravitational waves from these phase transitions. In this article we present PhaseTracer2, an improved version of the C++ software package PhaseTracer, designed for mapping cosmological phases and transitions in Standard Model extensions of multiple scalar fields. Building on the robust framework of its predecessor, PhaseTracer2 extends its capabilities by including new features crucial for a more comprehensive analysis of cosmological phase transitions. It can calculate more complex properties, such as the bounce action through the path deformation method or an interface with BubbleProfiler, thermodynamic parameters, and gravitational wave spectra. Its applicability has also been broadened via incorporating the dimensionally reduced effective potential for models obtained from DRalgo, as well as calculations in the MSbar and OS-like renormalisation schemes. This modular, flexible, and practical upgrade retains the speed and stability of the original PhaseTracer, while significantly expanding its utility.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04881
MozzaVID: Mozzarella Volumetric Image Dataset,['Computer Vision and Pattern Recognition'],"['Pawel Tomasz Pieta', 'Peter Winkel Rasmussen', 'Anders Bjorholm Dahl', 'Jeppe Revall Frisvad', 'Siavash Arjomand Bigdeli', 'Carsten Gundlach', 'Anders Nymark Christensen']","Influenced by the complexity of volumetric imaging, there is a shortage of established datasets useful for benchmarking volumetric deep-learning models. As a consequence, new and existing models are not easily comparable, limiting the development of architectures optimized specifically for volumetric data. To counteract this trend, we introduce MozzaVID - a large, clean, and versatile volumetric classification dataset. Our dataset contains X-ray computed tomography (CT) images of mozzarella microstructure and enables the classification of 25 cheese types and 149 cheese samples. We provide data in three different resolutions, resulting in three dataset instances containing from 591 to 37,824 images. While being general-purpose, the dataset also facilitates investigating mozzarella structure properties. The structure of food directly affects its functional properties and thus its consumption experience. Understanding food structure helps tune the production and mimicking it enables sustainable alternatives to animal-derived food products. The complex and disordered nature of food structures brings a unique challenge, where a choice of appropriate imaging method, scale, and sample size is not trivial. With this dataset we aim to address these complexities, contributing to more robust structural analysis models. The dataset can be downloaded from: https://archive.compute.dtu.dk/files/public/projects/MozzaVID/.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04880
Automatic Tissue Differentiation in Parotidectomy using Hyperspectral Imaging,['Image and Video Processing'],"['Eric L. Wisotzky', 'Alexander Schill', 'Anna Hilsmann', 'Peter Eisert', 'Michael Knoke']","In head and neck surgery, continuous intraoperative tissue differentiation is of great importance to avoid injury to sensitive structures such as nerves and vessels. Hyperspectral imaging (HSI) with neural network analysis could support the surgeon in tissue differentiation. A 3D Convolutional Neural Network with hyperspectral data in the range of $400-1000$ nm is used in this work. The acquisition system consisted of two multispectral snapshot cameras creating a stereo-HSI-system. For the analysis, 27 images with annotations of glandular tissue, nerve, muscle, skin and vein in 18 patients undergoing parotidectomy are included. Three patients are removed for evaluation following the leave-one-subject-out principle. The remaining images are used for training, with the data randomly divided into a training group and a validation group. In the validation, an overall accuracy of $98.7\%$ is achieved, indicating robust training. In the evaluation on the excluded patients, an overall accuracy of $83.4\%$ has been achieved showing good detection and identification abilities. The results clearly show that it is possible to achieve robust intraoperative tissue differentiation using hyperspectral imaging. Especially the high sensitivity in parotid or nerve tissue is of clinical importance. It is interesting to note that vein was often confused with muscle. This requires further analysis and shows that a very good and comprehensive data basis is essential. This is a major challenge, especially in surgery.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04879
Dynamic Interference Prediction for In-X 6G Sub-networks,['Information Theory'],"['Pramesh Gautam', 'Ravi Sharan B A G', 'Paolo Baracca', 'Carsten Bockelmann', 'Thorsten Wild', 'Armin Dekorsy']","The sixth generation (6G) industrial Sub-networks (SNs) face several challenges in meeting extreme latency and reliability requirements in the order of 0.1-1 ms and 99.999 -to-99.99999 percentile, respectively. Interference management (IM) plays an integral role in addressing these requirements, especially in ultra-dense SN environments with rapidly varying interference induced by channel characteristics, mobility, and resource limitations. In general, IM can be achieved using resource allocation and \textit{accurate} Link adaptation (LA). In this work, we focus on the latter, where we first model interference at SN devices using the spatially consistent 3GPP channel model. Following this, we present a discrete-time dynamic state space model (DSSM) at a SN access point (AP), where interference power values (IPVs) are modeled as latent variables incorporating underlying modeling errors as well as transmission/protocol delays. Necessary approximations are then presented to simplify the DSSM and to efficiently employ the extended Kalman filter (EKF) for interference prediction. Unlike baseline methods, our proposed approach predicts IPVs solely based on the channel quality indicator (CQI) reports available at the SN AP at every transmission time interval (TTI). Numerical results demonstrate that our proposed approach clearly outperforms the conventional baseline. Furthermore, we also show that despite predicting with limited information, our proposed approach consistently achieves a comparable performance w.r.t the off-the-shelf supervised learning based baseline.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04876
On estimates of the Bures distance between bosonic Gaussian states,['Quantum Physics'],['A. S. Holevo'],The aim of the present note is to show that the method of our paper ArXiv:2408.11400 with minor extra efforts can be extended to obtain upper bounds for the Bures distance between quantum Gaussian states. We argue that these bounds are better adapted to the Bures distance and hence to the state estimation and learning with the Bures distance rather than that with the trace-norm distance.△ Less,"6 December, 2024;",https://arxiv.org/pdf/2412.04875
Self-Organizing Complex Networks with AI-Driven Adaptive Nodes for Optimized Connectivity and Energy Efficiency,['Computational Physics'],"['Azra Seyyedi', 'Mahdi Bohlouli', 'SeyedEhsan Nedaaee Oskoee']","High connectivity and robustness are critical requirements in distributed networks, as they ensure resilience, efficient communication, and adaptability in dynamic environments. Additionally, optimizing energy consumption is also paramount for ensuring sustainability of networks composed of energy-constrained devices and prolonging their operational lifespan. In this study, we introduce an Artificial Intelligence (AI)-enhanced self-organizing network model, where each adaptive node autonomously adjusts its transmission power to optimize network connectivity and redundancy while lowering energy consumption. Building on our previous Hamiltonian-based methodology, which is designed to lead networks toward globally optimized states of complete connectivity and minimal energy usage, this research integrates a Multi-Layer Perceptron (MLP)-based decision-making model at each node. By leveraging a dataset from the Hamiltonian approach, each node independently learns and adapts its transmission power in response to local conditions, resulting in emergent global behaviors marked by high connectivity and resilience against structural disruptions. This distributed, MLP-driven adaptability allows nodes to make context-aware power adjustments autonomously, enabling the network to maintain its optimized state over time. Simulation results show that the proposed AI-driven adaptive nodes collectively achieve stable complete connectivity, significant robustness, and optimized energy usage under various conditions, including static and mobile network scenarios. This work contributes to the growing field of self-organizing networks by illustrating the potential of AI to enhance complex network design, supporting the development of scalable, resilient, and energy-efficient distributed systems across diverse applications.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04874
$\PGL$ orbits on products of flag varieties,['Algebraic Geometry'],"['Izzet Coskun', 'Abuzer Gündüz']","In this paper, we study the existence of a dense orbit for the diagonal $\PGL(n)$ action on self-products of partial flag varieties. We determine when there exists a dense orbit for flag varieties of the form $F(k_1, \dots, k_r; n)^m$ when $k_i = i$ or when $m=3$ and $k_i = \ell + i$ for some $\ell \geq 0$. We also show that certain infinite families of products do not have a dense orbit.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04870
"Non-rotational measures in the Bargmann space, and entanglement of bipartite squeezed coherent states",['Quantum Physics'],"['K. Górska', 'A. Horzela', 'D. Kołaczek', 'B. J. Spisak', 'F. H. Szafraniec']",Entanglement of bipartite squeezed states generated by holomorphic Hermite functions of two complex variables is investigated using phase-space approach based on the Wigner distribution function. Orthogonality of the holomorphic Hermite functions implies the relationship between certain real parameter associated with the non-rotational measure in the Bargmann space and the squeezing parameter. The mutual relation between squeezing and entanglement is elucidated with the help of Peres-Horodecki positive partial transpose criterion formulated in the phase-space version for continuous-variable systems. The quantitative characteristics of the entanglement is determined using the log-negativity criterion. The oscillator-like model of a two-particle quantum-mechanical system is developed to illustrate the presented findings.△ Less,"6 December, 2024;",https://arxiv.org/pdf/2412.04863
EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,['Computation and Language'],"['LG AI Research', 'Soyoung An', 'Kyunghoon Bae', 'Eunbi Choi', 'Kibong Choi', 'Stanley Jungkyu Choi', 'Seokhee Hong', 'Junwon Hwang', 'Hyojin Jeon', 'Gerrard Jeongwon Jo', 'Hyunjik Jo', 'Jiyeon Jung', 'Yountae Jung', 'Hyosang Kim', 'Joonkee Kim', 'Seonghwan Kim', 'Soyeon Kim', 'Sunkyoung Kim', 'Yireun Kim', 'Yongil Kim', 'Youchul Kim', 'Edward Hwayoung Lee', 'Haeju Lee', 'Honglak Lee', 'Jinsik Lee']","This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04862
First search for atmospheric millicharged particles with the LUX-ZEPLIN experiment,['High Energy Physics - Experiment'],"['J. Aalbers', 'D. S. Akerib', 'A. K. Al Musalhi', 'F. Alder', 'C. S. Amarasinghe', 'A. Ames', 'T. J. Anderson', 'N. Angelides', 'H. M. Araújo', 'J. E. Armstrong', 'M. Arthurs', 'A. Baker', 'S. Balashov', 'J. Bang', 'J. W. Bargemann', 'E. E. Barillier', 'D. Bauer', 'K. Beattie', 'T. Benson', 'A. Bhatti', 'A. Biekert', 'T. P. Biesiadzinski', 'H. J. Birch', 'E. Bishop', 'G. M. Blockinger']","We report on a search for millicharged particles (mCPs) produced in cosmic ray proton atmospheric interactions using data collected during the first science run of the LUX-ZEPLIN experiment. The mCPs produced by two processes -- meson decay and proton bremsstrahlung -- are considered in this study. This search utilized a novel signature unique to liquid xenon (LXe) time projection chambers (TPCs), allowing sensitivity to mCPs with masses ranging from 10 to 1000 MeV/c$^2$ and fractional charges between 0.001 and 0.02 of the electron charge e. With an exposure of 60 live days and a 5.5 tonne fiducial mass, we observed no significant excess over background. This represents the first experimental search for atmospheric mCPs and the first search for mCPs using an underground LXe experiment.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04854
Quadratic Modelings of Syndrome Decoding,['Cryptography and Security'],"['Alessio Caminata', 'Ryann Cartor', 'Alessio Meneghetti', 'Rocco Mora', 'Alex Pellegrini']","This paper presents enhanced reductions of the bounded-weight and exact-weight Syndrome Decoding Problem (SDP) to a system of quadratic equations. Over $\mathbb{F}_2$, we improve on a previous work and study the degree of regularity of the modeling of the exact weight SDP. Additionally, we introduce a novel technique that transforms SDP instances over $\mathbb{F}_q$ into systems of polynomial equations and thoroughly investigate the dimension of their varieties. Experimental results are provided to evaluate the complexity of solving SDP instances using our models through Gröbner bases techniques.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04848
MTSpark: Enabling Multi-Task Learning with Spiking Neural Networks for Generalist Agents,['Neural and Evolutionary Computing'],"['Avaneesh Devkota', 'Rachmad Vidya Wicaksana Putra', 'Muhammad Shafique']","Currently, state-of-the-art RL methods excel in single-task settings, but they still struggle to generalize across multiple tasks due to catastrophic forgetting challenges, where previously learned tasks are forgotten as new tasks are introduced. This multi-task learning capability is significantly important for generalist agents, where adaptation features are highly required (e.g., autonomous robots). On the other hand, Spiking Neural Networks (SNNs) have emerged as alternative energy-efficient neural network algorithms due to their sparse spike-based operations. Toward this, we propose MTSpark, a novel methodology to enable multi-task RL using spiking networks. Specifically, MTSpark develops a Deep Spiking Q-Network (DSQN) with active dendrites and dueling structure by leveraging task-specific context signals. Specifically, each neuron computes task-dependent activations that dynamically modulate inputs, forming specialized sub-networks for each task. Moreover, this bioplausible network model also benefits from SNNs, enhancing energy efficiency and making the model suitable for hardware implementation. Experimental results show that, our MTSpark effectively learns multiple tasks with higher performance compared to the state-of-the-art. Specifically, MTSpark successfully achieves high score in three Atari games (i.e., Pong: -5.4, Breakout: 0.6, and Enduro: 371.2), reaching human-level performance (i.e., Pong: -3, Breakout: 31, and Enduro: 368), where state-of-the-art struggle to achieve. In addition, our MTSpark also shows better accuracy in image classification tasks than the state-of-the-art. These results highlight the potential of our MTSpark methodology to develop generalist agents that can learn multiple tasks by leveraging both RL and SNN concepts.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04847
Cutting is All You Need: Execution of Large-Scale Quantum Neural Networks on Limited-Qubit Devices,['Quantum Physics'],"['Alberto Marchisio', 'Emman Sychiuco', 'Muhammad Kashif', 'Muhammad Shafique']","The rapid advancement in Quantum Computing (QC), particularly through Noisy-Intermediate Scale Quantum (NISQ) devices, has spurred significant interest in Quantum Machine Learning (QML) applications. Despite their potential, fully-quantum QML algorithms remain impractical due to the limitations of current NISQ devices. Hybrid quantum-classical neural networks (HQNNs) have emerged as a viable alternative, leveraging both quantum and classical computations to enhance machine learning capabilities. However, the constrained resources of NISQ devices, particularly the limited number of qubits, pose significant challenges for executing large-scale quantum circuits.
  This work addresses these current challenges by proposing a novel and practical methodology for quantum circuit cutting of HQNNs, allowing large quantum circuits to be executed on limited-qubit NISQ devices. Our approach not only preserves the accuracy of the original circuits but also supports the training of quantum parameters across all subcircuits, which is crucial for the learning process in HQNNs. We propose a cutting methodology for HQNNs that employs a greedy algorithm for identifying efficient cutting points, and the implementation of trainable subcircuits, all designed to maximize the utility of NISQ devices in HQNNs. The findings suggest that quantum circuit cutting is a promising technique for advancing QML on current quantum hardware, since the cut circuit achieves comparable accuracy and much lower qubit requirements than the original circuit.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04844
Asynchronous Random Access in Massive MIMO Systems Facilitated by the Delay-Angle Domain,['Signal Processing'],"['Ao Chen', 'Wei Chen', 'Bo Ai', 'Petar Popovski']","The problem of uplink transmissions in massive connectivity is commonly dealt with using schemes for grant-free random access. When a large number of devices transmit almost synchronously, the receiver may not be able to resolve the collision. This could be addressed by assigning dedicated pilots to each user, leading to a contention-free random access (CFRA), which suffers from low scalability and efficiency. This paper explores contention-based random access (CBRA) schemes for asynchronous access in massive multiple-input multiple-output (MIMO) systems. The symmetry across the accessing users with the same pilots is broken by leveraging the delay information inherent to asynchronous systems and the angle information from massive MIMO to enhance activity detection (AD) and channel estimation (CE). The problem is formulated as a sparse recovery in the delay-angle domain. The challenge is that the recovery signal exhibits both row-sparse and cluster-sparse structure, with unknown cluster sizes and locations. We address this by a cluster-extended sparse Bayesian learning (CE-SBL) algorithm that introduces a new weighted prior to capture the signal structure and extends the expectation maximization (EM) algorithm for hyperparameter estimation. Simulation results demonstrate the superiority of the proposed method in joint AD and CE.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04841
Classical-quantum scattering,['High Energy Physics - Theory'],"['Daniel Carney', 'Akira Matsumura']","We analyze the framework recently proposed by Oppenheim et al. to model relativistic quantum fields coupled to relativistic, classical, stochastic fields (in particular, as a model of quantum matter coupled to ``classical gravity''). Perhaps surprisingly, we find that we can define and calculate scattering probabilities which are Lorentz-covariant and conserve total probability, at least at tree level. As a concrete example, we analyze $2 \to 2$ scattering of quantum matter mediated by a classical Yukawa field. Mapping this to a gravitational coupling in the non-relativistic limit, and assuming that we can treat large objects as point masses, we find that the simplest possible ``classical-quantum'' gravity theory constructed this way gives predictions for $2 \to 2$ gravitational scattering which are inconsistent with simple observations of, e.g., spacecraft undergoing slingshot maneuvers. We comment on lessons learned for attempts to couple quantum matter to ``non-quantum'' gravity, or more generally, for attempts to couple relativistic quantum and classical systems.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04839
Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment,['Robotics'],"['Ran Tian', 'Yilin Wu', 'Chenfeng Xu', 'Masayoshi Tomizuka', 'Jitendra Malik', 'Andrea Bajcsy']","Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04835
Learning-based Control for Tendon-Driven Continuum Robotic Arms,['Robotics'],"['Nima Maghooli', 'Omid Mahdizadeh', 'Mohammad Bajelani', 'S. Ali A. Moosavian']","This paper presents a learning-based approach for centralized position control of Tendon Driven Continuum Robots (TDCRs) using Deep Reinforcement Learning (DRL), with a particular focus on the Sim-to-Real transfer of control policies. The proposed control method employs the Modified Transpose Jacobian (MTJ) control strategy, with its parameters optimally tuned using the Deep Deterministic Policy Gradient (DDPG) algorithm. Classical model-based controllers encounter significant challenges due to the inherent uncertainties and nonlinear dynamics of continuum robots. In contrast, model-free control strategies require efficient gain-tuning to handle diverse operational scenarios. This research aims to develop a model-free controller with performance comparable to model-based strategies by integrating an optimal adaptive gain-tuning system. Both simulations and real-world implementations demonstrate that the proposed method significantly enhances the trajectory-tracking performance of continuum robots independent of initial conditions and paths within the operational task-space, effectively establishing a task-free controller.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04829
PanoDreamer: 3D Panorama Synthesis from a Single Image,['Computer Vision and Pattern Recognition'],"['Avinash Paliwal', 'Xilong Zhou', 'Andrii Tsarov', 'Nima Khademi Kalantari']","In this paper, we present PanoDreamer, a novel method for producing a coherent 360$^\circ$ 3D scene from a single input image. Unlike existing methods that generate the scene sequentially, we frame the problem as single-image panorama and depth estimation. Once the coherent panoramic image and its corresponding depth are obtained, the scene can be reconstructed by inpainting the small occluded regions and projecting them into 3D space. Our key contribution is formulating single-image panorama and depth estimation as two optimization tasks and introducing alternating minimization strategies to effectively solve their objectives. We demonstrate that our approach outperforms existing techniques in single-image 360$^\circ$ scene reconstruction in terms of consistency and overall quality.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04827
Taylor spectrum of a Banach module over the quantum plane,['Functional Analysis'],['Anar Dosi'],"In the paper we investigate the joint spectra of Banach space representations of the quantum q-plane called Banach q-modules. Based on the transversality relation from the topological homology of the trivial modules versus given a left Banach q-module, we introduce the joint (essential) spectra of a Banach q-module. In particular, we have the well defined Taylor joint spectrum of a Banach q-module. The noncommutative projection q-property is proved for the Taylor spectrum, which stands out the conventional projection property in the commutative case. It is provided the key examples of the Banach q-modules, which do not possesses nether forward nor backward projection properties.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04824
Noncommutative complex analytic geometry of a contractive quantum plane,['Functional Analysis'],['Anar Dosi'],"In the paper we investigate the Banach space representations of Manin's quantum q-plane for |q| is not 1. The Arens-Michael envelope of the quantum plane is extended up to a Frechet algebra presheaf over its spectrum. The obtained ringed space represents the geometry of the quantum plane as a union of two irreducible components being copies of the complex plane equipped with the q-topology and the disk topology, respectively. It turns out that the Frechet algebra presheaf is commutative modulo its Jacobson radical, which is decomposed into a topological direct sum. The related noncommutative functional calculus problem and the spectral mapping property are solved in terms of the noncommutative Harte spectrum.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04823
Naturally graded nilpotent associative algebras of nilindex $n-3$,['Rings and Algebras'],['I. A. Karimjanov'],"In the present paper, we give the classification of a subclass of n-dimensional naturally graded associative algebras with nilindex $n-3$. The subclass has the characteristic sequence $C(\mathcal{A})=(n-3,2,1)$. The result completes the classification of naturally graded associative algebras with nilindex $n-3$ for $n>6$.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04817
Linear Regressions with Combined Data,['Econometrics'],"[""Xavier D'Haultfoeuille"", 'Christophe Gaillac', 'Arnaud Maurel']","We study best linear predictions in a context where the outcome of interest and some of the covariates are observed in two different datasets that cannot be matched. Traditional approaches obtain point identification by relying, often implicitly, on exclusion restrictions. We show that without such restrictions, coefficients of interest can still be partially identified and we derive a constructive characterization of the sharp identified set. We then build on this characterization to develop computationally simple and asymptotically normal estimators of the corresponding bounds. We show that these estimators exhibit good finite sample performances.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04816
Automatic Prediction of Stroke Treatment Outcomes: Latest Advances and Perspectives,['Image and Video Processing'],"['Zeynel A. Samak', 'Philip Clatworthy', 'Majid Mirmehdi']","Stroke is a major global health problem that causes mortality and morbidity. Predicting the outcomes of stroke intervention can facilitate clinical decision-making and improve patient care. Engaging and developing deep learning techniques can help to analyse large and diverse medical data, including brain scans, medical reports and other sensor information, such as EEG, ECG, EMG and so on. Despite the common data standardisation challenge within medical image analysis domain, the future of deep learning in stroke outcome prediction lie in using multimodal information, including final infarct data, to achieve better prediction of long-term functional outcomes. This article provides a broad review of recent advances and applications of deep learning in the prediction of stroke outcomes, including (i) the data and models used, (ii) the prediction tasks and measures of success, (iii) the current challenges and limitations, and (iv) future directions and potential benefits. This comprehensive review aims to provide researchers, clinicians, and policy makers with an up-to-date understanding of this rapidly evolving and promising field.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04812
Light-induced magnetic trapping for cold alkali atoms using a combined optical tweezers and nanofibre platform,['Atomic Physics'],"['Alexey Vylegzhanin', 'Dylan James Brown', 'Sergey Abdrakhmanov', 'Sile Nice Chormaic']","We present a magnetic trapping method for cold $^{87}$Rb atoms, utilising the light-induced magnetic fields from the evanescent field of an optical nanofibre (ONF) in conjunction with an optical tweezers. We calculate and plot the trapping potentials for both Gaussian and Laguerre-Gaussian modes of the optical tweezers, and quasi-linear polarisation of the ONF-guided field. Based on the optical powers in the tweezers beam and the ONF-guided mode, we analyse the trap depths and the distances of the trap minima from the surface of the nanofibre. We show that, by controlling the powers in both of the optical fields, one can vary the trap position over a few hundreds of nanometres, while also influencing the trap depth and trap frequencies. Such control over atom position is essential both for studying distance-dependent effects on atoms trapped near dielectric surfaces, and minimising these effects for quantum technology applications.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04809
Quantum-hydrodynamic modal perspective on plasmonic gap structures,['Mesoscale and Nanoscale Physics'],"['Pu Zhang', 'Christos Tserkezis', 'N. Asger Mortensen']","Plasmonic gap structures are among the few configurations capable of generating extreme light confinement, finding applications in surface-enhanced spectroscopy, ultrasensitive detection, photocatalysis and more. Their plasmonic response undergoes a dramatic, quantum effect-driven transition as the gap size approaches zero. Modal analysis can reveal insights into the mechanisms governing this process, which are otherwise obscured by nonlocal damping effects. Here, we offer a fresh modal perspective on the transition of the plasmonic response using quantum hydrodynamic theory (QHT)-based quasinormal mode (QNM) analysis. Focusing on the bonding dipolar and charge-transfer plasmons of a nanosphere dimer, we examine the detailed mode transition through the touching regime as well as the asymptotic behavior compared with the classical results as the constituent nanoparticles either separate or overlap. The complex eigenfrequency particularly provides accurate information on the linewidth and quality factor of the plasmon modes. We introduce an index to characterize charge-transfer efficiency, especially for the charge-transfer plasmon. The significant role of nonlocal damping in the mode evolution is elucidated by our mode-resolved QHT-QNM analysis. The insights from our theoretical study provide an integrated understanding of mode evolution in plasmonic gap structures, which can further advance gap structure-based applications.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04804
A Multi-physics Model of Flow from Coronary Angiography: Insights into Microvascular Function,"['Computational Engineering, Finance, and Science']","['Haizhou Yang', 'Jiyang Zhang', 'Ismael Assi', 'Brahmajee K Nallamothu', 'Krishna Garikipati', 'C. Alberto Figueroa']","Coronary Artery Disease (CAD) and Coronary Microvascular Disease (CMD) can lead to insufficient blood flow to the myocardium, affecting millions of people globally. Coronary angiography, one of the most commonly used imaging modalities, offers valuable information that assists in diagnosing these diseases. However, these benefits are not fully understood or utilized in current clinical practice. In this study, a 3D-0D coupled multi-physics computational fluid dynamics (CFD) model was developed and calibrated to simulate and better understand the process of contrast injection and washout during clinical angiography. A contrast intensity profile (CIP) was introduced to capture the dynamics of coronary angiography data. Additionally, a sensitivity study was conducted to assess the influence of various coronary artery model parameters on CIP. The results demonstrate that the calibrated 3D-0D coupled multi-physics models are physiologically meaningful and produce accurate hemodynamic results. The sensitivity study further reveals that resistance has a greater impact on CIP than capacitance, with higher resistance amplifying this effect.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04798
"Multi-class heart disease Detection, Classification, and Prediction using Machine Learning Models",['Artificial Intelligence'],"['Mahfuzul Haque', 'Abu Saleh Musa Miah', 'Debashish Gupta', 'Md. Maruf Al Hossain Prince', 'Tanzina Alam', 'Nusrat Sharmin', 'Mohammed Sowket Ali', 'Jungpil Shin']","Heart disease is a leading cause of premature death worldwide, particularly among middle-aged and older adults, with men experiencing a higher prevalence. According to the World Health Organization (WHO), non-communicable diseases, including heart disease, account for 25\% (17.9 million) of global deaths, with over 43,204 annual fatalities in Bangladesh. However, the development of heart disease detection (HDD) systems tailored to the Bangladeshi population remains underexplored due to the lack of benchmark datasets and reliance on manual or limited-data approaches. This study addresses these challenges by introducing new, ethically sourced HDD dataset, BIG-Dataset and CD dataset which incorporates comprehensive data on symptoms, examination techniques, and risk factors. Using advanced machine learning techniques, including Logistic Regression and Random Forest, we achieved a remarkable testing accuracy of up to 96.6\% with Random Forest. The proposed AI-driven system integrates these models and datasets to provide real-time, accurate diagnostics and personalized healthcare recommendations. By leveraging structured datasets and state-of-the-art machine learning algorithms, this research offers an innovative solution for scalable and effective heart disease detection, with the potential to reduce mortality rates and improve clinical outcomes.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04792
Inhomogeneous reduction-annealing effects on the electron-doped cuprate superconductor revealed by micro-focused angle-resolved photoemission spectroscopy,['Superconductivity'],"['M. Miyamoto', 'M. Horio', 'K. Moriya', 'A. Takahashi', 'J. Osiecki', 'B. Thiagarajan', 'C. M. Polley', 'Y. Koike', 'T. Adachi', 'T. Mizokawa', 'I. Matsuda']","The development of the protect-annealing method has extended the superconductivity of the electron-doped cuprate \PLCCO ~(PLCCO) into lower Ce concentrations, while the superconducting volume fraction decreases with underdoping. Employing angle-resolved photoemission spectroscopy with a micro-focused beam, we investigated the electronic structure of protect-annealed PLCCO ($x=0.08$) with small superconducting volume fraction. Significant spatial variation of Fermi surface area and shape was observed, suggesting inhomogeneity in electron concentrations and the pseudogap that competes with superconductivity. By performing measurements at dozens of different sample positions, negative and non-monotonic correlation was found between the electron concentration and pseudogap magnitude. The established correlation illustrates a systematic annealing dependence of the electronic structure where a pseudogap abruptly opens with insufficient oxygen reduction.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04790
Differentially Private Random Feature Model,['Machine Learning'],"['Chunyang Liao', 'Deanna Needell', 'Alexander Xue']","Designing privacy-preserving machine learning algorithms has received great attention in recent years, especially in the setting when the data contains sensitive information. Differential privacy (DP) is a widely used mechanism for data analysis with privacy guarantees. In this paper, we produce a differentially private random feature model. Random features, which were proposed to approximate large-scale kernel machines, have been used to study privacy-preserving kernel machines as well. We consider the over-parametrized regime (more features than samples) where the non-private random feature model is learned via solving the min-norm interpolation problem, and then we apply output perturbation techniques to produce a private model. We show that our method preserves privacy and derive a generalization error bound for the method. To the best of our knowledge, we are the first to consider privacy-preserving random feature models in the over-parametrized regime and provide theoretical guarantees. We empirically compare our method with other privacy-preserving learning methods in the literature as well. Our results show that our approach is superior to the other methods in terms of generalization performance on synthetic data and benchmark data sets. Additionally, it was recently observed that DP mechanisms may exhibit and exacerbate disparate impact, which means that the outcomes of DP learning algorithms vary significantly among different groups. We show that both theoretically and empirically, random features have the potential to reduce disparate impact, and hence achieve better fairness.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04785
"A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges",['Artificial Intelligence'],"['Aditi Singh', 'Nirmal Prakashbhai Patel', 'Abul Ehtesham', 'Saket Kumar', 'Tala Talaei Khoei']","Large Language Models (LLMs) have transformed numerous domains by providing advanced capabilities in natural language understanding, generation, and reasoning. Despite their groundbreaking applications across industries such as research, healthcare, and creative media, their rapid adoption raises critical concerns regarding sustainability. This survey paper comprehensively examines the environmental, economic, and computational challenges associated with LLMs, focusing on energy consumption, carbon emissions, and resource utilization in data centers. By synthesizing insights from existing literature, this work explores strategies such as resource-efficient training, sustainable deployment practices, and lifecycle assessments to mitigate the environmental impacts of LLMs. Key areas of emphasis include energy optimization, renewable energy integration, and balancing performance with sustainability. The findings aim to guide researchers, practitioners, and policymakers in developing actionable strategies for sustainable AI systems, fostering a responsible and environmentally conscious future for artificial intelligence.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04782
Anomaly Detection and Classification in Knowledge Graphs,['Machine Learning'],"['Asara Senaratne', 'Peter Christen', 'Pouya Omran', 'Graham Williams']","Anomalies such as redundant, inconsistent, contradictory, and deficient values in a Knowledge Graph (KG) are unavoidable, as these graphs are often curated manually, or extracted using machine learning and natural language processing techniques. Therefore, anomaly detection is a task that can enhance the quality of KGs. In this paper, we propose SEKA (SEeking Knowledge graph Anomalies), an unsupervised approach for the detection of abnormal triples and entities in KGs. SEKA can help improve the correctness of a KG whilst retaining its coverage. We propose an adaption of the Path Rank Algorithm (PRA), named the Corroborative Path Rank Algorithm (CPRA), which is an efficient adaptation of PRA that is customized to detect anomalies in KGs. Furthermore, we also present TAXO (TAXOnomy of anomaly types in KGs), a taxonomy of possible anomaly types that can occur in a KG. This taxonomy provides a classification of the anomalies discovered by SEKA with an extensive discussion of possible data quality issues in a KG. We evaluate both approaches using the four real-world KGs YAGO-1, KBpedia, Wikidata, and DSKG to demonstrate the ability of SEKA and TAXO to outperform the baselines.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04780
Nonlocality-Assisted Enhancement of Error-Free Communication in Noisy Classical Channels,['Quantum Physics'],"['Kunika Agarwal', 'Sahil Gopalkrishna Naik', 'Ananya Chakraborty', 'Samrat Sen', 'Pratik Ghosal', 'Biswajit Paul', 'Manik Banik', 'Ram Krishna Patra']","The zero-error capacity of a noisy classical channel quantifies its ability to transmit information with absolute certainty, i.e., without any error. Unlike Shannon's standard channel capacity, which remains unaffected by pre-shared correlations, zero-error capacity can be enhanced through nonlocal correlations. In this work, we investigate zero-error communication utility of such correlations arising in the 2-2-m Bell scenario, where two parties have two inputs and m possible outcomes per input. For all m\geq2, we construct examples of noisy classical channels with zero zero-error capacity that, when assisted by extremal 2-2-m nonlocal correlations, can transmit one bit of information. While nonlocal correlations arising from quantum entangled states cannot achieve a positive zero-error capacity for these channels, they significantly enhance the probability of successfully transmitting a classical bit in a single use. Extending this analysis to the 2-m-2 Bell scenario, we identify channels with zero zero-error capacity that can nonetheless perfectly transmit log m bits of information when assisted by corresponding extremal nonlocal correlations. Our findings underscore the versatile utility of Bell nonlocal correlations in achieving zero-error communication.△ Less","6 December, 2024;",https://arxiv.org/pdf/2412.04779
Revitalizing Reconstruction Models for Multi-class Anomaly Detection via Class-Aware Contrastive Learning,['Computer Vision and Pattern Recognition'],"['Lei Fan', 'Junjie Huang', 'Donglin Di', 'Anyang Su', 'Maurice Pagnucco', 'Yang Song']","For anomaly detection (AD), early approaches often train separate models for individual classes, yielding high performance but posing challenges in scalability and resource management. Recent efforts have shifted toward training a single model capable of handling multiple classes. However, directly extending early AD methods to multi-class settings often results in degraded performance. In this paper, we analyze this degradation observed in reconstruction-based methods, identifying two key issues: catastrophic forgetting and inter-class confusion. To this end, we propose a plug-and-play modification by incorporating class-aware contrastive learning (CL). By explicitly leveraging raw object category information (e.g., carpet or wood) as supervised signals, we apply local CL to fine-tune multiscale features and global CL to learn more compact feature representations of normal patterns, thereby effectively adapting the models to multi-class settings. Experiments across four datasets (over 60 categories) verify the effectiveness of our approach, yielding significant improvements and superior performance compared to advanced methods. Notably, ablation studies show that even using pseudo-class labels can achieve comparable performance.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04769
Small Triangulations of $4$-Manifolds: Introducing the $4$-Manifold Census,['Geometric Topology'],"['Rhuaidi Antonio Burke', 'Benjamin A. Burton', 'Jonathan Spreer']","We present a framework to classify PL-types of large censuses of triangulated $4$-manifolds, which we use to classify the PL-types of all triangulated $4$-manifolds with up to $6$ pentachora. This is successful except for triangulations homeomorphic to the $4$-sphere, $\mathbb{C}P^2$, and the rational homology sphere $QS^4(2)$, where we find at most four, three, and two PL-types respectively. We conjecture that they are all standard. In addition, we look at the cases resisting classification and discuss the combinatorial structure of these triangulations -- which we deem interesting in their own rights.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04768
Towards counterfactual fairness thorough auxiliary variables,['Machine Learning'],"['Bowei Tian', 'Ziyao Wang', 'Shwai He', 'Wanghao Ye', 'Guoheng Sun', 'Yucong Dai', 'Yongkai Wu', 'Ang Li']","The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04767
Free-surface equatorial flows with surface tension in spherical coordinates,['Fluid Dynamics'],['Andrei Stan'],"In this paper, we determine an exact solution to the governing equations in spherical coordinates for an inviscid, incompressible fluid. This solution describes a steady, purely azimuthal equatorial flow with an associated free surface. Using functional analytic techniques, we demonstrate that if a free surface is known beforehand, the variations in pressure needed to achieve this surface implicitly define the shape of the free surface in a unique way.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04763
"'Being there together for health': A Systematic Review on the Feasibility, Effectiveness and Design Considerations of Immersive Collaborative Virtual Environments in Health Applications",['Human-Computer Interaction'],"['Tohid Zarei', 'Michelle Emery', 'Dimitrios Saredakis', 'Gun A. Lee', 'Ben Stubbs', 'Ancret Szpak', 'Tobias Loetscher']","Effectively using immersive multi-user environments for digital applications (via virtual, augmented and mixed reality technologies) beckons the future of healthcare delivery in the metaverse. We aimed to evaluate the feasibility and effectiveness of these environments used in health applications, while identifying their design features.
  We systematically searched MEDLINE, PsycINFO, and Emcare databases for peer-reviewed original reports, published in English, without date restrictions until Aug 30, 2023, and conducted manual citation searching in Feb 2024. All studies using fully immersive extended reality technologies (e.g., head-mounted displays, smart glasses) while engaging more than one participant in an intervention with direct health benefits were included. A qualitative synthesis of findings is reported. The quality of research was assessed using JBI Critical Checklists. The review was pre-registered on PROSPERO (CRD42023479155).
  Of 2862 identified records, 10 studies were eligible. Included studies were mostly conducted with healthy young adults (five studies) and older adults (four studies). While they all used different models of Oculus/Meta headsets, their environments' designs were distinctive and aligned with their objectives. Findings indicated varying degrees of positive health outcomes, for engagement in rehabilitation, meaningful interactions across distances, positive affect, transformative experiences, mental health therapies, and motor skill learning. Participants reported high usability, motivation, enjoyment, presence and copresence. They also expressed the need for more training time with technology.
  Adopting an intentional intervention design, considering factors affecting presence and copresence, as well as integrating co-creation of the program with participants, seems integral to achieving positive health outcomes.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04760
Latent Space Characterization of Autoencoder Variants,['Machine Learning'],"['Anika Shrivastava', 'Renu Rameshan', 'Samar Agnihotri']","Understanding the latent spaces learned by deep learning models is crucial in exploring how they represent and generate complex data. Autoencoders (AEs) have played a key role in the area of representation learning, with numerous regularization techniques and training principles developed not only to enhance their ability to learn compact and robust representations, but also to reveal how different architectures influence the structure and smoothness of the lower-dimensional non-linear manifold. We strive to characterize the structure of the latent spaces learned by different autoencoders including convolutional autoencoders (CAEs), denoising autoencoders (DAEs), and variational autoencoders (VAEs) and how they change with the perturbations in the input. By characterizing the matrix manifolds corresponding to the latent spaces, we provide an explanation for the well-known observation that the latent spaces of CAE and DAE form non-smooth manifolds, while that of VAE forms a smooth manifold. We also map the points of the matrix manifold to a Hilbert space using distance preserving transforms and provide an alternate view in terms of the subspaces generated in the Hilbert space as a function of the distortion in the input. The results show that the latent manifolds of CAE and DAE are stratified with each stratum being a smooth product manifold, while the manifold of VAE is a smooth product manifold of two symmetric positive definite matrices and a symmetric positive semi-definite matrix.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04755
Nonsingular black holes and spherically symmetric objects in nonlinear electrodynamics with a scalar field,['General Relativity and Quantum Cosmology'],"['Antonio De Felice', 'Shinji Tsujikawa']","In general relativity with vector and scalar fields given by the Lagrangian ${\cal L}(F,φ,X)$, where $F$ is a Maxwell term and $X$ is a kinetic term of the scalar field, we study the linear stability of static and spherically symmetric objects without curvature singularities at their centers. We show that the background solutions are generally described by either purely electrically or magnetically charged objects with a nontrivial scalar-field profile. In theories with the Lagrangian $\tilde{\cal L}(F)+K(φ, X)$, which correspond to nonlinear electrodynamics with a k-essence scalar field, angular Laplacian instabilities induced by vector-field perturbations exclude all the regular spherically symmetric solutions including nonsingular black holes. In theories described by the Lagrangian ${\cal L}=X+μ(φ)F^n$, where $μ$ is a function of $φ$ and $n$ is a constant, the absence of angular Laplacian instabilities of spherically symmetric objects requires that $n>1/2$, under which nonsingular black holes with event horizons are not present. However, for some particular ranges of $n$, there are horizonless compact objects with neither ghosts nor Laplacian instabilities in the small-scale limit. In theories given by ${\cal L}=X κ(F)$, where $κ$ is a function of $F$, regular spherically symmetric objects are prone to Laplacian instabilities either around the center or at spatial infinity. Thus, in our theoretical framework, we do not find any example of linearly stable nonsingular black holes.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04754
GABAR: Graph Attention-Based Action Ranking for Relational Policy Learning,['Machine Learning'],"['Rajesh Mangannavar', 'Stefan Lee', 'Alan Fern', 'Prasad Tadepalli']",We propose a novel approach to learn relational policies for classical planning based on learning to rank actions. We introduce a new graph representation that explicitly captures action information and propose a Graph Neural Network architecture augmented with Gated Recurrent Units (GRUs) to learn action rankings. Our model is trained on small problem instances and generalizes to significantly larger instances where traditional planning becomes computationally expensive. Experimental results across standard planning benchmarks demonstrate that our action-ranking approach achieves generalization to significantly larger problems than those used in training.△ Less,"5 December, 2024;",https://arxiv.org/pdf/2412.04752
Decomposed Distribution Matching in Dataset Condensation,['Computer Vision and Pattern Recognition'],"['Sahar Rahimi Malakshan', 'Mohammad Saeed Ebrahimi Saadabadi', 'Ali Dabouei', 'Nasser M. Nasrabadi']","Dataset Condensation (DC) aims to reduce deep neural networks training efforts by synthesizing a small dataset such that it will be as effective as the original large dataset. Conventionally, DC relies on a costly bi-level optimization which prohibits its practicality. Recent research formulates DC as a distribution matching problem which circumvents the costly bi-level optimization. However, this efficiency sacrifices the DC performance. To investigate this performance degradation, we decomposed the dataset distribution into content and style. Our observations indicate two major shortcomings of: 1) style discrepancy between original and condensed data, and 2) limited intra-class diversity of condensed dataset. We present a simple yet effective method to match the style information between original and condensed data, employing statistical moments of feature maps as well-established style indicators. Moreover, we enhance the intra-class diversity by maximizing the Kullback-Leibler divergence within each synthetic class, i.e., content. We demonstrate the efficacy of our method through experiments on diverse datasets of varying size and resolution, achieving improvements of up to 4.1% on CIFAR10, 4.2% on CIFAR100, 4.3% on TinyImageNet, 2.0% on ImageNet-1K, 3.3% on ImageWoof, 2.5% on ImageNette, and 5.5% in continual learning accuracy.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04748
Fair Diagnosis: Leveraging Causal Modeling to Mitigate Medical Bias,['Computer Vision and Pattern Recognition'],"['Bowei Tian', 'Yexiao He', 'Meng Liu', 'Yucong Dai', 'Ziyao Wang', 'Shwai He', 'Guoheng Sun', 'Zheyu Shen', 'Wanghao Ye', 'Yongkai Wu', 'Ang Li']","In medical image analysis, model predictions can be affected by sensitive attributes, such as race and gender, leading to fairness concerns and potential biases in diagnostic outcomes. To mitigate this, we present a causal modeling framework, which aims to reduce the impact of sensitive attributes on diagnostic predictions. Our approach introduces a novel fairness criterion, \textbf{Diagnosis Fairness}, and a unique fairness metric, leveraging path-specific fairness to control the influence of demographic attributes, ensuring that predictions are primarily informed by clinically relevant features rather than sensitive attributes. By incorporating adversarial perturbation masks, our framework directs the model to focus on critical image regions, suppressing bias-inducing information. Experimental results across multiple datasets demonstrate that our framework effectively reduces bias directly associated with sensitive attributes while preserving diagnostic accuracy. Our findings suggest that causal modeling can enhance both fairness and interpretability in AI-powered clinical decision support systems.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04739
Generative Humanization for Therapeutic Antibodies,['Machine Learning'],"['Cade Gordon', 'Aniruddh Raghu', 'Hunter Elliott', 'Peyton Greenside']","Antibody therapies have been employed to address some of today's most challenging diseases, but must meet many criteria during drug development before reaching a patient. Humanization is a sequence optimization strategy that addresses one critical risk called immunogenicity - a patient's immune response to the drug - by making an antibody more ""human-like"" in the absence of a predictive lab-based test for immunogenicity. However, existing humanization strategies generally yield very few humanized candidates, which may have degraded biophysical properties or decreased drug efficacy. Here, we re-frame humanization as a conditional generative modeling task, where humanizing mutations are sampled from a language model trained on human antibody data. We describe a sampling process that incorporates models of therapeutic attributes, such as antigen binding affinity, to obtain candidate sequences that have both reduced immunogenicity risk and maintained or improved therapeutic properties, allowing this algorithm to be readily embedded into an iterative antibody optimization campaign. We demonstrate in silico and in lab validation that in real therapeutic programs our generative humanization method produces diverse sets of antibodies that are both (1) highly-human and (2) have favorable therapeutic properties, such as improved binding to target antigens.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04737
Sensing-Aided 6G Drone Communications: Real-World Datasets and Demonstration,['Signal Processing'],"['Gouranga Charan', 'Ahmed Alkhateeb']","In the advent of next-generation wireless communication, millimeter-wave (mmWave) and terahertz (THz) technologies are pivotal for their high data rate capabilities. However, their reliance on large antenna arrays and narrow directive beams for ensuring adequate receive signal power introduces significant beam training overheads. This becomes particularly challenging in supporting highly-mobile applications such as drone communication, where the dynamic nature of drones demands frequent beam alignment to maintain connectivity. Addressing this critical bottleneck, our paper introduces a novel machine learning-based framework that leverages multi-modal sensory data, including visual and positional information, to expedite and refine mmWave/THz beam prediction. Unlike conventional approaches that solely depend on exhaustive beam training methods, our solution incorporates additional layers of contextual data to accurately predict beam directions, significantly mitigating the training overhead. Additionally, our framework is capable of predicting future beam alignments ahead of time. This feature enhances the system's responsiveness and reliability by addressing the challenges posed by the drones' mobility and the computational delays encountered in real-time processing. This capability for advanced beam tracking asserts a critical advancement in maintaining seamless connectivity for highly-mobile drones. We validate our approach through comprehensive evaluations on a unique, real-world mmWave drone communication dataset, which integrates concurrent camera visuals, practical GPS coordinates, and mmWave beam training data...△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04734
Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model,['Computer Vision and Pattern Recognition'],"['Keunwoo Peter Yu', 'Achal Dave', 'Rares Ambrus', 'Jean Mercat']","Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we propose Espresso, a novel method that extracts and compresses spatial and temporal information separately. Through extensive evaluations, we show that spatial and temporal compression in Espresso each have a positive impact on the long-form video understanding capabilities; when combined, their positive impact increases. Furthermore, we show that Espresso's performance scales well with more training data, and that Espresso is far more effective than the existing projectors for VLMs in long-form video understanding. Moreover, we devise a more difficult evaluation setting for EgoSchema called ""needle-in-a-haystack"" that multiplies the lengths of the input videos. Espresso achieves SOTA performance on this task, outperforming the SOTA VLMs that have been trained on much more training data.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04729
BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English,['Computation and Language'],"['Dipankar Srirag', 'Aditya Joshi', 'Jordan Painter', 'Diptesh Kanojia']","Despite large language models (LLMs) being known to exhibit bias against non-mainstream varieties, there are no known labeled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). Using web-based content from two domains, namely, Google Place reviews and Reddit comments, we collect datasets for these language varieties using two methods: location-based and topic-based filtering. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. Subsequently, we fine-tune nine large language models (LLMs) (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results reveal that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), with significant performance drops for en-IN, particularly in sarcasm detection. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE datasets, code, and models are currently available on request, while the paper is under review. Please email aditya.joshi@unsw.edu.au.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04726
Imaging and Spectral Fitting of Bright Gamma-ray Sources with the COSI Balloon Payload,['High Energy Astrophysical Phenomena'],"['Jarred M. Roberts', 'Steven Boggs', 'Thomas Siegert', 'John A. Tomsick', 'Marco Ajello', 'Peter von Ballmoos', 'Jacqueline Beechert', 'Floriane Cangemi', 'Savitri Gallego', 'Pierre Jean', 'Chris Karwin', 'Carolyn Kierans', 'Hadar Lazar', 'Alex Lowell', 'Israel Martinez Castellanos', 'Sean Pike', 'Clio Sleator', 'Yong Sheng', 'Hiroki Yoneda', 'Andreas Zoglauer']","The Compton Spectrometer and Imager balloon payload (COSI-Balloon) is a wide-field-of-view Compton $γ$-ray telescope that operates in the 0.2 - 5 MeV bandpass. COSI-Balloon had a successful 46-day flight in 2016 during which the instrument observed the Crab Nebula, Cygnus X-1, and Centaurus A. Using the data collected by the COSI-Balloon instrument during this flight, we present the source flux extraction of signals from the variable balloon background environment and produce images of these background-dominated sources by performing Richardson-Lucy deconvolutions. We also present the spectra measured by the COSI-Balloon instrument, compare and combine them with measurements from other instruments, and fit the data. The Crab Nebula was observed by COSI-Balloon and we obtain a measured flux in the energy band 325 - 480 keV of (4.5 ${\pm}$ 1.6) ${\times}$ 10$^{-3}$ ph cm$^{-2}$ s$^{-1}$. The model that best fits the COSI-Balloon data combined with measurements from NuSTAR and Swift-BAT is a broken power law with a measured photon index $Γ$ = 2.20 ${\pm}$ 0.02 above the 43 keV break. Cygnus X-1 was also observed during this flight, and we obtain a measured flux of (1.4 ${\pm}$ 0.2) ${\times}$ 10$^{-3}$ ph cm$^{-2}$ s$^{-1}$ in the same energy band and a best-fit result (including data from NuSTAR, Swift-BAT, and INTEGRAL/ IBIS) was to a cutoff power law with a high-energy cutoff energy of 138.3 ${\pm}$ 1.0 keV and a photon index of $Γ$ = 1.358 ${\pm}$ 0.002. Lastly, we present the measured spectrum of Centaurus A and our best model fit to a power law with a photon index of $Γ$ = 1.73 ${\pm}$ 0.01.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04721
Fermionic quantum walkers coupled to a bosonic reservoir,['Mathematical Physics'],"['Olivier Bourget', 'Alain Joye', 'Dominique Spehner']","We analyze the discrete-time dynamics of a model of non-interacting fermions on a finite graph coupled to an infinite reservoir formed by a bosonic quantum walk on Z. This dynamics consists of consecutive applications of free evolutions of the fermions and bosons followed by a local coupling between them. The unitary operator implementing this coupling generates fermionic transitions between two fixed vertices induced by absorption and emission of bosons at a single lattice site. The free fermion evolution is given by a second-quantized single-particle unitary operator satisfying some genericity assumptions, in particular it produces hopping of fermions between all vertices. The free boson evolution is given by the second-quantized shift operator on Z. We derive explicitly the Heisenberg dynamics of fermionic observables and obtain a systematic expansion in the large-coupling regime, which we control by using spectral methods. We also prove that the reduced state of the fermions converges in the large-time limit to an infinite-temperature Gibbs△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04716
Simulation Tool Development and Sensitivity Analysis of 160Gd Double Beta Decay Search by the PIKACHU Project,['High Energy Physics - Experiment'],"['Takumi Omori', 'Takashi Iida', 'Nobuo Hinohara', 'Kotaro Takahashi', 'Ken-Ichi Fushimi', 'Azusa Gando', 'Keishi Hosokawa', 'Shotaro Ishidate', 'Motonao Ishigami', 'Kei Kamada', 'Keita Mizukoshi', 'Yasuhiro Shoji', 'Hisanori Suzuki', 'Masao Yoshino']","Neutrinoless double beta decay (0v2b) has been investigated as a physical process that can provide evidence for the Majorana nature of neutrinos. The theoretical predictions of the 0v2b rate are subject to significant uncertainty, primarily due to nuclear matrix elements (NME). To reduce this uncertainty, experimental measurements of the half-lives of two-neutrino double beta decay (2v2b) in various nuclei are essential as a benchmark for NME calculations. The PIKACHU (Pure Inorganic scintillator experiment in KAmioka for CHallenging Underground sciences) project searches for the previously unobserved 2v2b decay of 160Gd, employing Ce-doped Gd3Ga3Al2O12 (GAGG) single crystals. In the Phase 1 experiment, we aim to improve the current lower limit on the 2v2b half-life of 160Gd by a prior study using a Ce-doped Gd2SiO5 (GSO) crystal. Ultimately, in Phase 2, the project seeks to achieve a sensitivity surpassing the theoretical prediction of 7.4 x 10^20 years, enabling the potential discovery of the 160Gd 2v2b decay. In this paper, we describe the development of background models based on GEANT4 simulations. The modeled backgrounds are contributions from uranium and thorium decay chains, 40K present in GAGG, and 40K gamma-rays from outside of GAGG. Additionally, we developed models for both 2v2b and 0v2b decay by implementing the theoretical kinematics of two-electron emission in double beta decay in the GEANT4 simulation. As a result, our background models successfully reproduced the measured background spectrum through fitting. By generating pseudo background spectra expected in Phase 1 and analyzing them with the combined background and 2v2b models, we evaluated the 2v2b sensitivity of Phase 1 to be 2.78 x 10^19 years (90% C.L.). This paper presents the development of these simulation models and the expected sensitivities for both Phase 1 and Phase 2 based on the pseudo data analyses.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04712
QuTiP 5: The Quantum Toolbox in Python,['Quantum Physics'],"['Neill Lambert', 'Eric Giguère', 'Paul Menczel', 'Boxi Li', 'Patrick Hopf', 'Gerardo Suárez', 'Marc Gali', 'Jake Lishman', 'Rushiraj Gadhvi', 'Rochisha Agarwal', 'Asier Galicia', 'Nathan Shammah', 'Paul Nation', 'J. R. Johansson', 'Shahnawaz Ahmed', 'Simon Cross', 'Alexander Pitchford', 'Franco Nori']","QuTiP, the Quantum Toolbox in Python, has been at the forefront of open-source quantum software for the last ten years. It is used as a research, teaching, and industrial tool, and has been downloaded millions of times by users around the world. Here we introduce the latest developments in QuTiP v5, which are set to have a large impact on the future of QuTiP and enable it to be a modern, continuously developed and popular tool for another decade and more. We summarize the code design and fundamental data layer changes as well as efficiency improvements, new solvers, applications to quantum circuits with QuTiP-QIP, and new quantum control tools with QuTiP-QOC. Additional flexibility in the data layer underlying all ""quantum objects"" in QuTiP allows us to harness the power of state-of-the-art data formats and packages like JAX, CuPy, and more. We explain these new features with a series of both well-known and new examples. The code for these examples is available in a static form on GitHub and will be available also in a continuously updated and documented notebook form in the qutip-tutorials package.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04705
Transformers Struggle to Learn to Search,['Computation and Language'],"['Abulhair Saparov', 'Srushti Pawar', 'Shreyas Pimpalgaonkar', 'Nitish Joshi', 'Richard Yuanzhe Pang', 'Vishakh Padmakumar', 'Seyed Mehran Kazemi', 'Najoung Kim', 'He He']","Search is an ability foundational in many important tasks, and recent studies have shown that large language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer architecture. In this work, we use the foundational graph connectivity problem as a testbed to generate effectively limitless high-coverage data to train small transformers and test whether they can learn to perform search. We find that, when given the right training distribution, the transformer is able to learn to search.
  We analyze the algorithm that the transformer has learned through a novel mechanistic interpretability technique that enables us to extract the computation graph from the trained model. We find that for each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in the number of layers.
  However, we find that as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that increasing model scale will not lead to robust search abilities. We also find that performing search in-context (i.e., chain-of-thought) does not resolve this inability to learn to search on larger graphs.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04703
Engineering qubit dynamics in open systems with photonic synthetic lattices,['Quantum Physics'],"['Francesco Di Colandrea', 'Tareq Jaouni', 'John Grace', 'Dilip Paneru', 'Mirko Arienzo', ""Alessio D'Errico"", 'Ebrahim Karimi']","The evolution of a quantum system interacting with an environment can be described as a unitary process acting on both the system and the environment. In this framework, the system's evolution can be predicted by tracing out the environmental degrees of freedom. Here, we establish a precise mapping between the global unitary dynamics and the quantum operation involving the system, wherein the system is a single qubit, and the environment is modeled as a discrete lattice space. This approach enables the implementation of arbitrary noise operations on single-polarization qubits using a minimal set of three liquid-crystal metasurfaces, whose transverse distribution of the optic axes can be patterned to reproduce the target process. We experimentally validate this method by simulating common noise processes, such as phase errors and depolarization.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04701
Re-assembly and test of a COMB dipole magnet with STAR wires,['Accelerator Physics'],"['V. V. Kashikhin', 'S. Cohan', 'J. DiMarco', 'O. Kiemschies', 'S. Krave', 'V. Lombardo', 'V. Marinozzi', 'D. Orris', 'S. Stoynev', 'D. Turrioni', 'A. K. Chavda', 'U. Sambangi', 'S. Korupolu', 'J. Peram', 'A. Arjun', 'C. Goel', 'J. Sai Sandra', 'V. Yerraguravagari', 'R. Schmidt', 'V. Selvamanickam', 'G. Majkic', 'E. Galstyan', 'N. Mai', 'K. Selvamanickam']","Rare-Earth Barium Copper Oxide (REBCO) coated conductors are an attractive option for application in high field accelerator magnets due to their high critical field and the convenience of fabrication without heat treatment compared to some other superconductors. A small REBCO accelerator magnet was previously fabricated and tested in liquid nitrogen, demonstrating over 90% critical current retention in the coils. This paper describes the magnet re-assembly with a different support structure and its test in liquid helium at 1.8-4.5 K. The magnet quench history along with the instrumentation data is presented and discussed.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04696
Polariton-induced Purcell effects via a reduced semiclassical electrodynamics approach,['Chemical Physics'],"['Andres Felipe Bocanegra Vargas', 'Tao E. Li']","Recent experiments have demonstrated that polariton formation provides a novel strategy for modifying local molecular processes when a large ensemble of molecules is confined within an optical cavity. Herein, a numerical strategy based on coupled Maxwell--Schrödinger equations is examined for simulating local molecular processes in a realistic cavity structure under collective strong coupling. In this approach, only a few molecules, referred to as quantum impurities, are treated quantum mechanically, while the remaining macroscopic molecular layer and the cavity structure are modeled using dielectric functions. When a single electronic two-level system embedded in a Lorentz medium is confined in a two-dimensional Bragg resonator, our numerical simulations reveal a polariton-induced Purcell effect: the radiative decay rate of the quantum impurity is significantly enhanced by the cavity when the impurity frequency matches the polariton frequency, while the rate can sometimes be greatly suppressed when the impurity is near resonance with the bulk molecules forming strong coupling. Additionally, this approach demonstrates that the cavity absorption of light exhibits Rabi-splitting-dependent suppression due to the inclusion of a realistic cavity structure. Our simulations also identify a fundamental limitation of this approach -- an inaccurate description of polariton dephasing rates into dark modes. This arises because the dark-mode degrees of freedom are not explicitly included when most molecules are modeled using dielectric functions. As the polariton-induced Purcell effect alters molecular radiative decay differently from the Purcell effect under weak coupling, this polariton-induced effect may facilitate understanding the origin of polariton-modified photochemistry under electronic strong coupling.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04694
Sequential anomaly identification with observation control under generalized error metrics,['Statistics Theory'],"['Aristomenis Tsopelakos', 'Georgios Fellouris']","The problem of sequential anomaly detection and identification is considered, where multiple data sources are simultaneously monitored and the goal is to identify in real time those, if any, that exhibit ``anomalous"" statistical behavior. An upper bound is postulated on the number of data sources that can be sampled at each sampling instant, but the decision maker selects which ones to sample based on the already collected data. Thus, in this context, a policy consists not only of a stopping rule and a decision rule that determine when sampling should be terminated and which sources to identify as anomalous upon stopping, but also of a sampling rule that determines which sources to sample at each time instant subject to the sampling constraint. Two distinct formulations are considered, which require control of different, ``generalized"" error metrics. The first one tolerates a certain user-specified number of errors, of any kind, whereas the second tolerates distinct, user-specified numbers of false positives and false negatives. For each of them, a universal asymptotic lower bound on the expected time for stopping is established as the error probabilities go to 0, and it is shown to be attained by a policy that combines the stopping and decision rules proposed in the full-sampling case with a probabilistic sampling rule that achieves a specific long-run sampling frequency for each source. Moreover, the optimal to a first order asymptotic approximation expected time for stopping is compared in simulation studies with the corresponding factor in a finite regime, and the impact of the sampling constraint and tolerance to errors is assessed.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04693
PerfGen: Automated Performance Benchmark Generation for Big Data Analytics,['Software Engineering'],"['Jiyuan Wang', 'Jason Teoh', 'Muhammand Ali Gulza', 'Qian Zhang', 'Miryung Kim']","Many symptoms of poor performance in big data analytics such as computational skews, data skews, and memory skews are input dependent. However, due to the lack of inputs that can trigger such performance symptoms, it is hard to debug and test big data analytics.
  We design PerfGen to automatically generate inputs for the purpose of performance testing. PerfGen overcomes three challenges when naively using automated fuzz testing for the purpose of performance testing. First, typical greybox fuzzing relies on coverage as a guidance signal and thus is unlikely to trigger interesting performance behavior. Therefore, PerfGen provides performance monitor templates that a user can extend to serve as a set of guidance metrics for grey-box fuzzing. Second, performance symptoms may occur at an intermediate or later stage of a big data analytics pipeline. Thus, PerfGen uses a phased fuzzing approach. This approach identifies symptom-causing intermediate inputs at an intermediate stage first and then converts them to the inputs at the beginning of the program with a pseudo-inverse function generated by a large language model. Third, PerfGen defines sets of skew-inspired input mutations, which increases the chance of inducing performance problems.
  We evaluate PerfGen using four case studies. PerfGen achieves at least 11x speedup compared to a traditional fuzzing approach when generating inputs to trigger performance symptoms. Additionally, identifying intermediate inputs first and then converting them to original inputs enables PerfGen to generate such workloads in less than 0.004% of the iterations required by a baseline approach.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04687
Measuring the ATLAS ITk Pixel Detector Material via Multiple Scattering of Positrons at the CERN PS,['Instrumentation and Detectors'],"['Simon Florian Koch', 'Brian Moser', 'Antonín Lindner', 'Valerio Dao', 'Ignacio Asensi', 'Daniela Bortoletto', 'Marianne Brekkum', 'Florian Dachs', 'Hans Ludwig Joos', 'Milou van Rijnbach', 'Abhishek Sharma', 'Ismet Siral', 'Carlos Solans', 'Yingjie Wei']","The ITk is a new silicon tracker for the ATLAS experiment designed to increase detector resolution, readout capacity, and radiation hardness, in preparation for the larger number of simultaneous proton-proton interactions at the High Luminosity LHC. This paper presents the first direct measurement of the material budget of an ATLAS ITk pixel module, performed at a testbeam at the CERN Proton Synchrotron via the multiple scattering of low energy positrons within the module volume. Using a four plane telescope of thin monolithic pixel detectors from the MALTA collaboration, scattering datasets were recorded at a beam energy of $1.2\,\text{GeV}$. Kink angle distributions were extracted from tracks derived with and without information from the ITk pixel module, and were fit to extract the RMS scattering angle, which was converted to a fractional radiation length $x/X_0$. The average $x/X_0$ across the module was measured as $[0.89 \pm 0.01 \text{ (resolution)} \pm 0.01 \text{ (subtraction)} \pm 0.08 \text{ (beam momentum band)}]\%$, which agrees within uncertainties with an estimate of $0.88\%$ derived from material component expectations.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04686
Water Enrichment from Pebble Drift in Disks with Gap-forming Planets,['Earth and Planetary Astrophysics'],"['Whittney Easterwood', 'Anusha Kalyaan', 'Andrea Banzatti']","Volatiles like $H_2O$ are present as ice in solids in the outer cold regions of protoplanetary disks and as vapor in the warm inner regions within the water snow line. Icy pebbles drifting inwards from the outer disk sublimate after crossing the snow line, enriching the inner disk with solid mass and water vapor. Meanwhile, proto-planets forming within the disk open gaps in the disk gas, creating traps against the inward drift of pebbles and in turn reducing water enrichment in the inner disk. Recent disk observations from millimeter interferometry and infrared spectroscopy have supported this broad picture by finding a correlation between the outer radial distribution of pebbles and the properties of inner water vapor spectra. In this work, we aim at further informing previous and future observations by building on previous models to explore pebble drift in disks with multiple gaps. We systematically explore multiple gap locations and their depths (equivalent to specific masses of planets forming within), and different particle sizes to study their impact on inner disk water enrichment. We find that the presence of close-in deep gaps carved by a Jupiter-mass planet is likely crucial for blocking icy pebble delivery into the inner disk, while planets with lower masses only provide leaky traps. We also find that disks with multiple gaps show lower vapor enrichment in the inner disk. Altogether, these model results support the idea that inner disk water delivery and planet formation are regulated by the mass and location of the most massive planets.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04681
"Raspberry Pi multispectral imaging camera system (PiMICS): a low-cost, skills-based physics educational tool",['Physics Education'],"['John C. Howell', 'Brian Flores', 'Juan Javier Naranjo', 'Angel Mendez', 'Cesar Costa-Vera', 'Chris Koumriqian', 'Juliana Jordan', 'Pieter H. Neethling', 'Calvin Groenewald', 'Michael A. C. Lovemore', 'Patrick A. T. Kinsey', 'Tjaart P. J. Kruger']","We report on an educational pilot program for low-cost physics experimentation run in Ecuador, South Africa, and the United States. The program was developed after having needs-based discussions with African educators, researchers, and leaders. It was determined that the need and desire for low-cost, skills-building, and active-learning tools is very high. From this, we developed a 3D-printable, Raspberry Pi-based multispectral camera (15 to 25 spectral channels in the visible and near-IR) for as little as $100. The program allows students to learn 3D modeling, 3D printing, feedback, control, image analysis, Python programming, systems integration and artificial intelligence as well as spectroscopy. After completing their cameras, the students in the program studied plant health, plant stress, post-harvest fruit ripeness, and polarization and spectral analysis of nanostructured insect wings, the latter of which won the ``best-applied research"" award at a conference poster session and will be highlighted in this paper. Importantly, these cameras can be an integral part of any developing country's agricultural, recycling, medical, and pharmaceutical infrastructure. Thus, we believe this experiment can play an important role at the intersection of student training and developing countries' capacity building.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04679
Zephyr quantum-assisted hierarchical Calo4pQVAE for particle-calorimeter interactions,['Machine Learning'],"['Ian Lu', 'Hao Jia', 'Sebastian Gonzalez', 'Deniz Sogutlu', 'J. Quetzalcoatl Toledo-Marin', 'Sehmimul Hoque', 'Abhishek Abhishek', 'Colin Gay', 'Roger Melko', 'Eric Paquet', 'Geoffrey Fox', 'Maximilian Swiatlowski', 'Wojciech Fedorko']","With the approach of the High Luminosity Large Hadron Collider (HL-LHC) era set to begin particle collisions by the end of this decade, it is evident that the computational demands of traditional collision simulation methods are becoming increasingly unsustainable. Existing approaches, which rely heavily on first-principles Monte Carlo simulations for modeling event showers in calorimeters, are projected to require millions of CPU-years annually -- far exceeding current computational capacities. This bottleneck presents an exciting opportunity for advancements in computational physics by integrating deep generative models with quantum simulations. We propose a quantum-assisted hierarchical deep generative surrogate founded on a variational autoencoder (VAE) in combination with an energy conditioned restricted Boltzmann machine (RBM) embedded in the model's latent space as a prior. By mapping the topology of D-Wave's Zephyr quantum annealer (QA) into the nodes and couplings of a 4-partite RBM, we leverage quantum simulation to accelerate our shower generation times significantly. To evaluate our framework, we use Dataset 2 of the CaloChallenge 2022. Through the integration of classical computation and quantum simulation, this hybrid framework paves way for utilizing large-scale quantum simulations as priors in deep generative models.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04677
Cost optimized ab initio tensor network state methods: industrial perspectives,['Computational Physics'],"['Andor Menczer', 'Örs Legeza']","We introduce efficient solutions to optimize the cost of tree-like tensor network state method calculations when an expensive GPU-accelerated hardware is utilized. By supporting a main powerful compute node with additional auxiliary, but much cheaper nodes to store intermediate, precontracted tensor network scratch data, the IO time can be hidden behind the computation almost entirely without increasing memory peak. Our solution is based on the different bandwidths of the different communication channels, like NVLink, PCIe, InfiniBand and available storage media, which are utilized on different layers of the algorithm. This simple heterogeneous multiNode solution via asynchronous IO operation has the potential to minimize IO overhead, resulting in maximum performance rate for the main compute unit. In addition, we introduce an in-house developed massively parallel protocol to serialize and deserialize block sparse matrices and tensors, reducing data communication time tremendously. Performance profiles are presented for the spin adapted ab initio density matrix renormalization group method for corresponding U(1) bond dimension values up to 15400 on the active compounds of the FeMoco with complete active space (CAS) sizes of up to 113 electrons in 76 orbitals [CAS(113, 76)].△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04676
Comparison of Transcriptional Activation by Corticosteroids of Human MR (Ile-180) and Human MR Haplotype (Ile180Val),['Biomolecules'],"['Yoshinao Katsu', 'Jiawn Zhang', 'Ya Ao', 'Michael E. Baker']","While the classical function of human mineralocorticoid receptor (MR) is to regulate sodium and potassium homeostasis through aldosterone activation of the kidney MR, the MR also is highly expressed in the brain, where the MR is activated by cortisol in response to stress. Here, we report the half-maximal response (EC50) and fold-activation by cortisol, aldosterone and other corticosteroids of human MR rs5522, a haplotype containing valine at codon 180 instead of isoleucine found in the wild-type MR (Ile-180). MR rs5522 (Val-180) has been studied for its actions in the human brain involving coping with stress and depression. We compared the EC50 and fold-activation by corticosteroids of MR rs5522 and wild-type MR transfected into HEK293 cells with either the TAT3 promoter or the MMTV promoter. Parallel studies investigated the binding of MR antagonists, spironolactone and progesterone, to MR rs5522. In HEK293 cells with the MMTV promotor, MR rs5522 had a slightly higher EC50 compared to wild-type MR and a similar level of fold-activation for all corticosteroids. In contrast, in HEK293 cells with the TAT3 promoter, MR 5522 had a higher EC50 (lower affinity) and higher fold-activation for cortisol compared to wild-type MR (Ile-180), while compared to wild-type MR, the EC50s of MR rs5522 for aldosterone and corticosterone were slightly lower and fold-activation was higher. Spironolactone and progesterone had similar antagonist activity for MR rs5522 and MR (Ile-180) in the presence of MMTV and TAT3 promoters in HEK293 cells.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04674
Socially-Informed Reconstruction for Pedestrian Trajectory Forecasting,['Computer Vision and Pattern Recognition'],"['Haleh Damirchi', 'Ali Etemad', 'Michael Greenspan']","Pedestrian trajectory prediction remains a challenge for autonomous systems, particularly due to the intricate dynamics of social interactions. Accurate forecasting requires a comprehensive understanding not only of each pedestrian's previous trajectory but also of their interaction with the surrounding environment, an important part of which are other pedestrians moving dynamically in the scene. To learn effective socially-informed representations, we propose a model that uses a reconstructor alongside a conditional variational autoencoder-based trajectory forecasting module. This module generates pseudo-trajectories, which we use as augmentations throughout the training process. To further guide the model towards social awareness, we propose a novel social loss that aids in forecasting of more stable trajectories. We validate our approach through extensive experiments, demonstrating strong performances in comparison to state of-the-art methods on the ETH/UCY and SDD benchmarks.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04673
Exact Inversion from Space-filling Trajectories in Cone-beam Transmission Tomography,['Medical Physics'],"['Murdock Grewar', 'Glenn Myers', 'Andrew Kingston']","This article introduces a new theory of exact inversion in cone-beam transmission tomography where the source point locus is a 2D surface, 3D volume, or something more complex.
  We specialise the theory to the case of the cylinder-shaped source locus, and we describe in detail a functioning practical implementation of the inversion algorithm for this geometry. This locus is accessible to CT scanners which include translational and rotational manipulators (e.g. typical helical scanners). This serves as a concrete instantiation of the theory and as a reference implementation which is immediately applicable to many contemporary scanning apparatuses.
  We illustrate some error characteristics of the algorithm through reconstructions of a simulated dataset. We finish with a reconstruction performed on a set of experimental data acquired with a low-pitch sparsely sampled helical trajectory.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04669
Diffusion-Augmented Coreset Expansion for Scalable Dataset Distillation,['Computer Vision and Pattern Recognition'],"['Ali Abbasi', 'Shima Imani', 'Chenyang An', 'Gayathri Mahalingam', 'Harsh Shrivastava', 'Maurice Diesendruck', 'Hamed Pirsiavash', 'Pramod Sharma', 'Soheil Kolouri']","With the rapid scaling of neural networks, data storage and communication demands have intensified. Dataset distillation has emerged as a promising solution, condensing information from extensive datasets into a compact set of synthetic samples by solving a bilevel optimization problem. However, current methods face challenges in computational efficiency, particularly with high-resolution data and complex architectures. Recently, knowledge-distillation-based dataset condensation approaches have made this process more computationally feasible. Yet, with the recent developments of generative foundation models, there is now an opportunity to achieve even greater compression, enhance the quality of distilled data, and introduce valuable diversity into the data representation. In this work, we propose a two-stage solution. First, we compress the dataset by selecting only the most informative patches to form a coreset. Next, we leverage a generative foundation model to dynamically expand this compressed set in real-time, enhancing the resolution of these patches and introducing controlled variability to the coreset. Our extensive experiments demonstrate the robustness and efficiency of our approach across a range of dataset distillation benchmarks. We demonstrate a significant improvement of over 10% compared to the state-of-the-art on several large-scale dataset distillation benchmarks. The code will be released soon.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04668
ProPLIKS: Probablistic 3D human body pose estimation,['Computer Vision and Pattern Recognition'],"['Karthik Shetty', 'Annette Birkhold', 'Bernhard Egger', 'Srikrishna Jaganathan', 'Norbert Strobel', 'Markus Kowarschik', 'Andreas Maier']","We present a novel approach for 3D human pose estimation by employing probabilistic modeling. This approach leverages the advantages of normalizing flows in non-Euclidean geometries to address uncertain poses. Specifically, our method employs normalizing flow tailored to the SO(3) rotational group, incorporating a coupling mechanism based on the Möbius transformation. This enables the framework to accurately represent any distribution on SO(3), effectively addressing issues related to discontinuities. Additionally, we reinterpret the challenge of reconstructing 3D human figures from 2D pixel-aligned inputs as the task of mapping these inputs to a range of probable poses. This perspective acknowledges the intrinsic ambiguity of the task and facilitates a straightforward integration method for multi-view scenarios. The combination of these strategies showcases the effectiveness of probabilistic models in complex scenarios for human pose estimation techniques. Our approach notably surpasses existing methods in the field of pose estimation. We also validate our methodology on human pose estimation from RGB images as well as medical X-Ray datasets.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04665
Circumscribed Circles in Integer Geometry,['Number Theory'],"['Oleg Karpenkov', 'Anna Pratoussevitch', 'Rebecca Sheppard']",Integer geometry on a plane deals with objects whose vertices are points in $\mathbb Z^2$. The congruence relation is provided by all affine transformations preserving the lattice $\mathbb Z^2$. In this paper we study circumscribed circles in integer geometry. We introduce the notions of integer and rational circumscribed circles of integer sets. We determine the conditions for a finite integer set to admit an integer circumscribed circle and describe the spectra of radii for integer and rational circumscribed circles.△ Less,"5 December, 2024;",https://arxiv.org/pdf/2412.04662
Learning for Layered Safety-Critical Control with Predictive Control Barrier Functions,['Systems and Control'],"['William D. Compton', 'Max H. Cohen', 'Aaron D. Ames']","Safety filters leveraging control barrier functions (CBFs) are highly effective for enforcing safe behavior on complex systems. It is often easier to synthesize CBFs for a Reduced order Model (RoM), and track the resulting safe behavior on the Full order Model (FoM) -- yet gaps between the RoM and FoM can result in safety violations. This paper introduces \emph{predictive CBFs} to address this gap by leveraging rollouts of the FoM to define a predictive robustness term added to the RoM CBF condition. Theoretically, we prove that this guarantees safety in a layered control implementation. Practically, we learn the predictive robustness term through massive parallel simulation with domain randomization. We demonstrate in simulation that this yields safe FoM behavior with minimal conservatism, and experimentally realize predictive CBFs on a 3D hopping robot.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04658
An Efficient Model Maintenance Approach for MLOps,['Software Engineering'],"['Forough Majidi', 'Foutse Khomh', 'Heng Li', 'Amin Nikanjam']","In recent years, many industries have utilized machine learning models (ML) in their systems. Ideally, machine learning models should be trained on and applied to data from the same distributions. However, the data evolves over time in many application areas, leading to data and concept drift, which in turn causes the performance of the ML models to degrade over time. Therefore, maintaining up to date ML models plays a critical role in the MLOps pipeline. Existing ML model maintenance approaches are often computationally resource intensive, costly, time consuming, and model dependent. Thus, we propose an improved MLOps pipeline, a new model maintenance approach and a Similarity Based Model Reuse (SimReuse) tool to address the challenges of ML model maintenance. We identify seasonal and recurrent distribution patterns in time series datasets throughout a preliminary study. Recurrent distribution patterns enable us to reuse previously trained models for similar distributions in the future, thus avoiding frequent retraining. Then, we integrated the model reuse approach into the MLOps pipeline and proposed our improved MLOps pipeline. Furthermore, we develop SimReuse, a tool to implement the new components of our MLOps pipeline to store models and reuse them for inference of data segments with similar data distributions in the future. Our evaluation results on four time series datasets demonstrate that our model reuse approach can maintain the performance of models while significantly reducing maintenance time and costs. Our model reuse approach achieves ML performance comparable to the best baseline, while being 15 times more efficient in terms of computation time and costs. Therefore, industries and practitioners can benefit from our approach and use our tool to maintain the performance of their ML models in the deployment phase to reduce their maintenance costs.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04657
Drift-cyclotron loss-cone instability in 3D simulations of a sloshing-ion simple mirror,['Plasma Physics'],"['Aaron Tran', 'Samuel J. Frank', 'Ari Y. Le', 'Adam J. Stanier', 'Blake A. Wetherton', 'Jan Egedal', 'Douglass A. Endrizzi', 'Robert W. Harvey', 'Yuri V. Petrov', 'Tony M. Qian', 'Kunal Sanwalka', 'Jesse Viola', 'Cary B. Forest', 'Ellen G. Zweibel']","The kinetic stability of collisionless, sloshing beam-ion (45° pitch angle) plasma is studied in a 3D simple magnetic mirror, mimicking the Wisconsin High-temperature superconductor Axisymmetric Mirror (WHAM) experiment. The collisional Fokker-Planck code CQL3D-m provides a slowing-down beam-ion distribution to initialize the kinetic-ion/fluid-electron code Hybrid-VPIC, which then simulates free plasma decay without external heating or fueling. Over 1-10 $μ$s, drift-cyclotron loss-cone (DCLC) modes grow and saturate in amplitude. DCLC scatters ions to a marginally-stable distribution with gas-dynamic rather than classical-mirror confinement. Sloshing ions can trap cool (low-energy) ions in an electrostatic potential well to stabilize DCLC, but DCLC itself does not scatter sloshing beam-ions into said well. Instead, cool ions must come from external sources such as charge-exchange collisions with a low-density neutral population. Manually adding cool ~1 keV ions improves beam-ion confinement ~2-5x in Hybrid-VPIC simulations, which qualitatively corroborates measurements from real mirror devices with sloshing ions.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04656
One Communication Round is All It Needs for Federated Fine-Tuning Foundation Models,['Machine Learning'],"['Ziyao Wang', 'Bowei Tian', 'Yexiao He', 'Zheyu Shen', 'Luyang Liu', 'Ang Li']","The recent advancement of large foundation models (FMs) has increased the demand for fine-tuning these models on large-scale and cross-domain datasets. To address this, federated fine-tuning has emerged as a solution, allowing models to be fine-tuned on distributed datasets across multiple devices while ensuring data privacy. However, the substantial parameter size of FMs and the multi-round communication required by traditional federated fine-tuning algorithms result in prohibitively high communication costs, challenging the practicality of federated fine-tuning. In this paper, we are the first to reveal, both theoretically and empirically, that the traditional multi-round aggregation algorithms may not be necessary for federated fine-tuning large FMs. Our experiments reveal that a single round of communication (i.e., one-shot federated fine-tuning) yields a global model performance comparable to that achieved through multiple rounds of communication. Through rigorous mathematical and empirical analyses, we demonstrate that large FMs, due to their extensive parameter sizes and pre-training on general tasks, achieve significantly lower training loss in one-shot federated fine-tuning compared to smaller models. Our extensive experiments show that one-shot federated fine-tuning not only reduces communication costs but also enables asynchronous aggregation, enhances privacy, and maintains performance consistency with multi-round federated fine-tuning for models larger than 1 billion parameters, on text generation and text-to-image generation tasks. Our findings have the potential to revolutionize federated fine-tuning in practice, enhancing efficiency, reducing costs, and expanding accessibility for large-scale models. This breakthrough paves the way for broader adoption and application of federated fine-tuning across various domains.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04650
Fluid-structure coupled simulation framework for lightweight explosion containment structures under large deformations,['Fluid Dynamics'],"['Aditya Narkhede', 'Shafquat Islam', 'Xingsheng Sun', 'Kevin Wang']","Lightweight, single-use explosion containment structures provide an effective solution for neutralizing rogue explosives, combining affordability with ease of transport. This paper introduces a three-stage simulation framework that captures the distinct physical processes and time scales involved in detonation, shock propagation, and large, plastic structural deformations. The hypothesis is that as the structure becomes lighter and more flexible, its dynamic interaction with the gaseous explosion products becomes increasingly significant. Unlike previous studies that rely on empirical models to approximate pressure loads, this framework employs a partitioned procedure to couple a finite volume compressible fluid dynamics solver with a finite element structural dynamics solver. The level set and embedded boundary methods are utilized to track the fluid-fluid and fluid-structure interfaces. The interfacial mass, momentum, and energy fluxes are computed by locally constructing and solving one-dimensional bi-material Riemann problems. A case study is presented involving a thin-walled steel chamber subjected to an internal explosion of $250~\text{g}$ TNT. The result shows a $30\%$ increase in the chamber volume due to plastic deformation, with its strains remaining below the fracture limit. Although the incident shock pulse carries the highest pressure, the subsequent pulses from wave reflections also contribute significantly to structural deformation. The high energy and compressibility of the explosion products lead to highly nonlinear fluid dynamics, with shock speeds varying across both space and time. Comparisons with simpler simulation methods reveal that decoupling the fluid and structural dynamics overestimates the plastic strain by $43.75\%$, while modeling the fluid dynamics as a transient pressure load fitted to the first shock pulse underestimates the plastic strain by $31.25\%$.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04647
Improving LLM Group Fairness on Tabular Data via In-Context Learning,['Machine Learning'],"['Valeriia Cherepanova', 'Chia-Jung Lee', 'Nil-Jana Akpinar', 'Riccardo Fogliato', 'Martin Andres Bertran', 'Michael Kearns', 'James Zou']","Large language models (LLMs) have been shown to be effective on tabular prediction tasks in the low-data regime, leveraging their internal knowledge and ability to learn from instructions and examples. However, LLMs can fail to generate predictions that satisfy group fairness, that is, produce equitable outcomes across groups. Critically, conventional debiasing approaches for natural language tasks do not directly translate to mitigating group unfairness in tabular settings. In this work, we systematically investigate four empirical approaches to improve group fairness of LLM predictions on tabular datasets, including fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning. Through experiments on four tabular datasets using both open-source and proprietary LLMs, we show the effectiveness of these methods in enhancing demographic parity while maintaining high overall performance. Our analysis provides actionable insights for practitioners in selecting the most suitable approach based on their specific requirements and constraints.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04642
Multi-Quantile Estimators for the parameters of Generalized Extreme Value distribution,['Methodology'],"['Sen Lin', 'Ao Kong', 'Robert Azencott']","We introduce and study Multi-Quantile estimators for the parameters $( ξ, σ, μ)$ of Generalized Extreme Value (GEV) distributions to provide a robust approach to extreme value modeling. Unlike classical estimators, such as the Maximum Likelihood Estimation (MLE) estimator and the Probability Weighted Moments (PWM) estimator, which impose strict constraints on the shape parameter $ξ$, our estimators are always asymptotically normal and consistent across all values of the GEV parameters. The asymptotic variances of our estimators decrease with the number of quantiles increasing and can approach the Cramér-Rao lower bound very closely whenever it exists. Our Multi-Quantile Estimators thus offer a more flexible and efficient alternative for practical applications. We also discuss how they can be implemented in the context of Block Maxima method.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04640
Motion-Guided Deep Image Prior for Cardiac MRI,['Medical Physics'],"['Marc Vornehm', 'Chong Chen', 'Muhammad Ahmad Sultan', 'Syed Murtaza Arshad', 'Yuchi Han', 'Florian Knoll', 'Rizwan Ahmad']","Cardiovascular magnetic resonance imaging is a powerful diagnostic tool for assessing cardiac structure and function. Traditional breath-held imaging protocols, however, pose challenges for patients with arrhythmias or limited breath-holding capacity. We introduce Motion-Guided Deep Image prior (M-DIP), a novel unsupervised reconstruction framework for accelerated real-time cardiac MRI. M-DIP employs a spatial dictionary to synthesize a time-dependent template image, which is further refined using time-dependent deformation fields that model cardiac and respiratory motion. Unlike prior DIP-based methods, M-DIP simultaneously captures physiological motion and frame-to-frame content variations, making it applicable to a wide range of dynamic applications. We validate M-DIP using simulated MRXCAT cine phantom data as well as free-breathing real-time cine and single-shot late gadolinium enhancement data from clinical patients. Comparative analyses against state-of-the-art supervised and unsupervised approaches demonstrate M-DIP's performance and versatility. M-DIP achieved better image quality metrics on phantom data, as well as higher reader scores for in-vivo patient data.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04639
Semantic Retrieval at Walmart,['Information Retrieval'],"['Alessandro Magnani', 'Feng Liu', 'Suthee Chaidaroon', 'Sachin Yadav', 'Praveen Reddy Suram', 'Ajit Puthenputhussery', 'Sijie Chen', 'Min Xie', 'Anirudh Kashi', 'Tony Lee', 'Ciya Liao']","In product search, the retrieval of candidate products before re-ranking is more critical and challenging than other search like web search, especially for tail queries, which have a complex and specific search intent. In this paper, we present a hybrid system for e-commerce search deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to better answer user tail queries. Our system significantly improved the relevance of the search engine, measured by both offline and online evaluations. The improvements were achieved through a combination of different approaches. We present a new technique to train the neural model at scale. and describe how the system was deployed in production with little impact on response time. We highlight multiple learnings and practical tricks that were used in the deployment of this system.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04637
Smallest totient in a residue class,['Number Theory'],['Abhishek Jha'],"We obtain a totient analogue for Linnik's theorem in arithmetic progressions. Specifically, for any coprime pair of positive integers $(m,a)$ such that $m$ is odd, there exists $n\le m^{2+o(1)}$ such that $\varphi(n)\equiv a\,\mathrm{mod}\,{m}$.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04632
Asymptotic compatibility of parametrized optimal design problems,['Optimization and Control'],"['Tadele Mengesha', 'Abner J. Salgado', 'Joshua M. Siktar']","We study optimal design problems where the design corresponds to a coefficient in the principal part of the state equation. The state equation, in addition, is parameter dependent, and we allow it to change type in the limit of this (modeling) parameter. We develop a framework that guarantees asymptotic compatibility, that is unconditional convergence with respect to modeling and discretization parameters to the solution of the corresponding limiting problems. This framework is then applied to two distinct classes of problems where the modeling parameter represents the degree of nonlocality. Specifically, we show unconditional convergence of optimal design problems when the state equation is either a scalar-valued fractional equation, or a strongly coupled system of nonlocal equations derived from the bond-based model of peridynamics.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04630
BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks,['Machine Learning'],"['Juan Rodriguez', 'Xiangru Jian', 'Siba Smarak Panigrahi', 'Tianyu Zhang', 'Aarash Feizi', 'Abhay Puri', 'Akshay Kalkunte', 'François Savard', 'Ahmed Masry', 'Shravan Nayak', 'Rabiul Awal', 'Mahsa Massoud', 'Amirhossein Abaskohi', 'Zichao Li', 'Suyuchen Wang', 'Pierre-André Noël', 'Mats Leon Richter', 'Saverio Vadacchino', 'Shubbam Agarwal', 'Sanket Biswas', 'Sara Shanian', 'Ying Zhang', 'Noah Bolger', 'Kurt MacDonald', 'Simon Fauvel']","Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04626
Mixed Delay/Nondelay Embeddings Based Neuromorphic Computing with Patterned Nanomagnet Arrays,['Emerging Technologies'],"['Changpeng Ti', 'Usman Hassan', 'Sairam Sri Vatsavai', 'Margaret McCarter', 'Aastha Vasdev', 'Jincheng An', 'Barat Achinuq', 'Ulrich Welp', 'Sen-Ching Cheung', 'Ishan G Thakkar', 'J. Todd Hastings']","Patterned nanomagnet arrays (PNAs) have been shown to exhibit a strong geometrically frustrated dipole interaction. Some PNAs have also shown emergent domain wall dynamics. Previous works have demonstrated methods to physically probe these magnetization dynamics of PNAs to realize neuromorphic reservoir systems that exhibit chaotic dynamical behavior and high-dimensional nonlinearity. These PNA reservoir systems from prior works leverage echo state properties and linear/nonlinear short-term memory of component reservoir nodes to map and preserve the dynamical information of the input time-series data into nondelay spatial embeddings. Such mappings enable these PNA reservoir systems to imitate and predict/forecast the input time series data. However, these prior PNA reservoir systems are based solely on the nondelay spatial embeddings obtained at component reservoir nodes. As a result, they require a massive number of component reservoir nodes, or a very large spatial embedding (i.e., high-dimensional spatial embedding) per reservoir node, or both, to achieve acceptable imitation and prediction accuracy. These requirements reduce the practical feasibility of such PNA reservoir systems. To address this shortcoming, we present a mixed delay/nondelay embeddings-based PNA reservoir system. Our system uses a single PNA reservoir node with the ability to obtain a mixture of delay/nondelay embeddings of the dynamical information of the time-series data applied at the input of a single PNA reservoir node. Our analysis shows that when these mixed delay/nondelay embeddings are used to train a perceptron at the output layer, our reservoir system outperforms existing PNA-based reservoir systems for the imitation of NARMA 2, NARMA 5, NARMA 7, and NARMA 10 time series data, and for the short-term and long-term prediction of the Mackey Glass time series data.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04622
Non-smoothness of Moduli Spaces of Higher Genus Curves on Low Degree Hypersurfaces,['Algebraic Geometry'],"['Matthew Hase-Liu', 'Amal Mattoo']",We show that the moduli space of degree $e$ maps from smooth genus $g \ge 1$ curves to an arbitrary low degree smooth hypersurface is singular when $e$ is large compared to $g$. We also give a lower bound for the dimension of the singular locus.△ Less,"5 December, 2024;",https://arxiv.org/pdf/2412.04618
Assessing and Learning Alignment of Unimodal Vision and Language Models,['Computer Vision and Pattern Recognition'],"['Le Zhang', 'Qian Yang', 'Aishwarya Agrawal']","How well are unimodal vision and language models aligned? Although prior work have approached answering this question, their assessment methods do not directly translate to how these models are used in practical vision-language tasks. In this paper, we propose a direct assessment method, inspired by linear probing, to assess vision-language alignment. We identify that the degree of alignment of the SSL vision models depends on their SSL training objective, and we find that the clustering quality of SSL representations has a stronger impact on alignment performance than their linear separability. Next, we introduce Swift Alignment of Image and Language (SAIL), a efficient transfer learning framework that aligns pretrained unimodal vision and language models for downstream vision-language tasks. Since SAIL leverages the strengths of pretrained unimodal models, it requires significantly fewer (6%) paired image-text data for the multimodal alignment compared to models like CLIP which are trained from scratch. SAIL training only requires a single A100 GPU, 5 hours of training and can accommodate a batch size up to 32,768. SAIL achieves 73.4% zero-shot accuracy on ImageNet (vs. CLIP's 72.7%) and excels in zero-shot retrieval, complex reasoning, and semantic segmentation. Additionally, SAIL improves the language-compatibility of vision encoders that in turn enhance the performance of multimodal large language models. The entire codebase and model weights are open-source: https://lezhang7.github.io/sail.github.io/△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04616
Which subsets and when orbits of non-uniformly hyperbolic systems prefer to visit: operator renewal theory approach,['Dynamical Systems'],"['Leonid A. Bunimovich', 'Yaofeng Su']","The paper addresses some basic questions in the theory of finite time dynamics and finite time predictions for non-uniformly hyperbolic dynamical systems. We are concerned with transport in phase spaces of such systems, and analyze which subsets and when the orbits prefer to visit. An asymptotic expansion of the decay of polynomial escape rates is obtained. Our approach is based on the construction of operator renewal equations for open dynamical systems and on their spectral analysis. In order to do this, we generalize the Keller-Liverani perturbation technique. Applications to a large class of one-dimensional non-uniformly expanding systems are considered.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04615
Quantum Interference of Force with Entangled Photons,['Quantum Physics'],"['Gabriela S. Militani', 'Artur Matoso', 'Denise F. Ávila', 'Raul Corrêa', 'Reinaldo O. Vianna', 'Pablo L. Saldanha', 'Sebastião Pádua']","In this work we experimentally demonstrate the quantum interference of force effect using pairs of entangled photons. Although photons are massless particles, they have linear momentum, and our experiments show that the quantum superposition of a positive momentum transfer with a null momentum transfer may result in a negative momentum transfer to an ensemble of quantum particles (photons), a behavior with no classical analogue. The momentum transfer to each photon is defined by the result of a polarization measurement performed in a second photon, initially entangled with it.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04613
The 100 pc White Dwarf Sample in the SDSS Footprint II. A New Look at the Spectral Evolution of White Dwarfs,['Solar and Stellar Astrophysics'],"['Mukremin Kilic', 'Pierre Bergeron', 'Simon Blouin', 'Adam Moss', 'Warren R. Brown', 'Antoine Bedard', 'Gracyn Jewett', 'Marcel A. Agueros']","We increase the spectroscopic completeness of the 100 pc white dwarf sample in the SDSS footprint with 840 additional spectra. Our spectroscopy is 86% complete for white dwarfs hotter than $T_{\rm eff}= 5000$ K, where H$α$ remains visible and provides reliable constraints on the atmospheric composition. We identify 2108 DA white dwarfs with pure hydrogen atmospheres, and show that ultramassive DA white dwarfs with $M\geq1.1~M_{\odot}$ are an order of magnitude less common below 10,000 K. This is consistent with a fraction of them getting stuck on the crystallization sequence due to $^{22}$Ne distillation. In addition, there are no ultramassive DA white dwarfs with $M\geq1.1~M_{\odot}$ and $T_{\rm eff}\leq6000$ K in our sample, likely because Debye cooling makes them rapidly fade away. We detect a significant trend in the fraction of He-atmosphere white dwarfs as a function of temperature; the fraction increases from 9% at 20,000 K to 32% at 6000 K. This provides direct evidence of convective mixing in cool DA white dwarfs. Finally, we detect a relatively tight sequence of low-mass DQ white dwarfs in color-magnitude diagrams for the first time. We discuss the implications of this tight DQ sequence, and conclude with a discussion of the future prospects from the upcoming ULTRASAT mission and the large-scale multi-fiber spectroscopic surveys.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04611
Rational magnetic equivariant K-theory,['K-Theory and Homology'],"['Higinio Serrano', 'Bernardo Uribe', 'Miguel A. Xicoténcatl']","We introduce the magnetic equivariant K-theory groups as the K-theory groups associated to magnetic groups and their respective magnetic equivariant complex bundles. We restrict the magnetic group to its subgroup of elements that act complex linearly, and we show that this restriction induces a rational isomorphism with the conjugation invariant part of the complex equivariant K-theory of the restricted group. This isomorphism allows to calculate the torsion free part of the magnetic equivariant K-theory groups reducing it to known calculations in complex equivariant K-theory△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04603
SDSS J100711.74+193056.2: A Candidate Common Motion Substellar Companion to the Nearest B-Type Star Regulus,['Solar and Stellar Astrophysics'],"['Eric E. Mamajek', 'Adam J. Burgasser']","The L9 dwarf SDSS J100711.74+193056.2 is situated 7$^{\circ}$.5 north of the nearest B-type star Regulus (d = 24.3+-0.2 pc), part of a stellar quadruplet. The object is at similar distance (d = 21.9+-1.0 pc) as Regulus, with a 3D separation of 3.9+0.6-0.5 pc ($\sim$1.6 tidal radii from Regulus), and shares tangential motion within 2 km/s, hinting at a physical connection. Near-infrared spectroscopy with Keck/NIRES finds that SDSS J100711.74+193056.2 also has a comparable radial velocity as Regulus A and B, a metallicity similar to Regulus B, and a spectral morphology consistent with the estimated 1--2 Gyr total age of Regulus's close pre-white dwarf companion. Taken together, these observations indicate that SDSS J100711.74+193056.2 is a very widely-separated and potentially physically-bound companion to Regulus, with a binding energy and mass ratio comparable to other wide star-brown dwarf systems. It joins a growing list of brown dwarfs at the L dwarf/T dwarf transition with independent constraints on physical properties such as age and metallicity.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04599
Chondrites as thermal and mechanical archives of accretion processes in the Solar protoplanetary disk,['Earth and Planetary Astrophysics'],"['Anthony Seret', 'Guy Libourel']","As some of the most ancient materials in our Solar System, chondritic meteorites offer a valuable window into the early stages of planetary formation, particularly the accretion processes that built the most primitive asteroids. Until now, high energy shocks and collisions have been invoked to explain the deformation and fragmentation of chondrules, the main component of chondrites. However, simulating the cooling of chondrules using continuum mechanics and finite elements, we demonstrate that plastic deformation of chondrules can occur at low collision velocities of just a few meters per second and with kinetic energies less than tenths of a millijoule when temperatures exceed the glass transition temperature Tg ~ 1000 K. Conversely, below Tg, spontaneous chondrule cracking occurs due to differential thermal contraction between phases and is more pronounced in larger chondrules. Counterintuitively, our findings suggest that both ordinary and carbonaceous chondrites formed through similar low-energy processes, with varying degrees of ductility and brittleness depending on the amount of processed material. This implies that the environments where chondrites formed were likely less turbulent and more thermally active than previously thought.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04598
New Ultracool Companions to Nearby White Dwarfs,['Solar and Stellar Astrophysics'],"['Alexia Bravo', 'Adam C. Schneider', 'Sarah Casewell', 'Austin Rothermich', 'Jacqueline K. Faherty', 'Jenni R. French', 'Thomas P. Bickle', 'Aaron M. Meisner', 'J. Davy Kirkpatrick', 'Marc J. Kuchner', 'Adam J. Burgasser', 'Federico Marocco', 'John H. Debes', 'Arttu Sainio', 'Léopold Gramaize', 'Frank Kiwy', 'Peter A. Jalowiczor', 'Awab Abdullahi']","We conducted a search for new ultracool companions to nearby white dwarfs using multiple methods, including the analysis of colors and examination of images in both the optical and the infrared. Through this process, we identified fifty-one previously unrecognized systems with candidate ultracool companions. Thirty-one of these systems are resolved in at least one catalog, and all but six are confirmed as co-moving companions via common proper motion and consistent parallax measurements (when available). We have followed up four co-moving companions with near-infrared spectroscopy and confirm their ultracool nature. The remaining twenty candidates are unresolved, but show clear signs of infrared excess which is most likely due to the presence of a cold, low-mass companion or a dusty circumstellar disk. Three of these unresolved systems have existing optical spectra that clearly show the presence of a cool stellar companion to the white dwarf primary via spectral decomposition. These new discoveries, along with our age estimates for the primary white dwarfs, will serve as valuable benchmark systems for future characterization of ultracool dwarfs.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04597
Nonlinear Operator Learning Using Energy Minimization and MLPs,['Machine Learning'],"['Mats G. Larson', 'Carl Lundholm', 'Anna Persson']","We develop and evaluate a method for learning solution operators to nonlinear problems governed by partial differential equations. The approach is based on a finite element discretization and aims at representing the solution operator by an MLP that takes latent variables as input. The latent variables will typically correspond to parameters in a parametrization of input data such as boundary conditions, coefficients, and right-hand sides. The loss function is most often an energy functional and we formulate efficient parallelizable training algorithms based on assembling the energy locally on each element. For large problems, the learning process can be made more efficient by using only a small fraction of randomly chosen elements in the mesh in each iteration. The approach is evaluated on several relevant test cases, where learning the solution operator turns out to be beneficial compared to classical numerical methods.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04596
Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors,['Machine Learning'],"['Putri A. van der Linden', 'Alejandro García-Castellanos', 'Sharvaree Vadgama', 'Thijs P. Kuipers', 'Erik J. Bekkers']","Group equivariance has emerged as a valuable inductive bias in deep learning, enhancing generalization, data efficiency, and robustness. Classically, group equivariant methods require the groups of interest to be known beforehand, which may not be realistic for real-world data. Additionally, baking in fixed group equivariance may impose overly restrictive constraints on model architecture. This highlights the need for methods that can dynamically discover and apply symmetries as soft constraints. For neural network architectures, equivariance is commonly achieved through group transformations of a canonical weight tensor, resulting in weight sharing over a given group $G$. In this work, we propose to learn such a weight-sharing scheme by defining a collection of learnable doubly stochastic matrices that act as soft permutation matrices on canonical weight tensors, which can take regular group representations as a special case. This yields learnable kernel transformations that are jointly optimized with downstream tasks. We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions. Additionally, the flexibility of the method enables it to effectively pick up on partial symmetries.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04594
EgoPoints: Advancing Point Tracking for Egocentric Videos,['Computer Vision and Pattern Recognition'],"['Ahmad Darkhalil', 'Rhodri Guerrier', 'Adam W. Harley', 'Dima Damen']","We introduce EgoPoints, a benchmark for point tracking in egocentric videos. We annotate 4.7K challenging tracks in egocentric sequences. Compared to the popular TAP-Vid-DAVIS evaluation benchmark, we include 9x more points that go out-of-view and 59x more points that require re-identification (ReID) after returning to view. To measure the performance of models on these challenging points, we introduce evaluation metrics that specifically monitor tracking performance on points in-view, out-of-view, and points that require re-identification. We then propose a pipeline to create semi-real sequences, with automatic ground truth. We generate 11K such sequences by combining dynamic Kubric objects with scene points from EPIC Fields. When fine-tuning point tracking methods on these sequences and evaluating on our annotated EgoPoints sequences, we improve CoTracker across all metrics, including the tracking accuracy $δ^\star_{\text{avg}}$ by 2.7 percentage points and accuracy on ReID sequences (ReID$δ_{\text{avg}}$) by 2.4 points. We also improve $δ^\star_{\text{avg}}$ and ReID$δ_{\text{avg}}$ of PIPs++ by 0.3 and 2.8 respectively.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04592
Generating graph states with a single quantum emitter and the minimum number of fusions,['Quantum Physics'],"['Matthias C. Löbl', 'Love A. Pettersson', 'Andrew Jena', 'Luca Dellantonio', 'Stefano Paesani', 'Anders S. Sørensen']","Graph states are the key resources for measurement- and fusion-based quantum computing with photons, yet their creation is experimentally challenging. We optimize a hybrid graph-state generation scheme using a single quantum emitter and linear optics Bell-state measurements, called fusions. We first generate a restricted class of states from a single quantum emitter and then apply fusions to create a target graph state, where we use a dynamic programming approach to find the construction that requires the lowest possible number of fusions. Our analysis yields a lookup table for constructing $\sim 2.8\times 10^7$ non-isomorphic graph states with the minimum number of fusions. The lookup table covers all graph states with up to eight qubits and several other ones with up to 14 qubits. We present construction protocols of selected graph states and provide the lookup table. For large graph states that are not in the lookup table, we derive bounds for the required number of fusions using graph-theoretic properties. Finally, we use the lookup table to search for the best graph codes for loss-tolerant encodings, given a fixed number of fusions for their construction.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04587
The relevance of higher-order ties,['Physics and Society'],"['Alberto Ceria', 'Frank W. Takes']","Higher-order networks effectively represent complex systems with group interactions. Existing methods usually overlook the relative contribution of group interactions (hyperlinks) of different sizes to the overall network structure. Yet, this has many important applications, especially when the network has meaningful node labels. In this work, we propose a comprehensive methodology to precisely measure the contribution of different orders to the overall network structure. First, we propose the order contribution measure, which quantifies the contribution of hyperlinks of different orders to the link weights (local scale), number of triangles (mesoscale) and size of the largest connected component (global scale) of the pairwise weighted network. Second, we propose the measure of order relevance, which gives insights in how hyperlinks of different orders contribute to the considered network property. Most interestingly, it enables an assessment of whether this contribution is synergistic or redundant with respect to that of hyperlinks of other orders. Third, to account for labels, we propose a metric of label group balance to assess how hyperlinks of different orders connect label-induced groups of nodes. We applied these metrics to a large-scale board interlock network and scientific collaboration network, in which node labels correspond to geographical location of the nodes. Experiments including a comparison with randomized null models reveal how from the global level perspective, we observe synergistic contributions of orders in the board interlock network, whereas in the collaboration network there is more redundancy. The findings shed new light on social scientific debates on the role of busy directors in global business networks and the connective effects of large author teams in scientific collaboration networks.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04584
Analytic solutions for Vlasov equations with nonlinear zero-moment dependence,['Analysis of PDEs'],"['Nuno J. Alves', 'Peter Markowich', 'Athanasios E. Tzavaras']","We consider nonlinear Vlasov-type equations involving powers of the zero-order moment and obtain a local existence and uniqueness result within a framework of analytic functions. The proof employs a Banach fixed point argument, where a contraction mapping is built upon the solutions of a corresponding linearized problem. At a formal level, the considered nonlinear kinetic equations are derived from a generalized Vlasov-Poisson type equation under zero-electron-mass and quasi-neutrality assumptions, and are related to compressible Euler equations through monokinetic distributions.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04581
"Data-Driven, Parameterized Reduced-order Models for Predicting Distortion in Metal 3D Printing",['Machine Learning'],"['Indu Kant Deo', 'Youngsoo Choi', 'Saad A. Khairallah', 'Alexandre Reikher', 'Maria Strantza']","In Laser Powder Bed Fusion (LPBF), the applied laser energy produces high thermal gradients that lead to unacceptable final part distortion. Accurate distortion prediction is essential for optimizing the 3D printing process and manufacturing a part that meets geometric accuracy requirements. This study introduces data-driven parameterized reduced-order models (ROMs) to predict distortion in LPBF across various machine process settings. We propose a ROM framework that combines Proper Orthogonal Decomposition (POD) with Gaussian Process Regression (GPR) and compare its performance against a deep-learning based parameterized graph convolutional autoencoder (GCA). The POD-GPR model demonstrates high accuracy, predicting distortions within $\pm0.001mm$, and delivers a computational speed-up of approximately 1800x.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04577
Give me Some Hard Questions: Synthetic Data Generation for Clinical QA,['Computation and Language'],"['Fan Bai', 'Keith Harrigian', 'Joel Stremmel', 'Hamid Hassanzadeh', 'Ardavan Saeedi', 'Mark Dredze']","Clinical Question Answering (QA) systems enable doctors to quickly access patient information from electronic health records (EHRs). However, training these systems requires significant annotated data, which is limited due to the expertise needed and the privacy concerns associated with clinical data. This paper explores generating Clinical QA data using large language models (LLMs) in a zero-shot setting. We find that naive prompting often results in easy questions that do not reflect the complexity of clinical scenarios. To address this, we propose two prompting strategies: 1) instructing the model to generate questions that do not overlap with the input context, and 2) summarizing the input record using a predefined schema to scaffold question generation. Experiments on two Clinical QA datasets demonstrate that our method generates more challenging questions, significantly improving fine-tuning performance over baselines. We compare synthetic and gold data and find a gap between their training efficacy resulting from the quality of synthetically generated answers.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04573
An open-source library for performance-portable neutrino reaction rates: application to neutron star mergers,['High Energy Astrophysical Phenomena'],"['Leonardo Chiesa', 'Maitraya Bhattacharyya', 'Filippo Mazzini', 'Federico Maria Guercilena', 'Albino Perego', 'David Radice']","A realistic and detailed description of neutrinos in binary neutron star (BNS) mergers is essential to build reliable models of such systems. To this end, we present BNS_NURATES, a novel open-source numerical library designed for the efficient on-the-fly computation of neutrino interactions, with particular focus on regimes relevant to BNS mergers. BNS_NURATES targets an higher level of accuracy and realism in the implementation of commonly employed reactions by accounting for relevant microphysics effects on the interactions, such as weak magnetism and mean field effects. It also includes the contributions of inelastic neutrino scattering off electrons and positrons and (inverse) nucleon decays. Finally, it offers a way to reconstruct the neutrino distribution function in the framework of moment-based transport schemes. As a first application, we compute both energy-dependent and energy-integrated neutrino emissivities and opacities for conditions extracted from a BNS merger simulation with M1 transport scheme. We find some qualitative differences in the results when considering the impact of the additional relevant reactions and of microphysics effects. For example, neutrino-electron/positron scattering reactions are important for the energy exchange of heavy-type neutrinos as they do not undergo semi-leptonic charged-current processes, when $μ^\pm$ are not accounted for. Moreover, weak magnetism and mean field effects can significantly modify the contribution of $β$-processes for electron-type (anti)neutrinos, increasing at the same time the importance of (inverse) neutron decays. The improved treatment for the reaction rates also modify the conditions at which neutrinos decouple from matter in the system, potentially affecting their emission spectra.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04570
Towards Performance-Aware Allocation for Accelerated Machine Learning on GPU-SSD Systems,['Hardware Architecture'],"['Ayush Gundawar', 'Euijun Chung', 'Hyesoon Kim']","The exponential growth of data-intensive machine learning workloads has exposed significant limitations in conventional GPU-accelerated systems, especially when processing datasets exceeding GPU DRAM capacity. We propose MQMS, an augmented in-storage GPU architecture and simulator that is aware of internal SSD states and operations, enabling intelligent scheduling and address allocation to overcome performance bottlenecks caused by CPU-mediated data access patterns. MQMS introduces dynamic address allocation to maximize internal parallelism and fine-grained address mapping to efficiently handle small I/O requests without incurring read-modify-write overheads. Through extensive evaluations on workloads ranging from large language model inference to classical machine learning algorithms, MQMS demonstrates orders-of-magnitude improvements in I/O request throughput, device response time, and simulation end time compared to existing simulators.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04569
Solving High-dimensional Inverse Problems Using Amortized Likelihood-free Inference with Noisy and Incomplete Data,['Machine Learning'],"['Jice Zeng', 'Yuanzhe Wang', 'Alexandre M. Tartakovsky', 'David Barajas-Solano']","We present a novel likelihood-free probabilistic inversion method based on normalizing flows for high-dimensional inverse problems. The proposed method is comprised of two complementary networks: a summary network for data compression, and an inference network for parameter estimation. The summary network encodes raw observations into a fixed-size vector of summary statistics, while the inference network generates samples of the approximate posterior distribution of the model parameters based on these summary statistics. The posterior samples are produced in a deep generative fashion by sampling from a latent Gaussian distribution and passing these samples through an invertible transformation. We construct this invertible transformation by sequentially alternating conditional invertible neural network (cINN) and conditional neural spline flow (cNSF) layers. The summary and inference networks are trained simultaneously. We apply the proposed method to an inversion problem in groundwater hydrology to estimate the posterior distribution of the system's log-conductivity field conditioned on spatially sparse time-series observations of the system's hydraulic head responses. The conductivity field is represented with 706 degrees of freedom in the considered problem. The comparison with the likelihood-based iterative ensemble smoother PEST-IES method demonstrates that the proposed method accurately estimates the parameter posterior distribution and the observations' predictive posterior distribution at a fraction of the inference time of PEST-IES.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04565
Performance Evaluation of LoRa Technology for Rural Connectivity: An Experimental Analysis in Nepal,['Networking and Internet Architecture'],"['Atit Pokharel', 'Pratik Sapkota', 'Dilip Sapkota', 'Shashank Dahal', 'Sulav Karki', 'Ram Kaji Budhathoki']","LoRa technology has garnered significant interest in the Information and Communications Technology (ICT) field in recent years due to its ability to operate at low power while maintaining effective communication. Despite gaining attention, LoRa technology faces challenges in effectively facilitating communication in rural settings due to specific transmission and reception conditions. This research paper provides an in-depth analysis of using a LoRa mesh network that accesses the performance of different LoRa configurations by varying parameters like Bandwidth (BW), Spreading Factor (SF), and Coding Rate (CR). Metrics, like the Received Signal Strength Indicator (RSSI), Signal-Noise Ratio (SNR), and packet loss, are analyzed to check the optimal configurations for LoRa nodes, specifically in the context of rural areas of Nepal. Furthermore, the varying propagation loss concerning the change in physical layer parameters is also discussed. The experimental setup utilizes Arduino Uno and ESP 32 microcontroller boards with LoRa modules to build the transmitter and receiver nodes, which are paired with a self-constructed monopole antenna, showing superior gain compared to commercially available options. This paper also explores the potential of integrating the acquired data with cloud platforms such as ThingSpeak. This integration establishes a strong backbone for the Internet of Things (IoT), which can gather and analyze remote data, providing the capacity for remote access to the data. This paper finally recommends specific values for the examined parameters for the specific case of a particular type of hilly and mountainous terrain in a country like Nepal, keeping in mind the unique trade-offs each one offers, thereby enabling optimal rural wireless communication.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04563
"Differential operators, anisotropy, and simplicial spheres",['Combinatorics'],"['Kalle Karu', 'Matt Larson', 'Alan Stapledon']",We find identities involving differential operators in the generic artinian reduction of the Stanley-Reisner ring of a simplicial sphere in any positive characteristic. These identities generalize the characteristic 2 identities used by Papadakis and Petrotou to give a proof of the algebraic g-conjecture. We use these identities to prove the anisotropy of certain forms on the generic artinian reduction of the Stanley-Reisner ring and to prove weak Lefschetz results.△ Less,"5 December, 2024;",https://arxiv.org/pdf/2412.04561
X-raying CAMELS: Constraining Baryonic Feedback in the Circum-Galactic Medium with the CAMELS simulations and eRASS X-ray Observations,['Astrophysics of Galaxies'],"['Erwin T. Lau', 'Daisuke Nagai', 'Ákos Bogdán', 'Isabel Medlock', 'Benjamin D. Oppenheimer', 'Nicholas Battaglia', 'Daniel Anglés-Alcázar', 'Shy Genel', 'Yueying Ni', 'Francisco Villaescusa-Navarro']","The circumgalactic medium (CGM) around massive galaxies plays a crucial role in regulating star formation and feedback. Using the CAMELS simulation suite, we develop emulators for the X-ray surface brightness profile and the X-ray luminosity--stellar mass scaling relation to investigate how stellar and AGN feedback shape the X-ray properties of the hot CGM. Our analysis shows that at CGM scales ($10^{12} \lesssim M_{\rm halo}/M_\odot \lesssim 10^{13}$, $10\lesssim r/{\rm kpc} \lesssim 400$), stellar feedback more significantly impacts the X-ray properties than AGN feedback within the parameters studied. Comparing the emulators to recent eROSITA All-Sky Survey observations, it was found that stronger feedback than currently implemented in the IllustrisTNG, SIMBA, and Astrid simulations is required to match observed CGM properties. However, adopting these enhanced feedback parameters causes deviations in the stellar-mass-halo-mass relations from observational constraints below the group mass scale. This tension suggests possible unaccounted systematics in X-ray CGM observations or inadequacies in the feedback models of cosmological simulations.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04559
An unambiguous AGN and a Balmer break in an Ultraluminous Little Red Dot at z=4.47 from Ultradeep UNCOVER and All the Little Things Spectroscopy,['Astrophysics of Galaxies'],"['Ivo Labbe', 'Jenny E. Greene', 'Jorryt Matthee', 'Helena Treiber', 'Vasily Kokorev', 'Tim B. Miller', 'Ivan Kramarenko', 'David J. Setton', 'Yilun Ma', 'Andy D. Goulding', 'Rachel Bezanson', 'Rohan P. Naidu', 'Christina C. Williams', 'Hakim Atek', 'Gabriel Brammer', 'Sam E. Cutler', 'Iryna Chemerynska', 'Aidan P. Cloonan', 'Pratika Dayal', 'Anna de Graaff', 'Yoshinobu Fudamoto', 'Seiji Fujimoto', 'Lukas J. Furtak', 'Karl Glazebrook', 'Kasper E. Heintz']","We present a detailed exploration of the most optically-luminous Little Red Dot ($L_{Hα}=10^{44}$erg/s, $L_V=10^{45}$erg/s, F444W=22AB) found to date. Located in the Abell 2744 field, source A744-45924 was observed by NIRSpec/PRISM with ultradeep spectroscopy reaching SNR$\sim$100pix$^{-1}$, high-resolution 3-4 micron NIRCam/Grism spectroscopy, and NIRCam Medium Band imaging. The NIRCam spectra reveal high rest-frame EW $W_{Hα,0,broad}>800$Å, broad H$α$ emission (FWHM$\sim$4500 km/s), on top of narrow, complex absorption. NIRSpec data show exceptionally strong rest-frame UV to NIR Fe II emission ($W_{FeII-UV,0}\sim$340Å), N IV]$λλ$1483,1486 and N III]$λ$1750, and broad NIR O I $λ$8446 emission. The spectra unambiguously demonstrate a broad-line region associated with an inferred $M_{BH}\sim10^9M_\odot$ supermassive black hole embedded in dense gas, which might explain a non-detection in ultradeep Chandra X-ray data (>$10\times$ underluminous relative to broad $L_{Hα}$). Strong UV Nitrogen lines suggest supersolar N/O ratios due to rapid star formation or intense radiation near the AGN. The continuum shows a clear Balmer break at rest-frame 3650Å, which cannot be accounted for by an AGN power-law alone. A stellar population model produces an excellent fit with a reddened Balmer break and implying a massive ($M_*\sim8\times10^{10}M_\odot$), old $\sim$500 Myr, compact stellar core, among the densest stellar systems known ($ρ\sim3\times10^6M_\odot$/pc$^2$ for $R_{e,opt}=70\pm10$ pc), and AGN emission with extreme intrinsic EW $W_{Hα,0}\gg$1000Å. However, although high $M_*$ and $M_{BH}$ are supported by evidence of an overdensity containing 40 galaxies at $z=4.41-4.51$, deep high-resolution spectroscopy is required to confirm stellar absorption and rule out that dense gas around the AGN causes the Balmer break instead.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04557
Near-optimal pure state estimation with adaptive Fisher-symmetric measurements,['Quantum Physics'],"['C. Vargas', 'L. Pereira', 'A. Delgado']","Quantum state estimation is important for various quantum information processes, including quantum communications, computation, and metrology, which require the characterization of quantum states for evaluation and optimization. We present a three-stage adaptive method for estimating $d$-dimensional pure quantum states using Fisher symmetric measurements (FSM) and a single-shot measurement basis. The result of this measurement is used to generate two FSMs that jointly estimate any pure state up to a null measure set. This estimate is used to adapt a third FMS, which provides the final estimate of the unknown state. Our approach achieves an average estimation infidelity very close to the Gill-Massar lower bound (GMB) without requiring prior information beyond the purity of the unknown state, extending the applicability of FSM to any unknown state. The total number of measurement outcomes of the method scale linearly as $7d-3$, avoiding the need for collective measurements on multiple copies of the unknown state. This work highlights the potential of adaptive estimation techniques in quantum state characterization while maintaining efficiency in the number of measurement outcomes.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04555
On the signature of black holes on the quenched stellar mass function,['Astrophysics of Galaxies'],"['Antonio J. Porras-Valverde', 'John C. Forbes']","As star-forming galaxies approach or exceed a stellar mass around $10^{11} M_\odot$, they are increasingly likely to be quenched in a process generically called mass quenching. Central galaxies, which are quenched via mass rather than environmental quenching, therefore accumulate in a peak around this characteristic mass. While a number of processes may influence the shape of the quenched central stellar mass function (QCSMF), we find that its low-mass slope is strongly affected by the scatter in the mass of black holes at a given stellar mass, with higher scatters in the black hole population yielding shallower slopes. Higher scatters in the black hole mass spread out the stellar mass range over which quenching occurs, leading to shallower slopes. This trend holds across a variety of semi-analytic models and cosmological hydrodynamic simulations. A comparison with observations provides indirect evidence for a large scatter in black hole mass $σ(\log_{10}(M_\mathrm{BH})|M_*) \gtrsim 0.5$ dex, and a joint constraint on AGN feedback physics and the co-evolution of galaxies and black holes.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04553
True mass and atmospheric composition of the non-transiting hot Jupiter HD 143105 b,['Earth and Planetary Astrophysics'],"['Luke Finnerty', 'Yinzi Xin', 'Jerry W. Xuan', 'Julie Inglis', 'Michael P Fitzgerald', 'Shubh Agrawal', 'Ashley Baker', 'Geoffrey A. Blake', 'Benjamin Calvin', 'Sylvain Cetre', 'Jacques-Robert Delorme', 'Greg Doppman', 'Daniel Echeverri', 'Katelyn Horstman', 'Chih-Chun Hsu', 'Nemanja Jovanovic', 'Joshua Liberman', 'Ronald A. López', 'Emily C. Martin', 'Dimitri Mawet', 'Evan Morris', 'Jacklyn Pezzato-Rovner', 'Jean-Baptiste Ruffio', 'Ben Sappey', 'Tobias Schofield']","We present Keck/KPIC phase II $K$-band observations of the non-transiting hot Jupiter HD 143105 b. Using a cross-correlation approach, we make the first detection of the planetary atmosphere at $K_p = 185^{+11}_{-13}\rm km\ s^{-1}$ and an inferior conjunction time 2.5 hours before the previously-published ephemeris. The retrieved $K_p$ value, in combination with orbital period, mass of the host star, and lack of transit detection, gives an orbital inclination of $78^{\circ+2}_{-12}$ and a true planet mass of 1.23$\pm0.10\rm\ M_J$. While the equilibrium temperature of HD 143105 b is in the transition regime between non-inverted and inverted atmospheres, our analysis strongly prefers a non-inverted atmosphere. Retrieval analysis indicates the atmosphere of HD 143105 b is cloud-free to approximately 1 bar and dominated by H$_2$O absorption ($\log \rm H_2O_{MMR} = -3.9^{+0.8}_{-0.5}$), placing only an upper limit on the CO abundance ($\log \rm CO_{MMR} < -3.7$ at 95% confidence). We place no constraints on the abundances of Fe, Mg, or $^{13}$CO. From these abundances, we place an upper limit on the carbon-to-oxygen ratio for HD 143105 b, $\rm C/O < 0.2$ at 95% confidence, and find the atmospheric metallicity is approximately $0.1\times$ solar. The low metallicity may be responsible for the lack of a thermal inversion, which at the temperature of HD 143105 b would likely require significant opacity from TiO and/or VO. With these results, HD 143105 b joins the small number of non-transiting hot Jupiters with detected atmospheres.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04552
Recent Progress in Flavor Model Building,['High Energy Physics - Phenomenology'],"['Wolfgang Altmannshofer', 'Admir Greljo']","The flavor puzzles remain among the most compelling open questions in particle physics. The striking hierarchies observed in the masses and mixing of charged fermions define the Standard Model (SM) flavor puzzle, a profound structural enigma pointing to physics beyond the SM. Simultaneously, the absence of deviations from SM predictions in precision measurements of flavor-changing neutral currents imposes severe constraints on new physics at the TeV scale, giving rise to the new physics flavor puzzle. This review article provides an overview of a selection of recent advancements in flavor model building, with a particular focus on attempts to address one or both of these puzzles within the quark sector.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04549
Designing Flat Bands and Pseudo-Landau Levels in GaAs with Patterned Gates,['Mesoscale and Nanoscale Physics'],"['Pierre A. Pantaleon', 'Zhen Zhan', 'S. Morales', 'Gerardo G. Naumis']","We investigate the electronic properties of two-dimensional electron gases (2DEGs) subjected to a periodic patterned gate. By incorporating the superlattice potential (SL) induced by patterning into the Schrodinger equation, we develop a methodology for obtaining exact analytical solutions. These solutions enable us to construct a comprehensive phase diagram illustrating the emergence of narrow bands and pseudo-Landau levels driven by the SL potential. To complement the analytical approach, we employ a standard plane-wave formalism to track the evolution of the band structure as the SL strength increases. Furthermore, we introduce a self-consistent Hartree screening to account for the interplay between the SL potential and electronic interactions. Our findings not only reveal a competition between SL strength and electron-electron interactions, leading to a reduction in the effective potential, but also highlight the value of exact analytical solutions for understanding and engineering electronic phases in patterned 2DEG systems.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04547
Ab initio calculations of overlap integrals for $μ\to e$ conversion in nuclei,['Nuclear Theory'],"['Matthias Heinz', 'Martin Hoferichter', 'Takayuki Miyagi', 'Frederic Noël', 'Achim Schwenk']","In an effective-field-theory approach, the rate for $μ\to e$ conversion in nuclei depends on a set of effective operators mediating the lepton-flavor-violating interaction at a high scale, renormalization group corrections that describe the evolution to lower scales, and hadronic and nuclear matrix elements that turn quark-level interactions into hadronic ones and then embed the latter into nuclear responses. In particular, combining information from $μ\to eγ$, $μ\to 3e$, and $μ\to e $ conversion in nuclei, it becomes possible to disentangle different underlying sources of lepton flavor violation. However, to assess the discriminatory power it is critical that uncertainties at each step of the analysis be controlled and fully quantified. In this regard, nuclear response functions related to the coupling to neutrons are notoriously problematic, since they are not directly constrained by experiment. We address these shortcomings by combining ab initio calculations with a recently improved determination of charge distributions from electron scattering by exploiting strong correlations among charge, point-proton, and point-neutron radii and densities. We present overlap integrals for $^{27}$Al, $^{48}$Ca, and $^{48}$Ti including full covariance matrices, allowing, for the first time, for a comprehensive consideration of nuclear structure uncertainties in the interpretation of $μ\to e$ experiments.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04545
Spin dynamics of an easy-plane Dirac spin liquid in a frustrated XY model: Application to honeycomb cobaltates,['Strongly Correlated Electrons'],"['Anjishnu Bose', 'Arun Paramekanti']","Recent work has shown that the honeycomb lattice spin-$1/2$ $J_1$-$J_3$ XY model, with nearest-neighbor ferromagnetic exchange $J_1$ and frustration induced by third-neighbor antiferromagnetic exchange $J_3$, may be relevant to a wide range of cobaltate materials. We explore a variational Monte Carlo study of Gutzwiller projected wavefunctions for this model and show that an easy-plane Dirac spin liquid (DSL) is a viable `parent' state for the competing magnetic orders observed in these materials, including ferromagnetic, zig-zag, spiral, and double zig-zag orders at intermediate frustration, and show that such broken symmetry states can be easily polarized by a weak in-plane magnetic field consistent with experiments. We formulate a modified parton theory for such frustrated spin models, and explore the potential instabilities of the DSL due to residual parton interactions within a random phase approximation (RPA), both at zero magnetic field and in a nonzero in-plane field. The broken symmetry states which emerge in the vicinity of this Dirac spin liquid include ferromagnetic, zig-zag, and incommensurate spiral orders, with a phase diagram which is consistent with VMC and density matrix renormalization group studies. We calculate the dynamical spin response of the easy-plane DSL, including RPA corrections, near the boundary of the ordered states, and present results for THz spectroscopy and inelastic neutron scattering, at zero field as well as in an in-plane magnetic field, and discuss experimental implications.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04544
Going from 3D common-envelope simulations to fast 1D simulations,['Solar and Stellar Astrophysics'],"['V. A. Bronner', 'F. R. N. Schneider', 'Ph. Podsiadlowski', 'F. K. Roepke']","One-dimensional (1D) methods for simulating the common-envelope (CE) phase offer advantages over three-dimensional (3D) simulations regarding their computational speed and feasibility. We present the 1D CE method from Bronner et al. (2024), including the results of the CE simulations of an asymptotic giant branch star donor. We further test this method in the massive star regime by computing the CE event of a red supergiant with a neutron-star mass and a black-hole mass companion. The 1D model can reproduce the orbital evolution and the envelope ejection from 3D simulations when choosing suitable values for the free parameters in the model. The best-fitting values differ from the expectations based on the low mass simulations, indicating that the free parameters depend on the structure of the giant star. The released recombination energy from hydrogen and helium helps to expand the envelope, similar to the low-mass CE simulations.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04543
Efficient Ionizers with Low H$\boldsymbolβ$+[OIII] Equivalent Widths: JADES Spectroscopy of a Peculiar High-z Population,['Astrophysics of Galaxies'],"['Isaac H. Laseter', 'Michael V. Maseda', 'Charlotte Simmonds', 'Ryan Endsley', 'Daniel Stark', 'Andrew J. Bunker', 'Rachana Bhatawdekar', 'Kristan Boyett', 'Alex J. Cameron', 'Stefano Carniani', 'Mirko Curti', 'Zhiyuan Ji', 'Pierluigi Rinaldi', 'Aayush Saxena', 'Sandro Tacchella', 'Chris Willott', 'Joris Witstok', 'Yongda Zhu']","Early JWST photometric studies discovered a population of UV faint ($\rm <L^{*}_{UV}$) $z \sim 6.5-8$ Lyman break galaxies with spectral energy distributions implying young ages ($\sim10$ Myr) yet relatively weak H$β$+[OIII] equivalent widths ($\rm EW_{Hβ+[OIII]} \approx 400$Å). These galaxies seemingly contradict the implicit understanding that young star-forming galaxies are ubiquitously strong H$β$+[OIII] emitters, i.e., extreme emission line galaxies (EW $\rm \gtrsim 750$Å). Low metallicities, high Lyman continuum escape fractions, and rapidly declining star-formation histories have been proposed as primary drivers behind low H$β$+[OIII] equivalent widths, but the blend of H$β$+[OIII] in photometric studies makes proving one of these scenarios difficult. We aim to characterize this peculiar population with deep spectroscopy from the JWST Advanced Deep Extragalactic Survey (JADES). We find that a significant subset of these galaxies at $z\gtrsim2$ with modest H$β$+[OIII] equivalent widths ($\rm \approx 300-600$Å) have high ionization efficiencies ($\rm \log ξ_{ion} \gtrsim 25.5~[Hz~erg^{-1}]$). Suppressed [OIII] EW values yet elevated H$α$ and H$β$ EW values imply that the level of chemical enrichment is the primary culprit, supported by spectroscopic measurements of metallicities below 12+log(O/H)$\rm \approx 7.70~(10\%Z_{\odot})$. We demonstrate that integrated H$β$+[OIII] selections (e.g., H$β$+[OIII] EW $> 700$Å) exclude the most metal-poor efficient ionizers and favor 1) more chemically enriched systems with comparable extreme radiation fields and 2) older starbursting systems. In contrast, metallicity degeneracies are reduced in H$α$ space, enabling the identification of these metal-poor efficient ionizers by their specific star-formation rate.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04542
EMPRESS. X. Spatially resolved mass-metallicity relation in extremely metal-poor galaxies: evidence of episodic star-formation fueled by a metal-poor gas infall,['Astrophysics of Galaxies'],"['Kimihiko Nakajima', 'Masami Ouchi', 'Yuki Isobe', 'Yi Xu', 'Shinobu Ozaki', 'Tohru Nagao', 'Akio K. Inoue', 'Michael Rauch', 'Haruka Kusakabe', 'Masato Onodera', 'Moka Nishigaki', 'Yoshiaki Ono', 'Yuma Sugahara', 'Takashi Hattori', 'Yutaka Hirai', 'Takuya Hashimoto', 'Ji Hoon Kim', 'Takashi J. Moriya', 'Hiroto Yanagisawa', 'Shohei Aoyama', 'Seiji Fujimoto', 'Hajime Fukushima', 'Keita Fukushima', 'Yuichi Harikane', 'Shun Hatano']","Using the Subaru/FOCAS IFU capability, we examine the spatially resolved relationships between gas-phase metallicity, stellar mass, and star-formation rate surface densities (Sigma_* and Sigma_SFR, respectively) in extremely metal-poor galaxies (EMPGs) in the local universe. Our analysis includes 24 EMPGs, comprising 9,177 spaxels, which span a unique parameter space of local metallicity (12+log(O/H) = 6.9 to 7.9) and stellar mass surface density (Sigma_* ~ 10^5 to 10^7 Msun/kpc^2), extending beyond the range of existing large integral-field spectroscopic surveys. Through spatially resolved emission line diagnostics based on the [NII] BPT-diagram, we verify the absence of evolved active galactic nuclei in these EMPGs. Our findings reveal that, while the resolved mass-metallicity relation exhibits significant scatter in the low-mass regime, this scatter is closely correlated with local star-formation surface density. Specifically, metallicity decreases as Sigma_SFR increases for a given Sigma_*. Notably, half of the EMPGs show a distinct metal-poor horizontal branch on the resolved mass-metallicity relation. This feature typically appears at the peak clump with the highest Sigma_* and Sigma_SFR and is surrounded by a relatively metal-enriched ambient region. These findings support a scenario in which metal-poor gas infall fuels episodic star formation in EMPGs, consistent with the kinematic properties observed in these systems. In addition, we identify four EMPGs with exceptionally low central metallicities (12+log(O/H) <~ 7.2), which display only a metal-poor clump without a surrounding metal-rich region. This suggests that such ultra-low metallicity EMPGs, at less than a few percent of the solar metallicity, may serve as valuable analogs for galaxies in the early stages of galaxy evolution.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04541
Understanding Hidden Computations in Chain-of-Thought Reasoning,['Computation and Language'],['Aryasomayajula Ram Bharadwaj'],"Chain-of-Thought (CoT) prompting has significantly enhanced the reasoning abilities of large language models. However, recent studies have shown that models can still perform complex reasoning tasks even when the CoT is replaced with filler(hidden) characters (e.g., ""...""), leaving open questions about how models internally process and represent reasoning steps. In this paper, we investigate methods to decode these hidden characters in transformer models trained with filler CoT sequences. By analyzing layer-wise representations using the logit lens method and examining token rankings, we demonstrate that the hidden characters can be recovered without loss of performance. Our findings provide insights into the internal mechanisms of transformer models and open avenues for improving interpretability and transparency in language model reasoning.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04537
Dual approach to proving electoral fraud via statistics and forensics (Dvojnoe dokazatel'stvo falsifikazij na vyborah statistikoj i kriminalistikoj),['Applications'],"['Andrey Podlazov', 'Vadim Makarov']","Electoral fraud often manifests itself as statistical anomalies in election results, yet its extent can rarely be reliably confirmed by other evidence. Here we report complete results of municipal elections in Vlasikha town near Moscow, where we observe both statistical irregularities in the vote-counting transcripts and forensic evidence of tampering with ballots during their overnight storage. We evaluate two types of statistical signatures in the vote sequence that can prove batches of fraudulent ballots have been injected. We find that pairs of factory-made security bags with identical serial numbers are used in this fraud scheme. At 8 out of our 9 polling stations, the statistical and forensic evidence agrees (identifying 7 as fraudulent and 1 as honest), while at the remaining station the statistical evidence detects the fraud while the forensic one is insufficient. We also illustrate that the use of tamper-indicating seals at elections is inherently unreliable. -- --
  Tezis po-russki est' v russkoj versii stat'i (normal'noj kirillicej, ne translitom)△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04535
Quadrupole Mie-resonant metamaterial,['Optics'],"['I. M. Fradkin', 'A. V. Nikulin', 'N. S. Solodovchenko', 'D. S. Filonov', 'D. G. Baranov', 'M. V. Rybin', 'K. B. Samusev', 'M. F. Limonov', 'S. A. Dyakov', 'N. A. Gippius']","Dense lattices of photonic crystals can serve as artificial materials, with light propagation in these structures described by effective material parameters that surpass the capabilities of natural materials. In this study, we introduce a metamaterial that supports quadrupole magnetization, a characteristic rarely observed in existing structures. We experimentally demonstrate a magnetic quadrupole metamaterial associated with Mie-resonance-excited stop bands below the Bragg band. Additionally, we develop a theoretical model that addresses both dispersion and boundary conditions within this framework. Using a Fabry-Perot resonator as a case study, we validate our model and reveal that the quadrupole metamaterial can exhibit a markedly different reflection/transmission spectrum, including zero reflection at normal incidence. Our findings underscore the practical potential for both experimental and theoretical investigations of metamaterials that extend beyond the dipole approximation.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04530
ProtBoost: protein function prediction with Py-Boost and Graph Neural Networks -- CAFA5 top2 solution,['Quantitative Methods'],"['Alexander Chervov', 'Anton Vakhrushev', 'Sergei Fironov', 'Loredana Martignetti']","Predicting protein properties, functions and localizations are important tasks in bioinformatics. Recent progress in machine learning offers an opportunities for improving existing methods. We developed a new approach called ProtBoost, which relies on the strength of pretrained protein language models, the new Py-Boost gradient boosting method and Graph Neural Networks (GCN). The ProtBoost method was ranked second best model in the recent Critical Assessment of Functional Annotation (CAFA5) international challenge with more than 1600 participants. Py-Boost is the first gradient boosting method capable of predicting thousands of targets simultaneously, making it an ideal fit for tasks like the CAFA challange. Our GCN-based approach performs stacking of many individual models and boosts the performance significantly. Notably, it can be applied to any task where targets are arranged in a hierarchical structure, such as Gene Ontology. Additionally, we introduced new methods for leveraging the graph structure of targets and present an analysis of protein language models for protein function prediction task. ProtBoost is publicly available at: https://github.com/btbpanda/CAFA5-protein-function-prediction-2nd-place.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04529
Chemical Abundances in the Nuclear Star Cluster of the Milky Way: alpha-Element Trends and Their Similarities with the Inner Bulge,['Astrophysics of Galaxies'],"['N. Ryde', 'G. Nandakumar', 'M. Schultheis', 'G. Kordopatis', 'P. di Matteo', 'M. Haywood', 'R. Schödel', 'F. Nogueras-Lara', 'R. M. Rich', 'B. Thorsbro', 'G. Mace', 'O. Agertz', 'A. M. Amarsi', 'J. Kocher', 'M. Molero', 'L. Origlia', 'G. Pagnini', 'E. Spitoni']","A chemical characterization of the Galactic Center is essential for understanding its formation and structural evolution. Trends of alpha-elements, such as Mg, Si, and Ca, serve as powerful diagnostic tools, offering insights into star-formation rates and gas-infall history. However, high extinction has previously hindered such studies. In this study, we present a detailed chemical abundance analysis of M giants in the Milky Way's Nuclear Star Cluster (NSC), focusing on alpha-element trends with metallicity. High-resolution, near-infrared spectra were obtained using the IGRINS spectrograph on the Gemini South telescope for nine M giants. Careful selection of spectral lines, based on a solar-neighborhood control sample of 50 M giants, was implemented to minimize systematic uncertainties. Our findings show enhanced alpha-element abundances in the predominantly metal-rich NSC stars, consistent with trends in the inner bulge. The NSC stars follow the high-[alpha/Fe] envelope seen in the solar vicinity's metal-rich population, indicating a high star-formation rate. The alpha-element trends decrease with increasing metallicity, also at the highest metallicities. Our results suggest the NSC population likely shares a similar evolutionary history with the inner bulge, challenging the idea of a recent dominant star formation burst. This connection between the NSC and the inner-disk sequence suggests that the chemical properties of extragalactic NSCs of Milky Way type galaxies could serve as a proxy for understanding the host galaxies' evolutionary processes.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04528
2.5D Super-Resolution Approaches for X-ray Computed Tomography-based Inspection of Additively Manufactured Parts,['Image and Video Processing'],"['Haley Duba-Sullivan', 'Obaidullah Rahman', 'Singanallur Venkatakrishnan', 'Amirkoushyar Ziabari']","X-ray computed tomography (XCT) is a key tool in non-destructive evaluation of additively manufactured (AM) parts, allowing for internal inspection and defect detection. Despite its widespread use, obtaining high-resolution CT scans can be extremely time consuming. This issue can be mitigated by performing scans at lower resolutions; however, reducing the resolution compromises spatial detail, limiting the accuracy of defect detection.
  Super-resolution algorithms offer a promising solution for overcoming resolution limitations in XCT reconstructions of AM parts, enabling more accurate detection of defects. While 2D super-resolution methods have demonstrated state-of-the-art performance on natural images, they tend to under-perform when directly applied to XCT slices. On the other hand, 3D super-resolution methods are computationally expensive, making them infeasible for large-scale applications.
  To address these challenges, we propose a 2.5D super-resolution approach tailored for XCT of AM parts. Our method enhances the resolution of individual slices by leveraging multi-slice information from neighboring 2D slices without the significant computational overhead of full 3D methods. Specifically, we use neighboring low-resolution slices to super-resolve the center slice, exploiting inter-slice spatial context while maintaining computational efficiency. This approach bridges the gap between 2D and 3D methods, offering a practical solution for high-throughput defect detection in AM parts.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04525
Labeling questions inside issue trackers,['Software Engineering'],['Aidin Rasti'],"One of the issues faced by the maintainers of popular open source software is the triage of newly reported issues. Many of the issues submitted to issue trackers are questions. Many people ask questions on issue trackers about their problem instead of using a proper QA website like StackOverflow. This may seem insignificant but for many of the big projects with thousands of users, this leads to spamming of the issue tracker. Reading and labeling these unrelated issues manually is a serious time consuming task and these unrelated questions add to the burden. In fact, most often maintainers demand to not submit questions in the issue tracker. To address this problem, first, we leveraged dozens of patterns to clean text of issues, we removed noises like logs, stack traces, environment variables, error messages, etc. Second, we have implemented a classification-based approach to automatically label unrelated questions. Empirical evaluations on a dataset of more than 102,000 records show that our approach can label questions with an accuracy of over 81%.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04523
Modeling wildfire dynamics through a physics-based approach incorporating fuel moisture and landscape heterogeneity,['Fluid Dynamics'],"['Adrián Navas-Montilla', 'Cordula Reisch', 'Pablo Diaz', 'Ilhan Özgen-Xian']","Anthropogenic climate change has increased the probability, severity, and duration of heat waves and droughts, subsequently escalating the risk of wildfires. Mathematical and computational models can enhance our understanding of wildfire propagation dynamics. In this work, we present a simplified Advection-Diffusion-Reaction (ADR) model that accounts for the effect of fuel moisture, and also considers wind, local radiation, natural convection and topography. The model explicitly represents fuel moisture effects by means of the apparent calorific capacity method, distinguishing between live and dead fuel moisture content. Using this model, we conduct exploratory simulations and present theoretical insights into various modeling decisions in the context of ADR-based models. We aim to shed light on the interplay between the different modeled mechanisms in wildfire propagation to identify key factors influencing fire spread and to estimate the model's predictive capacity.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04517
Quasinormal Modes and GUP-Corrected Hawking Radiation of BTZ Black Holes within Modified Gravity Frameworks,['General Relativity and Quantum Cosmology'],"['Faizuddin Ahmed', 'Ahmad Al-Badawi', 'İzzet Sakallı', 'Abdelmalek Bouzenadad']","This paper aims to explore the quasinormal modes (QNMs) and effective potential profiles of massless and rotating BTZ black holes within the frameworks of $f(\mathcal{R})$ and Ricci-Inverse ($\mathcal{RI}$) modified gravity theories, which, while producing similar space-time structures, exhibit variations due to distinct cosmological constants, $Λ_m$. We derive wave equations for these black hole perturbations and analyze the behavior of the effective potential $V_{\text{eff}}(r)$ under different values of mass $m$, cosmological constant $Λ_m$, and modified gravity parameters $α_1$, $α_2$, $β_1$, $β_2$, and $γ$. The findings indicate that increasing mass and parameter values results in a raised potential barrier, implying stronger confinement of perturbations and impacting black hole stability. Incorporating the generalized uncertainty principle, we also study its effect on the thermodynamics of rotating BTZ black holes, demonstrating how GUP modifies black hole radiation, potentially observable in QNM decay rates. Additionally, we investigate the motion of particles through null and timelike geodesics in static BTZ space-time, observing asymptotic behaviors for null geodesics and parameter-dependent shifts in potential for timelike paths. The study concludes that modified gravity parameters significantly influence QNM frequencies and effective potential profiles, offering insights into black hole stability and suggesting that these theoretical predictions may be tested through gravitational wave observations.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.04513
A Taxonomy of System-Level Attacks on Deep Learning Models in Autonomous Vehicles,['Cryptography and Security'],"['Masoud Jamshidiyan Tehrani', 'Jinhan Kim', 'Rosmael Zidane Lekeufack Foulefack', 'Alessandro Marchetto', 'Paolo Tonella']","The advent of deep learning and its astonishing performance in perception tasks, such as object recognition and classification, has enabled its usage in complex systems, including autonomous vehicles. On the other hand, deep learning models are susceptible to mis-predictions when small, adversarial changes are introduced into their input. Such mis-predictions can be triggered in the real world and can propagate to a failure of the entire system, as opposed to a localized mis-prediction. In recent years, a growing number of research works have investigated ways to mount attacks against autonomous vehicles that exploit deep learning components for perception tasks. Such attacks are directed toward elements of the environment where these systems operate and their effectiveness is assessed in terms of system-level failures triggered by them. There has been however no systematic attempt to analyze and categorize such attacks. In this paper, we present the first taxonomy of system-level attacks against autonomous vehicles. We constructed our taxonomy by first collecting 8,831 papers, then filtering them down to 1,125 candidates and eventually selecting a set of 19 highly relevant papers that satisfy all inclusion criteria. Then, we tagged them with taxonomy categories, involving three assessors per paper. The resulting taxonomy includes 12 top-level categories and several sub-categories. The taxonomy allowed us to investigate the attack features, the most attacked components, the underlying threat models, and the propagation chains from input perturbation to system-level failure. We distilled several lessons for practitioners and identified possible directions for future work for researchers.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.04510
Pragmatic Metacognitive Prompting Improves LLM Performance on Sarcasm Detection,['Computation and Language'],"['Joshua Lee', 'Wyatt Fong', 'Alexander Le', 'Sur Shah', 'Kevin Han', 'Kevin Zhu']","Sarcasm detection is a significant challenge in sentiment analysis due to the nuanced and context-dependent nature of verbiage. We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm. Using state-of-the-art LLMs such as LLaMA-3-8B, GPT-4o, and Claude 3.5 Sonnet, PMP achieves state-of-the-art performance on GPT-4o on MUStARD and SemEval2018. This study demonstrates that integrating pragmatic reasoning and metacognitive strategies into prompting significantly enhances LLMs' ability to detect sarcasm, offering a promising direction for future research in sentiment analysis.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.04509
Video Quality Assessment: A Comprehensive Survey,['Image and Video Processing'],"['Qi Zheng', 'Yibo Fan', 'Leilei Huang', 'Tianyu Zhu', 'Jiaming Liu', 'Zhijian Hao', 'Shuo Xing', 'Chia-Ju Chen', 'Xiongkuo Min', 'Alan C. Bovik', 'Zhengzhong Tu']","Video quality assessment (VQA) is an important processing task, aiming at predicting the quality of videos in a manner highly consistent with human judgments of perceived quality. Traditional VQA models based on natural image and/or video statistics, which are inspired both by models of projected images of the real world and by dual models of the human visual system, deliver only limited prediction performances on real-world user-generated content (UGC), as exemplified in recent large-scale VQA databases containing large numbers of diverse video contents crawled from the web. Fortunately, recent advances in deep neural networks and Large Multimodality Models (LMMs) have enabled significant progress in solving this problem, yielding better results than prior handcrafted models. Numerous deep learning-based VQA models have been developed, with progress in this direction driven by the creation of content-diverse, large-scale human-labeled databases that supply ground truth psychometric video quality data. Here, we present a comprehensive survey of recent progress in the development of VQA algorithms and the benchmarking studies and databases that make them possible. We also analyze open research directions on study design and VQA algorithm architectures.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.04508
YSO implantation detector for beta-delayed neutron spectroscopy,['Instrumentation and Detectors'],"['M. Singh', 'R. Yokoyama', 'R. Grzywacz', 'A. Keeler', 'T. T. King', 'J. Agramunt', 'N. T. Brewer', 'S. Go', 'J. Liu', 'S. Nishimura', 'P. Parkhurst', 'V. H. Phong', 'M. M. Rajabali', 'B. C. Rasco', 'K. P. Rykaczewski', 'D. W. Stracener', 'A. Tolosa-Delgado', 'K. Vaigneur', 'M. Wolinska-Cichocka']","A segmented-scintillator-based implantation detector was developed to study the energy distribution of beta-delayed neutrons emitted from exotic isotopes. The detector comprises a 34 $\times$ 34 YSO scintillator coupled to an 8 $\times$ 8 Position-Sensitive Photo-Multiplier Tube (PSPMT) via a tapered light guide. The detector was used at RIBF, RIKEN, for time-of-flight-based neutron spectroscopy measurement in the $^{78}$Ni region. The detector provides the position and timing resolution necessary for ion-beta correlations and ToF measurements. The detector provides a high $\sim$ 80 $\%$ beta-detection efficiency and a sub-nanosecond timing resolution. This contribution discusses the details of the design, operation, implementation, and analysis developed to obtain neutron time-of-flight spectrum and the analysis methods in the context of neutron-rich nuclei in the $^{78}$Ni region.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.04507
Physics-informed Gaussian Processes as Linear Model Predictive Controller,['Optimization and Control'],"['Jörn Tebbe', 'Andreas Besginow', 'Markus Lange-Hegermann']","We introduce a novel algorithm for controlling linear time invariant systems in a tracking problem. The controller is based on a Gaussian Process (GP) whose realizations satisfy a system of linear ordinary differential equations with constant coefficients. Control inputs for tracking are determined by conditioning the prior GP on the setpoints, i.e. control as inference. The resulting Model Predictive Control scheme incorporates pointwise soft constraints by introducing virtual setpoints to the posterior Gaussian process. We show theoretically that our controller satisfies asymptotical stability for the optimal control problem by leveraging general results from Bayesian inference and demonstrate this result in a numerical example.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.04502
Organic electronic-based neutron detectors,['Instrumentation and Detectors'],"['Adrian J. Bevan', 'Fani E. Taifakou', 'Choudhry Z. Amjad', 'Aled Horner', 'C. Allwork', 'A. J. Drew']","In recent decades organic electronics has entered the mainstream of consumer electronics, driven by innovations in scalability and low power applications, and low-cost fabrication methods. The potential for using organic semiconductor electronic devices as radiation detectors, and in particular for neutron detection is reported. We report results of laboratory tests using alpha particles as well as the response to thermal and fast neutrons covering the energy range 0.025 eV to 16.5 MeV. GEANT4 simulations are used to provide a detailed understanding of the performance and potential of this emerging technology for radiation detection.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.04501
Approximate Computation of Loss Probability for Queueing System with Capacity Sharing Discipline,['Optimization and Control'],"['M. V. Yashina', 'A. G. Tatashev']","A multi-channel queueing system is considered. The arriving requests differ in their type. Requests of each type arrive according to a Poisson process. The number of channels required for service with the rate equal to 1 depends of the request type. If a request is serviced with the rate equal to 1, then, by definition, the length of the request equals to the total service time. If at arrival moment, the idle channels is sufficient, then the arriving request is serviced with the rate 1. If, at the arrival moment, there are no idle channel, then the arriving request is lost. If, at arrival moment, there are idle channels but the number of idle channels is not sufficient for servicing with rate 1, then the request begins to be in service with rate equal to the ratio of the number of idle channels to the number of the channels required for service with the rate 1. If a request is serviced with a rate less than 1 and another request leaves the system, then the service rate increases for the request in consideration. Approximate formula for loss probability has been proposed. The accuracy of approximation is estimated. Approximate values are compared with exact values found from the system of equations for the related Markov chain stationary state probabilities.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.04500
Stokes-Lagrange and Stokes-Dirac representations of $N$-dimensional port-Hamiltonian systems for modelling and control,['Optimization and Control'],"['Antoine Bendimerad-Hohl', 'Ghislain Haine', 'Laurent Lefèvre', 'Denis Matignon']","In this paper, we extend the port-Hamiltonian framework by introducing the concept of Stokes-Lagrange structure, which enables the implicit definition of a Hamiltonian over an $N$-dimensional domain and incorporates energy ports into the system. This new framework parallels the existing Dirac and Stokes-Dirac structures. We propose the Stokes-Lagrange structure as a specific case where the subspace is explicitly defined via differential operators that satisfy an integration by parts formula. By examining various examples through the lens of the Stokes-Lagrange structure, we demonstrate the existence of multiple equivalent system representations. These representations provide significant advantages for both numerical simulation and control design, offering additional tools for the modelling and control of port-Hamiltonian systems.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.04499
Artificial intelligence and cybersecurity in banking sector: opportunities and risks,['Cryptography and Security'],"['Ana Kovacevic', 'Sonja D. Radenkovic', 'Dragana Nikolic']","The rapid advancements in artificial intelligence (AI) have presented new opportunities for enhancing efficiency and economic competitiveness across various industries, espcially in banking. Machine learning (ML), as a subset of artificial intelligence, enables systems to adapt and learn from vast datasets, revolutionizing decision-making processes, fraud detection, and customer service automation. However, these innovations also introduce new challenges, particularly in the realm of cybersecurity. Adversarial attacks, such as data poisoning and evasion attacks, represent critical threats to machine learning models, exploiting vulnerabilities to manipulate outcomes or compromise sensitive information. Furthermore, this study highlights the dual-use nature of AI tools, which can be used by malicious users. To address these challenges, the paper emphasizes the importance of developing machine learning models with key characteristics such as security, trust, resilience and robustness. These features are essential to mitigating risks and ensuring the secure deployment of AI technologies in banking sectors, where the protection of financial data is paramount. The findings underscore the urgent need for enhanced cybersecurity frameworks and continuous improvements in defensive mechanisms. By exploring both opportunities and risks, this paper aims to guide the responsible integration of AI in the banking sector, paving the way for innovation while safeguarding against emerging threats.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.04495
MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification,['Computation and Language'],"['Saptarshi Sengupta', 'Kristal Curtis', 'Akshay Mallipeddi', 'Abhinav Mathur', 'Joseph Ross', 'Liang Gou']","Extending the capabilities of Large Language Models (LLMs) with functions or tools for environment interaction has led to the emergence of the agent paradigm. In industry, training an LLM is not always feasible because of the scarcity of domain data, legal holds on proprietary customer data, rapidly changing business requirements, and the need to prototype new assistants. Agents provide an elegant solution to the above by relying on the zero-shot reasoning abilities of the underlying LLM and utilizing tools to explore and reason over customer data and respond to user requests. However, there are two concerns here: (I) acquiring large scale customer queries for agent testing is time-consuming, and (II) high reliance on the tool call sequence (or trajectory) followed by the agent to respond to user queries may lead to unexpected or incorrect behavior. To address this, we propose MAG-V, a multi-agent framework to first generate a dataset of questions that mimic customer queries; and second, reverse-engineer alternate questions from the responses for trajectory verification. Initial results indicate that our synthetic data can improve agent performance on actual customer queries. Furthermore, our trajectory verification methodology, inspired by distant supervision and using traditional machine learning (ML) models, outperforms a GPT-4o judge baseline by 11% accuracy and matches the performance of a GPT-4 judge on our constructed dataset. Overall, our approach is a step towards unifying diverse task agents into a cohesive framework for achieving an aligned objective.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.04494
Socio-Emotional Response Generation: A Human Evaluation Protocol for LLM-Based Conversational Systems,['Computation and Language'],"['Lorraine Vanel', 'Ariel R. Ramos Vela', 'Alya Yacoubi', 'Chloé Clavel']","Conversational systems are now capable of producing impressive and generally relevant responses. However, we have no visibility nor control of the socio-emotional strategies behind state-of-the-art Large Language Models (LLMs), which poses a problem in terms of their transparency and thus their trustworthiness for critical applications. Another issue is that current automated metrics are not able to properly evaluate the quality of generated responses beyond the dataset's ground truth. In this paper, we propose a neural architecture that includes an intermediate step in planning socio-emotional strategies before response generation. We compare the performance of open-source baseline LLMs to the outputs of these same models augmented with our planning module. We also contrast the outputs obtained from automated metrics and evaluation results provided by human annotators. We describe a novel evaluation protocol that includes a coarse-grained consistency evaluation, as well as a finer-grained annotation of the responses on various social and emotional criteria. Our study shows that predicting a sequence of expected strategy labels and using this sequence to generate a response yields better results than a direct end-to-end generation scheme. It also highlights the divergences and the limits of current evaluation metrics for generated content. The code for the annotation platform and the annotated data are made publicly available for the evaluation of future models.△ Less","26 November, 2024;",https://arxiv.org/pdf/2412.04492
"Soft cells, Kelvin's foam and the minimal surface of Schwarz",['Computational Geometry'],"['Gábor Domokos', 'Alain Goriely', 'Ákos G. Horváth', 'Krisztina Regős']","In a recent article we introduced a new class of shapes, called \emph{soft cells} which fill space as \emph{soft tilings} without gaps and overlaps while minimizing the number of sharp corners. We defined the \textit{edge bending algorithm} deforming a polyhedral tiling into a soft tiling and we proved that by this algorithm an infinite class of polyhedral tilings can be smoothly deformed into soft tilings. Using the algorithm we constructed the soft versions of all Dirichlet-Voronoi cells associated with point lattices in two and three dimensions; for brevity we will refer to the soft cells produced by the edge bending algorithm as the \textit{standard} soft cells. Here we show that the edge bending algorithm has at least one alternative which is specific to one particular polyhedral tiling. By using the Dirichlet-Voronoi tiling based on the $bcc$ lattice, we demonstrate a new type of soft cell which is non-standard, i.e. it can not be produced by the universal edge bending algorithm and it shows a marked difference compared to the standard soft cell with identical combinatorial properties. We use the new, tiling-specific algorithm to construct a one parameter family of space-filling cells, the end members of which are soft cells and one member of which is the Kelvin cell. By using the same, tiling-specific algorithm we point out the connection of the new, non-standard soft cells to Schwarz minimal surfaces. Our findings indicate that soft cells may be identified by algorithms which are specific for the tiling. Also, results show that not only soft cells themselves, but also the geometric constructions resulting in soft cells could be of immediate interest when studying natural shapes.△ Less","23 November, 2024;",https://arxiv.org/pdf/2412.04491
AI-powered Digital Framework for Personalized Economical Quality Learning at Scale,['Computers and Society'],"['Mrzieh VatandoustMohammadieh', 'Mohammad Mahdi Mohajeri', 'Ali Keramati', 'Majid Nili Ahmadabadi']","The disparity in access to quality education is significant, both between developed and developing countries and within nations, regardless of their economic status. Socioeconomic barriers and rapid changes in the job market further intensify this issue, highlighting the need for innovative solutions that can deliver quality education at scale and low cost. This paper addresses these challenges by proposing an AI-powered digital learning framework grounded in Deep Learning (DL) theory. The DL theory emphasizes learner agency and redefines the role of teachers as facilitators, making it particularly suitable for scalable educational environments. We outline eight key principles derived from learning science and AI that are essential for implementing DL-based Digital Learning Environments (DLEs). Our proposed framework leverages AI for learner modelling based on Open Learner Modeling (OLM), activity suggestions, and AI-assisted support for both learners and facilitators, fostering collaborative and engaging learning experiences. Our framework provides a promising direction for scalable, high-quality education globally, offering practical solutions to some of the AI-related challenges in education.△ Less","20 November, 2024;",https://arxiv.org/pdf/2412.04483
LibEvolutionEval: A Benchmark and Study for Version-Specific Code Generation,['Software Engineering'],"['Sachit Kuhar', 'Wasi Uddin Ahmad', 'Zijian Wang', 'Nihal Jain', 'Haifeng Qian', 'Baishakhi Ray', 'Murali Krishna Ramanathan', 'Xiaofei Ma', 'Anoop Deoras']","Recent advancements in code completion models have primarily focused on local file contexts. However, these studies do not fully capture the complexity of real-world software development, which often requires the use of rapidly-evolving public libraries. To fill the gap, we introduce LibEvolutionEval, a detailed study requiring an understanding of library evolution to perform in-line code completion accurately. LibEvolutionEval provides a version-specific code-completion task comprised of eight libraries (torch, torchvision, scipy, pil, tqdm, pyyaml, matplotlib, and pandas) as they evolve over the year along with a detailed analysis of the evolution of two popular and well-maintained public libraries: PyTorch and Matplotlib. We evaluate popular public models and find that public library evolution significantly influences model performance. We explored mitigation methods by studying how retrieved version-specific library documentation and prompting can improve the model's capability in handling these fast-evolving packages, paving a promising future path in better handling fast-evolving libraries.△ Less","19 November, 2024;",https://arxiv.org/pdf/2412.04478
Intelligent Tutors for Adult Learners: An Analysis of Needs and Challenges,['Computers and Society'],"['Adit Gupta', 'Momin Siddiqui', 'Glen Smith', 'Jenn Reddig', 'Christopher MacLellan']","This paper aims to uncover needs of adult learners when using pedagogical technologies such as intelligent tutoring systems. Further, our aim with this work is to understand the usability challenges when deploying tutors at scale within the adult learning audience. As educational technologies become more ubiquitous within k-12 education, this paper aims to bridge the gap in understanding on how adult users might utilize intelligent tutors. In pursuit of this, we built four intelligent tutors, and deployed them to 110 classrooms at a state technical college for an entire academic year. Following this deployment, we conducted focus groups amongst users to gather data to understand how learners perceived the optional educational technology during their academic journey. We further analyzed this data using foundational HCI methodologies to extract leanings and design recommendations on how developers might craft educational technologies for adoption at scale for the adult learning population.△ Less","19 November, 2024;",https://arxiv.org/pdf/2412.04477
The Moral Mind(s) of Large Language Models,['Computers and Society'],['Avner Seror'],"As large language models (LLMs) become integrated to decision-making across various sectors, a key question arises: do they exhibit an emergent ""moral mind"" - a consistent set of moral principles guiding their ethical judgments - and is this reasoning uniform or diverse across models? To investigate this, we presented about forty different models from the main providers with a large array of structured ethical scenarios, creating one of the largest datasets of its kind. Our rationality tests revealed that at least one model from each provider demonstrated behavior consistent with stable moral principles, effectively acting as approximately optimizing a utility function encoding ethical reasoning. We identified these utility functions and observed a notable clustering of models around neutral ethical stances. To investigate variability, we introduced a novel non-parametric permutation approach, revealing that the most rational models shared 59% to 76% of their ethical reasoning patterns. Despite this shared foundation, differences emerged: roughly half displayed greater moral adaptability, bridging diverse perspectives, while the remainder adhered to more rigid ethical structures.△ Less","19 November, 2024;",https://arxiv.org/pdf/2412.04476
QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos,['Computer Vision and Pattern Recognition'],"['Sharath Girish', 'Tianye Li', 'Amrita Mazumdar', 'Abhinav Shrivastava', 'David Luebke', 'Shalini De Mello']","Online free-viewpoint video (FVV) streaming is a challenging problem, which is relatively under-explored. It requires incremental on-the-fly updates to a volumetric representation, fast training and rendering to satisfy real-time constraints and a small memory footprint for efficient transmission. If achieved, it can enhance user experience by enabling novel applications, e.g., 3D video conferencing and live volumetric video broadcast, among others. In this work, we propose a novel framework for QUantized and Efficient ENcoding (QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly learns Gaussian attribute residuals between consecutive frames at each time-step without imposing any structural constraints on them, allowing for high quality reconstruction and generalizability. To efficiently store the residuals, we further propose a quantization-sparsity framework, which contains a learned latent-decoder for effectively quantizing attribute residuals other than Gaussian positions and a learned gating module to sparsify position residuals. We propose to use the Gaussian viewspace gradient difference vector as a signal to separate the static and dynamic content of the scene. It acts as a guide for effective sparsity learning and speeds up training. On diverse FVV benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all metrics. Notably, for several highly dynamic scenes, it reduces the model size to just 0.7 MB per frame while training in under 5 sec and rendering at 350 FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04469
NVILA: Efficient Frontier Visual Language Models,['Computer Vision and Pattern Recognition'],"['Zhijian Liu', 'Ligeng Zhu', 'Baifeng Shi', 'Zhuoyang Zhang', 'Yuming Lou', 'Shang Yang', 'Haocheng Xi', 'Shiyi Cao', 'Yuxian Gu', 'Dacheng Li', 'Xiuyu Li', 'Yunhao Fang', 'Yukang Chen', 'Cheng-Yu Hsieh', 'De-An Huang', 'An-Chieh Cheng', 'Vishwesh Nath', 'Jinyi Hu', 'Sifei Liu', 'Ranjay Krishna', 'Daguang Xu', 'Xiaolong Wang', 'Pavlo Molchanov', 'Jan Kautz', 'Hongxu Yin']","Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This ""scale-then-compress"" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04468
UnZipLoRA: Separating Content and Style from a Single Image,['Computer Vision and Pattern Recognition'],"['Chang Liu', 'Viraj Shah', 'Aiyu Cui', 'Svetlana Lazebnik']","This paper introduces UnZipLoRA, a method for decomposing an image into its constituent subject and style, represented as two distinct LoRAs (Low-Rank Adaptations). Unlike existing personalization techniques that focus on either subject or style in isolation, or require separate training sets for each, UnZipLoRA disentangles these elements from a single image by training both the LoRAs simultaneously. UnZipLoRA ensures that the resulting LoRAs are compatible, i.e., they can be seamlessly combined using direct addition. UnZipLoRA enables independent manipulation and recontextualization of subject and style, including generating variations of each, applying the extracted style to new subjects, and recombining them to reconstruct the original image or create novel variations. To address the challenge of subject and style entanglement, UnZipLoRA employs a novel prompt separation technique, as well as column and block separation strategies to accurately preserve the characteristics of subject and style, and ensure compatibility between the learned LoRAs. Evaluation with human studies and quantitative metrics demonstrates UnZipLoRA's effectiveness compared to other state-of-the-art methods, including DreamBooth-LoRA, Inspiration Tree, and B-LoRA.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04465
DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction,['Computer Vision and Pattern Recognition'],"['Ben Kaye', 'Tomas Jakab', 'Shangzhe Wu', 'Christian Rupprecht', 'Andrea Vedaldi']","The choice of data representation is a key factor in the success of deep learning in geometric tasks. For instance, DUSt3R has recently introduced the concept of viewpoint-invariant point maps, generalizing depth prediction, and showing that one can reduce all the key problems in the 3D reconstruction of static scenes to predicting such point maps. In this paper, we develop an analogous concept for a very different problem, namely, the reconstruction of the 3D shape and pose of deformable objects. To this end, we introduce the Dual Point Maps (DualPM), where a pair of point maps is extracted from the {same} image, one associating pixels to their 3D locations on the object, and the other to a canonical version of the object at rest pose. We also extend point maps to amodal reconstruction, seeing through self-occlusions to obtain the complete shape of the object. We show that 3D reconstruction and 3D pose estimation reduce to the prediction of the DualPMs. We demonstrate empirically that this representation is a good target for a deep network to predict; specifically, we consider modeling horses, showing that DualPMs can be trained purely on 3D synthetic data, consisting of a single model of a horse, while generalizing very well to real images. With this, we improve by a large margin previous methods for the 3D analysis and reconstruction of this type of objects.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04464
"MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos",['Computer Vision and Pattern Recognition'],"['Zhengqi Li', 'Richard Tucker', 'Forrester Cole', 'Qianqian Wang', 'Linyi Jin', 'Vickie Ye', 'Angjoo Kanazawa', 'Aleksander Holynski', 'Noah Snavely']","We present a system that allows for accurate, fast, and robust estimation of camera parameters and depth maps from casual monocular videos of dynamic scenes. Most conventional structure from motion and monocular SLAM techniques assume input videos that feature predominantly static scenes with large amounts of parallax. Such methods tend to produce erroneous estimates in the absence of these conditions. Recent neural network-based approaches attempt to overcome these challenges; however, such methods are either computationally expensive or brittle when run on dynamic videos with uncontrolled camera motion or unknown field of view. We demonstrate the surprising effectiveness of a deep visual SLAM framework: with careful modifications to its training and inference schemes, this system can scale to real-world videos of complex dynamic scenes with unconstrained camera paths, including videos with little camera parallax. Extensive experiments on both synthetic and real videos demonstrate that our system is significantly more accurate and robust at camera pose and depth estimation when compared with prior and concurrent work, with faster or comparable running times. See interactive results on our project page: https://mega-sam.github.io/△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04463
4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion,['Computer Vision and Pattern Recognition'],"['Chaoyang Wang', 'Peiye Zhuang', 'Tuan Duc Ngo', 'Willi Menapace', 'Aliaksandr Siarohin', 'Michael Vasilkovsky', 'Ivan Skorokhodov', 'Sergey Tulyakov', 'Peter Wonka', 'Hsin-Ying Lee']","We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04462
Cubify Anything: Scaling Indoor 3D Object Detection,['Computer Vision and Pattern Recognition'],"['Justin Lazarow', 'David Griffiths', 'Gefen Kohavi', 'Francisco Crespo', 'Afshin Dehghan']","We consider indoor 3D object detection with respect to a single RGB(-D) frame acquired from a commodity handheld device. We seek to significantly advance the status quo with respect to both data and modeling. First, we establish that existing datasets have significant limitations to scale, accuracy, and diversity of objects. As a result, we introduce the Cubify-Anything 1M (CA-1M) dataset, which exhaustively labels over 400K 3D objects on over 1K highly accurate laser-scanned scenes with near-perfect registration to over 3.5K handheld, egocentric captures. Next, we establish Cubify Transformer (CuTR), a fully Transformer 3D object detection baseline which rather than operating in 3D on point or voxel-based representations, predicts 3D boxes directly from 2D features derived from RGB(-D) inputs. While this approach lacks any 3D inductive biases, we show that paired with CA-1M, CuTR outperforms point-based methods - accurately recalling over 62% of objects in 3D, and is significantly more capable at handling noise and uncertainty present in commodity LiDAR-derived depth maps while also providing promising RGB only performance without architecture changes. Furthermore, by pre-training on CA-1M, CuTR can outperform point-based methods on a more diverse variant of SUN RGB-D - supporting the notion that while inductive biases in 3D are useful at the smaller sizes of existing datasets, they fail to scale to the data-rich regime of CA-1M. Overall, this dataset and baseline model provide strong evidence that we are moving towards models which can effectively Cubify Anything.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04458
Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps,['Computer Vision and Pattern Recognition'],"['Yiqing Liang', 'Mikhail Okunev', 'Mikaela Angelina Uy', 'Runfeng Li', 'Leonidas Guibas', 'James Tompkin', 'Adam W. Harley']","Gaussian splatting methods are emerging as a popular approach for converting multi-view image data into scene representations that allow view synthesis. In particular, there is interest in enabling view synthesis for dynamic scenes using only monocular input data -- an ill-posed and challenging problem. The fast pace of work in this area has produced multiple simultaneous papers that claim to work best, which cannot all be true. In this work, we organize, benchmark, and analyze many Gaussian-splatting-based methods, providing apples-to-apples comparisons that prior works have lacked. We use multiple existing datasets and a new instructive synthetic dataset designed to isolate factors that affect reconstruction quality. We systematically categorize Gaussian splatting methods into specific motion representation types and quantify how their differences impact performance. Empirically, we find that their rank order is well-defined in synthetic data, but the complexity of real-world data currently overwhelms the differences. Furthermore, the fast rendering speed of all Gaussian-based methods comes at the cost of brittleness in optimization. We summarize our experiments into a list of findings that can help to further progress in this lively problem setting. Project Webpage: https://lynl7130.github.io/MonoDyGauBench.github.io/△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04457
Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction,['Computation and Language'],"['Yiheng Xu', 'Zekun Wang', 'Junli Wang', 'Dunjie Lu', 'Tianbao Xie', 'Amrita Saha', 'Doyen Sahoo', 'Tao Yu', 'Caiming Xiong']","Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at https://aguvis-project.github.io/.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04454
NaVILA: Legged Robot Vision-Language-Action Model for Navigation,['Robotics'],"['An-Chieh Cheng', 'Yandong Ji', 'Zhaojing Yang', 'Xueyan Zou', 'Jan Kautz', 'Erdem Bıyık', 'Hongxu Yin', 'Sifei Liu', 'Xiaolong Wang']","This paper proposes to solve the problem of Vision-and-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-Language-Action model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., ""moving forward 75cm""), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, low-level controls, and real-world robot experiments. We show more results at https://navila-bot.github.io/△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04453
Four-Plane Factorized Video Autoencoders,['Computer Vision and Pattern Recognition'],"['Mohammed Suhail', 'Carlos Esteves', 'Leonid Sigal', 'Ameesh Makadia']","Latent variable generative models have emerged as powerful tools for generative tasks including image and video synthesis. These models are enabled by pretrained autoencoders that map high resolution data into a compressed lower dimensional latent space, where the generative models can subsequently be developed while requiring fewer computational resources. Despite their effectiveness, the direct application of latent variable models to higher dimensional domains such as videos continues to pose challenges for efficient training and inference. In this paper, we propose an autoencoder that projects volumetric data onto a four-plane factorized latent space that grows sublinearly with the input size, making it ideal for higher dimensional data like videos. The design of our factorized model supports straightforward adoption in a number of conditional generation tasks with latent diffusion models (LDMs), such as class-conditional generation, frame prediction, and video interpolation. Our results show that the proposed four-plane latent space retains a rich representation needed for high-fidelity reconstructions despite the heavy compression, while simultaneously enabling LDMs to operate with significant improvements in speed and memory.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04452
Learning Artistic Signatures: Symmetry Discovery and Style Transfer,['Computer Vision and Pattern Recognition'],"['Emma Finn', 'T. Anderson Keller', 'Emmanouil Theodosis', 'Demba E. Ba']","Despite nearly a decade of literature on style transfer, there is no undisputed definition of artistic style. State-of-the-art models produce impressive results but are difficult to interpret since, without a coherent definition of style, the problem of style transfer is inherently ill-posed. Early work framed style-transfer as an optimization problem but treated style as a measure only of texture. This led to artifacts in the outputs of early models where content features from the style image sometimes bled into the output image. Conversely, more recent work with diffusion models offers compelling empirical results but provides little theoretical grounding. To address these issues, we propose an alternative definition of artistic style. We suggest that style should be thought of as a set of global symmetries that dictate the arrangement of local textures. We validate this perspective empirically by learning the symmetries of a large dataset of paintings and showing that symmetries are predictive of the artistic movement to which each painting belongs. Finally, we show that by considering both local and global features, using both Lie generators and traditional measures of texture, we can quantitatively capture the stylistic similarity between artists better than with either set of features alone. This approach not only aligns well with art historians' consensus but also offers a robust framework for distinguishing nuanced stylistic differences, allowing for a more interpretable, theoretically grounded approach to style transfer.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04441
Overdensity of Lyman-Break Galaxy Candidates Around Hot Dust-Obscured Galaxies,['Astrophysics of Galaxies'],"['Dejene Zewdie', 'Roberto J. Assef', 'Trystan Lambert', 'Chiara Mazzucchelli', 'S. Ilani Loubser', 'Manuel Aravena', 'Jorge González-López', 'Hyunsung D. Jun', 'Chao-Wei Tsai', 'Daniel Stern', 'Guodong Li', 'Román Fernández Aranda', 'Tanio Díaz-Santos', 'Peter R. M. Eisenhardt', 'Andrey Vayner', 'Lee R. Martin', 'Andrew W. Blain', 'Jingwen Wu']","Hot dust-obscured galaxies (Hot DOGs), are a family of hyper-luminous, heavily obscured quasars. A number of studies have shown that these objects reside in significantly overdense regions of the Universe based on the identification of companions at optical through far-IR wavelengths. Here we present further characterization of their environments by studying the surface density of Lyman break galaxy (LBG) candidates in the vicinity of three Hot DOGs. For two of them, WISE J041010.60-091305.2 at z=3.631 and WISE J083153.25+014010.8 at z=3.912, we identify the candidate LBG companions using deep observations obtained with Baade/IMACS. For the third, WISE J224607.56-052634.9 at z=4.601, we re-analyse previously published data obtained with Gemini-S/GMOS-S. We optimise the LBG photometric selection criteria at the redshift of each target using the COSMOS2020 catalog. When comparing the density of LBG candidates found in the vicinity of these Hot DOGs with that in the COSMOS2020 catalog, we find overdensities of $δ=1.83\pm 0.08$ ($δ' = 7.49\pm 0.68$), $δ=4.67\pm 0.21$ ($δ' = 29.17\pm 2.21$), and $δ= 2.36\pm 0.25$ ($δ' = 11.60\pm 1.96$) around W0410-0913, W0831+0140, and W2246-0526, respectively, without (with) contamination correction. Additionally, we find that the overdensities are centrally concentrated around each Hot DOG. Our analysis also reveals that the overdensity of the fields surrounding W0410-0913 and W0831+0140 declines steeply beyond physical scales of $\sim$2 Mpc. If these overdensities evolve to clusters by z=0, these results suggest that the Hot DOG may correspond to the early formation stages of the brightest cluster galaxy. We were unable to determine if this is also the case for W2246-0526 due to the smaller field of view of the GMOS-S observations. Our results imply that Hot DOGs may be excellent tracers of protoclusters.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04436
CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing,['Audio and Speech Processing'],"['Yen-Ju Lu', 'Jing Liu', 'Thomas Thebaud', 'Laureano Moro-Velazquez', 'Ariya Rastrow', 'Najim Dehak', 'Jesus Villalba']","We introduce Condition-Aware Self-Supervised Learning Representation (CA-SSLR), a generalist conditioning model broadly applicable to various speech-processing tasks. Compared to standard fine-tuning methods that optimize for downstream models, CA-SSLR integrates language and speaker embeddings from earlier layers, making the SSL model aware of the current language and speaker context. This approach reduces the reliance on input audio features while preserving the integrity of the base SSLR. CA-SSLR improves the model's capabilities and demonstrates its generality on unseen tasks with minimal task-specific tuning. Our method employs linear modulation to dynamically adjust internal representations, enabling fine-grained adaptability without significantly altering the original model behavior. Experiments show that CA-SSLR reduces the number of trainable parameters, mitigates overfitting, and excels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves a 10% relative reduction in LID errors, a 37% improvement in ASR CER on the ML-SUPERB benchmark, and a 27% decrease in SV EER on VoxCeleb-1, demonstrating its effectiveness.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04425
Single-qubit gates with errors at the $10^{-7}$ level,['Quantum Physics'],"['M. C. Smith', 'A. D. Leu', 'K. Miyanishi', 'M. F. Gely', 'D. M. Lucas']","We report the achievement of single-qubit gates with sub-part-per-million error rates, in a trapped-ion $^{43}$Ca$^{+}$ hyperfine clock qubit. We explore the speed/fidelity trade-off for gate times $4.4\leq t_{g}\leq35~μ$s, and benchmark a minimum error of $1.5(4) \times 10^{-7}$. Gate calibration errors are suppressed to $< 10^{-8}$, leaving qubit decoherence ($T_{2}\approx 70$ s), leakage and measurement as the dominant error contributions. The ion is held above a microfabricated surface-electrode trap which incorporates a chip-integrated microwave resonator for electronic qubit control; the trap is operated at room temperature without magnetic shielding.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04421
ACE2-SOM: Coupling to a slab ocean and learning the sensitivity of climate to changes in CO$_2$,['Atmospheric and Oceanic Physics'],"['Spencer K. Clark', 'Oliver Watt-Meyer', 'Anna Kwa', 'Jeremy McGibbon', 'Brian Henn', 'W. Andre Perkins', 'Elynn Wu', 'Christopher S. Bretherton', 'Lucas M. Harris']","While autoregressive machine-learning-based emulators have been trained to produce stable and accurate rollouts in the climate of the present-day and recent past, none so far have been trained to emulate the sensitivity of climate to substantial changes in CO$_2$ or other greenhouse gases. As an initial step we couple the Ai2 Climate Emulator version 2 to a slab ocean model (hereafter ACE2-SOM) and train it on output from a collection of equilibrium-climate physics-based reference simulations with varying levels of CO$_2$. We test it in equilibrium and non-equilibrium climate scenarios with CO$_2$ concentrations seen and unseen in training.
  ACE2-SOM performs well in equilibrium-climate inference with both in-sample and out-of-sample CO$_2$ concentrations, accurately reproducing the emergent time-mean spatial patterns of surface temperature and precipitation change with CO$_2$ doubling, tripling, or quadrupling. In addition, the vertical profile of atmospheric warming and change in extreme precipitation rates with increased CO$_2$ closely agree with the reference model. Non-equilibrium-climate inference is more challenging. With CO$_2$ increasing gradually at a rate of 2% year$^{-1}$, ACE2-SOM can accurately emulate the global annual mean trends of surface and lower-to-middle atmosphere fields but produces unphysical jumps in stratospheric fields. With an abrupt quadrupling of CO$_2$, ML-controlled fields transition unrealistically quickly to the 4xCO$_2$ regime. In doing so they violate global energy conservation and exhibit unphysical sensitivities of and surface and top of atmosphere radiative fluxes to instantaneous changes in CO$_2$. Future emulator development needed to address these issues should improve its generalizability to diverse climate change scenarios.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04418
Resurgence number and convex body associated to pairs of graded families of ideals,['Commutative Algebra'],"['Tai Huy Ha', 'A. V. Jayanthan', 'Arvind Kumar', 'Thai Thanh Nguyen']","We discuss how to understand the asymptotic resurgence number of a pair of graded families of ideals from combinatorial data of their associated convex bodies. When the families consist of monomial ideals, the convex bodies being considered are the Newton-Okounkov bodies of the families. When ideals in the second family are classical invariant ideals, for instance, determinantal ideals or ideals of Pfaffians, these convex bodies are constructed from the associated Rees packages.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04417
FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning,['Machine Learning'],"['Pranab Sahoo', 'Ashutosh Tripathi', 'Sriparna Saha', 'Samrat Mondal']","Federated Learning (FL) marks a transformative approach to distributed model training by combining locally optimized models from various clients into a unified global model. While FL preserves data privacy by eliminating centralized storage, it encounters significant challenges such as performance degradation, slower convergence, and reduced robustness of the global model due to the heterogeneity in client data distributions. Among the various forms of data heterogeneity, label skew emerges as a particularly formidable and prevalent issue, especially in domains such as image classification. To address these challenges, we begin with comprehensive experiments to pinpoint the underlying issues in the FL training process. Based on our findings, we then introduce an innovative dual-strategy approach designed to effectively resolve these issues. First, we introduce an adaptive loss function for client-side training, meticulously crafted to preserve previously acquired knowledge while maintaining an optimal equilibrium between local optimization and global model coherence. Secondly, we develop a dynamic aggregation strategy for aggregating client models at the server. This approach adapts to each client's unique learning patterns, effectively addressing the challenges of diverse data across the network. Our comprehensive evaluation, conducted across three diverse real-world datasets, coupled with theoretical convergence guarantees, demonstrates the superior efficacy of our method compared to several established state-of-the-art approaches.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04416
Efficient Task Grouping Through Samplewise Optimisation Landscape Analysis,['Machine Learning'],"['Anshul Thakur', 'Yichen Huang', 'Soheila Molaei', 'Yujiang Wang', 'David A. Clifton']","Shared training approaches, such as multi-task learning (MTL) and gradient-based meta-learning, are widely used in various machine learning applications, but they often suffer from negative transfer, leading to performance degradation in specific tasks. While several optimisation techniques have been developed to mitigate this issue for pre-selected task cohorts, identifying optimal task combinations for joint learning - known as task grouping - remains underexplored and computationally challenging due to the exponential growth in task combinations and the need for extensive training and evaluation cycles. This paper introduces an efficient task grouping framework designed to reduce these overwhelming computational demands of the existing methods. The proposed framework infers pairwise task similarities through a sample-wise optimisation landscape analysis, eliminating the need for the shared model training required to infer task similarities in existing methods. With task similarities acquired, a graph-based clustering algorithm is employed to pinpoint near-optimal task groups, providing an approximate yet efficient and effective solution to the originally NP-hard problem. Empirical assessments conducted on 8 different datasets highlight the effectiveness of the proposed framework, revealing a five-fold speed enhancement compared to previous state-of-the-art methods. Moreover, the framework consistently demonstrates comparable performance, confirming its remarkable efficiency and effectiveness in task grouping.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04413
Estimation of correlation coefficients and spin angular distributions of fission fragments,['Nuclear Theory'],"['D. E. Lyubashevsky', 'A. A. Pisklyukov', 'S. V. Klyuchnikov', 'P. V. Kostryukov']","This study proposes a theoretical model for studying the spin characteristics and angular correlations of fission fragments of heavy nuclei. The mechanisms of spin formation, including the influence of transverse vibrations, are considered and the relationship between the anisotropy of the angular distribution and the correlation coefficient is revealed. The theoretical predictions are compared with experimental data and various models developed by other research groups.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04411
Spin distribution of fission fragments involving bending and wriggling modes,['Nuclear Theory'],"['D. E. Lyubashevsky', 'A. A. Pisklyukov', 'D. A. Stepanov', 'T. Yu. Shashkina', 'P. V. Kostryukov']","This paper presents a theoretical description of the spin distributions of fragments from low-energy induced and spontaneous nuclear fission, expressed in an analytical form. The mechanism of pumping high spin values for deformed fission fragments is explained. The idea is that the source of the generation of high relative orbital moments and spins of the fragments are the transverse wriggling and bending vibrations of the pre-fragments, while the nucleus remains ""cold"" until the moment of fission. To verify this hypothesis, experimental distributions for the induced fission of $\rm ^{232}Th$ and $\rm ^{238}U$ nuclei, as well as the spontaneous fission of $\rm ^{252}Cf$, were compared. The results show reasonable agreement both in the magnitude of the mean spin values and in the sawtooth shape of the sip distribution with respect to the fragment mass number. The results are also compared with other approaches to the description of these quantities, and possible reasons for their discrepancies are discussed.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04410
Providing Differential Privacy for Federated Learning Over Wireless: A Cross-layer Framework,['Information Theory'],"['Jiayu Mao', 'Tongxin Yin', 'Aylin Yener', 'Mingyan Liu']","Federated Learning (FL) is a distributed machine learning framework that inherently allows edge devices to maintain their local training data, thus providing some level of privacy. However, FL's model updates still pose a risk of privacy leakage, which must be mitigated. Over-the-air FL (OTA-FL) is an adapted FL design for wireless edge networks that leverages the natural superposition property of the wireless medium. We propose a wireless physical layer (PHY) design for OTA-FL which improves differential privacy (DP) through a decentralized, dynamic power control that utilizes both inherent Gaussian noise in the wireless channel and a cooperative jammer (CJ) for additional artificial noise generation when higher privacy levels are required. Although primarily implemented within the Upcycled-FL framework, where a resource-efficient method with first-order approximations is used at every even iteration to decrease the required information from clients, our power control strategy is applicable to any FL framework, including FedAvg and FedProx as shown in the paper. This adaptation showcases the flexibility and effectiveness of our design across different learning algorithms while maintaining a strong emphasis on privacy. Our design removes the need for client-side artificial noise injection for DP, utilizing a cooperative jammer to enhance privacy without affecting transmission efficiency for higher privacy demands. Privacy analysis is provided using the Moments Accountant method. We perform a convergence analysis for non-convex objectives to tackle heterogeneous data distributions, highlighting the inherent trade-offs between privacy and accuracy. Numerical results show that our approach with various FL algorithms outperforms the state-of-the-art under the same DP conditions on the non-i.i.d. FEMNIST dataset, and highlight the cooperative jammer's effectiveness in ensuring strict privacy.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04408
Establishing Task Scaling Laws via Compute-Efficient Model Ladders,['Computation and Language'],"['Akshita Bhagia', 'Jiacheng Liu', 'Alexander Wettig', 'David Heineman', 'Oyvind Tafjord', 'Ananya Harsh Jha', 'Luca Soldaini', 'Noah A. Smith', 'Dirk Groeneveld', 'Pang Wei Koh', 'Jesse Dodge', 'Hannaneh Hajishirzi']","We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. We train a set of small-scale ""ladder"" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04403
Enhanced Sampling of Protein Conformational Changes via True Reaction Coordinates from Energy Relaxation,['Chemical Physics'],"['Huiyu Li', 'Ao Ma']","The bottleneck in enhanced sampling lies in finding collective variables (CVs) that can effectively accelerate protein conformational changes. True reaction coordinates (tRCs) that can predict the committor are considered the optimal CVs, but identifying them requires unbiased natural reactive trajectories, which, paradoxically, depend on effective enhanced sampling. Using the generalized work functional method, we found that tRCs control both conformational changes and energy relaxation, enabling us to compute tRCs from energy relaxation simulations. Applying bias to tRCs accelerated conformational changes and ligand dissociation in HIV-1 protease and the PDZ2 domain by 10^5 to 10^15-fold. The resulting trajectories follow natural transition pathways, enabling efficient generation of natural reactive trajectories. In contrast, biased trajectories from empirical CVs often display non-physical features. Furthermore, by computing tRCs from a single protein structure, our method enables predictive sampling of conformational changes. These findings significantly broaden the range of protein functional processes accessible to molecular dynamics simulations.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04400
ERF: Energy Research and Forecasting Model,['Atmospheric and Oceanic Physics'],"['Aaron Lattanzi', 'Ann Almgren', 'Eliot Quon', 'Mahesh Natarajan', 'Branko Kosovic', 'Jeff Mirocha', 'Bruce Perry', 'David Wiersema', 'Donald Willcox', 'Xingqiu Yuan', 'Weiqun Zhang']","High performance computing (HPC) architectures have undergone rapid development in recent years. As a result, established software suites face an ever increasing challenge to remain performant on and portable across modern systems. Many of the widely adopted atmospheric modeling codes cannot fully (or in some cases, at all) leverage the acceleration provided by General-Purpose Graphics Processing Units (GPGPUs), leaving users of those codes constrained to increasingly limited HPC resources. Energy Research and Forecasting (ERF) is a regional atmospheric modeling code that leverages the latest HPC architectures, whether composed of only Central Processing Units (CPUs) or incorporating GPUs. ERF contains many of the standard discretizations and basic features needed to model general atmospheric dynamics as well as flows relevant to renewable energy. The modular design of ERF provides a flexible platform for exploring different physics parameterizations and numerical strategies. ERF is built on a state-of-the-art, well-supported, software framework (AMReX) that provides a performance portable interface and ensures ERF's long-term sustainability on next generation computing systems. This paper details the numerical methodology of ERF and presents results for a series of verification and validation cases.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04395
Bayesian Quantum Amplitude Estimation,['Quantum Physics'],"['Alexandra Ramôa', 'Luis Paulo Santos']","Quantum amplitude estimation is a fundamental routine that offers a quadratic speed-up over classical approaches. The original QAE protocol is based on phase estimation. The associated circuit depth and width, and the assumptions of fault tolerance, are unfavorable for near-term quantum technology. Subsequent approaches attempt to replace the original protocol with hybrid iterative quantum-classical strategies. In this work, we introduce BAE, a noise-aware Bayesian algorithm for QAE that combines quantum circuits with a statistical inference backbone. BAE can dynamically characterize device noise and adapt to it in real-time. Problem-specific insights and approximations are used to keep the problem tractable. We further propose an annealed variant of BAE, drawing on methods from statistical inference, to enhance statistical robustness. Our proposal is parallelizable in both quantum and classical components, offers tools for fast noise model assessment, and can leverage preexisting information. Additionally, it accommodates experimental limitations and preferred cost trade-offs. We show that BAE achieves Heisenberg-limited estimation and benchmark it against other approaches, demonstrating its competitive performance in both noisy and noiseless scenarios.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04394
Asynchronous Batch Bayesian Optimization with Pipelining Evaluations for Experimental Resource$\unicode{x2013}$constrained Conditions,['Machine Learning'],"['Yujin Taguchi', 'Yusuke Shibuya', 'Yusuke Hiki', 'Takashi Morikura', 'Takahiro G. Yamada', 'Akira Funahashi']","Bayesian optimization is efficient even with a small amount of data and is used in engineering and in science, including biology and chemistry. In Bayesian optimization, a parameterized model with an uncertainty is fitted to explain the experimental data, and then the model suggests parameters that would most likely improve the results. Batch Bayesian optimization reduces the processing time of optimization by parallelizing experiments. However, batch Bayesian optimization cannot be applied if the number of parallelized experiments is limited by the cost or scarcity of equipment; in such cases, sequential methods require an unrealistic amount of time. In this study, we developed pipelining Bayesian optimization (PipeBO) to reduce the processing time of optimization even with a limited number of parallel experiments. PipeBO was inspired by the pipelining of central processing unit architecture, which divides computational tasks into multiple processes. PipeBO was designed to achieve experiment parallelization by overlapping various processes of the experiments. PipeBO uses the results of completed experiments to update the parameters of running parallelized experiments. Using the Black-Box Optimization Benchmarking, which consists of 24 benchmark functions, we compared PipeBO with the sequential Bayesian optimization methods. PipeBO reduced the average processing time of optimization to about 56% for the experiments that consisted of two processes or even less for those with more processes for 20 out of the 24 functions. Overall, PipeBO parallelizes Bayesian optimization in the resource-constrained settings so that efficient optimization can be achieved.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04392
Electronic structure of Ruddlesden-Popper nickelates: strain to mimic the effects pressure,['Superconductivity'],"['Yi-Feng Zhao', 'Antia S. Botana']","Signatures of superconductivity under pressure have recently been reported in the bilayer La$_3$Ni$_2$O$_7$ and trilayer La$_4$Ni$_3$O$_{10}$ Ruddlesden-Popper (RP) nickelates with general chemical formula La$_{n+1}$Ni$_n$O$_{3n+1}$ ($n=$ number of perovskite layers along the $c$-axis). The emergence of superconductivity is always concomitant with a structural transition in which the octahedral tilts are suppressed causing an increase in the out-of-plane $d_{z^2}$ orbital overlap. Here, using first-principles calculations, we explore biaxial strain (both compressive and tensile) as a means to mimic the electronic structure characteristics of RP nickelates (up to $n=5$) under hydrostatic pressure. Our findings highlight that strain allows to decouple the structural and electronic structure effects obtained under hydrostatic pressure, with tensile strain reproducing the known electronic structure characteristics of the pressurized bilayer and trilayer compounds. Overall, strain represents a promising way to tune the electronic structure of RP nickelates and could be an alternative route to achieve superconductivity in this family of materials.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04391
"Hypergraph burning, matchings, and zero forcing",['Combinatorics'],"['Anthony Bonato', 'Caleb Jones', 'Trent G. Marbach', 'Teddy Mishura', 'Zhiyuan Zhang']","Lazy burning is a recently introduced variation of burning where only one set of vertices is chosen to burn in the first round. In hypergraphs, lazy burning spreads when all but one vertex in a hyperedge is burned. The lazy burning number is the minimum number of initially burned vertices that eventually burns all vertices. We give several equivalent characterizations of lazy burning on hypergraphs using matchings and zero forcing, and then apply these to give new bounds and complexity results.
  We prove that the lazy burning number of a hypergraph $H$ equals its order minus the maximum cardinality of a certain matching on its incidence graph. Using this characterization, we give a formula for the lazy burning number of a dual hypergraph and give new bounds on the lazy burning number based on various hypergraph parameters. We show that the lazy burning number of a hypergraph may be characterized by a maximal subhypergraph that results from iteratively deleting vertices in singleton hyperedges.
  We prove that lazy burning on a hypergraph is equivalent to zero forcing on its incidence graph and show an equivalence between skew zero forcing on a graph and lazy burning on its neighborhood hypergraph. As a result, we show that finding an upper bound on the lazy burning number of a hypergraph is NP-complete, which resolves a conjecture from \cite{BJR}. By applying lazy burning, we show that computing an upper bound on the skew zero forcing number for bipartite graphs is NP-complete. We finish with open problems.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04389
Unitarity bounds with subthreshold and anomalous cuts for $b$-hadron decays,['High Energy Physics - Phenomenology'],"['Abinand Gopal', 'Nico Gubernari']","We derive a generalisation of the Boyd-Grinstein-Lebed (BGL) parametrization. Most form factors (FFs) in $b$-hadron decays exhibit additional branch cuts -- namely subthreshold and anomalous branch cuts -- beyond the ``standard'' unitarity cut. These additional cuts cannot be adequately accounted for by the BGL parametrization. For instance, these cuts arise in the FFs for $B\to D^{(*)}$, $B\to K^{(*)}$, and $Λ_b\to Λ$ processes, which are particularly relevant from a phenomenological standpoint. We demonstrate how to parametrize such FFs and derive unitarity bounds in the presence of subthreshold and/or anomalous branch cuts. Our work paves the way for a wide range of new FF analyses based solely on first principles, thereby minimising systematic uncertainties.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04388
On the $1/c$ expansion in $2d$ CFTs with degenerate operators,['High Energy Physics - Theory'],"['Agnese Bissi', 'Nicola Dondi', 'Alessandro Piazza', 'Tomas Reis', 'Marco Serone']","We analytically determine the large central charge asymptotic expansion of the Virasoro conformal blocks entering in four-point functions with external degenerate operators on a sphere in $2d$ CFTs, and study its resurgence properties as a function of the conformal cross-ratio $z$. We focus on the cases of four heavy level-two $(2,1)$ degenerate operators, and two $(2,1)$ heavy degenerate ones plus two arbitrary light operators. The $1/c$ asymptotic series is Borel summable for generic values of $z$, but it jumps when a Stokes line is crossed. Starting from the $1/c$ series of the identity block, we show how a resurgent analysis allows us to completely determine the other Virasoro block and in fact to reconstruct the full correlator. We also show that forbidden singularities, known to exist in correlators with two heavy and two light operators, appear with four heavy operators as well. In both cases, they are turning points emanating Stokes lines, artefacts of the asymptotic expansion, and we show how they are non perturbatively resolved. More general correlators and implications for gravitational theories in $\text{AdS}_{3}$ are briefly discussed. Our results are based on new asymptotic expansions for large parameters $(a,b,c)$ of certain hypergeometric functions $\,_2 F_1(a,b,c;z)$ which can be useful in general.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04387
The Thermal Sunyaev-Zel'dovich Effect from the Epoch of Reionization,['Cosmology and Nongalactic Astrophysics'],"['Ilian T. Iliev', 'Azizah R. Hosein', 'Jens Chluba', 'Luke Conaboy', 'David Attard', 'Rajesh Mondal', 'Kyungjin Ahn', 'Stefan Gottlöber', 'Joseph Lewis', 'Pierre Ocvirk', 'Hyunbae Park', 'Paul R. Shapiro', 'Jenny G. Sorce', 'Gustavo Yepes']","The thermal Sunyaev-Zel'dovich (tSZ) effect arises from inverse Compton scattering of low energy photons onto thermal electrons, proportional to the integrated electron pressure, and is usually observed from galaxy clusters. However, we can expect that the Epoch of Reionization (EoR) also contributes to this signal, but that contribution has not been previously evaluated. In this work we analyse a suite of fully-coupled radiation-hydrodynamics simulations based on RAMSES-CUDATON to calculate and study the tSZ signal from the Reionization Epoch. We construct lightcones of the electron pressure in the intergalactic medium for $6<z$ to calculate the resulting Compton y-parameters. We vary the box sizes, resolutions and star formation parameters to investigate how these factors affect the tSZ effect. We produce plots of maps and distributions of y, as well as angular temperature power spectra of the tSZ signal obtained from integrating the lightcones constructed for each simulation. We find that the tSZ signal from reionization is generally sub-dominant to the post-reionization one at larger scales ($\ell< 10^4$), but can contribute non-trivially and potentially contaminate the measured signals. At scales probed by current experiments like SPT ($\ell\sim10^3-10^4$), we find that the tSZ signal power spectrum from reionization contributes at roughly a percent level compared to the current templates, with the quadratic Doppler effect contributing an additional $\sim10\%$ to the tSZ signal. At smaller scales the tSZ from reionization peaks and can potentially dominate the total signal and is thus a potentially much more important contribution to take into account in any future, more sensitive experiments.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04385
GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction,['Computer Vision and Pattern Recognition'],"['Yuanhui Huang', 'Amonnut Thammatadatrakoon', 'Wenzhao Zheng', 'Yunpeng Zhang', 'Dalong Du', 'Jiwen Lu']","3D semantic occupancy prediction is an important task for robust vision-centric autonomous driving, which predicts fine-grained geometry and semantics of the surrounding scene. Most existing methods leverage dense grid-based scene representations, overlooking the spatial sparsity of the driving scenes. Although 3D semantic Gaussian serves as an object-centric sparse alternative, most of the Gaussians still describe the empty region with low efficiency. To address this, we propose a probabilistic Gaussian superposition model which interprets each Gaussian as a probability distribution of its neighborhood being occupied and conforms to probabilistic multiplication to derive the overall geometry. Furthermore, we adopt the exact Gaussian mixture model for semantics calculation to avoid unnecessary overlapping of Gaussians. To effectively initialize Gaussians in non-empty region, we design a distribution-based initialization module which learns the pixel-aligned occupancy distribution instead of the depth of surfaces. We conduct extensive experiments on nuScenes and KITTI-360 datasets and our GaussianFormer-2 achieves state-of-the-art performance with high efficiency. Code: https://github.com/huang-yh/GaussianFormer.△ Less",v1,https://arxiv.org/pdf/2412.04384
Discriminative Fine-tuning of LVLMs,['Computer Vision and Pattern Recognition'],"['Yassine Ouali', 'Adrian Bulat', 'Alexandros Xenos', 'Anestis Zaganidis', 'Ioannis Maniadis Metaxas', 'Georgios Tzimiropoulos', 'Brais Martinez']","Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a ""bag of words"" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.
  In this work, we propose to combine ""the best of both worlds"": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.
  Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04378
A Hitchhiker's Guide to Understanding Performances of Two-Class Classifiers,['Computer Vision and Pattern Recognition'],"['Anaïs Halin', 'Sébastien Piérard', 'Anthony Cioppa', 'Marc Van Droogenbroeck']","Properly understanding the performances of classifiers is essential in various scenarios. However, the literature often relies only on one or two standard scores to compare classifiers, which fails to capture the nuances of application-specific requirements, potentially leading to suboptimal classifier selection. Recently, a paper on the foundations of the theory of performance-based ranking introduced a tool, called the Tile, that organizes an infinity of ranking scores into a 2D map. Thanks to the Tile, it is now possible to evaluate and compare classifiers efficiently, displaying all possible application-specific preferences instead of having to rely on a pair of scores. In this paper, we provide a first hitchhiker's guide for understanding the performances of two-class classifiers by presenting four scenarios, each showcasing a different user profile: a theoretical analyst, a method designer, a benchmarker, and an application developer. Particularly, we show that we can provide different interpretative flavors that are adapted to the user's needs by mapping different values on the Tile. As an illustration, we leverage the newly introduced Tile tool and the different flavors to rank and analyze the performances of 74 state-of-the-art semantic segmentation models in two-class classification through the eyes of the four user profiles. Through these user profiles, we demonstrate that the Tile effectively captures the behavior of classifiers in a single visualization, while accommodating an infinite number of ranking scores.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04377
Restricted Phase Space Thermodynamics of 4D Dyonic AdS Black Holes: Insights from Kaniadakis Statistics and Emergence of Superfluid $λ$-Phase Transition,['High Energy Physics - Theory'],"['Abhishek Baruah', 'Prabwal Phukon']","We study the thermodynamics of $4D$ dyonic AdS black hole in the Kaniadakis statistics framework using the Restricted Phase Space (RPST) formalism. This framework provides a non-extensive extension of classical statistical mechanics, drawing inspiration from relativistic symmetries and presenting a fresh perspective on black hole thermodynamics. Our study analyzes how including Kaniadakis entropy modifies the phase transition of the dyonic black holes. We consider the central charge $C$ and its conjugate chemical potential $μ$ as the thermodynamic variable along with others except the pressure and volume. Due to the addition of the magnetic charge $\tilde{Q}_m$, the study of the phase transition becomes much richer by obtaining a non-equilibrium phase transition from an unstable small black hole to a stable large black hole along with the Van der Waals phase transition in the $T-S$ processes. In the $F-T$ plot, we get an extra Hawking-Page phase transition. Including the deformation parameter $κ$ introduces an unstable (ultra-large BH) branch seen in almost all the plots. Turning off the magnetic charge flips the direction of the phase transition seen during its presence. We observe a novel phenomenon that is the superfluid $λ$ phase transition in the mixed $(\tildeΦ_e,\tilde{Q}_m)$ which is due to the additional $\tilde{Q}_m$ inclusion. Also, in the plots varying $κ$ match with the plot varying $C$ which underlines some sort of correspondence in its meaning which is not possible to observe in Gibbs-Boltzmann statistics. As the entropy models change the homogeneity is not lost where mass is of the first order and the rest is zeroth order. Finally, the $μ-C$ processes in quite similar across black hole systems and entropy formulation marking some kind of universality of this process.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04375
Distributed Inference with Minimal Off-Chip Traffic for Transformers on Low-Power MCUs,['Hardware Architecture'],"['Severin Bochem', 'Victor J. B. Jung', 'Arpan Prasad', 'Francesco Conti', 'Luca Benini']","Contextual Artificial Intelligence (AI) based on emerging Transformer models is predicted to drive the next technology revolution in interactive wearable devices such as new-generation smart glasses. By coupling numerous sensors with small, low-power Micro-Controller Units (MCUs), these devices will enable on-device intelligence and sensor control. A major bottleneck in this class of systems is the small amount of on-chip memory available in the MCUs. In this paper, we propose a methodology to deploy real-world Transformers on low-power wearable devices with minimal off-chip traffic exploiting a distributed system of MCUs, partitioning inference across multiple devices and enabling execution with stationary on-chip weights. We validate the scheme by deploying the TinyLlama-42M decoder-only model on a system of 8 parallel ultra-low-power MCUs. The distributed system achieves an energy consumption of 0.64 mJ, a latency of 0.54 ms per inference, a super-linear speedup of 26.1 x, and an Energy Delay Product (EDP) improvement of 27.2 x, compared to a single-chip system. On MobileBERT, the distributed system's runtime is 38.8 ms, with a super-linear 4.7 x speedup when using 4 MCUs compared to a single-chip system.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04372
Interatomic Coulombic decay in lithium-doped large helium nanodroplets induced by photoelectron impact excitation,['Atomic and Molecular Clusters'],"['L. Ben Ltaief', 'K. Sishodia', 'J. D. Asmussen', 'A. R. Abid', 'S. R. Krishnan', 'H. B. Pedersen', 'N. Sisourat', 'M. Mudrich']","Irradiation of condensed matter with ionizing radiation generally causes direct photoionization as well as secondary processes that often dominate the ionization dynamics. Here, large helium (He) nanodroplets with radius >40 nm doped with lithium (Li) atoms are irradiated with extreme ultraviolet (XUV) photons of energy >44.4 eV and indirect ionization of the Li dopants is observed in addition to direct photoionization of the He droplets. Specifically, Li ions are efficiently produced by an interatomic Coulombic decay (ICD) process involving metastable He atoms and He_2 excimers which are populated by elastic and inelastic scattering of photoelectrons in the nanodroplets as well as by electron-ion recombination. This type of indirect ICD, observed in large He nanodroplets in nearly the entire XUV range, turns out to be more efficient than Li dopant ionization by ICD following direct resonant photoexcitation at a photon energy of 21.6 eV and by charge-transfer ionization. Indirect ICD processes induced by scattering of photoelectrons likely play an important role in other condensed phase systems exposed to ionizing radiation as well, including biological matter.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04371
Precision calibration of calorimeter signals in the ATLAS experiment using an uncertainty-aware neural network,['High Energy Physics - Experiment'],['ATLAS Collaboration'],The ATLAS experiment at the Large Hadron Collider explores the use of modern neural networks for a multi-dimensional calibration of its calorimeter signal defined by clusters of topologically connected cells (topo-clusters). The Bayesian neural network (BNN) approach not only yields a continuous and smooth calibration function that improves performance relative to the standard calibration but also provides uncertainties on the calibrated energies for each topo-cluster. The results obtained by using a trained BNN are compared to the standard local hadronic calibration and to a calibration provided by training a deep neural network. The uncertainties predicted by the BNN are interpreted in the context of a fractional contribution to the systematic uncertainties of the trained calibration. They are also compared to uncertainty predictions obtained from an alternative estimator employing repulsive ensembles.△ Less,"5 December, 2024;",https://arxiv.org/pdf/2412.04370
Finer Behavioral Foundation Models via Auto-Regressive Features and Advantage Weighting,['Machine Learning'],"['Edoardo Cetin', 'Ahmed Touati', 'Yann Ollivier']","The forward-backward representation (FB) is a recently proposed framework (Touati et al., 2023; Touati & Ollivier, 2021) to train behavior foundation models (BFMs) that aim at providing zero-shot efficient policies for any new task specified in a given reinforcement learning (RL) environment, without training for each new task. Here we address two core limitations of FB model training. First, FB, like all successor-feature-based methods, relies on a linear encoding of tasks: at test time, each new reward function is linearly projected onto a fixed set of pre-trained features. This limits expressivity as well as precision of the task representation. We break the linearity limitation by introducing auto-regressive features for FB, which let finegrained task features depend on coarser-grained task information. This can represent arbitrary nonlinear task encodings, thus significantly increasing expressivity of the FB framework. Second, it is well-known that training RL agents from offline datasets often requires specific techniques.We show that FB works well together with such offline RL techniques, by adapting techniques from (Nair et al.,2020b; Cetin et al., 2024) for FB. This is necessary to get non-flatlining performance in some datasets, such as DMC Humanoid. As a result, we produce efficient FB BFMs for a number of new environments. Notably, in the D4RL locomotion benchmark, the generic FB agent matches the performance of standard single-task offline agents (IQL, XQL). In many setups, the offline techniques are needed to get any decent performance at all. The auto-regressive features have a positive but moderate impact, concentrated on tasks requiring spatial precision and task generalization beyond the behaviors represented in the trainset.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04368
Short hierarchically hyperbolic groups II: quotients and the Hopf property for Artin groups,['Group Theory'],"['Giorgio Mangioni', 'Alessandro Sisto']","We prove that most Artin groups of large and hyperbolic type are Hopfian, meaning that every self-epimorphism is an isomorphism. The class covered by our result is generic, in the sense of Goldsborough-Vaskou. Moreover, assuming the residual finiteness of certain hyperbolic groups with an explicit presentation, we get that all large and hyperbolic type Artin groups are residually finite. We also show that most quotients of the five-holed sphere mapping class group are hierarchically hyperbolic, up to taking powers of the normal generators of the kernels. The main tool we use to prove both results is a Dehn-filling-like procedure for short hierarchically hyperbolic groups (these also include e.g. non-geometric 3-manifolds, and triangle- and square-free RAAGs).△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04364
Challenges in Trustworthy Human Evaluation of Chatbots,['Human-Computer Interaction'],"['Wenting Zhao', 'Alexander M. Rush', 'Tanya Goyal']","Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10\% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high-quality human annotations.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04363
The spin-phonon relaxation mechanism of single-molecule magnets in the presence of strong exchange coupling,['Materials Science'],"['Sourav Mondal', 'Julia Netz', 'David Hunger', 'Simon Suhr', 'Biprajit Sarkar', 'Joris van Slageren', 'Andreas Köhn', 'Alessandro Lunghi']","Magnetic relaxation in coordination compounds is largely dominated by the interaction of the spin with phonons. Large zero-field splitting and exchange coupling values have been empirically found to strongly suppress spin relaxation and have been used as the main guideline for designing new molecular compounds. Although a comprehensive understanding of spin-phonon relaxation has been achieved for mononuclear complexes, only a qualitative picture is available for polynuclear compounds. Here we fill this critical knowledge gap by providing a full first-principle description of spin-phonon relaxation in an air-stable Co(II) dimer with both large single-ion anisotropy and exchange coupling. Simulations reproduce the experimental relaxation data with excellent accuracy and provide a microscopic understanding of Orbach and Raman relaxation pathways and their dependency on exchange coupling, zero-field splitting, and molecular vibrations. Theory and numerical simulations show that increasing cluster nuclearity to just four cobalt units would lead to a complete suppression of Raman relaxation. These results hold a general validity for single-molecule magnets, providing a deeper understanding of their relaxation and revised strategies for their improvement.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04362
Fractionalized Magnetization Plateaus in the Shastry-Sutherland Lattice Material Er$_2$Be$_2$GeO$_7$,['Strongly Correlated Electrons'],"['M. Pula', 'S. Sharma', 'J. Gautreau', 'Sajilesh K. P.', 'A. Kanigel', 'C. R. dela Cruz', 'T. N. Dolling', 'L. Clark', 'G. M. Luke']","The experimental study of magnetism on the Shastry-Sutherland lattice has been ongoing for more than two decades, following the discovery of the first Shastry-Sutherland lattice materials SrCu$_2$(BO$_3$)$_2$. However, the study of Shastry-Sutherland systems is often complicated by the requirements of high magnetic fields ($>$~20~T SrCu$_2$(BO$_3$)$_2$) or the presence of itinerate electrons (e.g. REB$_4$). In this paper, we present the magnetic properties of the Shastry-Sutherland lattice material Er$_2$Be$_2$GeO$_7$. Like SrCu$_2$(BO$_3$)$_2$, Er$_2$Be$_2$GeO$_7$ exhibits fractionalized magnetization plateaus. Unlike SrCu$_2$(BO$_3$)$_2$, Er$_2$Be$_2$GeO$_7$ exhibits long-range order below $\sim1~$K, and the plateaus are accessible using commercial laboratory equipment, occurring for fields <~1~T. The fractions of magnetization present are closest to $\frac{1}{4}$ and $\frac{1}{2}$ of the full powder moment; we show that the $\frac{1}{4}$ magnetization plateau in Er$_2$Be$_2$GeO$_7$ has a classical analog, well represented by the magnetic structure (canted antiferromagnetic) observed in powder neutron diffraction. The lack of itinerate electrons, chemical disorder, and the low fields required to access the fractionalized magnetization plateaus promises Er$_2$Be$_2$GeO$_7$ to be a prime candidate for the study of frustrated magnetism on the Shastry-Sutherland lattice.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04360
DARWEN: Data-driven Algorithm for Reduction of Wide Exoplanetary Networks,['Earth and Planetary Astrophysics'],"['A. Lira-Barria', 'J. N. Harvey', 'T. Konings', 'R. Baeyens', 'C. Henríquez', 'L. Decin', 'O. Venot', 'R. Veillet']","Exoplanet atmospheric modeling is advancing from chemically diverse one-dimensional (1D) models to three-dimensional (3D) global circulation models (GCMs), which are crucial for interpreting observations from facilities like the James Webb Space Telescope (JWST) and Extremely Large Telescope (ELT). However, maintaining chemical diversity in models, especially in GCMs, is computationally expensive, limiting their complexity. Optimizing the number of reactions and species can address this tradeoff, but transparent and efficient methods for such optimization are lacking in current exoplanet literature. We aim to develop a systematic approach for reducing chemical networks in exoplanetary atmospheres while balancing accuracy and computational efficiency. Our data-driven method selects optimal reduced chemical networks based on accuracy and computational efficiency metrics. This approach can optimize networks for similar planets simultaneously, assign weights to prioritize accuracy or efficiency, and is applicable when including photochemistry. We base our method on sensitivity analysis of a typical 1D chemical kinetics model, applying principal component analysis to the sensitivities. To achieve fast and reliable network reduction, we utilize a genetic algorithm, a machine-learning optimization method that mimics natural selection. We present three schemes tailored for different priorities (accuracy, computational efficiency, and adaptability to photochemistry) that demonstrate improved performance and reduced computational costs. Our genetic algorithm-based method, the first to reduce a chemical network including photochemistry in exoplanet research, offers a versatile and efficient approach to enhance both accuracy and computational efficiency.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04359
Approximate Top-$k$ for Increased Parallelism,['Machine Learning'],"['Oscar Key', 'Luka Ribar', 'Alberto Cattaneo', 'Luke Hudlass-Galley', 'Douglas Orr']","We present an evaluation of bucketed approximate top-$k$ algorithms. Computing top-$k$ exactly suffers from limited parallelism, because the $k$ largest values must be aggregated along the vector, thus is not well suited to computation on highly-parallel machine learning accelerators. By relaxing the requirement that the top-$k$ is exact, bucketed algorithms can dramatically increase the parallelism available by independently computing many smaller top-$k$ operations. We explore the design choices of this class of algorithms using both theoretical analysis and empirical evaluation on downstream tasks. Our motivating examples are sparsity algorithms for language models, which often use top-$k$ to select the most important parameters or activations. We also release a fast bucketed top-$k$ implementation for PyTorch.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04358
VMGuard: Reputation-Based Incentive Mechanism for Poisoning Attack Detection in Vehicular Metaverse,['Cryptography and Security'],"['Ismail Lotfi', 'Marwa Qaraqe', 'Ali Ghrayeb', 'Dusit Niyato']","The vehicular Metaverse represents an emerging paradigm that merges vehicular communications with virtual environments, integrating real-world data to enhance in-vehicle services. However, this integration faces critical security challenges, particularly in the data collection layer where malicious sensing IoT (SIoT) devices can compromise service quality through data poisoning attacks. The security aspects of the Metaverse services should be well addressed both when creating the digital twins of the physical systems and when delivering the virtual service to the vehicular Metaverse users (VMUs). This paper introduces vehicular Metaverse guard (VMGuard), a novel four-layer security framework that protects vehicular Metaverse systems from data poisoning attacks. Specifically, when the virtual service providers (VSPs) collect data about physical environment through SIoT devices in the field, the delivered content might be tampered. Malicious SIoT devices with moral hazard might have private incentives to provide poisoned data to the VSP to degrade the service quality (QoS) and user experience (QoE) of the VMUs. The proposed framework implements a reputation-based incentive mechanism that leverages user feedback and subjective logic modeling to assess the trustworthiness of participating SIoT devices. More precisely, the framework entails the use of reputation scores assigned to participating SIoT devices based on their historical engagements with the VSPs. Ultimately, we validate our proposed model using comprehensive simulations. Our key findings indicate that our mechanism effectively prevents the initiation of poisoning attacks by malicious SIoT devices. Additionally, our system ensures that reliable SIoT devices, previously missclassified, are not barred from participating in future rounds of the market.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04349
Quantized Hall drift in a frequency-encoded photonic Chern insulator,['Optics'],"['Alexandre Chénier', ""Bosco d'Aligny"", 'Félix Pellerin', 'Paul-Édouard Blanchard', 'Tomoki Ozawa', 'Iacopo Carusotto', 'Philippe St-Jean']","The prospect of developing more efficient classical or quantum photonic devices through the suppression of backscattering is a major driving force for the field of topological photonics. However, genuine protection against backscattering in photonics requires implementing architectures with broken time-reversal which is technically challenging. Here, we make use of a frequency-encoded synthetic dimension scheme in an optical fibre loop platform to experimentally realise a photonic Chern insulator inspired from the Haldane model where time-reversal is explicitly broken through temporal modulation. The bands' topology is assessed by reconstructing the Bloch states' geometry across the Brillouin zone. We further highlight its consequences by measuring a driven-dissipative analogue of the quantized transverse Hall conductivity. Our results thus open the door to harnessing topologically protected unidirectional transport of light in frequency-multiplexed photonic systems.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04347
Coordinate- and spacetime-independent quantum physics,['General Relativity and Quantum Cosmology'],"['V. A. Emelyanov', 'D. Robertz']","The concept of a particle is ambiguous in quantum field theory. It is generally agreed that particles depend not only on spacetime, but also on coordinates used to parametrise spacetime points. One of us has in contrast proposed a coordinate-frame-independent model of quantum particles within the framework of quantum field theory in curved spacetime. The aim of this article is to present a scalar-field-equation solution that is not only a zero-rank tensor under general coordinate transformations, but also common for anti-de-Sitter, de-Sitter, closed and open Einstein static universes. Moreover, it locally reduces to a Minkowski plane-wave solution and is non-perturbative in curvature. The former property makes it suitable for the standard applications of quantum theory in particle physics, while the latter allows then to gain insights into quantum physics in the strong-gravity regime.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04345
Disentangling the influence of excitation energy and compound nucleus angular momentum on fission fragment angular momentum,['Nuclear Experiment'],"['Simone Cannarozzo', 'Stephan Pomp', 'Andreas Solders', 'Ali Al-Adili', 'Zhihao Gao', 'Mattias Lantz', 'Heikki Penttilä', 'Anu Kankainen', 'Iain Moore', 'Tommi Eronen', 'Jouni Ruotsalainen', 'Zhuang Ge', 'Arthur Jaries', 'Maxime Mougeot', 'Andrea Raggio', 'Ville Virtanen']","The origin of the large angular momenta observed for fission fragments is still a question under discussion. To address this, we study isomeric yield ratios (IYR), \textit{i.e.} the relative population of two or more long-lived metastable states with different spins, of fission products.
  We report on IYR of 17 isotopes produced in the 28 MeV $α$-induced fission of $^{232}$Th at the IGISOL facility of the University of Jyv{ä}skyl{ä}. The fissioning nuclei in this reaction are $^{233,234,235}$U*. We compare our data to IYR from thermal neutron-induced fission of $^{233}$U and $^{235}$U, and we observe statistically significant larger IYR in the $^{232}$Th($α$,f) reaction, where the average compound nucleus (CN) spin is 7.5 $\hbar$, than in $^{233,235}$U(n$_{th}$,f), with average spins 2.5 and 3.5 $\hbar$, respectively.
  To assess the influence of the excitation energy, we study literature data of IYR from photon-induced fission reactions, and find that the IYR are independent of the CN excitation energy. We conclude that the different IYR must be explained by the different CN spin alone. This implies that the FF angular momentum only partly comes from the fission process itself, and is in addition influenced by the angular momentum present in the CN.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04340
Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET Image Reconstruction,['Medical Physics'],"['George Webber', 'Yuya Mizuno', 'Oliver D. Howes', 'Alexander Hammers', 'Andrew P. King', 'Andrew J. Reader']","Medical image reconstruction with pre-trained score-based generative models (SGMs) has advantages over other existing state-of-the-art deep-learned reconstruction methods, including improved resilience to different scanner setups and advanced image distribution modeling. SGM-based reconstruction has recently been applied to simulated positron emission tomography (PET) datasets, showing improved contrast recovery for out-of-distribution lesions relative to the state-of-the-art. However, existing methods for SGM-based reconstruction from PET data suffer from slow reconstruction, burdensome hyperparameter tuning and slice inconsistency effects (in 3D). In this work, we propose a practical methodology for fully 3D reconstruction that accelerates reconstruction and reduces the number of critical hyperparameters by matching the likelihood of an SGM's reverse diffusion process to a current iterate of the maximum-likelihood expectation maximization algorithm. Using the example of low-count reconstruction from simulated $[^{18}$F]DPA-714 datasets, we show our methodology can match or improve on the NRMSE and SSIM of existing state-of-the-art SGM-based PET reconstruction while reducing reconstruction time and the need for hyperparameter tuning. We evaluate our methodology against state-of-the-art supervised and conventional reconstruction algorithms. Finally, we demonstrate a first-ever implementation of SGM-based reconstruction for real 3D PET data, specifically $[^{18}$F]DPA-714 data, where we integrate perpendicular pre-trained SGMs to eliminate slice inconsistency issues.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04339
Numerical Aspects of Large Deviations,['Computational Physics'],['Alexander K. Hartmann'],"An introduction to numerical large-deviation sampling is provided. First, direct biasing with a known distribution is explained. As simple example, the Bernoulli experiment is used throughout the text. Next, Markov chain Monte Carlo (MCMC) simulations are introduced. In particular, the Metropolis-Hastings algorithm is explained. As first implementation of MCMC, sampling of the plain Bernoulli model is shown. Next, an exponential bias is used for the same model, which allows one to obtain the tails of the distribution of a measurable quantity. This approach is generalized to MCMC simulations, where the states are vectors of $U(0,1)$ random entries. This allows one to use the exponential or any other bias to access the large-deviation properties of rather arbitrary random processes. Finally, some recent research applications to study more complex models are discussed.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04338
Reflective Teacher: Semi-Supervised Multimodal 3D Object Detection in Bird's-Eye-View via Uncertainty Measure,['Computer Vision and Pattern Recognition'],"['Saheli Hazra', 'Sudip Das', 'Rohit Choudhary', 'Arindam Das', 'Ganesh Sistu', 'Ciaran Eising', 'Ujjwal Bhattacharya']","Applying pseudo labeling techniques has been found to be advantageous in semi-supervised 3D object detection (SSOD) in Bird's-Eye-View (BEV) for autonomous driving, particularly where labeled data is limited. In the literature, Exponential Moving Average (EMA) has been used for adjustments of the weights of teacher network by the student network. However, the same induces catastrophic forgetting in the teacher network. In this work, we address this issue by introducing a novel concept of Reflective Teacher where the student is trained by both labeled and pseudo labeled data while its knowledge is progressively passed to the teacher through a regularizer to ensure retention of previous knowledge. Additionally, we propose Geometry Aware BEV Fusion (GA-BEVFusion) for efficient alignment of multi-modal BEV features, thus reducing the disparity between the modalities - camera and LiDAR. This helps to map the precise geometric information embedded among LiDAR points reliably with the spatial priors for extraction of semantic information from camera images. Our experiments on the nuScenes and Waymo datasets demonstrate: 1) improved performance over state-of-the-art methods in both fully supervised and semi-supervised settings; 2) Reflective Teacher achieves equivalent performance with only 25% and 22% of labeled data for nuScenes and Waymo datasets respectively, in contrast to other fully supervised methods that utilize the full labeled dataset.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04337
On the Replica Symmetry of a Variant of the Sherrington-Kirkpatrick Spin Glass,['Probability'],"['Christian Brennecke', 'Adrien Schertzer']","We consider $N$ i.i.d. Ising spins with mean $m\in (-1,1)$ whose interactions are described by a Sherrington-Kirkpatrick Hamiltonian with a quartic correction. This model was recently introduced by Bolthausen in \cite{Bolt2} as a toy model to understand whether a second moment argument can be used to derive the replica symmetric formula in the full high temperature regime if $m\neq 0$. In \cite{Bolt2}, Bolthausen suggested that a natural analogue of the de Almeida-Thouless condition for the toy model is
  \begin{equation}\label{eq:conj} β^2(1-m^2)^2\leq 1. \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, (1)\end{equation} Here, $β\geq 0$ corresponds to the inverse temperature. While the second moment method implies replica symmetry for $β$ sufficiently small, Bolthausen showed that the method fails to prove replica symmetry in the full region described by (1). A natural question that was left open in \cite{Bolt2} is whether (1) correctly characterizes the high temperature phase of the toy model. In this note, we show that this is indeed not the case. We prove that if $|m| \geq m_*$, for some $m_* \in (0,1)$, the limiting free energy of the toy model is negative for suitable $β$ that satisfy (1).△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04336
Fractional counting process at Lévy times and its applications,['Probability'],"['Shilpa Garg', 'Ashok Kumar Pathak', 'Aditya Maheshwari']","Traditionally, fractional counting processes, such as the fractional Poisson process, etc. have been defined using fractional differential and integral operators. Recently, Laskin (2024) introduced a generalized fractional counting process (FCP) by changing the probability mass function (pmf) of the time fractional Poisson process using the generalized three-parameter Mittag-Leffler function. Here, we study some additional properties for the FCP and introduce a time-changed fractional counting process (TCFCP), defined by time-changing the FCP with an independent Lévy subordinator. We derive distributional properties such as the Laplace transform, probability generating function, the moments generating function, mean, and variance for the TCFCP. Some results related to waiting time distribution and the first passage time distribution are also discussed. We define the multiplicative and additive compound variants for the FCP and the TCFCP and examine their distributional characteristics with some typical examples. We explore some interesting connections of the TCFCP with Bell polynomials by introducing subordinated generalized fractional Bell polynomials. It is shown that the moments of the TCFCP can be represented in terms of the subordinated generalized fractional Bell polynomials. Finally, we present the application of the FCP in a shock deterioration model.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04334
Beta delayed neutron emission of $N=84$ $^{132}$Cd,['Nuclear Experiment'],"['M. Madurga', 'Z. Y. Xu', '1 R. Grzywacz', 'A. Andreyev', 'G. Benzoni', 'M. J. G. Borge', 'C. Costache', 'I. Cox', 'B. Dimitrov', 'P. Van Duppen', 'L. M. Fraile', 'S. Franchoo', 'H. Fynbo', 'B. Gonsalves', 'A. Gottardo', 'P. T. Greenless', 'C. J. Gross', 'L. J. Harkness-Brennan', 'M. Hyuse', 'D. S. Judson', 'S. Kisyov', 'K. Kolos', 'J. Konki', 'J. Kurzewicz', 'I. Lazarus']","Using the time-of-flight technique, we measured the beta-delayed neutron emission of $^{132}$Cd. From our large-scale shell model (LSSM) calculation using the N$^3$LO interaction [Z.Y. Xu et al., Phys. Rev. Lett. 131, 022501 (2023)], we suggest the decay is dominated by the transformation of a neutron in the $g_{7/2}$ orbital, deep below the Fermi surface, into a proton in the $g_{9/2}$ orbital. We compare the beta-decay half-lives and neutron branching ratios of nuclei with $Z<50$ and $N\geq82$ obtained with our LSSM with those of leading ""global"" models. Our calculations match known half-lives and neutron branching ratios well and suggest that current leading models overestimate the yet-to-be-measured half-lives. Our model, backed by the $^{132}$Cd decay data presented here, offers robust predictive power for nuclei of astrophysical interest such as $r$-process waiting points.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04333
High-energy cLFV at $μ$TRISTAN: HNL extensions of the Standard Model,['High Energy Physics - Phenomenology'],"['J. Kriewald', 'E. Pinsard', 'A. M. Teixeira']","Within the context of heavy neutral lepton (HNL) extensions of the Standard Model, we compute the cross-sections for $μ^+ e^-\to \ell_α^+\ell_β^-$ scattering, as well as several angular observables. In particular, we investigate the future sensitivity of a $μ$TRISTAN collider in discovering such charged lepton flavour violating processes and the potential constraining power of these searches on the parameter space of HNL models. Our results show that while low-energy probes of $μ-e$ flavour violation do offer the most promising potential, the prospects for $eτ$ and $μτ$ flavour violation searches at $μ$TRISTAN can exceed those of related low-energy probes (as well as flavour violating $Z$-pole processes at FCC-ee) by several orders of magnitude.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04331
The GAPS programme at TNG LXVI. A homogeneous search for Na i and its possible variability in ten gas giant exoplanets,['Earth and Planetary Astrophysics'],"['D. Sicilia', 'L. Malavolta', 'G. Scandariato', 'L. Fossati', 'A. F. Lanza', 'A. S. Bonomo', 'F. Borsa', 'G. Guilluy', 'V. Nascimbeni', 'L. Pino', 'F. Biassoni', ""M. C. D'Arpa"", 'I. Pagano', 'A. Sozzetti', 'M. Stangret', 'R. Cosentino', 'P. Giacobbe', 'M. Lodi', 'J. Maldonado', 'D. Nardiello', 'M. Pedani']","The neutral sodium resonance doublet (Na i D) has been detected in the upper atmosphere of several close-in gas giants, through high-resolution transmission spectroscopy. We aim to investigate whether its variability is linked to the planets' properties, the data quality, or the accuracy of the system parameters used.
  Using the public code SLOPpy, we extracted the transmission spectrum in the Na i D region of ten gas giants for which a large number of HARPS-N observations are available. We modelled the absorption signals found, performing an MCMC analysis, and converted the measured absorption depth to the corresponding atmospheric height over which most sodium absorption occurs.
  While two targets (GJ 436 b and KELT-7 b) show no Na i D feature, we found variability in the transmission spectrum of the other targets. Three of them (HD 209458 b, WASP-80 b, and WASP-127 b) present absorption on only some nights, while in the other five targets (HD 189733 b, KELT-9 b, KELT-20 b, WASP-69 b, and WASP-76 b), a significant absorption signal is present on most of the nights analysed. Except for WASP-69 b, the measured absorption depths lead to a ratio of the two Na I D depths that is compatible with or slightly larger than one.
  As was expected from literature, the relative atmospheric height follows an empirical exponential trend as a function of a scaled product of the planet's equilibrium temperature and surface gravity.
  We confirm the sodium detection on HD 189733 b, KELT-9 b, KELT-20 b, WASP-69 b, and WASP-76 b. The signal detected in WASP-127 b requires further observations for definitive confirmation. We exclude a planetary origin for the signals found on HD 209458 b and WASP-80 b. The sodium absorption variability does not appear to be related to planetary properties, but rather to data quality, sub-optimal data treatment, or stellar activity.△ Less",v1,https://arxiv.org/pdf/2412.04330
The critical Karp--Sipser core of Erdős--Rényi random graphs,['Probability'],"['Thomas Budzinski', 'Alice Contat']","The Karp--Sipser algorithm consists in removing recursively the leaves as well their unique neighbours and all isolated vertices of a given graph. The remaining graph obtained when there is no leaf left is called the Karp--Sipser core. When the underlying graph is the classical sparse Erdős--Rényi random graph $ \mathrm{G}[n, λ/n]$, it is known to exhibit a phase transition at $λ= \mathrm{e}$. We show that at criticality, the Karp--Sipser core has size of order $n^{3/5}$, which proves a conjecture of Bauer and Golinelli. We provide the asymptotic law of this renormalized size as well as a description of the distribution of the core as a graph. Our approach relies on the differential equation method, and builds up on a previous work on a configuration model with bounded degrees.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04328
Action Mapping for Reinforcement Learning in Continuous Environments with Constraints,['Machine Learning'],"['Mirco Theile', 'Lukas Dirnberger', 'Raphael Trumpp', 'Marco Caccamo', 'Alberto L. Sangiovanni-Vincentelli']","Deep reinforcement learning (DRL) has had success across various domains, but applying it to environments with constraints remains challenging due to poor sample efficiency and slow convergence. Recent literature explored incorporating model knowledge to mitigate these problems, particularly through the use of models that assess the feasibility of proposed actions. However, integrating feasibility models efficiently into DRL pipelines in environments with continuous action spaces is non-trivial. We propose a novel DRL training strategy utilizing action mapping that leverages feasibility models to streamline the learning process. By decoupling the learning of feasible actions from policy optimization, action mapping allows DRL agents to focus on selecting the optimal action from a reduced feasible action set. We demonstrate through experiments that action mapping significantly improves training performance in constrained environments with continuous action spaces, especially with imperfect feasibility models.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04327
Clustering-induced localization of quantum walks on networks,['Quantum Physics'],"['Lucas Böttcher', 'Mason A. Porter']","Quantum walks on networks are a paradigmatic model in quantum information theory. Quantum-walk algorithms have been developed for various applications, including spatial-search problems, element-distinctness problems, and node centrality analysis. Unlike their classical counterparts, the evolution of quantum walks is unitary, so they do not converge to a stationary distribution. However, it is important for many applications to understand the long-time behavior of quantum walks and the impact of network structure on their evolution. In the present paper, we study the localization of quantum walks on networks. We demonstrate how localization emerges in highly clustered networks that we construct by recursively attaching triangles, and we derive an analytical expression for the long-time inverse participation ratio that depends on products of eigenvectors of the quantum-walk Hamiltonian. Building on the insights from this example, we then show that localization also occurs in Kleinberg navigable small-world networks and Holme--Kim power-law cluster networks. Our results illustrate that local clustering, which is a key structural feature of networks, can induce localization of quantum walks.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04325
Multi-Subject Image Synthesis as a Generative Prior for Single-Subject PET Image Reconstruction,['Medical Physics'],"['George Webber', 'Yuya Mizuno', 'Oliver D. Howes', 'Alexander Hammers', 'Andrew P. King', 'Andrew J. Reader']","Large high-quality medical image datasets are difficult to acquire but necessary for many deep learning applications. For positron emission tomography (PET), reconstructed image quality is limited by inherent Poisson noise. We propose a novel method for synthesising diverse and realistic pseudo-PET images with improved signal-to-noise ratio. We also show how our pseudo-PET images may be exploited as a generative prior for single-subject PET image reconstruction. Firstly, we perform deep-learned deformable registration of multi-subject magnetic resonance (MR) images paired to multi-subject PET images. We then use the anatomically-learned deformation fields to transform multiple PET images to the same reference space, before averaging random subsets of the transformed multi-subject data to form a large number of varying pseudo-PET images. We observe that using MR information for registration imbues the resulting pseudo-PET images with improved anatomical detail compared to the originals. We consider applications to PET image reconstruction, by generating pseudo-PET images in the same space as the intended single-subject reconstruction and using them as training data for a diffusion model-based reconstruction method. We show visual improvement and reduced background noise in our 2D reconstructions as compared to OSEM, MAP-EM and an existing state-of-the-art diffusion model-based approach. Our method shows the potential for utilising highly subject-specific prior information within a generative reconstruction framework. Future work may compare the benefits of our approach to explicitly MR-guided reconstruction methodologies.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04324
Strange metal transport from coupling to fluctuating spins,['Strongly Correlated Electrons'],"['Simone Fratini', 'Arnaud Ralko', 'Sergio Ciuchi']","Metals hosting strong electronic interactions, including high-temperature superconductors, behave in ways that do not conform to the normal Fermi liquid theory. To pinpoint the microscopic origin of this strange metal behavior, here we reexamine the transport and optical properties of the two-dimensional t-J model taking advantage of recent improvements made on the finite temperature Lanczos method (FTLM), enabling numerically exact calculations at unprecedentedly low temperatures and high spectral resolution. We find that strange metallicity is pervasive in the temperature-doping phase diagram wherever magnetic order is suppressed: it is driven by the presence of a fluctuating spin background, and it is therefore independent on hole concentration and unrelated to quantum criticality. Our results point to a two-step transport mechanism, with short- and long-time processes associated respectively with the charge and spin dynamics, the latter being responsible for both the strange metal character and the unconventional optical conductivities seen in experiments.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04322
No evidence of magma ocean on Io based on Juno/JIRAM data,['Earth and Planetary Astrophysics'],"['Federico Tosi', 'Alessandro Mura', 'Francesca Zambon']","A recent paper (ref. 1) used infrared images of Io acquired by the Juno/JIRAM instrument to derive a latitudinal dependence of the spectral radiance and conclude that such latitudinal dependence is consistent with a magma ocean model. We challenge their conclusions, and we draw attention to some potential issues with their analysis. In this letter, we will use three arguments to show that: (1) the (ref. 1) paper uses saturated data; (2) the M-filter of the JIRAM imager is only a weak and incomplete proxy for the total power output; and finally (3) even assuming that the radiance was correctly estimated, the latitudinal dependence of the 4.8-$μ$m spectral radiance is not statistically significant. These facts, taken together, demonstrate that the results presented in (ref. 1) are not sufficient to confirm consistency with a magma ocean model on Io.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04321
Egorov's theorem in the Weyl--Hörmander calculus,['Analysis of PDEs'],['Antoine Prouff'],"We prove a general version of Egorov's theorem for evolution propagators in the Euclidean space, in the Weyl--Hörmander framework of metrics on the phase space. Mild assumptions on the Hamiltonian allow for a wide range of applications that we describe in the paper, including Schrödinger, wave and transport evolutions. We also quantify an Ehrenfest time and describe the full symbol of the conjugated operator. Our main result is a consequence of a stronger theorem on the propagation of quantum partitions of unity.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04320
Generative-Model-Based Fully 3D PET Image Reconstruction by Conditional Diffusion Sampling,['Medical Physics'],"['George Webber', 'Yuya Mizuno', 'Oliver D. Howes', 'Alexander Hammers', 'Andrew P. King', 'Andrew J. Reader']","Score-based generative models (SGMs) have recently shown promising results for image reconstruction on simulated positron emission tomography (PET) datasets. In this work we have developed and implemented practical methodology for 3D image reconstruction with SGMs, and perform (to our knowledge) the first SGM-based reconstruction of real fully 3D PET data. We train an SGM on full-count reference brain images, and extend methodology to allow SGM-based reconstructions at very low counts (1% of original, to simulate low-dose or short-duration scanning). We then perform reconstructions for multiple independent realisations of 1% count data, allowing us to analyse the bias and variance characteristics of the method. We sample from the learned posterior distribution of the generative algorithm to calculate uncertainty images for our reconstructions. We evaluate the method's performance on real full- and low-count PET data and compare with conventional OSEM and MAP-EM baselines, showing that our SGM-based low-count reconstructions match full-dose reconstructions more closely and in a bias-variance trade-off comparison, our SGM-reconstructed images have lower variance than existing baselines. Future work will compare to supervised deep-learned methods, with other avenues for investigation including how data conditioning affects the SGM's posterior distribution and the algorithm's performance with different tracers.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04319
LocalSR: Image Super-Resolution in Local Region,['Computer Vision and Pattern Recognition'],"['Bo Ji', 'Angela Yao']","Standard single-image super-resolution (SR) upsamples and restores entire images. Yet several real-world applications require higher resolutions only in specific regions, such as license plates or faces, making the super-resolution of the entire image, along with the associated memory and computational cost, unnecessary. We propose a novel task, called LocalSR, to restore only local regions of the low-resolution image. For this problem setting, we propose a context-based local super-resolution (CLSR) to super-resolve only specified regions of interest (ROI) while leveraging the entire image as context. Our method uses three parallel processing modules: a base module for super-resolving the ROI, a global context module for gathering helpful features from across the image, and a proximity integration module for concentrating on areas surrounding the ROI, progressively propagating features from distant pixels to the target region. Experimental results indicate that our approach, with its reduced low complexity, outperforms variants that focus exclusively on the ROI.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04314
A solution to the extreme point problem and other applications of Choquet theory to Lipschitz-free spaces,['Functional Analysis'],"['Ramón J. Aliaga', 'Eva Pernecká', 'Richard J. Smith']","We prove that every element of a Lipschitz-free space admits an expression as a convex series of elements with compact support. As a consequence, we conclude that all extreme points of the unit ball of Lipschitz-free spaces are elementary molecules, solving a long-standing problem. We also deduce that all elements of a Lipschitz-free space with the Radon-Nikodým property can be expressed as convex integrals of molecules. Our results are based on a recent theory of integral representation for functionals on Lipschitz spaces which draws on classical Choquet theory, due to the third named author.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04312
Lorentzian metric spaces and GH-convergence: the unbounded case,['Metric Geometry'],"['A. Bykov', 'E. Minguzzi', 'S. Suhr']","We introduce a notion of Lorentzian metric space which drops the boundedness condition from our previous work and argue that the properties defining our spaces are minimal. In fact, they are defined by three conditions given by (a) the reverse triangle inequality for chronologically related events, (b) Lorentzian distance continuity and relative compactness of chronological diamonds, and (c) a distinguishing condition via the Lorentzian distance function. By adding a countably generating condition we confirm the validity of desirable properties for our spaces including the Polish property. The definition of (pre)length space given in our previous work on the bounded case is generalised to this setting. We also define a notion of Gromov-Hausdorff convergence for Lorentzian metric spaces and prove that (pre)length spaces are GH-stable. It is also shown that our (sequenced) Lorentzian metric space bring a natural quasi-uniformity (resp.\ quasi-metric). Finally, an explicit comparison with other recent constructions based on our previous work on bounded Lorentzian metric spaces is presented.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04311
Quantum trails and memory effects in the phase space of chaotic quantum systems,['Quantum Physics'],['Andrea Pizzi'],"The eigenstates of a chaotic system can be enhanced along underlying unstable periodic orbits in so-called quantum scars, making it more likely for a particle launched along one such orbits to be found still there at long times. Unstable periodic orbits are however a negligible part of the phase space, and a question arises regarding the structure of the wavefunction elsewhere. Here, we address this question and show that a weakly-dispersing dynamics of a localized wavepacket in phase space leaves a ""quantum trail"" on the eigenstates, that is, makes them vary slowly when moving along a trajectories in phase space, even if not periodic. The quantum trails underpin a remarkable dynamical effect: for a system initialized in a localized wavepacket, the long-time phase-space distribution is enhanced along the short-time trajectory, which can result in ergodicity breaking. We provide the general intuition for these effects and prove them in the stadium billiard, for which an unwarping procedure allows to visualize the phase space on the two-dimensional space of the page.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04310
The Tile: A 2D Map of Ranking Scores for Two-Class Classification,['Computer Vision and Pattern Recognition'],"['Sébastien Piérard', 'Anaïs Halin', 'Anthony Cioppa', 'Adrien Deliège', 'Marc Van Droogenbroeck']","In the computer vision and machine learning communities, as well as in many other research domains, rigorous evaluation of any new method, including classifiers, is essential. One key component of the evaluation process is the ability to compare and rank methods. However, ranking classifiers and accurately comparing their performances, especially when taking application-specific preferences into account, remains challenging. For instance, commonly used evaluation tools like Receiver Operating Characteristic (ROC) and Precision/Recall (PR) spaces display performances based on two scores. Hence, they are inherently limited in their ability to compare classifiers across a broader range of scores and lack the capability to establish a clear ranking among classifiers. In this paper, we present a novel versatile tool, named the Tile, that organizes an infinity of ranking scores in a single 2D map for two-class classifiers, including common evaluation scores such as the accuracy, the true positive rate, the positive predictive value, Jaccard's coefficient, and all F-beta scores. Furthermore, we study the properties of the underlying ranking scores, such as the influence of the priors or the correspondences with the ROC space, and depict how to characterize any other score by comparing them to the Tile. Overall, we demonstrate that the Tile is a powerful tool that effectively captures all the rankings in a single visualization and allows interpreting them.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04309
ALMA: Alignment with Minimal Annotation,['Computation and Language'],"['Michihiro Yasunaga', 'Leonid Shamis', 'Chunting Zhou', 'Andrew Cohen', 'Jason Weston', 'Luke Zettlemoyer', 'Marjan Ghazvininejad']","Recent approaches to large language model (LLM) alignment typically require millions of human annotations or rely on external aligned models for synthetic data generation. This paper introduces ALMA: Alignment with Minimal Annotation, demonstrating that effective alignment can be achieved using only 9,000 labeled examples -- less than 1% of conventional approaches. ALMA generates large amounts of high-quality synthetic alignment data through new techniques: diverse prompt synthesis via few-shot learning, diverse response generation with multiple model checkpoints, and judge (reward model) enhancement through score aggregation and self-distillation. Using only a pretrained Llama3 base model, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves performance close to Llama3-Instruct across diverse alignment benchmarks (e.g., 0.1% difference on AlpacaEval 2.0 score). These results are achieved with a multi-round, self-bootstrapped data synthesis and training recipe that continues to improve for 10 rounds, surpassing the typical 3-round ceiling of previous methods. These results suggest that base models already possess sufficient knowledge for effective alignment, and that synthetic data generation methods can expose it.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04305
Ultrafast core-to-core luminescence in BaF$_2$-LaF$_3$ single crystals,['Materials Science'],"['Roman Shendrik', 'Evgeny Radzhabov', 'Alexandra Myasnikova', 'Viktorija Pankratova', 'Anatolijs Šarakovskis', 'Alexander Nepomnyashchikh', 'Alexander Bogdanov', 'Veronika Gavrilenko', 'Vladimir Pankratov']","This study investigates the mechanisms underlying ultrafast cross-luminescence observed in BaF$_2$ crystals doped with LaF$_3$. We identified an ultrafast luminescent component with a decay time of approximately 150 ps, which emerges under excitation energies exceeding 24 eV as a novel radiative recombination process between electrons in the 5p core band of Ba2+ and holes in the 5p core band of La$^{3+}$. Ab initio calculations support this hypothesis, showing that the energy levels of the core bands facilitate such transitions. The findings indicate that BaF$_2$-LaF$_3$ scintillators hold significant promise for applications in time-of-flight tomography.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04303
SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion,['Computer Vision and Pattern Recognition'],"['Trong-Tung Nguyen', 'Quang Nguyen', 'Khoi Nguyen', 'Anh Tran', 'Cuong Pham']","Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04301
Investigating Kozai-Lidov Oscillations and Disc Tearing in Be Star Discs,['Solar and Stellar Astrophysics'],"['M. W. Suffak', 'C. E. Jones', 'A. C. Carciofi']","Recent simulations of Be stars in misaligned binary systems have revealed that misalignment between the disc and binary orbit can cause the disc to undergo Kozai-Lidov (KL) oscillations or disc-tearing. We build on our previous suite of three-dimensional smoothed particle hydrodynamics simulations of equal-mass systems by simulating eight new misaligned Be star binary systems, with mass-ratios of 0.1 and 0.5, or equal-mass systems with varying viscosity. We find the same phenomena occur as previously for mass ratios of 0.5, while the mass ratio of 0.1 does not cause KL oscillations or disc-tearing for the parameters examined. With increased viscosity in our equal-mass simulations, we show that these phenomena and other oscillations are damped out and do not occur. We also briefly compare two viscosity prescriptions and find they can produce the same qualitative disc evolution. Next, we use the radiative transfer code HDUST to predict observable trends of a KL oscillation, and show how the observables oscillate in sync with disc inclination and cause large changes in the polarization position angle. Our models generate highly complex line profiles, including triple-peak profiles that are known to occur in Be stars. The mapping between the SPH simulations and these triple-peak features gives us hints as to where they originate. Finally, we construct interferometric predictions of how a gap in the disc, produced by KL oscillations or disc-tearing, perturbs the visibility versus baseline curve at multiple wavelengths, and can cause large changes to the differential phase profile across an emission line.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04299
Energy Scalability Limits of Dissipative Solitons,['Optics'],"['Vladimir L. Kalashnikov', 'Alexander Rudenkov', 'Evgeni Sorokin', 'Irina T. Sorokina']","In this study, we apply a thermodynamical approach to elucidate the primary constraints on the energy scaling of dissipative solitons (DS). We rely on the adiabatic theory of strongly chirped DS and define the DS energy scaling in terms of dissipative soliton resonance (DSR). Three main experimentally verifiable signatures identify a transition to DSR: i) growth of a Lorentzian spike at the centrum of the DS spectrum, which resembles a spectral condensation in Bose-Einstein condensate (BEC), ii) saturation of the spectrum broadening, and iii) asymptotical DS stretching. We connect the DSR breakup with three critical factors: i) decoupling of two correlation scales inherent in strongly chirped DS, ii) resulting rise of the DS entropy with energy, which provokes its disintegration, and iii) transition to a nonequilibrium phase, which is characterized by negative temperature. The breakup results in multiple stable DSs with lower energy. Theoretical results are in good qualitative agreement with the experimental data from a Kerr-lens mode-locked Cr$^{2+}$:ZnS chirped-pulse oscillator (CPO) that paves the way for optimizing high-energy femtosecond pulse generation in solid-state CPO and all-normal-dispersion fiber lasers.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04297
Delay-Doppler Signal Processing with Zadoff-Chu Sequences,['Signal Processing'],"['Sandesh Rao Mattu', 'Imran Ali Khan', 'Venkatesh Khammammetti', 'Beyza Dabak', 'Saif Khan Mohammed', 'Krishna Narayanan', 'Robert Calderbank']","Much of the engineering behind current wireless systems has focused on designing an efficient and high-throughput downlink to support human-centric communication such as video streaming and internet browsing. This paper looks ahead to design of the uplink, anticipating the emergence of machine-type communication (MTC) and the confluence of sensing, communication, and distributed learning. We demonstrate that grant-free multiple access is possible even in the presence of highly time-varying channels. Our approach provides a pathway to standards adoption, since it is built on enhancing the 2-step random access procedure which is already part of the 5GNR standard. This 2-step procedure uses Zadoff-Chu (ZC) sequences as preambles that point to radio resources which are then used to upload data. We also use ZC sequences as preambles / pilots, but we process signals in the Delay-Doppler (DD) domain rather than the time-domain. We demonstrate that it is possible to detect multiple preambles in the presence of mobility and delay spread using a receiver with no knowledge of the channel other than the worst case delay and Doppler spreads. Our approach depends on the mathematical properties of ZC sequences in the DD domain. We derive a closed form expression for ZC pilots in the DD domain, we characterize the possible self-ambiguity functions, and we determine the magnitude of the possible cross-ambiguity functions. These mathematical properties enable detection of multiple pilots through solution of a compressed sensing problem. The columns of the compressed sensing matrix are the translates of individual ZC pilots in delay and Doppler. We show that columns in the design matrix satisfy a coherence property that makes it possible to detect multiple preambles in a single Zak-OTFS subframe using One-Step Thresholding (OST), which is an algorithm with low complexity.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04295
Evolutionary Pre-Prompt Optimization for Mathematical Reasoning,['Computation and Language'],"['Mathurin Videau', 'Alessandro Leite', 'Marc Schoenauer', 'Olivier Teytaud']","Recent advancements have highlighted that large language models (LLMs), when given a small set of task-specific examples, demonstrate remarkable proficiency, a capability that extends to complex reasoning tasks. In particular, the combination of few-shot learning with the chain-of-thought (CoT) approach has been pivotal in steering models towards more logically consistent conclusions. This paper explores the optimization of example selection for designing effective CoT pre-prompts and shows that the choice of the optimization algorithm, typically in favor of comparison-based methods such as evolutionary computation, significantly enhances efficacy and feasibility. Specifically, thanks to a limited exploitative and overfitted optimization, Evolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the naive few-shot approach exceeding 10 absolute points in exact match scores on benchmark datasets such as GSM8k and MathQA. These gains are consistent across various contexts and are further amplified when integrated with self-consistency (SC)△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04291
Approximately Jumping Towards the Origin,['Probability'],"['Alex Albors', 'François Clément', 'Shosuke Kiami', 'Braeden Sodt', 'Ding Yifan', 'Tony Zeng']","Given an initial point $x_0 \in \mathbb{R}^d$ and a sequence of vectors $v_1, v_2, \dots$ in $\mathbb{R}^d$, we define a greedy sequence by setting $x_{n} = x_{n-1} \pm v_n$ where the sign is chosen so as to minimize $\|x_n\|$. We prove that if the vectors $v_i$ are chosen uniformly at random from $\mathbb{S}^{d-1}$ then elements of the sequence are, on average, approximately at distance $\|x_n\| \sim \sqrt{πd/8}$ from the origin. We show that the sequence $(\|x_n\|)_{n=1}^{\infty}$ has an invariant measure $π_d$ depending only on $d$ and we determine its mean and study its decay for all $d$. We also investigate a completely deterministic example in $d=2$ where the $v_n$ are derived from the van der Corput sequence. Several additional examples are considered.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04284
Targeted Hard Sample Synthesis Based on Estimated Pose and Occlusion Error for Improved Object Pose Estimation,['Computer Vision and Pattern Recognition'],"['Alan Li', 'Angela P. Schoellig']","6D Object pose estimation is a fundamental component in robotics enabling efficient interaction with the environment. It is particularly challenging in bin-picking applications, where objects may be textureless and in difficult poses, and occlusion between objects of the same type may cause confusion even in well-trained models. We propose a novel method of hard example synthesis that is model-agnostic, using existing simulators and the modeling of pose error in both the camera-to-object viewsphere and occlusion space. Through evaluation of the model performance with respect to the distribution of object poses and occlusions, we discover regions of high error and generate realistic training samples to specifically target these regions. With our training approach, we demonstrate an improvement in correct detection rate of up to 20% across several ROBI-dataset objects using state-of-the-art pose estimation models.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04279
Peccei-Quinn charges in the 3-3-1 model with $U(1)_{B-L}$ symmetry,['High Energy Physics - Phenomenology'],"['H. N. Long', 'H. T. Hung', 'V. H. Binh', 'A. B. Arbuzov']","The Peccei-Quinn (PQ) mechanism is applied within the $\mathrm{SU(3)_c \otimes SU(3)_L \otimes U(1)_X}$ model with B-L symmetry. The structures in the PQ charges
  of all fermions and scalar fields in the model are investigated by applying the invariant condition under
  the symmetry group on all Yukawa interaction terms. All defined PQ charges just depend on PQ charge of the only complex singlet scalar in the model. The mixing and mass hierachy in the scalar sector of the model are studied in detail. As the result, the physical state of axion is a linear combination
  of all imaginary parts of scalar multiplets as general form. Explicit expressions for axion and light Standard Model like Higgs boson are shown. Mass of the axion and its coupling to photon are derived.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04269
Integrated Minimum Mean Squared Error Algorithms for Combined Acoustic Echo Cancellation and Noise Reduction,['Audio and Speech Processing'],"['Arnout Roebben', 'Toon van Waterschoot', 'Jan Wouters', 'Marc Moonen']","In many speech recording applications, noise and acoustic echo corrupt the desired speech. Consequently, combined noise reduction (NR) and acoustic echo cancellation (AEC) is required. Generally, a cascade approach is followed, i.e., the AEC and NR are designed in isolation by selecting a separate signal model, formulating a separate cost function, and using a separate solution strategy. The AEC and NR are then cascaded one after the other, not accounting for their interaction. In this paper, however, an integrated approach is proposed to consider this interaction in a general multi-microphone/multi-loudspeaker setup. Therefore, a single signal model of either the microphone signal vector or the extended signal vector, obtained by stacking microphone and loudspeaker signals, is selected, a single mean squared error cost function is formulated, and a common solution strategy is used. Using this microphone signal model, a multi channel Wiener filter (MWF) is derived. Using the extended signal model, an extended MWF (MWFext) is derived, and several equivalent expressions are found, which nevertheless are interpretable as cascade algorithms. Specifically, the MWFext is shown to be equivalent to algorithms where the AEC precedes the NR (AEC NR), the NR precedes the AEC (NR-AEC), and the extended NR (NRext) precedes the AEC and post-filter (PF) (NRext-AECPF). Under rank-deficiency conditions the MWFext is non-unique, such that this equivalence amounts to the expressions being specific, not necessarily minimum-norm solutions for this MWFext. The practical performances nonetheless differ due to non-stationarities and imperfect correlation matrix estimation, resulting in the AEC-NR and NRext-AEC-PF attaining best overall performance.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04267
Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier,['Computation and Language'],"['John Dang', 'Shivalika Singh', ""Daniel D'souza"", 'Arash Ahmadian', 'Alejandro Salamanca', 'Madeline Smith', 'Aidan Peppin', 'Sungjin Hong', 'Manoj Govindassamy', 'Terrence Zhao', 'Sandra Kublik', 'Meor Amer', 'Viraat Aryabumi', 'Jon Ander Campos', 'Yi-Chern Tan', 'Tom Kocmi', 'Florian Strub', 'Nathan Grinsztajn', 'Yannis Flet-Berliac', 'Acyr Locatelli', 'Hangyu Lin', 'Dwarak Talupuru', 'Bharat Venkitesh', 'David Cairuz', 'Bowen Yang']","We introduce the Aya Expanse model family, a new generation of 8B and 32B parameter multilingual language models, aiming to address the critical challenge of developing highly performant multilingual models that match or surpass the capabilities of monolingual models. By leveraging several years of research at Cohere For AI and Cohere, including advancements in data arbitrage, multilingual preference training, and model merging, Aya Expanse sets a new state-of-the-art in multilingual performance. Our evaluations on the Arena-Hard-Auto dataset, translated into 23 languages, demonstrate that Aya Expanse 8B and 32B outperform leading open-weight models in their respective parameter classes, including Gemma 2, Qwen 2.5, and Llama 3.1, achieving up to a 76.6% win-rate. Notably, Aya Expanse 32B outperforms Llama 3.1 70B, a model with twice as many parameters, achieving a 54.0% win-rate. In this short technical report, we present extended evaluation results for the Aya Expanse model family and release their open-weights, together with a new multilingual evaluation dataset m-ArenaHard.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04261
SCADE: Scalable Command-line Anomaly Detection Engine,['Cryptography and Security'],"['Vaishali Vinay', 'Anjali Mangal']","As command-line interfaces remain an integral part of high-computation environments, the risk of exploitation through stealthy, complex command-line abuse continues to grow. Conventional security solutions often struggle with these command-line-based anomalies due to their context-specific nature and lack of labeled data, especially in detecting rare, malicious patterns amidst legitimate, high-volume activity. This gap has left organizations vulnerable to sophisticated threats like Living-off-the-Land (LOL) attacks, where standard detection tools frequently miss or misclassify anomalous command-line behavior. We introduce Scalable Command-Line Anomaly Detection Engine (SCADE), who addresses these challenges by introducing a dual-layered detection framework that combines a global statistical analysis with local context-specific anomaly detection, innovatively using a novel ensemble of statistical models such as BM25 and Log Entropy, adapted for command-line data. The framework also features a dynamic thresholding mechanism for adaptive anomaly detection, ensuring high precision and recall even in environments with extremely high Signal-to-Noise Ratios (SNRs). Initial experimental results demonstrate the effectiveness of the framework, achieving above 98% SNR in identifying unusual command-line behavior while minimizing false positives. In this paper, we present SCADE's core architecture, including its metadata-enriched approach to anomaly detection and the design choices behind its scalability for enterprise-level deployment. We argue that SCADE represents a significant advancement in command-line anomaly detection, offering a robust, adaptive framework for security analysts and researchers seeking to enhance detection accuracy in high-computation environments.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04259
Model-Agnostic Meta-Learning for Fault Diagnosis of Induction Motors in Data-Scarce Environments with Varying Operating Conditions and Electric Drive Noise,['Systems and Control'],"['Ali Pourghoraba', 'MohammadSadegh KhajueeZadeh', 'Ali Amini', 'Abolfazl Vahedi', 'Gholam Reza Agah', 'Akbar Rahideh']","Reliable mechanical fault detection with limited data is crucial for the effective operation of induction machines, particularly given the real-world challenges present in industrial datasets, such as significant imbalances between healthy and faulty samples and the scarcity of data representing faulty conditions. This research introduces an innovative meta-learning approach to address these issues, focusing on mechanical fault detection in induction motors across diverse operating conditions while mitigating the adverse effects of drive noise in scenarios with limited data. The process of identifying faults under varying operating conditions is framed as a few-shot classification challenge and approached through a model-agnostic meta-learning strategy. Specifically, this approach begins with training a meta-learner across multiple interconnected fault-diagnosis tasks conducted under different operating conditions. In this stage, cross-entropy is utilized to optimize parameters and develop a robust representation of the tasks. Subsequently, the parameters of the meta-learner are fine-tuned for new tasks, enabling rapid adaptation using only a small number of samples. This method achieves excellent accuracy in fault detection across various conditions, even when data availability is restricted. The findings indicate that the proposed model outperforms other sophisticated techniques, providing enhanced generalization and quicker adaptation. The accuracy of fault diagnosis reaches a minimum of 99%, underscoring the model's effectiveness for reliable fault identification.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04255
CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations,['Computation and Language'],"['Subash Neupane', 'Himanshu Tripathi', 'Shaswata Mitra', 'Sean Bozorgzad', 'Sudip Mittal', 'Shahram Rahimi', 'Amin Amirlatifi']","This paper presents ClinicSum, a novel framework designed to automatically generate clinical summaries from patient-doctor conversations. It utilizes a two-module architecture: a retrieval-based filtering module that extracts Subjective, Objective, Assessment, and Plan (SOAP) information from conversation transcripts, and an inference module powered by fine-tuned Pre-trained Language Models (PLMs), which leverage the extracted SOAP data to generate abstracted clinical summaries. To fine-tune the PLM, we created a training dataset of consisting 1,473 conversations-summaries pair by consolidating two publicly available datasets, FigShare and MTS-Dialog, with ground truth summaries validated by Subject Matter Experts (SMEs). ClinicSum's effectiveness is evaluated through both automatic metrics (e.g., ROUGE, BERTScore) and expert human assessments. Results show that ClinicSum outperforms state-of-the-art PLMs, demonstrating superior precision, recall, and F-1 scores in automatic evaluations and receiving high preference from SMEs in human assessment, making it a robust solution for automated clinical summarization.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04254
GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities,['Computer Vision and Pattern Recognition'],"['Rao Fu', 'Dingxi Zhang', 'Alex Jiang', 'Wanjia Fu', 'Austin Funk', 'Daniel Ritchie', 'Srinath Sridhar']","Understanding bimanual human hand activities is a critical problem in AI and robotics. We cannot build large models of bimanual activities because existing datasets lack the scale, coverage of diverse hand activities, and detailed annotations. We introduce GigaHands, a massive annotated dataset capturing 34 hours of bimanual hand activities from 56 subjects and 417 objects, totaling 14k motion clips derived from 183 million frames paired with 84k text annotations. Our markerless capture setup and data acquisition protocol enable fully automatic 3D hand and object estimation while minimizing the effort required for text annotation. The scale and diversity of GigaHands enable broad applications, including text-driven action synthesis, hand motion captioning, and dynamic radiance field reconstruction.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04244
Quantifying the Limits of Segment Anything Model: Analyzing Challenges in Segmenting Tree-Like and Low-Contrast Structures,['Computer Vision and Pattern Recognition'],"['Yixin Zhang', 'Nicholas Konz', 'Kevin Kramer', 'Maciej A. Mazurowski']","Segment Anything Model (SAM) has shown impressive performance in interactive and zero-shot segmentation across diverse domains, suggesting that they have learned a general concept of ""objects"" from their large-scale training. However, we observed that SAM struggles with certain types of objects, particularly those featuring dense, tree-like structures and low textural contrast from their surroundings. These failure modes are critical for understanding its limitations in real-world use. In order to systematically examine this issue, we propose metrics to quantify two key object characteristics: tree-likeness and textural separability. Through extensive controlled synthetic experiments and testing on real datasets, we demonstrate that SAM's performance is noticeably correlated with these factors. We link these behaviors under the concept of ""textural confusion"", where SAM misinterprets local structure as global texture, leading to over-segmentation, or struggles to differentiate objects from similarly textured backgrounds. These findings offer the first quantitative framework to model SAM's challenges, providing valuable insights into its limitations and guiding future improvements for vision foundation models.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04243
On Deep-Learning-Based Closures for Algebraic Surrogate Models of Turbulent Flows,['Fluid Dynamics'],"['Benet Eiximeno', 'Marcial Sanchís-Agudo', 'Arnau Miró', 'Ivette Rodríguez', 'Ricardo Vinuesa', 'Oriol Lehmkuhl']","A deep-learning-based closure model to address energy loss in low-dimensional surrogate models based on proper-orthogonal-decomposition (POD) modes is introduced. Using a transformer-encoder block with easy-attention mechanism, the model predicts the spatial probability density function of fluctuations not captured by the truncated POD modes. The methodology is demonstrated on the wake of the Windsor body at yaw angles of [2.5,5,7.5,10,12.5], with 7.5 as a test case. Key coherent modes are identified by clustering them based on dominant frequency dynamics using Hotelling T2 on the spectral properties of temporal coefficients. These coherent modes account for nearly 60% of the total energy while comprising less than 10% of all modes. A common POD basis is created by concatenating coherent modes from training angles and orthonormalizing the set, reducing the basis vectors from 142 to 90 without losing information. Transformers with different size on the attention layer, (64, 128 and 256), are trained to model the missing fluctuations. Larger attention sizes always improve predictions for the training set, but the transformer with an attention layer of size 256 overshoots the fluctuations predictions in the test set because they have lower intensity than in the training cases. Adding the predicted fluctuations closes the energy gap between the reconstruction and the original flow field, improving predictions for energy, root-mean-square velocity fluctuations, and instantaneous flow fields. The deepest architecture reduces mean energy error from 37% to 12% and decreases the Kullback--Leibler divergence of velocity distributions from KL=0.2 to below KL=0.026.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04239
VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction,['Computer Vision and Pattern Recognition'],"['Jiahao Zhang', 'Ryota Yoshihashi', 'Shunsuke Kitada', 'Atsuki Osanai', 'Yuta Nakashima']","Large language models (LLMs) have proven effective for layout generation due to their ability to produce structure-description languages, such as HTML or JSON, even without access to visual information. Recently, LLM providers have evolved these models into large vision-language models (LVLM), which shows prominent multi-modal understanding capabilities. Then, how can we leverage this multi-modal power for layout generation? To answer this, we propose Visual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based content-aware layout generation. In our method, LVLMs iteratively refine their outputs with reference to rendered layout images, which are visualized as colored bounding boxes on poster backgrounds. In experiments, we demonstrate that our method combined with the Gemini. Without any additional training, VASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming both existing layout-specific generative models and other LLM-based methods.△ Less",v1,https://arxiv.org/pdf/2412.04237
HyperMARL: Adaptive Hypernetworks for Multi-Agent RL,['Machine Learning'],"['Kale-ab Abebe Tessera', 'Arrasy Rahman', 'Stefano V. Albrecht']","Balancing individual specialisation and shared behaviours is a critical challenge in multi-agent reinforcement learning (MARL). Existing methods typically focus on encouraging diversity or leveraging shared representations. Full parameter sharing (FuPS) improves sample efficiency but struggles to learn diverse behaviours when required, while no parameter sharing (NoPS) enables diversity but is computationally expensive and sample inefficient. To address these challenges, we introduce HyperMARL, a novel approach using hypernetworks to balance efficiency and specialisation. HyperMARL generates agent-specific actor and critic parameters, enabling agents to adaptively exhibit diverse or homogeneous behaviours as needed, without modifying the learning objective or requiring prior knowledge of the optimal diversity. Furthermore, HyperMARL decouples agent-specific and state-based gradients, which empirically correlates with reduced policy gradient variance, potentially offering insights into its ability to capture diverse behaviours. Across MARL benchmarks requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL consistently matches or outperforms FuPS, NoPS, and diversity-focused methods, achieving NoPS-level diversity with a shared architecture. These results highlight the potential of hypernetworks as a versatile approach to the trade-off between specialisation and shared behaviours in MARL.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04233
Optimal Low-Thrust Orbit Transfers Connecting Gateway with Earth and Moon,['Optimization and Control'],"['Chiara Pozzi', 'Mauro Pontani', 'Alessandro Beolchi', 'Elena Fantino']","Gateway will represent a primary logistic infrastructure in cislunar space. The identification of efficient orbit transfers capable of connecting Earth, Moon, and Gateway paves the way for enabling refurbishment, servicing, and utilization of this orbiting platform. This study is devoted to determining two-way minimum-time low-thrust orbit transfers that connect both Earth and Moon to Gateway. Backward time propagation is proposed as a very convenient option for trajectories that approach and rendezvous with Gateway, and a unified formulation is introduced for minimum-time orbit transfers, using either forward or backward propagation. Two-way transfers between Gateway and a specified low-altitude lunar orbit are first determined, using an indirect heuristic method, which employs the necessary conditions for optimality and a heuristic algorithm. Second, two-way orbit transfers that connect Earth and Gateway are addressed. Because these trajectories exist under the influence of two major attracting bodies, the underlying optimal control problem is formulated as a multi-arc trajectory optimization problem, involving three different representations for the spacecraft state. Multi-arc optimal control problems are associated with several corner conditions to be enforced at the junction time that separates distinct arcs. However, these conditions are shown to be solvable sequentially for the problem at hand, leveraging implicit costate transformation. This implies that the multi-arc problem has a set of unknown quantities with the same size as that of a single-arc optimal control problem, with apparent advantages of computational nature. The indirect heuristic method is also applied in this second mission scenario.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04229
Foundations of the Theory of Performance-Based Ranking,['Machine Learning'],"['Sébastien Piérard', 'Anaïs Halin', 'Anthony Cioppa', 'Adrien Deliège', 'Marc Van Droogenbroeck']","Ranking entities such as algorithms, devices, methods, or models based on their performances, while accounting for application-specific preferences, is a challenge. To address this challenge, we establish the foundations of a universal theory for performance-based ranking. First, we introduce a rigorous framework built on top of both the probability and order theories. Our new framework encompasses the elements necessary to (1) manipulate performances as mathematical objects, (2) express which performances are worse than or equivalent to others, (3) model tasks through a variable called satisfaction, (4) consider properties of the evaluation, (5) define scores, and (6) specify application-specific preferences through a variable called importance. On top of this framework, we propose the first axiomatic definition of performance orderings and performance-based rankings. Then, we introduce a universal parametric family of scores, called ranking scores, that can be used to establish rankings satisfying our axioms, while considering application-specific preferences. Finally, we show, in the case of two-class classification, that the family of ranking scores encompasses well-known performance scores, including the accuracy, the true positive rate (recall, sensitivity), the true negative rate (specificity), the positive predictive value (precision), and F1. However, we also show that some other scores commonly used to compare classifiers are unsuitable to derive performance orderings satisfying the axioms. Therefore, this paper provides the computer vision and machine learning communities with a rigorous framework for evaluating and ranking entities.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04227
The radio properties of the JWST-discovered AGN,['Astrophysics of Galaxies'],"['G. Mazzolari', 'R. Gilli', 'R. Maiolino', 'I. Prandoni', 'I. Delvecchio', 'C. Norman', 'E. F. Jimenez-Andrade', 'S. Belladitta', 'F. Vito', 'E. Momjian', 'M. Chiaberge', 'B. Trefoloni', 'M. Signorini', 'X. Ji', ""Q. D'Amato"", 'G. Risaliti', 'R. D. Baldi', 'A. Fabian', 'H. Übler', ""F. D'Eugenio"", 'J. Scholtz', 'I. Juodžbalis', 'M. Mignoli', 'M. Brusa', 'E. Murphy']","We explore the radio emission of spectroscopically confirmed, X-ray weak, Broad Line AGN (BLAGN, or type 1) selected with JWST in the GOODS-N field, one of the fields with the best combination of deep radio observations and statistics of JWST-selected BLAGN. We use deep radio data at different frequencies (144\,MHz, 1.5\,GHz, 3\,GHz, 5.5\,GHz, 10\,GHz), and we find that none of the 22 sources investigated is detected at any of the aforementioned frequencies. Similarly, the radio stacking analysis does not reveal any detection down to an rms of $\sim 0.2μ$Jy beam$^{-1}$, corresponding to a $3σ$ upper limit at rest frame 5 GHz of $L_{5GHz}=2\times10^{39}$ erg s$^{-1}$ at the mean redshift of the sample $z\sim 5.2$. We compared this and individual sources upper limits with expected radio luminosities estimated assuming different AGN scaling relations. For most of the sources the radio luminosity upper limits are still compatible with expectations for radio-quiet (RQ) AGN; nevertheless, the more stringent stacking upper limits and the fact that no detection is found would suggest that JWST-selected BLAGN are weaker than standard AGN even at radio frequencies. We discuss some scenarios that could explain the possible radio weakness, such as free-free absorption from a dense medium, or the lack of either magnetic field or a corona, possibly as a consequence of super-Eddington accretion. These scenarios would also explain the observed X-ray weakness. We also conclude that $\sim$1 dex more sensitive radio observations are needed to better constrain the level of radio emission (or lack thereof) for the bulk of these sources. The Square Kilometer Array Observatory (SKAO) will likely play a crucial role in assessing the properties of this AGN population.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04224
Generation of Subfemtosecond Deep and Vacuum UV pulses via Two-Photon Rabi Oscillations in Alkali Atoms or Alkaline Earth Ions,['Optics'],"['I. R. Khairulin', 'A. A. Romanov', 'A. A. Silaev', 'M. Yu. Ryabikin', 'V. A. Antonov']","A method is proposed for the formation of femto- and subfemtosecond pulses of the deep ultraviolet and vacuum ultraviolet radiation via generating the third harmonic of femtosecond laser pulses during their resonant interaction with alkali atoms or alkaline earth ions. The pulse formation occurs due to two-photon Rabi oscillations between quasi-equidistant energy levels of atoms or ions. The duration of the generated third harmonic pulse is several times shorter than far from resonance, while the generation efficiency is up to 3-4 orders of magnitude higher.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04223
DistB-VNET: Distributed Cluster-based Blockchain Vehicular Ad-Hoc Networks through SDN-NFV for Smart City,['Cryptography and Security'],"['Anichur Rahman', 'MD. Zunead Abedin Eidmum', 'Dipanjali Kundu', 'Mahir Hossain', 'MD Tanjum An Tashrif', 'Md Ahsan Karim', 'Md. Jahidul Islam']","In the developing topic of smart cities, Vehicular Ad-Hoc Networks (VANETs) are crucial for providing successful interaction between vehicles and infrastructure. This research proposes a distributed Blockchain-based Vehicular Ad-hoc Network (DistB-VNET) architecture that includes binary malicious traffic classification, Software Defined Networking (SDN), and Network Function Virtualization (NFV) to ensure safe, scalable, and reliable vehicular networks in smart cities. The suggested framework is the decentralized blockchain for safe data management and SDN-NFV for dynamic network management and resource efficiency and a noble isolation forest algorithm works as an IDS (Intrusion Detection System). Further, ""DistB-VNET"" offers a dual-layer blockchain system, where a distributed blockchain provides safe communication between vehicles, while a centralized blockchain in the cloud is in charge of data verification and storage. This improves security, scalability, and adaptability, ensuring better traffic management, data security, and privacy in VANETs. Furthermore, the unsupervised isolation forest model achieves a high accuracy of 99.23% for detecting malicious traffic. Additionally, reveals that our method greatly improves network performance, offering decreased latency, increased security, and reduced congestion, an effective alternative for existing smart city infrastructures.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04222
Ionospheric conductances at the giant planets of the Solar System:a comparative study of ionization sources and the impact of meteoric ions,['Earth and Planetary Astrophysics'],"['Noé Clément', 'Yuki Nakamura', 'Michel Blanc', 'Yuxian Wang', 'Sariah Al Saati']","The dynamics of giant planet magnetospheres is controlled by a complex interplay between their fast rotation, their interaction with the solar wind, and their diverse internal plasma and momentum sources. In the ionosphere, the Hall and Pedersen conductances are two key parameters that regulate the intensity of currents coupling the magnetosphere and the ionosphere, and the rate of angular momentum transfer and power carried by these currents. We perform a comparative study of Hall and Pedersen conductivities and conductances in the four giant planets of our Solar System - Jupiter, Saturn, Uranus and Neptune. We use a generic ionospheric model (restraining the studied ions to H3+, CH5+, and meteoric ions) to study the dependence of conductances on the structure and composition of these planets' upper atmospheres and on the main ionization sources (photoionization, ionization by precipitating electrons, and meteoroid ablation). After checking that our model reproduces the conclusions of Nakamura et al. (2022, https://doi.org/10.1029/2022JA030312) at Jupiter, i.e. the contribution of meteoric ions to the height-integrated conductances is non-negligible, we show that this contribution could also be non-negligible at Saturn, Uranus and Neptune, compared with ionization processes caused by precipitating electrons of energies lower than a few keV (typical energies on these planets). However, because of their weaker magnetic field, the conductive layer of these planets is higher than the layer where meteoric ions are mainly produced, limiting their role in magnetosphere-ionosphere coupling.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04219
Aligned Music Notation and Lyrics Transcription,['Computer Vision and Pattern Recognition'],"['Eliseo Fuentes-Martínez', 'Antonio Ríos-Vila', 'Juan C. Martinez-Sevilla', 'David Rizo', 'Jorge Calvo-Zaragoza']","The digitization of vocal music scores presents unique challenges that go beyond traditional Optical Music Recognition (OMR) and Optical Character Recognition (OCR), as it necessitates preserving the critical alignment between music notation and lyrics. This alignment is essential for proper interpretation and processing in practical applications. This paper introduces and formalizes, for the first time, the Aligned Music Notation and Lyrics Transcription (AMNLT) challenge, which addresses the complete transcription of vocal scores by jointly considering music symbols, lyrics, and their synchronization. We analyze different approaches to address this challenge, ranging from traditional divide-and-conquer methods that handle music and lyrics separately, to novel end-to-end solutions including direct transcription, unfolding mechanisms, and language modeling. To evaluate these methods, we introduce four datasets of Gregorian chants, comprising both real and synthetic sources, along with custom metrics specifically designed to assess both transcription and alignment accuracy. Our experimental results demonstrate that end-to-end approaches generally outperform heuristic methods in the alignment challenge, with language models showing particular promise in scenarios where sufficient training data is available. This work establishes the first comprehensive framework for AMNLT, providing both theoretical foundations and practical solutions for preserving and digitizing vocal music heritage.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04217
A minimal tensor network beyond free fermions,['Strongly Correlated Electrons'],"['Carolin Wille', 'Maksimilian Usoltcev', 'Jens Eisert', 'Alexander Altland']","This work proposes a minimal model extending the duality between classical statistical spin systems and fermionic systems beyond the case of free fermions. A Jordan-Wigner transformation applied to a two-dimensional tensor network maps the partition sum of a classical statistical mechanics model to a Grassmann variable integral, structurally similar to the path integral for interacting fermions in two dimensions. The resulting model is simple, featuring only two parameters: one governing spin-spin interaction (dual to effective hopping strength in the fermionic picture), the other measuring the deviation from the free fermion limit. Nevertheless, it exhibits a rich phase diagram, partially stabilized by elements of topology, and featuring three phases meeting at a tricritical point. Besides the interpretation as a spin and fermionic system, the model is closely related to loop gas and vertex models and can be interpreted as a parity-preserving (non-unitary) circuit. Its minimal construction makes it an ideal reference system for studying non-linearities in tensor networks and deriving results by means of duality.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04216
Near-infrared spectroscopy of the LMC recurrent nova LMCN 1968-12a,['Solar and Stellar Astrophysics'],"['A. Evans', 'D. P. K. Banerjee', 'T. R. Geballe', 'A. Polin', 'E. Y. Hsiao', 'K. L. Page', 'C. E. Woodward', 'S. Starrfield']","We have obtained near-infrared ($0.80-2.45μ$m) spectra of the recurrent nova LMCN 1968-12a on two occasions during its 2024 August eruption. This is the first near-infrared spectroscopy of an extragalactic nova. The initial spectrum, on day 8.48, caught the nova in the coronal phase, with the [SiX] $1.43μ$m line being extremely strong. This line had a luminosity of $\sim95$L$_\odot$, and is clearly a very powerful coolant. Its presence, together with the absence of [SiIX] 1.56$μ$m, implies a coronal temperature $\gtrsim3\times10^6$K, possibly amongst the highest recorded coronal temperature in a nova eruption. With the exception of the [SiX] line, the near-infrared spectra are remarkable for being devoid of metal lines. We suggest that this is due, in part, to the exceptionally high temperature of the coronal gas, causing ions, whose emission lines would normally appear in the near-infrared spectrum, to be collisionally ionised to higher stages.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04215
BEACON: JWST NIRCam Pure-parallel Imaging Survey. I. Survey Design and Initial Results,['Astrophysics of Galaxies'],"['Takahiro Morishita', 'Charlotte A. Mason', 'Kimi C. Kreilgaard', 'Michele Trenti', 'Tommaso Treu', 'Benedetta Vulcani', 'Yechi Zhang', ""Abdurro'uf"", 'Anahita Alavi', 'Hakim Atek', 'Yannick Bahe', 'Marusa Bradac', 'Larry D. Bradley', 'Andrew J. Bunker', 'Dan Coe', 'James Colbert', 'Viola Gelli', 'Matthew J. Hayes', 'Tucker Jones', 'Tadayuki Kodama', 'Nicha Leethochawalit', 'Zhaoran Liu', 'Matthew A. Malkan', 'Vihang Mehta', 'Benjamin Metha']","We introduce the Bias-free Extragalactic Analysis for Cosmic Origins with NIRCam (BEACON) survey, a JWST Cycle2 program allocated up to 600 pure-parallel hours of observations. BEACON explores high-latitude areas of the sky with JWST/NIRCam over $\sim100$ independent sightlines, totaling $\sim0.3$deg$^2$, reaching a median F444W depth of $\approx28.2$AB mag (5$σ$). Based on existing JWST observations in legacy fields, we estimate that BEACON will photometrically identify 25--150 galaxies at $z>10$ and 500--1000 at $z\sim7$--10 uniquely enabled by an efficient multiple filter configuration spanning $0.9$--5.0$μ$m. The expected sample size of $z>10$ galaxies will allow us to obtain robust number density estimates and to discriminate between different models of early star formation. In this paper, we present an overview of the survey design and initial results using the first 19 fields. We present 129 galaxy candidates at $z>7$ identified in those fields, including 11 galaxies at $z>10$ and several UV-luminous ($M_{\rm UV}<-21$mag) galaxies at $z\sim8$. The number densities of $z<13$ galaxies inferred from the initial fields are overall consistent with those in the literature. Despite reaching a considerably large volume ($\sim10^5$Mpc$^3$), however, we find no galaxy candidates at $z>13$, providing us with a complimentary insight into early galaxy evolution with minimal cosmic variance. We publish imaging and catalog data products for these initial fields. Upon survey completion, all BEACON data will be coherently processed and distributed to the community along with catalogs for redshift and other physical quantities.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04211
Four-fold Anisotropic Magnetoresistance in Antiferromagnetic Epitaxial Thin Films of MnPt$_{x}$Pd$_{1-x}$,['Materials Science'],"['Shivesh Yadav', 'Mohit Verma', 'Shikhar Kumar Gupta', 'Debjoty Paul', 'Nilesh Kulkarni', 'Arti Kashyap', 'Shouvik Chatterjee']","Antiferromagnets are emerging as promising alternatives to ferromagnets in spintronics applications. A key feature of antiferromagnets is their anisotropic magnetoresistance (AMR), which has the potential to serve as a sensitive marker for the antiferromagnetic order parameter. However, the underlying origins of this behavior remain poorly understood. In this study, we report the observation of AMR in epitaxial thin films of the collinear L1$_{0}$ antiferromagnet MnPt$_{x}$Pd$_{1-x}$. In thicker films, the AMR is dominated by a non-crystalline two-fold component. As the film thickness is reduced, however, a crystalline four-fold component emerges, accompanied by the appearance of uncompensated magnetic moment, which strongly modifies the magnetotransport properties in the thinner films. We demonstrate that interfacial interactions lead to a large density of states (DOS) at the Fermi level. This enhanced DOS, combined with disorder in thinner films, stabilizes the uncompensated moment and induces a four-fold modulation of the DOS as the Neel vector rotates, explaining the observed AMR behavior.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04207
Numerical Estimation of Limiting Large-Deviation Rate Functions,"['Data Analysis, Statistics and Probability']","['Peter Werner', 'Alexander K. Hartmann']","For statistics of rare events in systems obeying a large-deviation principle, the rate function is a key quantity. When numerically estimating the rate function one is always restricted to finite system sizes. Thus, if the interest is in the limiting rate function for infinite system sizes, first, several system sizes have to be studied numerically. Here, rare-event algorithms using biased ensembles give access to the low-probability region. Second, some kind of system-size extrapolation has to be performed.
  Here we demonstrate how rare-event importance sampling schemes can be combined with multi-histogram reweighting, which allows for rather general applicability of the approach, independent of specific sampling algorithms. We study two ways of performing the system-size extrapolation, either directly acting on the empirical rate functions, or on the scaled cumulant generating functions, to obtain the infinite-size limit. The presented method is demonstrated for a binomial distributed variable and the largest connected component in Erdös-Rényi random graphs. Analytical solutions are available in both cases for direct comparison. It is observed in particular that phase transitions appearing in the biased ensembles can lead to systematic deviations from the true result.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04206
A Context-aware Framework for Translation-mediated Conversations,['Computation and Language'],"['José Pombal', 'Sweta Agrawal', 'Patrick Fernandes', 'Emmanouil Zaranis', 'André F. T. Martins']","Effective communication is fundamental to any interaction, yet challenges arise when participants do not share a common language. Automatic translation systems offer a powerful solution to bridge language barriers in such scenarios, but they introduce errors that can lead to misunderstandings and conversation breakdown. A key issue is that current systems fail to incorporate the rich contextual information necessary to resolve ambiguities and omitted details, resulting in literal, inappropriate, or misaligned translations. In this work, we present a framework to improve large language model-based translation systems by incorporating contextual information in bilingual conversational settings. During training, we leverage context-augmented parallel data, which allows the model to generate translations sensitive to conversational history. During inference, we perform quality-aware decoding with context-aware metrics to select the optimal translation from a pool of candidates. We validate both components of our framework on two task-oriented domains: customer chat and user-assistant interaction. Across both settings, our framework consistently results in better translations than state-of-the-art systems like GPT-4o and TowerInstruct, as measured by multiple automatic translation quality metrics on several language pairs. We also show that the resulting model leverages context in an intended and interpretable way, improving consistency between the conveyed message and the generated translations.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04205
PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models,['Computer Vision and Pattern Recognition'],"['Valerio Marsocci', 'Yuru Jia', 'Georges Le Bellier', 'David Kerekes', 'Liang Zeng', 'Sebastian Hafner', 'Sebastian Gerard', 'Eric Brune', 'Ritu Yadav', 'Ali Shibli', 'Heng Fang', 'Yifang Ban', 'Maarten Vergauwen', 'Nicolas Audebert', 'Andrea Nascetti']","Geospatial Foundation Models (GFMs) have emerged as powerful tools for extracting representations from Earth observation data, but their evaluation remains inconsistent and narrow. Existing works often evaluate on suboptimal downstream datasets and tasks, that are often too easy or too narrow, limiting the usefulness of the evaluations to assess the real-world applicability of GFMs. Additionally, there is a distinct lack of diversity in current evaluation protocols, which fail to account for the multiplicity of image resolutions, sensor types, and temporalities, which further complicates the assessment of GFM performance. In particular, most existing benchmarks are geographically biased towards North America and Europe, questioning the global applicability of GFMs. To overcome these challenges, we introduce PANGAEA, a standardized evaluation protocol that covers a diverse set of datasets, tasks, resolutions, sensor modalities, and temporalities. It establishes a robust and widely applicable benchmark for GFMs. We evaluate the most popular GFMs openly available on this benchmark and analyze their performance across several domains. In particular, we compare these models to supervised baselines (e.g. UNet and vanilla ViT), and assess their effectiveness when faced with limited labeled data. Our findings highlight the limitations of GFMs, under different scenarios, showing that they do not consistently outperform supervised models. PANGAEA is designed to be highly extensible, allowing for the seamless inclusion of new datasets, models, and tasks in future research. By releasing the evaluation code and benchmark, we aim to enable other researchers to replicate our experiments and build upon our work, fostering a more principled evaluation protocol for large pre-trained geospatial models. The code is available at https://github.com/VMarsocci/pangaea-bench.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04204
On Local Irregularity Conjecture for 2-multigraphs,['Combinatorics'],"['Igor Grzelec', 'Alfréd Onderko', 'Mariusz Woźniak']","A multigraph in which adjacent vertices have different degrees is called locally irregular. The locally irregular edge coloring is an edge coloring of a multigraph $G$ in which every color induces a locally irregular submultigraph of $G$. We denote by $\operatorname{lir}(G)$ the locally irregular chromatic index of a multigraph $G$, which is the smallest number of colors required in a locally irregular edge coloring of $G$, given that such a coloring of $G$ exists. By $^2G$ we denote a 2-multigraph obtained from a simple graph $G$ by doubling each its edge. In 2022 Grzelec and Woźniak conjectured that $\operatorname{lir}(^2G) \leq 2$ for every connected simple graph $G$ different from $K_2$; the conjecture is known as Local Irregularity Conjecture for 2-multigraphs. In this paper, we prove this conjecture in the case of regular graphs, split graphs, and some particular families of subcubic graphs. Moreover, we provide a constant upper bound on the locally irregular chromatic index of planar 2-multigraphs (except for $^2K_2$), and we obtain a better constant upper bound on $\operatorname{lir}(^2G)$ if $G$ is a simple subcubic graph different from $K_2$. In the proofs, special decompositions of graphs and the relation of Local Irregularity Conjecture to the well-known 1-2-3 Conjecture are utilized.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04200
Monolithic integration of blue light sources into silicon nitride photonic chips,['Optics'],"['Ivan A. Pshenichnyuk', 'Muneeb Farooq', 'Daniil S. Zemtsov', 'Denis M. Zhigunov', 'Sergey S. Kosolobov', 'Vladimir P. Drachev']","We investigate theoretically photonic chips with monolithically integrated blue light sources. According to our evaluations, a group-III nitride light emitting heterostructure can be efficiently combined with silicon nitride waveguiding layers. Low losses, high level of miniaturization and built-in light injection mechanism potentially make the selected platform attractive for applications and draw up an addition or even alternative to silicon photonics. We use a combination of drift-diffusion and Maxwell equations to build a model of the proposed multilayer structure. The model allows to choose the best parameters for both light emitting sandwich and waveguiding layers as well as to pick an optimal coupling regime. High transmittance coefficients are obtained. Various optimal geometries are analised with respect to captured power, desired polarization and excited modes. The obtained numbers are promising and allow in the future to pave the way towards hetero-integrated blue photonic circuits.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04198
"GARFIELD, a toolkit for interpreting ultrafast electron diffraction data of imperfect quasi-single crystals",['Materials Science'],"['Alexander Marx', 'Sascha W. Epp']","The analysis of ultrafast electron diffraction (UED) data from low-symmetry single crystals of small molecules is often challenged by the difficulty of assigning unique Laue indices to the observed Bragg reflections. For a variety of technical and physical reasons, UED diffraction images are typically of lower quality when viewed from the perspective of structure determination by single-crystal X-ray or electron diffraction. Nevertheless, time series of UED images can provide valuable insight into structural dynamics, provided that an adequate interpretation of the diffraction patterns can be achieved. GARFIELD is a collection of tools with a graphical user interface designed to facilitate the interpretation of diffraction patterns and to index Bragg reflections in challenging cases where other indexing tools are ineffective. To this end, GARFIELD enables the user to interactively create, explore, and optimize sets of parameters that define the diffraction geometry and characteristic properties of the sample.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04197
Partial Betti splittings with applications to binomial edge ideals,['Commutative Algebra'],"['A. V. Jayanthan', 'Aniketh Sivakumar', 'Adam Van Tuyl']","We introduce the notion of a partial Betti splitting of a homogeneous ideal, generalizing the notion of a Betti splitting first given by Francisco, Hà, and Van Tuyl. Given a homogeneous ideal $I$ and two ideals $J$ and $K$ such that $I = J+K$, a partial Betti splitting of $I$ relates some of the graded Betti of $I$ with those of $J, K$, and $J\cap K$. As an application, we focus on the partial Betti splittings of binomial edge ideals. Using this new technique, we generalize results of Saeedi Madani and Kiani related to binomial edge ideals with cut edges, we describe a partial Betti splitting for all binomial edge ideals, and we compute the total second Betti number of binomial edge ideals of trees.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04195
Predictive Strategies for the Control of Complex Motor Skills: Recent Insights into Individual and Joint Actions,['Neurons and Cognition'],"['Marta Russo', 'Antonella Maselli', 'Dagmar Sternad', 'Giovanni Pezzulo']","Humans can perform exquisite sensorimotor skills, both individually and in teams, from athletes performing rhythmic gymnastics to everyday tasks like carrying a cup of coffee. The ""predictive brain"" framework suggests that mastering these tasks relies on predictive mechanisms, raising the question of how we deploy such predictions for real-time control and coordination. This review highlights two lines of research: one showing that during the control of complex objects people make the interaction with 'tools' predictable; the second one examines dyadic coordination showing that people make their behavior predictable for their partners. These studies demonstrate that to achieve sophisticated motor skills, we play ""prediction tricks"": we select subspaces of predictable solutions and make sensorimotor interactions more predictable and legible by and for others. This synthesis underscores the critical role of predictability in optimizing control strategies across various contexts and establishes a link between predictive processing and closed-loop control theories of behavior.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04191
Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects,['Artificial Intelligence'],"['Dominic Lohr', 'Marc Berges', 'Abhishek Chugh', 'Michael Kohlhase', 'Dennis Müller']","Background: Over the past few decades, the process and methodology of automated question generation (AQG) have undergone significant transformations. Recent progress in generative natural language models has opened up new potential in the generation of educational content.
  Objectives: This paper explores the potential of large language models (LLMs) for generating computer science questions that are sufficiently annotated for automatic learner model updates, are fully situated in the context of a particular course, and address the cognitive dimension understand.
  Methods: Unlike previous attempts that might use basic methods like ChatGPT, our approach involves more targeted strategies such as retrieval-augmented generation (RAG) to produce contextually relevant and pedagogically meaningful learning objects.
  Results and Conclusions: Our results show that generating structural, semantic annotations works well. However, this success was not reflected in the case of relational annotations. The quality of the generated questions often did not meet educational standards, highlighting that although LLMs can contribute to the pool of learning materials, their current level of performance requires significant human intervention to refine and validate the generated content.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04185
Modeling Eye Gaze Velocity Trajectories using GANs with Spectral Loss for Enhanced Fidelity,['Neural and Evolutionary Computing'],"['Shailendra Bhandari', 'Pedro Lencastre', 'Rujeena Mathema', 'Alexander Szorkovszky', 'Anis Yazidi', 'Pedro Lind']","Accurate modeling of eye gaze dynamics is essential for advancement in human-computer interaction, neurological diagnostics, and cognitive research. Traditional generative models like Markov models often fail to capture the complex temporal dependencies and distributional nuance inherent in eye gaze trajectories data. This study introduces a GAN framework employing LSTM and CNN generators and discriminators to generate high-fidelity synthetic eye gaze velocity trajectories. We conducted a comprehensive evaluation of four GAN architectures: CNN-CNN, LSTM-CNN, CNN-LSTM, and LSTM-LSTM trained under two conditions: using only adversarial loss and using a weighted combination of adversarial and spectral losses. Our findings reveal that the LSTM-CNN architecture trained with this new loss function exhibits the closest alignment to the real data distribution, effectively capturing both the distribution tails and the intricate temporal dependencies. The inclusion of spectral regularization significantly enhances the GANs ability to replicate the spectral characteristics of eye gaze movements, leading to a more stable learning process and improved data fidelity. Comparative analysis with an HMM optimized to four hidden states further highlights the advantages of the LSTM-CNN GAN. Statistical metrics show that the HMM-generated data significantly diverges from the real data in terms of mean, standard deviation, skewness, and kurtosis. In contrast, the LSTM-CNN model closely matches the real data across these statistics, affirming its capacity to model the complexity of eye gaze dynamics effectively. These results position the spectrally regularized LSTM-CNN GAN as a robust tool for generating synthetic eye gaze velocity data with high fidelity.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04184
Linear Discriminant Analysis in Credit Scoring: A Transparent Hybrid Model Approach,['Machine Learning'],"['Md Shihab Reza', 'Monirul Islam Mahmud', 'Ifti Azad Abeer', 'Nova Ahmed']","The development of computing has made credit scoring approaches possible, with various machine learning (ML) and deep learning (DL) techniques becoming more and more valuable. While complex models yield more accurate predictions, their interpretability is often weakened, which is a concern for credit scoring that places importance on decision fairness. As features of the dataset are a crucial factor for the credit scoring system, we implement Linear Discriminant Analysis (LDA) as a feature reduction technique, which reduces the burden of the models complexity. We compared 6 different machine learning models, 1 deep learning model, and a hybrid model with and without using LDA. From the result, we have found our hybrid model, XG-DNN, outperformed other models with the highest accuracy of 99.45% and a 99% F1 score with LDA. Lastly, to interpret model decisions, we have applied 2 different explainable AI techniques named LIME (local) and Morris Sensitivity Analysis (global). Through this research, we showed how feature reduction techniques can be used without affecting the performance and explainability of the model, which can be very useful in resource-constrained settings to optimize the computational workload.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04183
Fixed-Mean Gaussian Processes for Post-hoc Bayesian Deep Learning,['Machine Learning'],"['Luis A. Ortega', 'Simón Rodríguez-Santana', 'Daniel Hernández-Lobato']","Recently, there has been an increasing interest in performing post-hoc uncertainty estimation about the predictions of pre-trained deep neural networks (DNNs). Given a pre-trained DNN via back-propagation, these methods enhance the original network by adding output confidence measures, such as error bars, without compromising its initial accuracy. In this context, we introduce a novel family of sparse variational Gaussian processes (GPs), where the posterior mean is fixed to any continuous function when using a universal kernel. Specifically, we fix the mean of this GP to the output of the pre-trained DNN, allowing our approach to effectively fit the GP's predictive variances to estimate the DNN prediction uncertainty. Our approach leverages variational inference (VI) for efficient stochastic optimization, with training costs that remain independent of the number of training points, scaling efficiently to large datasets such as ImageNet. The proposed method, called fixed mean GP (FMGP), is architecture-agnostic, relying solely on the pre-trained model's outputs to adjust the predictive variances. Experimental results demonstrate that FMGP improves both uncertainty estimation and computational efficiency when compared to state-of-the-art methods.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04177
Supertoroid fitting of objects with holes for robotic grasping and scene generation,['Image and Video Processing'],"['Joan Badia Torres', 'Eric Carmona', 'Abhijit Makhal', 'Omid Heidari', 'Alba Perez Gracia']","One of the strategies to detect the pose and shape of unknown objects is their geometric modeling, consisting on fitting known geometric entities. Classical geometric modeling fits simple shapes such as spheres or cylinders, but often those don't cover the variety of shapes that can be encountered. For those situations, one solution is the use of superquadrics, which can adapt to a wider variety of shapes.
  One of the limitations of superquadrics is that they cannot model objects with holes, such as those with handles. This work aims to fit supersurfaces of degree four, in particular supertoroids, to objects with a single hole. Following the results of superquadrics, simple expressions for the major and minor radial distances are derived, which lead to the fitting of the intrinsic and extrinsic parameters of the supertoroid. The differential geometry of the surface is also studied as a function of these parameters. The result is a supergeometric modeling that can be used for symmetric objects with and without holes with a simple distance function for the fitting. The proposed algorithm expands considerably the amount of shapes that can be targeted for geometric modeling.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04174
Neuromodulation and homeostasis: complementary mechanisms for robust neural function,['Neurons and Cognition'],"['Arthur Fyon', 'Guillaume Drion']","Neurons depend on two interdependent mechanisms-homeostasis and neuromodulation-to maintain robust and adaptable functionality. Homeostasis stabilizes neuronal activity by adjusting ionic conductances, whereas neuromodulation dynamically modifies ionic properties in response to external signals. Combining these mechanisms in conductance-based models often produces unreliable outcomes, particularly when sharp neuromodulation interferes with homeostatic tuning. This study explores how a biologically inspired neuromodulation controller can harmonize with homeostasis to ensure reliable neuronal function. Using computational models of stomatogastric ganglion and dopaminergic neurons, we demonstrate that controlled neuromodulation preserves neuronal firing patterns while maintaining intracellular calcium levels. Unlike sharp neuromodulation, the neuromodulation controller integrates activity-dependent feedback through mechanisms mimicking G-protein-coupled receptor cascades. The interaction between these controllers critically depends on the existence of an intersection in conductance space, representing a balance between target calcium levels and neuromodulated firing patterns. Maximizing neuronal degeneracy enhances the likelihood of such intersections, enabling robust modulation and compensation for channel blockades. We further show that this controller pairing extends to network-level activity, reliably modulating central pattern generators in crustaceans. These findings suggest that targeting neuromodulation pathways-rather than ion channels directly-may offer safer pharmacological strategies to manage neuronal dysfunctions. This study highlights the complementary roles of homeostasis and neuromodulation, proposing a unified control framework for maintaining robust and adaptive neural activity under physiological and pathological conditions.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04172
Deep imaging of three accelerating stars using SHARK-NIR and LMIRCam at LBT,['Earth and Planetary Astrophysics'],"['D. Mesa', 'R. Gratton', ""V. D'Orazi"", 'E. Carolo', 'D. Vassallo', 'J. Farinato', 'L. Marafatto', 'K. Wagner', 'J. Hom', 'S. Ertel', 'Th. Henning', 'C. Desgrange', 'D. Barbato', 'M. Bergomi', 'P. Cerpelloni', 'S. Desidera', 'S. Di Filippo', 'D. Doelman', 'T. S. Gomes Machado', 'D. Greggio', 'P. Grenz', 'M. Kenworthy', 'F. Laudisio', 'C. Lazzoni', 'J. Leisenring']","The combination of detection techniques enhances our ability to identify companions orbiting nearby stars. We employed high-contrast imaging to constrain mass and separation of possible companions responsible for the significant proper motion anomalies of the nearby stars HIP 11696, HIP 47110 and HIP 36277. These targets were observed using the LBT's high-contrast camera, SHARK-NIR, in H-band using a Gaussian coronagraph, and with the LMIRCam instrument in the L'-band and using a vAPP coronagraph. Both observations were conducted simultaneously. Additionally, constraints at short separations from the host star are derived analyzing the renormalized unit weight error (RUWE) values from the Gaia catalogue. We find that the companion responsible for the anomaly signal of HIP 11696 is likely positioned at a distance from 2.5 to 28 astronomical units from its host. Its mass is estimated to be between 4 and 16 Jupiter masses, with the greater mass possible only at the upper end of the separation range. Similar limits were obtained for HIP 47110 where the companion should reside between 3 and 30 au with a mass between 3 and 10 MJup. For HIP 36277, we identified a faint stellar companion at large separation, though it might be substellar depending on the assumed age for the star. Considering the older age, this object accounts for the absolute value of the PMa vector but not for its direction. Additionally, we found a substellar candidate companion at a closer separation that could explain the PMa signal, considering a younger age for the system.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04171
A note on high-dimensional discrepancy of subtrees,['Combinatorics'],"['Lawrence Hollom', 'Lyuben Lichev', 'Adva Mond', 'Julien Portier']","For a tree $T$ and a function $f \colon E(T)\to \mathbb{S}^d$, the imbalance of a subtree $T'\subseteq T$ is given by $|\sum_{e \in E(T')} f(e)|$. The $d$-dimensional discrepancy of the tree $T$ is the minimum, over all functions $f$ as above, of the maximum imbalance of a subtree of $T$. We prove tight asymptotic bounds for the discrepancy of a tree $T$, confirming a conjecture of Krishna, Michaeli, Sarantis, Wang and Wang. We also settle a related conjecture on oriented discrepancy of subtrees by the same authors.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04170
Differential Magnetic Force Microscopy with a Switchable Tip,['Applied Physics'],"['Shobhna Misra', 'Reshma Peremadathil Pradeep', 'Yaoxuan Feng', 'Urs Grob', 'Andrada Oana Mandru', 'Christian L. Degen', 'Hans J. Hug', 'Alexander Eichler']","The separation of physical forces acting on the tip of a magnetic force microscope (MFM) is essential for correct magnetic imaging. Electrostatic forces can be modulated by varying the tip-sample potential and minimized to map the local Kelvin potential. However, distinguishing magnetic forces from van der Waals forces typically requires two measurements with opposite tip magnetizations under otherwise identical measurement conditions. Here, we present an inverted magnetic force microscope where the sample is mounted on a flat cantilever for force sensing, and the magnetic tip is attached to a miniaturized electromagnet that periodically flips the tip magnetization. This setup enables the extraction of magnetic tip-sample interactions from the sidebands occurring at the switching rate in the cantilever oscillation spectrum. Our method achieves the separation of magnetic signals from other force contributions in a single-scan mode. Future iterations of this setup may incorporate membrane, trampoline, or string resonators with ultra-high quality factors, potentially improving measurement sensitivity by up to three orders of magnitude compared to the state-of-the-art MFM systems using cantilevers.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04165
On the Lack of Robustness of Binary Function Similarity Systems,['Cryptography and Security'],"['Gianluca Capozzi', 'Tong Tang', 'Jie Wan', 'Ziqi Yang', ""Daniele Cono D'Elia"", 'Giuseppe Antonio Di Luna', 'Lorenzo Cavallaro', 'Leonardo Querzoni']","Binary function similarity, which often relies on learning-based algorithms to identify what functions in a pool are most similar to a given query function, is a sought-after topic in different communities, including machine learning, software engineering, and security. Its importance stems from the impact it has in facilitating several crucial tasks, from reverse engineering and malware analysis to automated vulnerability detection. Whereas recent work cast light around performance on this long-studied problem, the research landscape remains largely lackluster in understanding the resiliency of the state-of-the-art machine learning models against adversarial attacks. As security requires to reason about adversaries, in this work we assess the robustness of such models through a simple yet effective black-box greedy attack, which modifies the topology and the content of the control flow of the attacked functions. We demonstrate that this attack is successful in compromising all the models, achieving average attack success rates of 57.06% and 95.81% depending on the problem settings (targeted and untargeted attacks). Our findings are insightful: top performance on clean data does not necessarily relate to top robustness properties, which explicitly highlights performance-robustness trade-offs one should consider when deploying such models, calling for further research.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04163
In-situ Investigation of the Phase Formation and Superconductivity in V$_3$Si Thin Films at High Temperatures,['Superconductivity'],"['Manjith Bose', 'David L. Cortie', 'Sergey Rubanov', 'Anton P. Le Brun', 'Trevor R. Finlayson', 'Jeffrey C. McCallum']","Vanadium silicide (V$_3$Si) is a promising superconductor for integration with silicon-based electronics, however the interfacial growth kinetics have a strong influence on the resulting superconducting properties and are not yet fully understood. In this study, we have used neutron reflectometry to reveal the phase transformation during thin film growth driven by different annealing strategies. We examined the silicide formation when a thin layer of vanadium undergoes reactive diffusion with a silicon dioxide film on silicon at temperatures from 650-800 °C. To further investigate the time evolution of different phases under various annealing temperatures, a chemical model was developed and subsequent simulations were performed. The results of this model were validated using X-ray diffraction and cross-sectional TEM analysis. Correlations were observed between the structure and superconducting properties. Over-annealing films leads to complete depletion of the SiO$_2$ barrier layer, forming diffuse interfaces and driving the formation of undesirable silicon-rich silicides. Avoiding this by controlling time and temperature, allows higher quality superconducting films to be achieved. The $T_c$ of the films was found to be 13 K, and the annealing conditions influenced the critical fields and the paramagnetic Meissner effect near $T_c$. For optimally-annealed films, superconducting order parameters were calculated. Ginzberg-Landau theory was applied to explain flux penetration.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04159
LossVal: Efficient Data Valuation for Neural Networks,['Machine Learning'],"['Tim Wibiral', 'Mohamed Karim Belaid', 'Maximilian Rabus', 'Ansgar Scherp']","Assessing the importance of individual training samples is a key challenge in machine learning. Traditional approaches retrain models with and without specific samples, which is computationally expensive and ignores dependencies between data points. We introduce LossVal, an efficient data valuation method that computes importance scores during neural network training by embedding a self-weighting mechanism into loss functions like cross-entropy and mean squared error. LossVal reduces computational costs, making it suitable for large datasets and practical applications. Experiments on classification and regression tasks across multiple datasets show that LossVal effectively identifies noisy samples and is able to distinguish helpful from harmful samples. We examine the gradient calculation of LossVal to highlight its advantages. The source code is available at: https://github.com/twibiral/LossVal△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04158
WalkSAT is linear on random 2-SAT,['Combinatorics'],"['Petra Berenbrink', 'Amin Coja-Oghlan', 'Colin Cooper', 'Thorsten Götte', 'Lukas Hintze', 'Pavel Zakharov']","In an influential article Papadimitriou [FOCS 1991] proved that a local search algorithm called WalkSAT finds a satisfying assignment of a satisfiable 2-CNF with $n$ variables in $O(n^2)$ expected time. Variants of the WalkSAT algorithm have become a mainstay of practical SAT solving (e.g., [Hoos and Stützle 2000]). In the present article we analyse the expected running time of WalkSAT on random 2-SAT instances. Answering a question raised by Alekhnovich and Ben-Sasson [SICOMP 2007], we show that WalkSAT runs in linear expected time for all clause/variable densities up to the random 2-SAT satisfiability threshold.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04156
Marginal Analysis of Convex Optimization Problems with Set-Valued Inclusion Constraints,['Optimization and Control'],['Amos Uderzo'],"In this paper, stability and sensitivity properties of a class of parametric constrained optimization problem, whose feasible region is defined by a set-valued inclusion, are investigated through the associated optimal value function. Set-valued inclusions are a kind of constraint system, which naturally emerges in contexts requiring the robust fulfilment of traditional cone constraints, where data are affected by uncertain elements having a non stochastic nature, or in (MPEC) as a vector equilibrium constraint, where feasible solutions are intended as equilibrium point in a strong sense. Under proper convexity assumptions on the objective function and the constraining set-valued term, combined with a global qualification condition, a class of parametric optimization problems is singled out, which displays a global Lipschitz behaviour. By employing recent results of variational analysis, elements for a sensitivity analysis of this class of problems are provided via exact subgradient formulae for the optimal value function. Further consequences of the stability behaviour are explored in terms of problem calmness and viability of penalization techniques.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04155
On unipolar and bipolar HiPIMS pulse configurations to enhance energy flux to insulating surfaces,['Plasma Physics'],"['M. Farahani', 'T. Kozák', 'A. D. Pajdarová', 'T. Tölg', 'J. Čapek']","High-power impulse magnetron sputtering (HiPIMS) delivers a high target power in short pulses, enhancing the ionization and energy of sputtered atoms and providing thus more possibilities to control the film properties. This study explores the effect of various pulse configurations (unipolar HiPIMS, bipolar HiPIMS, chopped unipolar, and chopped bipolar HiPIMS) to increase energy flux to an insulated surface (e.g., substrate or growing film). The chopped bipolar HiPIMS configuration, featuring several short positive pulses replacing a single long positive pulse, is introduced, and the total energy fluxes are subsequently measured using a passive thermal probe. Moreover, the effect of the probe's capacitance with respect to the ground is systematically investigated by connecting an external capacitor. Results show that for an insulated surface with low capacitance, bipolar pulse configurations do not significantly increase energy flux to the surface due to its rapid charging by plasma ions. Conversely, high surface capacitance facilitates an increase in energy flux, as a large potential difference between the plasma and the surface remains even for a long positive pulse. For medium surface capacitance (tens of nF), chopping the positive pulse in bipolar HiPIMS effectively increases the energy delivered to the film by discharging the surface in the off-times. The thermal probe measurements also confirm that energy to the film can be increased for unipolar HiPIMS configurations by splitting the negative pulse into several shorter pulses.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04154
A Dynamic Safety Shield for Safe and Efficient Reinforcement Learning of Navigation Tasks,['Robotics'],"['Murad Dawood', 'Ahmed Shokry', 'Maren Bennewitz']","Reinforcement learning (RL) has been successfully applied to a variety of robotics applications, where it outperforms classical methods. However, the safety aspect of RL and the transfer to the real world remain an open challenge. A prominent field for tackling this challenge and ensuring the safety of the agents during training and execution is safe reinforcement learning. Safe RL can be achieved through constrained RL and safe exploration approaches. The former learns the safety constraints over the course of training to achieve a safe behavior by the end of training, at the cost of high number of collisions at earlier stages of the training. The latter offers robust safety by enforcing the safety constraints as hard constraints, which prevents collisions but hinders the exploration of the RL agent, resulting in lower rewards and poor performance. To overcome those drawbacks, we propose a novel safety shield, that combines the robustness of the optimization-based controllers with the long prediction capabilities of the RL agents, allowing the RL agent to adaptively tune the parameters of the controller. Our approach is able to improve the exploration of the RL agents for navigation tasks, while minimizing the number of collisions. Experiments in simulation show that our approach outperforms state-of-the-art baselines in the reached goals-to-collisions ratio in different challenging environments. The goals-to-collisions ratio metrics emphasizes the importance of minimizing the number of collisions, while learning to accomplish the task. Our approach achieves a higher number of reached goals compared to the classic safety shields and fewer collisions compared to constrained RL approaches. Finally, we demonstrate the performance of the proposed method in a real-world experiment.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04153
Uncovering Hidden Variables: A Physics Classroom Activity on Correlation and Causation,['Physics Education'],"['Alvaro Suarez', 'Marcelo Vachetta']","One of the main goals of physics education is to develop in students the ability to think critically about data and scientific models. This skill is key in everyday life, as it allows one to interpret data accurately, evaluate whether conclusions are based on evidence, and distinguish between meaningful patterns and random occurrences. Although ""correlation does not imply causation"" it is common to make mistakes when interpreting relationships between variables, either due to bias or the human tendency to seek causal explanations. The history of science is filled with examples where causality was incorrectly inferred from correlation. In today's society, with the rise of fake news, misinterpretation, and data manipulation, educating students about scientific reasoning is more important than ever. In this paper, we present an activity aimed at high school physics students as part of an introductory module on scientific work. The learning objectives of this activity are to adequately plot a set of data, to extract relevant information from these data, and to clearly distinguish between correlation and causation. A brief overview of some types of correlation that do not imply causation follows, followed by a description of the activity, the results obtained, and finally the conclusions.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04150
"If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs",['Computation and Language'],"['Muhammad Khalifa', 'Yi-Chern Tan', 'Arash Ahmadian', 'Tom Hosking', 'Honglak Lee', 'Lu Wang', 'Ahmet Üstün', 'Tom Sherborne', 'Matthias Gallé']","Model merging has shown great promise at combining expert models, but the benefit of merging is unclear when merging ``generalist'' models trained on many tasks. We explore merging in the context of large ($\sim100$B) models, by \textit{recycling} checkpoints that exhibit tradeoffs among different tasks. Such checkpoints are often created in the process of developing a frontier model, and many suboptimal ones are usually discarded. Given a pool of model checkpoints obtained from different training runs (e.g., different stages, objectives, hyperparameters, and data mixtures), which naturally show tradeoffs across different language capabilities (e.g., instruction following vs. code generation), we investigate whether merging can recycle such suboptimal models into a Pareto-optimal one. Our optimization algorithm tunes the weight of each checkpoint in a linear combination, resulting in a Pareto-optimal models that outperforms both individual models and merge-based baselines. Further analysis shows that good merges tend to include almost all checkpoints with with non-zero weights, indicating that even seemingly bad initial checkpoints can contribute to good final merges.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04144
Methodology for Online Estimation of Rheological Parameters in Polymer Melts Using Deep Learning and Microfluidics,['Fluid Dynamics'],"['Juan Sandubete-López', 'José L. Risco-Martín', 'Alexander H. McMillan', 'Eva Besada-Portas']","Microfluidic devices are increasingly used in biological and chemical experiments due to their cost-effectiveness for rheological estimation in fluids. However, these devices often face challenges in terms of accuracy, size, and cost. This study presents a methodology, integrating deep learning, modeling and simulation to enhance the design of microfluidic systems, used to develop an innovative approach for viscosity measurement of polymer melts. We use synthetic data generated from the simulations to train a deep learning model, which then identifies rheological parameters of polymer melts from pressure drop and flow rate measurements in a microfluidic circuit, enabling online estimation of fluid properties. By improving the accuracy and flexibility of microfluidic rheological estimation, our methodology accelerates the design and testing of microfluidic devices, reducing reliance on physical prototypes, and offering significant contributions to the field.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04142
Understanding Memorization in Generative Models via Sharpness in Probability Landscapes,['Machine Learning'],"['Dongjae Jeon', 'Dueun Kim', 'Albert No']","In this paper, we introduce a geometric framework to analyze memorization in diffusion models using the eigenvalues of the Hessian of the log probability density. We propose that memorization arises from isolated points in the learned probability distribution, characterized by sharpness in the probability landscape, as indicated by large negative eigenvalues of the Hessian. Through experiments on various datasets, we demonstrate that these eigenvalues effectively detect and quantify memorization. Our approach provides a clear understanding of memorization in diffusion models and lays the groundwork for developing strategies to ensure secure and reliable generative models△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04140
SRAM-Based PUF Reliability Prediction Using Cell-Imbalance Characterization in the State Space Diagram,['Cryptography and Security'],"['Gabriel Torrens', 'Abdel Alheyasat', 'Bartomeu Alorda', 'Sebastia A. Bota']","This work proposes a methodology to estimate the statistical distribution of the probability that a 6T bit-cell starts up to a given logic value in SRAM memories for PUF applications. First, the distribution is obtained experimentally in a 65-nm CMOS device. As this distribution cannot be reproduced by electrical simulation, we explore the use of an alternative parameter defined as the distance between the origin and the separatrix in the bit-cell state space to quantify the mismatch of the cell. The resulting distribution of this parameter obtained from Monte Carlo simulations is then related to the start-up probability distribution using a two-component logistic function. The reported results show that the proposed imbalance factor is a good predictor for PUF-related reliability estimation with the advantage that can be applied at the early design stages.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04125
Insights on the Rotational State and Shape of Asteroid (203) Pompeja from TESS Photometry,['Earth and Planetary Astrophysics'],"['Oriel A. Humes', 'Josef Hanuš']","The Main Belt asteroid (203) Pompeja shows evidence of extreme variability in visible and near-infrared spectral slope with time. The observed spectral variability has been hypothesized to be attributed to spatial variations across Pompeja's surface. In this scenario, the observed spectrum of Pompeja is dependent on the geometry of the Sun and the observer relative to the asteroid's spin pole and surface features. Knowledge of the rotational spin pole and shape can be gleaned from light curves and photometric measurements. However, dense light curves of Pompeja are only available from two apparitions. Further, previous estimates of Pompeja's sidereal period are close to being Earth-commensurate, making ground-based light curves difficult to obtain. To overcome these difficulties, we implement a pipeline to extract a dense light curve of Pompeja from cutouts of TESS Full Frame Images. We succeeded in obtaining a dense light curve of Pompeja covering $\sim$22 complete rotations. We measure a synodic period of $P_{syn} =24.092 \pm 0.005$ hours and amplitude of 0.073 $\pm$ 0.002 magnitudes during Pompeja's 2021 apparition in the TESS field of view. We use this light curve to refine models of Pompeja's shape and spin pole orientation, yielding two spin pole solutions with sidereal periods and spin pole ecliptic coordinates of $P_{\mathrm{sid}, 1} = 24.0485 \pm 0.0001$ hours, $λ_1 = 132^{\circ}$, $β_1 = +41^{\circ}$ and $P_{\mathrm{sid}, 2} = 24.0484 \pm 0.0001$ hours, $λ_2 =307^{\circ}$, $β_2 =+34^{\circ}$. Finally, we discuss the implications of the derived shape and spin models for spectral variability on Pompeja.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04123
CrossSDF: 3D Reconstruction of Thin Structures From Cross-Sections,['Computer Vision and Pattern Recognition'],"['Thomas Walker', 'Salvatore Esposito', 'Daniel Rebain', 'Amir Vaxman', 'Arno Onken', 'Changjian Li', 'Oisin Mac Aodha']","Reconstructing complex structures from planar cross-sections is a challenging problem, with wide-reaching applications in medical imaging, manufacturing, and topography. Out-of-the-box point cloud reconstruction methods can often fail due to the data sparsity between slicing planes, while current bespoke methods struggle to reconstruct thin geometric structures and preserve topological continuity. This is important for medical applications where thin vessel structures are present in CT and MRI scans. This paper introduces \method, a novel approach for extracting a 3D signed distance field from 2D signed distances generated from planar contours. Our approach makes the training of neural SDFs contour-aware by using losses designed for the case where geometry is known within 2D slices. Our results demonstrate a significant improvement over existing methods, effectively reconstructing thin structures and producing accurate 3D models without the interpolation artifacts or over-smoothing of prior approaches.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04120
Thermal and RGB Images Work Better Together in Wind Turbine Damage Detection,['Computer Vision and Pattern Recognition'],"['Serhii Svystun', 'Oleksandr Melnychenko', 'Pavlo Radiuk', 'Oleg Savenko', 'Anatoliy Sachenko', 'Andrii Lysyi']","The inspection of wind turbine blades (WTBs) is crucial for ensuring their structural integrity and operational efficiency. Traditional inspection methods can be dangerous and inefficient, prompting the use of unmanned aerial vehicles (UAVs) that access hard-to-reach areas and capture high-resolution imagery. In this study, we address the challenge of enhancing defect detection on WTBs by integrating thermal and RGB images obtained from UAVs. We propose a multispectral image composition method that combines thermal and RGB imagery through spatial coordinate transformation, key point detection, binary descriptor creation, and weighted image overlay. Using a benchmark dataset of WTB images annotated for defects, we evaluated several state-of-the-art object detection models. Our results show that composite images significantly improve defect detection efficiency. Specifically, the YOLOv8 model's accuracy increased from 91% to 95%, precision from 89% to 94%, recall from 85% to 92%, and F1-score from 87% to 93%. The number of false positives decreased from 6 to 3, and missed defects reduced from 5 to 2. These findings demonstrate that integrating thermal and RGB imagery enhances defect detection on WTBs, contributing to improved maintenance and reliability.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04114
Adult Glioma Segmentation in Sub-Saharan Africa using Transfer Learning on Stratified Finetuning Data,['Image and Video Processing'],"['Abhijeet Parida', 'Daniel Capellán-Martín', 'Zhifan Jiang', 'Austin Tapp', 'Xinyang Liu', 'Syed Muhammad Anwar', 'María J. Ledesma-Carbayo', 'Marius George Linguraru']","Gliomas, a kind of brain tumor characterized by high mortality, present substantial diagnostic challenges in low- and middle-income countries, particularly in Sub-Saharan Africa. This paper introduces a novel approach to glioma segmentation using transfer learning to address challenges in resource-limited regions with minimal and low-quality MRI data. We leverage pre-trained deep learning models, nnU-Net and MedNeXt, and apply a stratified fine-tuning strategy using the BraTS2023-Adult-Glioma and BraTS-Africa datasets. Our method exploits radiomic analysis to create stratified training folds, model training on a large brain tumor dataset, and transfer learning to the Sub-Saharan context. A weighted model ensembling strategy and adaptive post-processing are employed to enhance segmentation accuracy. The evaluation of our proposed method on unseen validation cases on the BraTS-Africa 2024 task resulted in lesion-wise mean Dice scores of 0.870, 0.865, and 0.926, for enhancing tumor, tumor core, and whole tumor regions and was ranked first for the challenge. Our approach highlights the ability of integrated machine-learning techniques to bridge the gap between the medical imaging capabilities of resource-limited countries and established developed regions. By tailoring our methods to a target population's specific needs and constraints, we aim to enhance diagnostic capabilities in isolated environments. Our findings underscore the importance of approaches like local data integration and stratification refinement to address healthcare disparities, ensure practical applicability, and enhance impact.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04111
Pseudo-Observations for Bivariate Survival Data,['Methodology'],"['Yael Travis-Lumer', 'Micha Mandel', 'Rebecca A. Betensky']","The pseudo-observations approach has been gaining popularity as a method to estimate covariate effects on censored survival data. It is used regularly to estimate covariate effects on quantities such as survival probabilities, restricted mean life, cumulative incidence, and others. In this work, we propose to generalize the pseudo-observations approach to situations where a bivariate failure-time variable is observed, subject to right censoring. The idea is to first estimate the joint survival function of both failure times and then use it to define the relevant pseudo-observations. Once the pseudo-observations are calculated, they are used as the response in a generalized linear model. We consider two common nonparametric estimators of the joint survival function: the estimator of Lin and Ying (1993) and the Dabrowska estimator (Dabrowska, 1988). For both estimators, we show that our bivariate pseudo-observations approach produces regression estimates that are consistent and asymptotically normal. Our proposed method enables estimation of covariate effects on quantities such as the joint survival probability at a fixed bivariate time point, or simultaneously at several time points, and consequentially can estimate covariate-adjusted conditional survival probabilities. We demonstrate the method using simulations and an analysis of two real-world datasets.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04109
The cosmic globular cluster formation history in the E-MOSAICS simulations,['Astrophysics of Galaxies'],"['Philipp S. Joschko', 'J. M. Diederik Kruijssen', 'Sebastian Trujillo-Gomez', 'Joel L. Pfeffer', 'Nate Bastian', 'Robert A. Crain', 'Marta Reina-Campos']","We present a comprehensive analysis of globular cluster (GC) formation and evolution across the $34^3$ Mpc$^3$ volume of the E-MOSAICS galaxy formation simulations. Defining GCs as surviving, high-mass ($>10^5$ M$_\odot$) clusters, we analyse their formation histories as a function of their metallicity and host galaxy mass, also distinguishing between central and satellite galaxies. The redshift of peak GC formation rate increases weakly with galaxy mass, decreases with metallicity, and does not differ between centrals and satellites. The epoch of peak GC formation precedes that of the stars by a factor of $1.1{-}1.6$, primarily due to `downsizing', i.e. low-mass galaxies form their stars later. Consequently, this offset decreases with galaxy mass, leading to nearly coeval stellar and GC populations in massive galaxies ($>10^{11}$ M$_\odot$). GCs themselves do not exhibit strong downsizing, because they predominantly formed at early cosmic epochs conducive to the formation (through high gas pressures) and survival (through high galaxy merger and GC migration rates) of massive, compact stellar systems. The total GC formation rate in the volume peaks at $z\approx 2.5$, shortly before star formation peaks at $z\approx 2$, but well after the general cluster formation rate at $z\approx 4$, reflecting a survivor bias where surviving GCs formed more recently. We find that GC formation commenced early, at $z>10$, such that the results of this work may provide a framework for interpreting direct observations of proto-GC formation with the JWST, especially as these observations accumulate to enable statistical studies.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04105
Restricted Boltzmann machine network versus Jastrow correlated wave function for the two-dimensional Hubbard model,['Strongly Correlated Electrons'],"['Karthik V', 'Amal Medhi']","We consider a restricted Boltzmann Machine (RBM) correlated BCS wave function as the ground state of the two-dimensional Hubbard model and study its electronic and magnetic properties as a function of hole doping. We compare the results with those obtained by using conventional Jastrow projectors. The results show that the RBM wave function outperforms the Jastrow projected ones in the underdoped region inmterms of the variational energy. Computation of superconducting (SC) correlations in the model shows that the RBM wave function gives slightly weaker SC correlations as compared to the Jastrow projected wave functions. A significant advantage of the RBM wave function is that it spontaneously gives rise to strong antiferromagnetic (AF) correlations in the underdoped region even though the wave function does not incorporate any explicit AF order. In comparison, AF correlations in the Jastrow projected wave functions are found to be very weak. These and other results obtained show that the RBM wave function provides an improved description of the phase diagram of the model. The work also demonstrates the power of neural-network quantum state (NQS) wave functions in the study of strongly correlated electron systems.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04103
"Missing Melodies: AI Music Generation and its ""Nearly"" Complete Omission of the Global South",['Sound'],"['Atharva Mehta', 'Shivam Chauhan', 'Monojit Choudhury']","Recent advances in generative AI have sparked renewed interest and expanded possibilities for music generation. However, the performance and versatility of these systems across musical genres are heavily influenced by the availability of training data. We conducted an extensive analysis of over one million hours of audio datasets used in AI music generation research and manually reviewed more than 200 papers from eleven prominent AI and music conferences and organizations (AAAI, ACM, EUSIPCO, EURASIP, ICASSP, ICML, IJCAI, ISMIR, NeurIPS, NIME, SMC) to identify a critical gap in the fair representation and inclusion of the musical genres of the Global South in AI research. Our findings reveal a stark imbalance: approximately 86% of the total dataset hours and over 93% of researchers focus primarily on music from the Global North. However, around 40% of these datasets include some form of non-Western music, genres from the Global South account for only 14.6% of the data. Furthermore, approximately 51% of the papers surveyed concentrate on symbolic music generation, a method that often fails to capture the cultural nuances inherent in music from regions such as South Asia, the Middle East, and Africa. As AI increasingly shapes the creation and dissemination of music, the significant underrepresentation of music genres in datasets and research presents a serious threat to global musical diversity. We also propose some important steps to mitigate these risks and foster a more inclusive future for AI-driven music generation.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04100
Magnetically-controlled Vortex Dynamics in a Ferromagnetic Superconductor,['Superconductivity'],"['Joseph Alec Wilcox', 'Lukas Schneider', 'Estefani Marchiori', 'Vadim Plastovets', 'Alexandre Buzdin', 'Pardis Sahafi', 'Andrew Jordan', 'Raffi Budakian', 'Tong Ren', 'Ivan Veschunov', 'Tsuyoshi Tamegai', 'Sven Friedemann', 'Martino Poggio', 'Simon John Bending']","Ferromagnetic superconductors are exceptionally rare because the strong ferromagnetic exchange field usually destroys singlet superconductivity. EuFe$_2$(As$_{1-x}$P$_x$)$_2$, an iron-based superconductor with a maximum critical temperature of $\sim$25 K, is a unique material that exhibits full coexistence with ferromagnetic order below $T_\mathrm{FM} \approx 19$ K. The interplay between the two leads to a narrowing of ferromagnetic domains at higher temperatures and the spontaneous nucleation of vortices/antivortices at lower temperatures. Here we demonstrate how the underlying magnetic structure directly controls the superconducting vortex dynamics in applied magnetic fields. Just below $T_\mathrm{FM}$ we observe a pronounced temperature-dependent peak in both the coercivity and the creep activation energy, the latter becoming rapidly suppressed in large applied magnetic fields. We attribute this behaviour to the formation of vortex polarons arising from the unique interaction between free vortices and magnetic stripe domains. We present a theoretical description of the properties of vortex polarons that explains our main observations, showing how they lead to vortex trapping and an attractive vortex-vortex interaction at short distances. In stark contrast, strong magnetic irreversibility at low temperatures is linked to a critical current governed by giant flux creep over an activation barrier for vortex-antivortex annihilation near domain walls. Our work reveals unexplored new routes for the magnetic enhancement of vortex pinning with particularly important applications in high-current conductors for operation at high magnetic fields.△ Less",v1,https://arxiv.org/pdf/2412.04098
Three-dimensional velocity fields in the silicon- and sulfur-reach ejecta in the remnant of Tycho supernova,['High Energy Astrophysical Phenomena'],"['O. Petruk', 'M. Patrii', 'T. Kuzyo', 'A. Baldyniuk', 'V. Marchenko', 'V. Beshley']","The three-dimensional velocity structure of the shock-heated Si-reach and S-reach ejecta were reconstructed in Tycho supernova remnant from Doppler-shifted lines. The vector components along the line of sight were restored from the spatially resolved spectral analysis of the Doppler shifts of Si XIII and S XV lines. The components in the plane of the sky were derived from analysis of the proper motion of the remnant's edge at different azimuths. This has been done by using the data of X-ray observations from Chandra observatory as well as the radio data from the Very Large Array.
  Differences in Doppler velocities over the Tycho's SNR are of the order of thousands of km/s. The speed of the ejecta on the opposite sides of the remnant as a three-dimensional object differs on 20-30%. There are asymmetries and differences in the spatial distributions between the Si-reach and S-reach ejecta components. Namely, the level of isotropy is higher in Si while the vector components directed outward of the observer are larger in S. This puts limitations on the level of deviation of the internal structure of the progenitor star from the ideal layered structure as well as on the level of asymmetries in supernova explosion.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04096
Magnetic Resonance Imaging Feature-Based Subtyping and Model Ensemble for Enhanced Brain Tumor Segmentation,['Image and Video Processing'],"['Zhifan Jiang', 'Daniel Capellán-Martín', 'Abhijeet Parida', 'Austin Tapp', 'Xinyang Liu', 'María J. Ledesma-Carbayo', 'Syed Muhammad Anwar', 'Marius George Linguraru']","Accurate and automatic segmentation of brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is essential for quantitative measurements, which play an increasingly important role in clinical diagnosis and prognosis. The International Brain Tumor Segmentation (BraTS) Challenge 2024 offers a unique benchmarking opportunity, including various types of brain tumors in both adult and pediatric populations, such as pediatric brain tumors (PED), meningiomas (MEN-RT) and brain metastases (MET), among others. Compared to previous editions, BraTS 2024 has implemented changes to substantially increase clinical relevance, such as refined tumor regions for evaluation. We propose a deep learning-based ensemble approach that integrates state-of-the-art segmentation models. Additionally, we introduce innovative, adaptive pre- and post-processing techniques that employ MRI-based radiomic analyses to differentiate tumor subtypes. Given the heterogeneous nature of the tumors present in the BraTS datasets, this approach enhances the precision and generalizability of segmentation models. On the final testing sets, our method achieved mean lesion-wise Dice similarity coefficients of 0.926, 0.801, and 0.688 for the whole tumor in PED, MEN-RT, and MET, respectively. These results demonstrate the effectiveness of our approach in improving segmentation performance and generalizability for various brain tumor types.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04094
Cyclic sum formula for certain parametrized multiple zeta values,['Number Theory'],"['Hanamichi Kawamura', 'Anju Yokoi']","Ohno-Wakabayashi's cyclic sum formula for multiple zeta-star values is generalized by Igarashi with one or two parameters. In this article, we give a possible answer for one of his problems about a generalization with three parameters.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04089
A Spectrophotometric analysis and dust properties of classical nova V5584 Sgr,['Solar and Stellar Astrophysics'],"['Mohit Singh Bisht', 'A. Raj', 'F. M. Walter', 'D. Bisht', 'Gargi Shaw', 'K. Belwal', 'S. Biswas']","In this work, optical observations of the nova V5584 Sgr are presented. These observations cover different phases including pre-maximum, early decline, and nebular. The spectra are dominated by hydrogen Balmer, Fe II, and O I lines with P-Cygni profiles in the early phase, which are subsequently observed in complete emission. The presence of numerous Fe II lines and low ejecta velocity aligns with the Fe II type nova classification. From optical and NIR colors it is clear that this nova manifests dust formation in the ejecta. The dust temperature and mass were estimated from a spectral energy distribution (SED) fit to the JHK band magnitudes and the WISE data. Light curve analysis shows t$_2$ and t$_3$ values of $\sim$ 26 and $\sim$ 48 days, classifying the nova as moderately fast. The physical and chemical properties during early decline and later phases were evaluated using the photoionization code CLOUDY. The best-fit model parameters from two epochs of multiwavelength spectra are compatible with a hot white dwarf source with a roughly constant luminosity of $\sim$ (2.08 $\pm$ 0.10) $\times$ 10$^{36}$ erg s$^{-1}$. We find an ejected mass of $\sim$ (1.59 $\pm$ 0.04) $\times$ 10$^{-4}$M$_{\odot}$. Abundance analysis indicates that the ejecta is significantly enriched relative to solar values, with O/H = 30.2, C/H = 10.8, He/H = 1.8, Mg/H = 1.68, Na/H = 1.55, and N/H = 45.5 in the early decline phase, and O/H = 4.5, Ne/H = 1.5, and N/H = 24.5 in the nebular phase.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04088
BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image Generation,['Computer Vision and Pattern Recognition'],"['Nefeli Andreou', 'Varsha Vivek', 'Ying Wang', 'Alex Vorobiov', 'Tiffany Deng', 'Raja Bala', 'Larry Davis', 'Betty Mohler Tesch']","Accurately generating images of human bodies from text remains a challenging problem for state of the art text-to-image models. Commonly observed body-related artifacts include extra or missing limbs, unrealistic poses, blurred body parts, etc. Currently, evaluation of such artifacts relies heavily on time-consuming human judgments, limiting the ability to benchmark models at scale. We address this by proposing BodyMetric, a learnable metric that predicts body realism in images. BodyMetric is trained on realism labels and multi-modal signals including 3D body representations inferred from the input image, and textual descriptions. In order to facilitate this approach, we design an annotation pipeline to collect expert ratings on human body realism leading to a new dataset for this task, namely, BodyRealism. Ablation studies support our architectural choices for BodyMetric and the importance of leveraging a 3D human body prior in capturing body-related artifacts in 2D images. In comparison to concurrent metrics which evaluate general user preference in images, BodyMetric specifically reflects body-related artifacts. We demonstrate the utility of BodyMetric through applications that were previously infeasible at scale. In particular, we use BodyMetric to benchmark the generation ability of text-to-image models to produce realistic human bodies. We also demonstrate the effectiveness of BodyMetric in ranking generated images based on the predicted realism scores.△ Less",v1,https://arxiv.org/pdf/2412.04086
Unified Framework for Open-World Compositional Zero-shot Learning,['Computer Vision and Pattern Recognition'],"['Hirunima Jayasekara', 'Khoi Pham', 'Nirat Saini', 'Abhinav Shrivastava']","Open-World Compositional Zero-Shot Learning (OW-CZSL) addresses the challenge of recognizing novel compositions of known primitives and entities. Even though prior works utilize language knowledge for recognition, such approaches exhibit limited interactions between language-image modalities. Our approach primarily focuses on enhancing the inter-modality interactions through fostering richer interactions between image and textual data. Additionally, we introduce a novel module aimed at alleviating the computational burden associated with exhaustive exploration of all possible compositions during the inference stage. While previous methods exclusively learn compositions jointly or independently, we introduce an advanced hybrid procedure that leverages both learning mechanisms to generate final predictions. Our proposed model, achieves state-of-the-art in OW-CZSL in three datasets, while surpassing Large Vision Language Models (LLVM) in two datasets.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04083
Electron-Induced Radiolysis of Water Ice and the Buildup of Oxygen,['Earth and Planetary Astrophysics'],"['Chantal Tinner', 'André Galli', 'Fiona Bär', 'Antoine Pommerol', 'Martin Rubin', 'Audrey Vorburger', 'Peter Wurz']","Irradiation by energetic ions, electrons, and UV photons induces sputtering and chemical processes (radiolysis) in the surfaces of icy moons, comets, and icy grains. Laboratory experiments, both of ideal surfaces and of more complex and realistic analog samples, are crucial to understand the interaction of surfaces of icy moons and comets with their space environment. This study shows the first results of mass spectrometry measurements from porous water ice regolith samples irradiated with electrons as a representative analogy to water-ice rich surfaces in the solar system. Previous studies have shown that most electron-induced H2O radiolysis products leave the ice as H2 and O2 and that O2 can be trapped under certain conditions in the irradiated ice. Our new laboratory experiments confirm these findings. Moreover, they quantify residence times and saturation levels of O2 in originally pure water ice. H2O may also be released from the water ice by irradiation, but the quantification of the released H2O is more difficult and the total amount is sensitive to the electron flux and energy.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04079
TransAdapter: Vision Transformer for Feature-Centric Unsupervised Domain Adaptation,['Computer Vision and Pattern Recognition'],"['A. Enes Doruk', 'Erhan Oztop', 'Hasan F. Ates']","Unsupervised Domain Adaptation (UDA) aims to utilize labeled data from a source domain to solve tasks in an unlabeled target domain, often hindered by significant domain gaps. Traditional CNN-based methods struggle to fully capture complex domain relationships, motivating the shift to vision transformers like the Swin Transformer, which excel in modeling both local and global dependencies. In this work, we propose a novel UDA approach leveraging the Swin Transformer with three key modules. A Graph Domain Discriminator enhances domain alignment by capturing inter-pixel correlations through graph convolutions and entropy-based attention differentiation. An Adaptive Double Attention module combines Windows and Shifted Windows attention with dynamic reweighting to align long-range and local features effectively. Finally, a Cross-Feature Transform modifies Swin Transformer blocks to improve generalization across domains. Extensive benchmarks confirm the state-of-the-art performance of our versatile method, which requires no task-specific alignment modules, establishing its adaptability to diverse applications.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04073
Boundary-Guided Learning for Gene Expression Prediction in Spatial Transcriptomics,['Machine Learning'],"['Mingcheng Qu', 'Yuncong Wu', 'Donglin Di', 'Anyang Su', 'Tonghua Su', 'Yang Song', 'Lei Fan']","Spatial transcriptomics (ST) has emerged as an advanced technology that provides spatial context to gene expression. Recently, deep learning-based methods have shown the capability to predict gene expression from WSI data using ST data. Existing approaches typically extract features from images and the neighboring regions using pretrained models, and then develop methods to fuse this information to generate the final output. However, these methods often fail to account for the cellular structure similarity, cellular density and the interactions within the microenvironment. In this paper, we propose a framework named BG-TRIPLEX, which leverages boundary information extracted from pathological images as guiding features to enhance gene expression prediction from WSIs. Specifically, our model consists of three branches: the spot, in-context and global branches. In the spot and in-context branches, boundary information, including edge and nuclei characteristics, is extracted using pretrained models. These boundary features guide the learning of cellular morphology and the characteristics of microenvironment through Multi-Head Cross-Attention. Finally, these features are integrated with global features to predict the final output. Extensive experiments were conducted on three public ST datasets. The results demonstrate that our BG-TRIPLEX consistently outperforms existing methods in terms of Pearson Correlation Coefficient (PCC). This method highlights the crucial role of boundary features in understanding the complex interactions between WSI and gene expression, offering a promising direction for future research.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04072
Multiwavelength picture of the misaligned BL Lac object 3C 371,['High Energy Astrophysical Phenomena'],"['J. Otero-Santos', 'C. M. Raiteri', 'A. Tramacere', 'J. Escudero Pedrosa', 'J. A. Acosta-Pulido', 'M. I. Carnerero', 'M. Villata', 'I. Agudo', 'I. A. Rahimov', 'T. S. Andreeva', 'D. V. Ivanov', 'N. Marchili', 'S. Righini', 'M. Giroletti', 'M. A. Gurwell', 'S. S. Savchenko', 'D. Carosati', 'W. P. Chen', 'S. O. Kurtanidze', 'M. D. Joner', 'E. Semkov', 'T. Pursimo', 'E. Benítez', 'G. Damljanovic', 'G. Andreuzzi']","The BL Lac object 3C 371 is one of the targets that are regularly monitored by the Whole Earth Blazar Telescope (WEBT) Collaboration to study blazar variability on both short and long timescales. We aim to evaluate the long-term multiwavelength (MWL) behaviour of 3C 371, comparing it with the results derived for its optical emission in our previous study. For this, we make use of the multi-band campaigns organized by the WEBT Collaboration in optical and radio between January 2018 and December 2020, and of public data from Swift and Fermi satellites and the MOJAVE Very Large Interferometry programme. We evaluate the variability shown by the source in each band with the amplitude variability quantification, as well as possible interband correlation using the z-Discrete Correlation Function. We also present a deep analysis of the optical-UV, X-ray and $γ$-ray spectral variability. With the MOJAVE data we perform a kinematics analysis, looking for components propagating along the jet, calculating its kinematics parameters. This set of parameters is later used for the interpretation of the source MWL behaviour, modelling the broadband spectral energy distribution (SED) of the source with theoretical blazar emission scenarios.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04068
Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning,['Computation and Language'],"['Amnon Bleich', 'Antje Linnemann', 'Bjoern H. Diem', 'Tim OF Conrad']","Recent advances in deep learning and natural language generation have significantly improved image captioning, enabling automated, human-like descriptions for visual content. In this work, we apply these captioning techniques to generate clinician-like interpretations of ECG data. This study leverages existing ECG datasets accompanied by free-text reports authored by healthcare professionals (HCPs) as training data. These reports, while often inconsistent, provide a valuable foundation for automated learning. We introduce an encoder-decoder-based method that uses these reports to train models to generate detailed descriptions of ECG episodes. This represents a significant advancement in ECG analysis automation, with potential applications in zero-shot classification and automated clinical decision support.
  The model is tested on various datasets, including both 1- and 12-lead ECGs. It significantly outperforms the state-of-the-art reference model by Qiu et al., achieving a METEOR score of 55.53% compared to 24.51% achieved by the reference model. Furthermore, several key design choices are discussed, providing a comprehensive overview of current challenges and innovations in this domain.
  The source codes for this research are publicly available in our Git repository https://git.zib.de/ableich/ecg-comment-generation-public△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04067
"A note on infinite versions of $(p,q)$-theorems",['Combinatorics'],"['Attila Jung', 'Dömötör Pálvölgyi']","We prove that fractional Helly and $(p,q)$-theorems imply $(\aleph_0,q)$-theorems in an entirely abstract setting. We give a plethora of applications, including reproving almost all earlier $(\aleph_0,q)$-theorems about geometric hypergraphs that were proved recently. Some of the corollaries are new results, for example, we prove that if $\mathcal{F}$ is an infinite family of convex compact sets in $\mathbb{R}^d$ and among every $\aleph_0$ of the sets some $d+1$ contain a point in their intersection with integer coordinates, then all the members of $\mathcal{F}$ can be hit with finitely many points with integer coordinates.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04066
Graph Neural Networks Need Cluster-Normalize-Activate Modules,['Machine Learning'],"['Arseny Skryagin', 'Felix Divo', 'Mohammad Amin Ali', 'Devendra Singh Dhami', 'Kristian Kersting']","Graph Neural Networks (GNNs) are non-Euclidean deep learning models for graph-structured data. Despite their successful and diverse applications, oversmoothing prohibits deep architectures due to node features converging to a single fixed point. This severely limits their potential to solve complex tasks. To counteract this tendency, we propose a plug-and-play module consisting of three steps: Cluster-Normalize-Activate (CNA). By applying CNA modules, GNNs search and form super nodes in each layer, which are normalized and activated individually. We demonstrate in node classification and property prediction tasks that CNA significantly improves the accuracy over the state-of-the-art. Particularly, CNA reaches 94.18% and 95.75% accuracy on Cora and CiteSeer, respectively. It further benefits GNNs in regression tasks as well, reducing the mean squared error compared to all baselines. At the same time, GNNs with CNA require substantially fewer learnable parameters than competing architectures.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04064
The puzzling long GRB 191019A: Evidence for Kilonova Light,['High Energy Astrophysical Phenomena'],"['G. Stratta', 'A. M. Nicuesa Guelbenzu', 'S. Klose', 'A. Rossi', 'P. Singh', 'E. Palazzi', 'C. Guidorzi', 'A. Camisasca', 'S. Bernuzzi', 'A. Rau', 'M. Bulla', 'F. Ragosta', 'E. Maiorano', 'D. Paris']","GRB 191019A was a long Gamma-ray burst (GRB) lasting about 65 s and, as such, originally thought to be linked to a core-collapse supernova. However, even though follow-up observations identified the optical counterpart close to the bright nucleus of a nearby ancient galaxy (z=0.248), no associated supernova was found. This led to the suggestion that the burst was caused by the merger of two compact stellar objects, likely in a dense circumnuclear environment. By using a recently developed diagnostic tool based on prompt emission temporal properties, we noticed that GRB 191019A falls among those long GRBs which are associated with compact mergers and with evidence of kilonova light. We thus re-analyzed unpublished GROND multi-color (g'r'i'z'JHK_s) data obtained between 0.4 and 15 days post trigger. Image subtraction confirmed the optical counterpart in all four optical bands, with GROND tracking its fading until 1.5 days post-burst. Incorporating publicly available Swift-XRT data, a joint fit of an afterglow plus a kilonova model revealed a better match than an afterglow-only scenario. The resulting kilonova properties resemble those of AT2017gfo associated with the binary neutron star merger GW170817, with a total ejected mass of about 0.06 solar mass. Contrary to previous findings inferring a high-density circumburst environment (n0=10^7-10^8 cm^-3), our analysis finds standard conditions (n0 = 1 cm^-3), suggesting the long duration of GRB 191019A was intrinsic rather than due to jet interaction with a dense external medium.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04059
From Code to Play: Benchmarking Program Search for Games Using Large Language Models,['Artificial Intelligence'],"['Manuel Eberhardinger', 'James Goodman', 'Alexander Dockhorn', 'Diego Perez-Liebana', 'Raluca D. Gaina', 'Duygu Çakmak', 'Setareh Maghsudi', 'Simon Lucas']","Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games. In this work, we explore the potential of LLMs to directly synthesize usable code for a wide range of gaming applications, focusing on two programming languages, Python and Java. We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs. For Python, the framework covers various game-related tasks, including five miniature versions of Atari games, ten levels of Baba is You, an environment inspired by Asteroids, and a maze generation task. For Java, the framework contains 12 games from the TAG tabletop games framework. Across 29 tasks, we evaluated 12 language models for Python and 8 for Java. Our findings suggest that the performance of LLMs depends more on the task than on model size. While larger models generate more executable programs, these do not always result in higher-quality solutions but are much more expensive. No model has a clear advantage, although on any specific task, one model may be better. Trying many models on a problem and using the best results across them is more reliable than using just one.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04057
On translocal entropy functions,['Dynamical Systems'],"['Andrzej Bis', 'Henk Bruin']","In 2007, Ye \& Zhang introduced a version of local topological entropy. Since their entropy function is, as we show under mild conditions, constant for topologically transitive dynamical systems, we propose to adjust the notion in a way that doesn't neglect the initial transient part of an orbit.
  We investigate the properties of this ``transient'' local entropy and compute it in terms of Lyapunov exponents for various dynamical systems.
  We also investigate how this adjustment affects measure-theoretic local (Brin-Katok) entropy.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04055
Demonstration of Enhanced Qubit Readout via Reinforcement Learning,['Quantum Physics'],"['Aniket Chatterjee', 'Jonathan Schwinger', 'Yvonne Y. Gao']","Measurement is an essential component for robust and practical quantum computation. For superconducting qubits, the measurement process involves the effective manipulation of the joint qubit-resonator dynamics, and should ideally provide the highest quality for qubit state discrimination with the shortest readout pulse and resonator reset time. Here, we harness model-free reinforcement learning (RL) together with a tailored training environment to achieve this multi-pronged optimization task. We demonstrate on the IBM quantum device that the measurement pulse obtained by the RL agent not only successfully achieves state-of-the-art performance, with an assignment error of $(4.6 \pm 0.4)\times10^{-3}$, but also executes the readout and the subsequent resonator reset almost 3x faster than the system's default process. Furthermore, the learned waveforms are robust against realistic parameter drifts and follow a generalized analytical form, making them readily implementable in practice with no significant computation overhead. Our results provide an effective readout strategy to boost the performance of superconducting quantum processors and demonstrate the prowess of RL in providing optimal and experimentally informed solutions for complex quantum information processing tasks.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04053
A Phase-Field-Micromechanics Study on the Microstructural Evolution during Viscous Sintering,"['Computational Engineering, Finance, and Science']","['Xiaoxu Dai', 'Bo Qian', 'Arkadz Kirshtein', 'Qingcheng Yang']","In the manufacturing process of high-performance particulate materials, viscous sintering plays a crucial role, particularly in fields such as polymer processing and additive manufacturing. The interactions between microscopic particles, their flow behavior, and the evolution of porosity during the viscous sintering process directly influence the material's density and mechanical properties. Therefore, developing efficient modeling techniques to simulate the viscous sintering process is essential for optimizing sintering technology. However, the large deformations and dynamic surface evolution inherent in the viscous sintering of particulate materials present challenges to traditional methods based on the sharp interface model. To address these challenges, we propose a thermodynamically consistent diffusion interface model, referred to as the phase-field-micromechanics model, to analyze the evolution of various physical quantities throughout the viscous sintering process. This model implicitly describes the evolution of particle morphology through an introduced phase-field variable. Through comparisons with analytical solutions and experimental data, we rigorously validate the correctness of the proposed model qualitatively and quantitatively under both isothermal and non-isothermal conditions. Using the proposed model, we explore the development of strain and stress during the sintering process, as well as the effects of particle size, shape and arrangement on the overall sintering behavior. The evolution of these characteristic indicators allows for a clear observation of the viscous sintering process, which is vital for understanding the mechanisms behind viscous sintering and for guiding industrial production.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04050
Pathwise optimization for bridge-type estimators and its applications,['Machine Learning'],"['Alessandro De Gregorio', 'Francesco Iafrate']","Sparse parametric models are of great interest in statistical learning and are often analyzed by means of regularized estimators. Pathwise methods allow to efficiently compute the full solution path for penalized estimators, for any possible value of the penalization parameter $λ$. In this paper we deal with the pathwise optimization for bridge-type problems; i.e. we are interested in the minimization of a loss function, such as negative log-likelihood or residual sum of squares, plus the sum of $\ell^q$ norms with $q\in(0,1]$ involving adpative coefficients. For some loss functions this regularization achieves asymptotically the oracle properties (such as the selection consistency). Nevertheless, since the objective function involves nonconvex and nondifferentiable terms, the minimization problem is computationally challenging.
  The aim of this paper is to apply some general algorithms, arising from nonconvex optimization theory, to compute efficiently the path solutions for the adaptive bridge estimator with multiple penalties. In particular, we take into account two different approaches: accelerated proximal gradient descent and blockwise alternating optimization. The convergence and the path consistency of these algorithms are discussed. In order to assess our methods, we apply these algorithms to the penalized estimation of diffusion processes observed at discrete times. This latter represents a recent research topic in the field of statistics for time-dependent data.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04047
AI4EF: Artificial Intelligence for Energy Efficiency in the Building Sector,['Machine Learning'],"['Alexandros Menelaos Tzortzis', 'Georgios Kormpakis', 'Sotiris Pelekis', 'Ariadni Michalitsi-Psarrou', 'Evangelos Karakolis', 'Christos Ntanos', 'Dimitris Askounis']","AI4EF, Artificial Intelligence for Energy Efficiency, is an advanced, user-centric tool designed to support decision-making in building energy retrofitting and efficiency optimization. Leveraging machine learning (ML) and data-driven insights, AI4EF enables stakeholders such as public sector representatives, energy consultants, and building owners to model, analyze, and predict energy consumption, retrofit costs, and environmental impacts of building upgrades. Featuring a modular framework, AI4EF includes customizable building retrofitting, photovoltaic installation assessment, and predictive modeling tools that allow users to input building parameters and receive tailored recommendations for achieving energy savings and carbon reduction goals. Additionally, the platform incorporates a Training Playground for data scientists to refine ML models used by said framework. Finally, AI4EF provides access to the Enershare Data Space to facilitate seamless data sharing and access within the ecosystem. Its compatibility with open-source identity management, Keycloak, enhances security and accessibility, making it adaptable for various regulatory and organizational contexts. This paper presents an architectural overview of AI4EF, its application in energy efficiency scenarios, and its potential for advancing sustainable energy practices through artificial intelligence (AI).△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04045
Recognizing 2-Layer and Outer $k$-Planar Graphs,['Data Structures and Algorithms'],"['Yasuaki Kobayashi', 'Yuto Okada', 'Alexander Wolff']","The crossing number of a graph is the least number of crossings over all drawings of the graph in the plane. Computing the crossing number of a given graph is NP-hard, but fixed-parameter tractable (FPT) with respect to the natural parameter. Two well-known variants of the problem are 2-layer crossing minimization and circular crossing minimization, where every vertex must lie on one of two layers, namely two parallel lines, or a circle, respectively. Both variants are NP-hard, but FPT with respect to the natural parameter.
  Recently, a local version of the crossing number has also received considerable attention. A graph is $k$-planar if it admits a drawing with at most $k$ crossings per edge. In contrast to the crossing number, recognizing $k$-planar graphs is NP-hard even if $k=1$.
  In this paper, we consider the two above variants in the local setting. The $k$-planar graphs that admit a straight-line drawing with vertices on two layers or on a circle are called 2-layer $k$-planar and outer $k$-planar graphs, respectively. We study the parameterized complexity of the two recognition problems with respect to $k$. For $k=0$, both problems can easily be solved in linear time. Two groups independently showed that outer 1-planar graphs can also be recognized in linear time [Algorithmica 2015/2016]. One group asked whether outer 2-planar graphs can be recognized in polynomial time.
  Our main contribution consists of XP-algorithms for recognizing 2-layer $k$-planar graphs and outer $k$-planar graphs. We complement these results by showing that both recognition problems are XNLP-hard. This implies that both problems are W$[t]$-hard for every $t$ and that it is unlikely that they admit FPT-algorithms. On the other hand, we present an FPT-algorithm for recognizing 2-layer $k$-planar graphs where the order of the vertices on one layer is specified.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04042
"The physical and chemical structure of Sagittarius B2, VIII. Full molecular line survey of hot cores",['Astrophysics of Galaxies'],"['T. Möller', 'P. Schilke', 'Á. Sánchez-Monge', 'A. Schmiedeke']","The giant molecular cloud complex Sagittarius B2 (Sgr~B2) in the central molecular zone of our Galaxy hosts several high-mass star formation sites, with Sgr~B2(M) and Sgr~B2(N) being the main centers of activity. This analysis aims to comprehensively model each core spectrum, considering molecular lines, dust attenuation, and free-free emission interactions. We describe the molecular content analysis of each hot core and identify the chemical composition of detected sources. Using ALMA's high sensitivity, we aim to characterize the hot core population in Sgr~B2(M) and N, gaining a better understanding of the different evolutionary phases of star formation processes in this complex. We conducted an unbiased ALMA spectral line survey of 47 sources in band 6 (211-275 GHz). Chemical composition and column densities were derived using XCLASS, assuming local thermodynamic equilibrium. Quantitative descriptions for each molecule were determined, considering all emission and absorption features across the spectral range. Temperature and velocity distributions were analyzed, and derived abundances were compared with other spectral line surveys. We identified 65 isotopologs from 48 different molecules, ranging from light molecules to complex organic compounds, originating from various environments. Most sources in the Sgr~B2 complex were assigned different evolutionary phases of high-mass star formation. Sgr~B2(N) hot cores show more complex molecules such as CH$_3$OH, CH$_3$OCHO, and CH$_3$OCH$_3$, while M cores contain lighter molecules such as SO$_2$, SO, and NO. Some sulfur-bearing molecules are more abundant in N than in M. The derived molecular abundances can be used for comparison and to constrain astrochemical models. Inner sources in both regions were generally more developed than outer sources, with some exceptions.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04040
Benchmarking and Enhancing Surgical Phase Recognition Models for Robotic-Assisted Esophagectomy,['Computer Vision and Pattern Recognition'],"['Yiping Li', 'Romy van Jaarsveld', 'Ronald de Jong', 'Jasper Bongers', 'Gino Kuiper', 'Richard van Hillegersberg', 'Jelle Ruurda', 'Marcel Breeuwer', 'Yasmina Al Khalil']","Robotic-assisted minimally invasive esophagectomy (RAMIE) is a recognized treatment for esophageal cancer, offering better patient outcomes compared to open surgery and traditional minimally invasive surgery. RAMIE is highly complex, spanning multiple anatomical areas and involving repetitive phases and non-sequential phase transitions. Our goal is to leverage deep learning for surgical phase recognition in RAMIE to provide intraoperative support to surgeons. To achieve this, we have developed a new surgical phase recognition dataset comprising 27 videos. Using this dataset, we conducted a comparative analysis of state-of-the-art surgical phase recognition models. To more effectively capture the temporal dynamics of this complex procedure, we developed a novel deep learning model featuring an encoder-decoder structure with causal hierarchical attention, which demonstrates superior performance compared to existing models.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04039
Dimension Reduction via Random Projection for Privacy in Multi-Agent Systems,['Cryptography and Security'],"['Puspanjali Ghoshal', 'Ashok Singh Sairam']","The agents in a Multi-Agent System (MAS) make observations about the system and send that information to a fusion center. The fusion center aggregates the information and concludes about the system parameters with as much accuracy as possible. However for the purposes of better efficiency of the system at large, the agents need to append some private parameters to the observed data. In this scenario, the data sent to the fusion center is faced with privacy risks. The data communicated to the fusion center must be secured against data privacy breaches and inference attacks in a decentralized manner. However, this in turn leads to a loss of utility of the data being sent to the fusion center. We quantify the utility and privacy of the system using Cosine similarity. We formulate our MAS problem in terms of deducing a concept for which compression-based methods are there in literature. Next, we propose a novel sanitization mechanism for our MAS using one such compression-based method while addressing the utility-privacy tradeoff problem.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04031
Mask of truth: model sensitivity to unexpected regions of medical images,['Computer Vision and Pattern Recognition'],"['Théo Sourget', 'Michelle Hestbek-Møller', 'Amelia Jiménez-Sánchez', 'Jack Junchi Xu', 'Veronika Cheplygina']","The development of larger models for medical image analysis has led to increased performance. However, it also affected our ability to explain and validate model decisions. Models can use non-relevant parts of images, also called spurious correlations or shortcuts, to obtain high performance on benchmark datasets but fail in real-world scenarios. In this work, we challenge the capacity of convolutional neural networks (CNN) to classify chest X-rays and eye fundus images while masking out clinically relevant parts of the image. We show that all models trained on the PadChest dataset, irrespective of the masking strategy, are able to obtain an Area Under the Curve (AUC) above random. Moreover, the models trained on full images obtain good performance on images without the region of interest (ROI), even superior to the one obtained on images only containing the ROI. We also reveal a possible spurious correlation in the Chaksu dataset while the performances are more aligned with the expectation of an unbiased model. We go beyond the performance analysis with the usage of the explainability method SHAP and the analysis of embeddings. We asked a radiology resident to interpret chest X-rays under different masking to complement our findings with clinical knowledge. Our code is available at https://github.com/TheoSourget/MMC_Masking and https://github.com/TheoSourget/MMC_Masking_EyeFundus△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04030
Atmospheric molecular blobs shape up circumstellar envelopes of AGB stars,['Solar and Stellar Astrophysics'],"['L. Velilla-Prieto', 'J. P. Fonfría', 'M. Agúndez', 'A. Castro-Carrizo', 'M. Guélin', 'G. Quintana-Lacaci', 'I. Cherchneff', 'C. Joblin', 'M. C. McCarthy', 'J. A. Martín-Gago', 'J. Cernicharo']","During their thermally pulsing phase, Asymptotic Giant Branch (AGB) stars eject material that forms extended dusty envelopes. Visible polarimetric imaging found clumpy dust clouds within two stellar radii of several oxygen-rich stars. Inhomogeneous molecular gas has also been observed in multiple emission lines within several stellar radii of different oxygen rich stars, including W Hya and Mira. At the stellar surface level, infrared images have revealed intricate structures around the carbon semi-regular variable R Scl and in the S-type star $π^{\mathrm{1}}$ Gru. Infrared images have also shown clumpy dust structures within a few stellar radii of the prototypical carbon AGB star IRC+10216, and studies of the molecular gas distribution beyond the dust formation zone have also shown complex circumstellar structures. Because of the lack of sufficient spatial resolution, however, the distribution of molecular gas in the stellar atmosphere and the dust formation zone of AGB carbon stars is not known, nor is how it is subsequently expelled.
  Here we report observations with a resolution of one stellar radius of the recently formed dust and molecular gas in the atmosphere of IRC+10216. Lines of HCN, SiS, and SiC$_2$ appear at different radii and in different clumps, which we interpret as large convective cells in the photosphere, as seen in Betelgeuse. The convective cells coalesce with pulsation causing anisotropies that, together with companions, shape its circumstellar envelope.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04027
A Tactile Void,['Soft Condensed Matter'],"['Pierre Tapie', 'Diogo Barreiros Scatamburlo', 'Antoine Chateauminois', 'Elie Wandersman']","We mimic the mechanical response of touch mechanoreceptors by that of a gas cavity embedded in an elastic semi-cylinder, as a fingertip analogue. Using tribological experiments combined with optical imaging, we measure the dynamics and deformation of the cavity as the semi-cylinder is put in static contact or slid against model rough surfaces at constant normal force and velocity. We propose an elastic model to predict the cavity deformation under normal load showing that membrane mechanical stresses are anisotropic and we discuss its possible biological consequences. In friction experiments, we show that the cavity shape fluctuations allow for texture discriminations.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04024
PT-Symmetry in $2\times 2$ Matrix Polynomials Formed by Pauli Matrices,['Mathematical Physics'],"['Stalin Abraham', 'Ameeya A. Bhagwat']","$2\times2$ matrix polynomials of the form $P_{n}(z)= Σ^{n}_{j=0}\,σ_{j}\,z^{j}$, for the cases $n=1,2,3$ are constructed, and the nature of PT-symmetry is examined across different points $z=(x,y)$ in the complex plane. The PT-symmetric properties of $P_{n}(z)$ can be characterized by two functions, denoted by $s(x,y)$ and $h(x,y)$. If the trace of the matrix polynomial is real, then the points at which it can exhibit PT-symmetry are defined by the family of curves $s(x,y)=0$. Additionally, at points where the function $h(x,y)\geq 0$, the matrix polynomial exhibits unbroken PT-symmetry; otherwise, it exhibits broken PT-symmetry. The intersection points of the curves $s(x,y)=0$ and $h(x,y)=k$, for a given $k\in \mathbb{R}$, are shown to lie on an ellipse, hyperbola, two lines passing through the origin, or a straight line, depending on the nature of PT-symmetry of the matrix polynomial. The PT-symmetric behaviour of $P_{n}(z)$ at the zeros of the matrix polynomial is also studied.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04022
(Blind) Users Really Do Heed Aural Telephone Scam Warnings,['Cryptography and Security'],"['Filipo Sharevski', 'Jennifer Vander Loop', 'Bill Evans', 'Alexander Ponticello']","This paper reports on a study exploring how two groups of individuals, legally blind (n=36) and sighted ones (n=36), react to aural telephone scam warnings in naturalistic settings. As spoofing a CallerID is trivial, communicating the context of an incoming call instead offers a better possibility to warn a receiver about a potential scam. Usually, such warnings are visual in nature and fail to cater to users with visual disabilities. To address this exclusion, we developed an aural variant of telephone scam warnings and tested them in three conditions: baseline (no warning), short warning, and contextual warning that preceded the scam's content. We tested the two most common scam scenarios: fraud (interest rate reduction) and identity theft (social security number) by cold-calling participants and recording their action, and debriefing and obtaining consent afterward. Only two participants ""pressed one"" as the scam demanded, both from the legally blind group that heard the contextual warning for the social security scenario. Upon close inspection, we learned that one of them did so because of accessibility issues with their screen reader and the other did so intentionally because the warning convinced them to waste the scammer's time, so they don't scam vulnerable people. Both the legally blind and the sighted participants found the contextual warnings as powerful usable security cues that, together with STIR/SHAKEN indicators like ""Scam Likely"", would provide robust protection against any type of scam. We also discussed the potential privacy implications of the contextual warnings and collected recommendations for usably accessible implementation.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04014
Deep-Unrolling Multidimensional Harmonic Retrieval Algorithms on Neuromorphic Hardware,['Signal Processing'],"['Vlad C. Andrei', 'Alexandru P. Drăguţoiu', 'Gabriel Béna', 'Mahmoud Akl', 'Yin Li', 'Matthias Lohrmann', 'Ullrich J. Mönich', 'Holger Boche']","This paper explores the potential of conversion-based neuromorphic algorithms for highly accurate and energy-efficient single-snapshot multidimensional harmonic retrieval (MHR). By casting the MHR problem as a sparse recovery problem, we devise the currently proposed, deep-unrolling-based Structured Learned Iterative Shrinkage and Thresholding (S-LISTA) algorithm to solve it efficiently using complex-valued convolutional neural networks with complex-valued activations, which are trained using a supervised regression objective. Afterward, a novel method for converting the complex-valued convolutional layers and activations into spiking neural networks (SNNs) is developed. At the heart of this method lies the recently proposed Few Spikes (FS) conversion, which is extended by modifying the neuron model's parameters and internal dynamics to account for the inherent coupling between real and imaginary parts in complex-valued computations. Finally, the converted SNNs are mapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of estimation accuracy and power efficiency between the original CNNs deployed on an NVIDIA Jetson Xavier and the SNNs is being conducted. The measurement results show that the converted SNNs achieve almost five-fold power efficiency at moderate performance loss compared to the original CNNs.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04008
Radial Oscillations in Hybrid Stars with Slow Quark Phase Transition,['Nuclear Theory'],"['Ishfaq Ahmad Rather', 'Kauan D. Marquez', 'Betânia C. Backes', 'Grigoris Panotopoulos', 'Ilídio Lopes']","This study investigates the radial oscillations of hybrid neutron stars, characterized by a composition of hadronic external layers and a quark matter core. Utilizing a density-dependent relativistic mean-field model that incorporates hyperons and baryons for describing hadronic matter, and a density-dependent quark model for quark matter, we analyze the ten lowest eigenfrequencies and their corresponding oscillation functions. Our focus lies on neutron stars with equations-of-state involving N, N + $Δ$, N + H, and N + H + $Δ$, featuring a phase transition to quark matter. Emphasizing the effects of a slow phase transition at the hadron-quark interface, we observe that the maximum mass is attained before the fundamental mode's frequency decreases for slow phase transitions. This observation implies the stability of stellar configurations with higher central densities than the maximum mass, called Slow Stable Hybrid Stars (SSHSs), even under small radial perturbations. The length of these SSHS branch depends upon the energy density jump between two phases and the stiffness of the quark EoS.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.04007
Transport Signatures of Radial Rashba Spin-Orbit Coupling at Ferromagnet/Superconductor Interfaces,['Superconductivity'],"['Andreas Costa', 'Jaroslav Fabian']","Spin-orbit coupling (SOC) emerging at the interfaces of superconducting magnetic tunnel junctions is at the heart of multiple unprecedented physical phenomena, covering triplet proximity effects induced by unconventional (spin-flip) Andreev reflections, giant transport magnetoansiotropies, sizable tunneling anomalous Hall effects, and electrically controlled current-reversing $ 0 $--$ π$(-like) transitions in Josephson contacts. Recent first-principles calculations proposed that the Rashba spin-orbit fields in twisted graphene/transition-metal dichalcogenide and van-der-Waals multilayers can -- owing to broken mirror symmetries -- exhibit an unconventional radial component (with spin parallel to the electron's momentum), which can be quantified by the Rashba angle $ θ_\mathrm{R} $. We theoretically explore the ramifications of radial Rashba SOC at the interfaces of vertical ferromagnet/superconductor tunnel junctions with a focus on the magnetoanisotropies of the tunneling and tunneling-anomalous-Hall-effect conductances. Our results demonstrate that $ θ_\mathrm{R} $ can be experimentally extracted from respective magnetization-angle shifts, providing a practical way to probe the radial Rashba SOC induced by twisted multilayers that are placed as tunneling barrier between ferromagnetic and superconducting electrodes.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03994
Kinematic distances of galaxies in the Local Volume,['Astrophysics of Galaxies'],"['I. D. Karachentsev', 'A. A. Popova']","We consider the kinematic distances to nearby galaxies obtained by the Numerical Action Method (NAM) based on the Cosmic-flow-3 survey data. NAM distances are compared with 418 high-precision distances measured by the Tip of the Red Giant Branch (TRGB) method using the Hubble Space Telescope. We estimated the average difference <D_NAM - D_TRGB> = -0.30 +- 0.08 Mpc and the standard deviation of 1.57 Mpc. Approximately the same difference in the distance scale is obtained in comparison with less accurate distance estimates through the membership of galaxies in known groups or from the Tully-Fisher relation. We conclude that the NAM method provides distance estimates with an accuracy of 20% within the Local Volume, which is valid for ~90% of the sky, except for the regions of the Virgo cluster and the Coma-I group.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03988
"Design, Characterization, and Validation of a Variable Stiffness Prosthetic Elbow",['Robotics'],"['Giuseppe Milazzo', 'Simon Lemerle', 'Giorgio Grioli', 'Antonio Bicchi', 'Manuel G. Catalano']","Intuitively, prostheses with user-controllable stiffness could mimic the intrinsic behavior of the human musculoskeletal system, promoting safe and natural interactions and task adaptability in real-world scenarios. However, prosthetic design often disregards compliance because of the additional complexity, weight, and needed control channels. This paper focuses on designing a Variable Stiffness Actuator (VSA) with weight, size, and performance compatible with prosthetic applications, addressing its implementation for the elbow joint. While a direct biomimetic approach suggests adopting an Agonist-Antagonist (AA) layout to replicate the biceps and triceps brachii with elastic actuation, this solution is not optimal to accommodate the varied morphologies of residual limbs. Instead, we employed the AA layout to craft an elbow prosthesis fully contained in the user's forearm, catering to individuals with distal transhumeral amputations. Additionally, we introduce a variant of this design where the two motors are split in the upper arm and forearm to distribute mass and volume more evenly along the bionic limb, enhancing comfort for patients with more proximal amputation levels. We characterize and validate our approach, demonstrating that both architectures meet the target requirements for an elbow prosthesis. The system attains the desired 120° range of motion, achieves the target stiffness range of [2, 60] Nm/rad, and can actively lift up to 3 kg. Our novel design reduces weight by up to 50% compared to existing VSAs for elbow prostheses while achieving performance comparable to the state of the art. Case studies suggest that passive and variable compliance could enable robust and safe interactions and task adaptability in the real world.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03985
Epoch-based Application of Problem-Aware Operators in a Multiobjective Memetic Algorithm for Portfolio Optimization,['Neural and Evolutionary Computing'],"['Feijoo Colomine Durán', 'Carlos Cotta', 'Antonio J. Fernández-Leiva']","We consider the issue of intensification/diversification balance in the context of a memetic algorithm for the multiobjective optimization of investment portfolios with cardinality constraints. We approach this issue in this work by considering the selective application of knowledge-augmented operators (local search and a memory of elite solutions) based on the search epoch in which the algorithm finds itself, hence alternating between unbiased search (guided uniquely by the built-in search mechanics of the algorithm) and focused search (intensified by the use of the problem-aware operators). These operators exploit Sharpe index (a measure of the relationship between return and risk) as a source of problem knowledge. We have conducted a sensibility analysis to determine in which phases of the search the application of these operators leads to better results. Our findings indicate that the resulting algorithm is quite robust in terms of parameterization from the point of view of this problem-specific indicator. Furthermore, it is shown that not only can other non-memetic counterparts be outperformed, but that there is a range of parameters in which the MA is also competitive when not better in terms of standard multiobjective performance indicators.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03981
Comprehensive Audio Query Handling System with Integrated Expert Models and Contextual Understanding,['Audio and Speech Processing'],"['Vakada Naveen', 'Arvind Krishna Sridhar', 'Yinyi Guo', 'Erik Visser']","This paper presents a comprehensive chatbot system designed to handle a wide range of audio-related queries by integrating multiple specialized audio processing models. The proposed system uses an intent classifier, trained on a diverse audio query dataset, to route queries about audio content to expert models such as Automatic Speech Recognition (ASR), Speaker Diarization, Music Identification, and Text-to-Audio generation. A 3.8 B LLM model then takes inputs from an Audio Context Detection (ACD) module extracting audio event information from the audio and post processes text domain outputs from the expert models to compute the final response to the user. We evaluated the system on custom audio tasks and MMAU sound set benchmarks. The custom datasets were motivated by target use cases not covered in industry benchmarks and included ACD-timestamp-QA (Question Answering) as well as ACD-temporal-QA datasets to evaluate timestamp and temporal reasoning questions, respectively. First we determined that a BERT based Intent Classifier outperforms LLM-fewshot intent classifier in routing queries. Experiments further show that our approach significantly improves accuracy on some custom tasks compared to state-of-the-art Large Audio Language Models and outperforms models in the 7B parameter size range on the sound testset of the MMAU benchmark, thereby offering an attractive option for on device deployment.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03980
AI-based Attacker Models for Enhancing Multi-Stage Cyberattack Simulations in Smart Grids Using Co-Simulation Environments,['Cryptography and Security'],"['Omer Sen', 'Christoph Pohl', 'Immanuel Hacker', 'Markus Stroot', 'Andreas Ulbig']","The transition to smart grids has increased the vulnerability of electrical power systems to advanced cyber threats. To safeguard these systems, comprehensive security measures-including preventive, detective, and reactive strategies-are necessary. As part of the critical infrastructure, securing these systems is a major research focus, particularly against cyberattacks. Many methods are developed to detect anomalies and intrusions and assess the damage potential of attacks. However, these methods require large amounts of data, which are often limited or private due to security concerns. We propose a co-simulation framework that employs an autonomous agent to execute modular cyberattacks within a configurable environment, enabling reproducible and adaptable data generation. The impact of virtual attacks is compared to those in a physical lab targeting real smart grids. We also investigate the use of large language models for automating attack generation, though current models on consumer hardware are unreliable. Our approach offers a flexible, versatile source for data generation, aiding in faster prototyping and reducing development resources and time.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03979
Revealing Physical Mechanisms of pattern formation and switching in ecosystems via Landscape and Flux,['Biological Physics'],"['Jie Su', 'Wei Wu', 'Denis Patterson', 'Simon Asher Levin', 'Jin Wang']","Spatial patterns are widely observed in numerous nonequilibrium natural systems, often undergoing complex transitions and bifurcations, thereby exhibiting significant importance in many physical and biological systems such as embryonic development, ecosystem desertification, and turbulence. However, how the pattern formation emerges and how the pattern switching occurs are not fully understood. Here, we developed a landscape-flux field theory via the spatial mode expansion method to uncover the underlying physical mechanism of the pattern formation and switching. We identified the landscape and flux field as the driving force for spatial dynamics and applied this theory to the critical transitions between spatial vegetation patterns in semi-arid ecosystems, revealing that the nonequilibrium flux drives transitions of spatial patterns. We uncovered how the pattern switching emerges through the optimal pathways and how fast this occurs via the speed of pattern switching. Furthermore, both the averaged flux and the entropy production rate exhibit peaks near pattern switching boundaries, revealing dynamical and thermodynamical origins for pattern transitions, and further offering early warning signals for anticipating pattern switching. Our work thus reveals physical mechanisms on spatial pattern-switching in semi-arid ecosystems and, more generally, introduces an useful approach for quantifying spatial pattern switching in nonequilibrium systems, which further offers practical applications such as early warning signals for critical transitions of spatial patterns.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03978
Digital Twin for Evaluating Detective Countermeasures in Smart Grid Cybersecurity,['Cryptography and Security'],"['Omer Sen', 'Nathalie Bleser', 'Andreas Ulbig']","As the integration of digital technologies and communication systems continues within distribution grids, new avenues emerge to tackle energy transition challenges. Nevertheless, this deeper technological immersion amplifies the necessity for resilience against threats, encompassing both systemic outages and targeted cyberattacks. To ensure the robustness and safeguarding of vital infrastructure, a thorough examination of potential smart grid vulnerabilities and subsequent countermeasure development is essential. This study delves into the potential of digital twins, replicating a smart grid's cyber-physical laboratory environment, thereby enabling focused cybersecurity assessments. Merging the nuances of communication network emulation and power network simulation, we introduce a flexible, comprehensive digital twin model equipped for hardware-in-the-loop evaluations. Through this innovative framework, we not only verify and refine security countermeasures but also underscore their role in maintaining grid stability and trustworthiness.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03973
First Measurements of the 4-Point Correlation Function of Magnetohydrodynamic Turbulence as a Novel Probe of the Interstellar Medium,['Astrophysics of Galaxies'],"['Victoria Williamson', 'James Sunseri', 'Zachary Slepian', 'Jiamin Hou', 'Alessandro Greco']","In the Interstellar Medium (ISM), gas and dust evolve under magnetohydrodynamic (MHD) turbulence. This produces dense, non-linear structures that then seed star formation. Observationally and theoretically, turbulence is quantified by summary statistics such as the 2-Point Correlation Function (2PCF) or its Fourier-space analog the power spectrum. These cannot capture the non-Gaussian correlations coming from turbulence's highly non-linear nature. We here for the first time apply the 4-Point Correlation Function (4PCF) to turbulence, measuring it on a large suite of MHD simulations that mirror, as well as currently possible, the conditions expected in the ISM. The 4PCF captures the dependence of correlations between quadruplets of density points on the geometry of the tetrahedron they form. Using a novel functionality added to the \textsc{sarabande} code specifically for this work, we isolate the purely non-Gaussian piece of the 4PCF. We then explore simulations with a range of pressures, $P$, and magnetic fields, $B$ (but without self-gravity); these are quantified by different sonic $(M_{\rm S})$ and Alfvénic $(M_{\rm A})$ Mach numbers. We show that the 4PCF has rich behavior that can in future be used as a diagnostic of ISM conditions. We also show that a large-scale coherent magnetic field leads to parity-odd modes of the 4PCF, a clean test of magnetic field coherence with observational ramifications. All our measurements of the 4PCF (10 $M_{\rm S}, M_{\rm A}$ combinations, 9 time-slices for each, 34 4PCF modes for each) are made public for the community to explore.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03967
Naturally graded p-filiform associative algebras,['Rings and Algebras'],['I. A. Karimjanov'],"In the paper, we describe $n$-dimensional naturally graded nilpotent associative algebras with the characteristic sequence $C(\mathcal{A})=(n-p,1,\dots,1)$ as called $p-$filiform algebras over the field of the complex numbers.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03964
CO-to-H$_2$ conversion factor and grain size distribution through the analysis of $α_\mathrm{CO}$-$q_\mathrm{PAH}$ relation,['Astrophysics of Galaxies'],"['I-Da Chiang', 'Hiroyuki Hirashita', 'Jeremy Chastenet', 'Karin M. Sandstrom', 'Eric W. Koch', 'Adam K. Leroy', 'Yu-Hsuan Teng', 'Thomas G. Williams']","The CO-to-H$_2$ conversion factor ($α_\mathrm{CO}$) is expected to vary with dust abundance and grain size distribution through the efficiency of shielding gas from CO-dissociation radiation. We present a comprehensive analysis of $α_\mathrm{CO}$ and grain size distribution for nearby galaxies, using the PAH fraction ($q_\mathrm{PAH}$) as an observable proxy of grain size distribution. We adopt the resolved observations at 2-kpc resolution in 42 nearby galaxies, where $α_\mathrm{CO}$ is derived from measured metallicity and surface densities of dust and HI assuming a fixed dust-to-metals ratio. We use an analytical model for the evolution of H$_2$ and CO, in which the evolution of grain size distribution is controlled by the dense gas fraction ($η$). We find that the observed level of $q_\mathrm{PAH}$ is consistent with the diffuse-gas-dominated model ($η=0.2$) where dust shattering is more efficient. Meanwhile, the slight decreasing trend of observed $q_\mathrm{PAH}$ with metallicity is more consistent with high-$η$ predictions, likely due to the more efficient loss of PAHs by coagulation. We discuss how grain size distribution (indicated by $q_\mathrm{PAH}$) and metallicity impact $α_\mathrm{CO}$; we however did not obtain conclusive evidence that the grain size distribution affects $α_\mathrm{CO}$. Observations and model predictions show similar anti-correlation between $α_\mathrm{CO}$ and 12+log(O/H). Meanwhile, there is a considerable difference in how resolved $α_\mathrm{CO}$ behaves with $q_\mathrm{PAH}$. The observed $α_\mathrm{CO}$ has a positive correlation with $q_\mathrm{PAH}$, while the model-predicted $α_\mathrm{CO}$ does not have a definite correlation with $q_\mathrm{PAH}$. This difference is likely due to the limitation of one-zone treatment in the model.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03954
Performance study for anisotropic flow measurements in the MPD (NICA) experiment with fixed target,['High Energy Physics - Experiment'],"['P. Parfenov', 'M. Mamaev', 'A. Taranenko']",Studying the properties of strongly-interacting matter at high relative baryon densities is one of the key scientific goals of the MPD (Multi-Purpose Detector) experiment at the NICA accelerator complex. The performance of measuring the azimuthal collective flow of identified charged hadrons at the MPD facility in fixed-target mode is studied in this work.△ Less,"5 December, 2024;",https://arxiv.org/pdf/2412.03947
The Mpemba effect in quantum oscillating and two-level systems,['Quantum Physics'],"['Fardin Kheirandish', 'Narges Cheraghpour', 'Adam Moradian']","The Empemba effect (ME) is investigated in the context of ubiquitous quantum oscillating and two-level systems (TLS) using a novel approach (DOI 10.1088/1402-4896/ad97f1). Exact reduced density matrices for various initial states are derived. The temporal behavior of the trace distance for these initial states is calculated analytically and presented. For a dissipative quantum oscillating system, it is demonstrated that number states $|N\ra$ intersect with coherent states $|α\ra$, with this intersection occurring earlier for smaller values of $N$. Additionally, thermal states intersect with coherent states for specific values of $|α|$, leading to the occurrence of the ME in these two scenarios. A weaker version of the ME is also observed for thermal and number states. In the case of a quantum TLS, it is shown that the ME effect occurs, and the potential for its realization and experimental observation is discussed, with reference to the Jaynes-Cummings model (JCM) involving a decaying time-dependent coupling function.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03943
"Exploring AI Text Generation, Retrieval-Augmented Generation, and Detection Technologies: a Comprehensive Overview",['Artificial Intelligence'],"['Fnu Neha', 'Deepshikha Bhati', 'Deepak Kumar Shukla', 'Angela Guercio', 'Ben Ward']","The rapid development of Artificial Intelligence (AI) has led to the creation of powerful text generation models, such as large language models (LLMs), which are widely used for diverse applications. However, concerns surrounding AI-generated content, including issues of originality, bias, misinformation, and accountability, have become increasingly prominent. This paper offers a comprehensive overview of AI text generators (AITGs), focusing on their evolution, capabilities, and ethical implications. This paper also introduces Retrieval-Augmented Generation (RAG), a recent approach that improves the contextual relevance and accuracy of text generation by integrating dynamic information retrieval. RAG addresses key limitations of traditional models, including their reliance on static knowledge and potential inaccuracies in handling real-world data. Additionally, the paper reviews detection tools that help differentiate AI-generated text from human-written content and discusses the ethical challenges these technologies pose. The paper explores future directions for improving detection accuracy, supporting ethical AI development, and increasing accessibility. The paper contributes to a more responsible and reliable use of AI in content creation through these discussions.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03933
A Physics-Informed Scenario Approach with Data Mitigation for Safety Verification of Nonlinear Systems,['Systems and Control'],"['Ali Aminzadeh', 'MohammadHossein Ashoori', 'Amy Nejati', 'Abolfazl Lavaei']","This paper develops a physics-informed scenario approach for safety verification of nonlinear systems using barrier certificates (BCs) to ensure that system trajectories remain within safe regions over an infinite time horizon. Designing BCs often relies on an accurate dynamics model; however, such models are often imprecise due to the model complexity involved, particularly when dealing with highly nonlinear systems. In such cases, while scenario approaches effectively address the safety problem using collected data to construct a guaranteed BC for the dynamical system, they often require substantial amounts of data-sometimes millions of samples-due to exponential sample complexity. To address this, we propose a physics-informed scenario approach that selects data samples such that the outputs of the physics-based model and the observed data are sufficiently close (within a specified threshold). This approach guides the scenario optimization process to eliminate redundant samples and significantly reduce the required dataset size. We demonstrate the capability of our approach in mitigating the amount of data required for scenario optimizations with both deterministic (i.e., confidence 1) and probabilistic (i.e., confidence between 0 and 1) guarantees. We validate our physics-informed scenario approach through two physical case studies, showcasing its practical application in reducing the required data.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03932
Near-ideal relaxed MHD in slab geometry,['Plasma Physics'],"['Arash Tavassoli', 'Stuart R. Hudson', 'Zhisong Qu', 'Matthew Hole']","We investigate the solutions of the relaxed MHD model (RxMHD) of Dewar \& Qu [J. Plasma Phys. {\bf 88}, 835880101 (2022)]. This model is a generalization of Taylor relaxation that allows the ideal Ohm's law constraint to be included, and this offers a pathway to extend the multi-region relaxed MHD (MRxMHD) model. By constructing solutions numerically, we show that the RxMHD model of Dewar \& Qu is mathematically well-defined and computationally feasible for constructing MHD equilibria. We also show that a cross-field flow can exist without enforcing an arbitrary constraint on the angular momentum (as is done in the case of MRxMHD with flow), and a pressure profile with a small gradient due to the Bernoulli flow. Our results also demonstrate the self-organization of fully relaxed regions during the optimization, which was an important motivation behind developing this model.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03931
Privacy-Preserving in Medical Image Analysis: A Review of Methods and Applications,['Computer Vision and Pattern Recognition'],"['Yanming Zhu', 'Xuefei Yin', 'Alan Wee-Chung Liew', 'Hui Tian']","With the rapid advancement of artificial intelligence and deep learning, medical image analysis has become a critical tool in modern healthcare, significantly improving diagnostic accuracy and efficiency. However, AI-based methods also raise serious privacy concerns, as medical images often contain highly sensitive patient information. This review offers a comprehensive overview of privacy-preserving techniques in medical image analysis, including encryption, differential privacy, homomorphic encryption, federated learning, and generative adversarial networks. We explore the application of these techniques across various medical image analysis tasks, such as diagnosis, pathology, and telemedicine. Notably, we organizes the review based on specific challenges and their corresponding solutions in different medical image analysis applications, so that technical applications are directly aligned with practical issues, addressing gaps in the current research landscape. Additionally, we discuss emerging trends, such as zero-knowledge proofs and secure multi-party computation, offering insights for future research. This review serves as a valuable resource for researchers and practitioners and can help advance privacy-preserving in medical image analysis.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03924
Deformation-Aware Segmentation Network Robust to Motion Artifacts for Brain Tissue Segmentation using Disentanglement Learning,['Image and Video Processing'],"['Sunyoung Jung', 'Yoonseok Choi', 'Mohammed A. Al-masni', 'Minyoung Jung', 'Dong-Hyun Kim']","Motion artifacts caused by prolonged acquisition time are a significant challenge in Magnetic Resonance Imaging (MRI), hindering accurate tissue segmentation. These artifacts appear as blurred images that mimic tissue-like appearances, making segmentation difficult. This study proposes a novel deep learning framework that demonstrates superior performance in both motion correction and robust brain tissue segmentation in the presence of artifacts. The core concept lies in a complementary process: a disentanglement learning network progressively removes artifacts, leading to cleaner images and consequently, more accurate segmentation by a jointly trained motion estimation and segmentation network. This network generates three outputs: a motioncorrected image, a motion deformation map that identifies artifact-affected regions, and a brain tissue segmentation mask. This deformation serves as a guidance mechanism for the disentanglement process, aiding the model in recovering lost information or removing artificial structures introduced by the artifacts. Extensive in-vivo experiments on pediatric motion data demonstrate that our proposed framework outperforms state-of-the-art methods in segmenting motion-corrupted MRI scans.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03922
Learning Robust Safety Controllers for Uncertain Input-Affine Polynomial Systems,['Systems and Control'],"['Omid Akbarzadeh', 'Abolfazl Lavaei']","This paper offers a direct data-driven approach for learning robust control barrier certificates (R-CBCs) and robust safety controllers (R-SCs) for discrete-time input-affine polynomial systems with unknown dynamics under unknown-but-bounded disturbances. The proposed method relies on data from input-state observations collected over a finite-time horizon while satisfying a specific rank condition to ensure the system is persistently excited. Our data-driven scheme enables the synthesis of R-CBCs and R-SCs directly from observed data, bypassing the need for explicit modeling of the system's dynamics and thus ensuring robust system safety against disturbances within a finite time horizon. Our proposed approach is formulated as a sum-of-squares (SOS) optimization problem, providing a structured design framework. Two case studies showcase our method's capability to provide robust safety guarantees for unknown input-affine polynomial systems under bounded disturbances, demonstrating its practical effectiveness.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03919
Quantized and Interpretable Learning Scheme for Deep Neural Networks in Classification Task,['Machine Learning'],"['Alireza Maleki', 'Mahsa Lavaei', 'Mohsen Bagheritabar', 'Salar Beigzad', 'Zahra Abadi']","Deep learning techniques have proven highly effective in image classification, but their deployment in resourceconstrained environments remains challenging due to high computational demands. Furthermore, their interpretability is of high importance which demands even more available resources. In this work, we introduce an approach that combines saliency-guided training with quantization techniques to create an interpretable and resource-efficient model without compromising accuracy. We utilize Parameterized Clipping Activation (PACT) to perform quantization-aware training, specifically targeting activations and weights to optimize precision while minimizing resource usage. Concurrently, saliency-guided training is employed to enhance interpretability by iteratively masking features with low gradient values, leading to more focused and meaningful saliency maps. This training procedure helps in mitigating noisy gradients and yields models that provide clearer, more interpretable insights into their decision-making processes. To evaluate the impact of our approach, we conduct experiments using famous Convolutional Neural Networks (CNN) architecture on the MNIST and CIFAR-10 benchmark datasets as two popular datasets. We compare the saliency maps generated by standard and quantized models to assess the influence of quantization on both interpretability and classification accuracy. Our results demonstrate that the combined use of saliency-guided training and PACT-based quantization not only maintains classification performance but also produces models that are significantly more efficient and interpretable, making them suitable for deployment in resource-limited settings.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03915
Final-Model-Only Data Attribution with a Unifying View of Gradient-Based Methods,['Machine Learning'],"['Dennis Wei', 'Inkit Padhi', 'Soumya Ghosh', 'Amit Dhurandhar', 'Karthikeyan Natesan Ramamurthy', 'Maria Chang']","Training data attribution (TDA) is the task of attributing model behavior to elements in the training data. This paper draws attention to the common setting where one has access only to the final trained model, and not the training algorithm or intermediate information from training. To serve as a gold standard for TDA in this ""final-model-only"" setting, we propose further training, with appropriate adjustment and averaging, to measure the sensitivity of the given model to training instances. We then unify existing gradient-based methods for TDA by showing that they all approximate the further training gold standard in different ways. We investigate empirically the quality of these gradient-based approximations to further training, for tabular, image, and text datasets and models. We find that the approximation quality of first-order methods is sometimes high but decays with the amount of further training. In contrast, the approximations given by influence function methods are more stable but surprisingly lower in quality.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03906
Certified Learning of Incremental ISS Controllers for Unknown Nonlinear Polynomial Dynamics,['Systems and Control'],"['Mahdieh Zaker', 'David Angeli', 'Abolfazl Lavaei']","Incremental input-to-state stability (delta-ISS) offers a robust framework to ensure that small input variations result in proportionally minor deviations in the state of a nonlinear system. This property is essential in practical applications where input precision cannot be guaranteed. However, analyzing delta-ISS demands detailed knowledge of system dynamics to assess the state's incremental response to input changes, posing a challenge in real-world scenarios where mathematical models are unknown. In this work, we develop a data-driven approach to design delta-ISS Lyapunov functions together with their corresponding delta-ISS controllers for continuous-time input-affine nonlinear systems with polynomial dynamics, ensuring the delta-ISS property is achieved without requiring knowledge of the system dynamics. In our data-driven scheme, we collect only two sets of input-state trajectories from sufficiently excited dynamics, as introduced by Willems et al.'s fundamental lemma. By fulfilling a specific rank condition, we design delta-ISS controllers using the collected samples through formulating a sum-of-squares optimization program. The effectiveness of our data-driven approach is evidenced by its application on a physical case study.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03901
Evolution of Critical Current Density in CaKFe$_4$As$_4$ with La-doping,['Superconductivity'],"['Yangsong Chen', 'Chunlei Wang', 'Yuhang Zu', 'Yuto Kobayashi', 'Ataru Ichinose', 'Ryosuke Sakagami', 'Tsuyoshi Tamegai']","Single crystals of (Ca$_{1-x}$La$_x$)KFe$_4$As$_4$ (0 <= x <=0.16) have been grown by using the self-flux method, and the evolution of physical properties including the critical current density (Jc) with La-doping has been investigated. Tc decreases monotonically with increasing x, while Jc at the same temperature and magnetic field increases initially and reach its maximum at x = 0.082. The increase in Jc is more obvious at low temperatures and high fields. At T = 5 K and H = 40 kOe, Jc reaches 0.34 MA/cm$^2$, which is ~4 times larger than that for pure crystals. It is also found that anomalous temperature dependence of Jc in CaKFe$_4$As$_4$ is wiped away as the La content is increased. However, Jc shows non-monotonic field dependence (peak effect) at high fields in crystals with large x. In addition, we found that despite weak anisotropy of H$_{c2}$, there is extremely large anisotropy of Jc up to ~15, which is most likely caused by novel planar defects in the crystal, similar to CaKFe$_4$As$_4$. Jc characteristics in (Ca$_{1-x}$La$_x$)KFe$_4$As$_4$ with disorder outside FeAs planes is compared with that in CaK(Fe$_{1-x}$Co$_x$)$_4$As$_4$ with disorder within FeAs planes.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03899
Machine Learning-based Android Intrusion Detection System,['Machine Learning'],"['Madiha Tahreem', 'Ifrah Andleeb', 'Bilal Zahid Hussain', 'Arsalan Hameed']","The android operating system is being installed in most of the smart devices. The introduction of intrusions in such operating systems is rising at a tremendous rate. With the introduction of such malicious data streams, the smart devices are being subjected to various attacks like Phishing, Spyware, SMS Fraud, Bots and Banking-Trojans and many such. The application of machine learning classification algorithms for the security of android APK files is used in this paper. Each apk data stream was marked to be either malicious or non malicious on the basis of different parameters. The machine learning classification techniques are then used to classify whether the newly installed applications' signature falls within the malicious or non-malicious domain. If it falls within the malicious category, appropriate action can be taken, and the Android operating system can be shielded against illegal activities.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03894
Abstraction-based Control of Unknown Continuous-Space Models with Just Two Trajectories,['Systems and Control'],"['Behrad Samari', 'Mahdieh Zaker', 'Abolfazl Lavaei']","Finite abstractions (a.k.a. symbolic models) offer an effective scheme for approximating the complex continuous-space systems with simpler models in the discrete-space domain. A crucial aspect, however, is to establish a formal relation between the original system and its symbolic model, ensuring that a discrete controller designed for the symbolic model can be effectively implemented as a hybrid controller (using an interface map) for the original system. This task becomes even more challenging when the exact mathematical model of the continuous-space system is unknown. To address this, the existing literature mainly employs scenario-based data-driven methods, which require collecting a large amount of data from the original system. In this work, we propose a data-driven framework that utilizes only two input-state trajectories collected from unknown nonlinear polynomial systems to synthesize a hybrid controller, enabling the desired behavior on the unknown system through the controller derived from its symbolic model. To accomplish this, we employ the concept of alternating simulation functions (ASFs) to quantify the closeness between the state trajectories of the unknown system and its data-driven symbolic model. By satisfying a specific rank condition on the collected data, which intuitively ensures that the unknown system is persistently excited, we directly design an ASF and its corresponding hybrid controller using finite-length data without explicitly identifying the unknown system, while providing correctness guarantees. This is achieved through proposing a data-based sum-of-squares (SOS) optimization program, enabling a systematic approach to the design process. We illustrate the effectiveness of our data-driven approach through a case study.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03892
MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous Navigation Application,['Robotics'],"['Hyesu Jang', 'Wooseong Yang', 'Hanguen Kim', 'Dongje Lee', 'Yongjin Kim', 'Jinbum Park', 'Minsoo Jeon', 'Jaeseong Koh', 'Yejin Kang', 'Minwoo Jung', 'Sangwoo Jung', 'Ayoung Kim']","Maritime environmental sensing requires overcoming challenges from complex conditions such as harsh weather, platform perturbations, large dynamic objects, and the requirement for long detection ranges. While cameras and LiDAR are commonly used in ground vehicle navigation, their applicability in maritime settings is limited by range constraints and hardware maintenance issues. Radar sensors, however, offer robust long-range detection capabilities and resilience to physical contamination from weather and saline conditions, making it a powerful sensor for maritime navigation. Among various radar types, X-band radar (e.g., marine radar) is widely employed for maritime vessel navigation, providing effective long-range detection essential for situational awareness and collision avoidance. Nevertheless, it exhibits limitations during berthing operations where close-range object detection is critical. To address this shortcoming, we incorporate W-band radar (e.g., Navtech imaging radar), which excels in detecting nearby objects with a higher update rate. We present a comprehensive maritime sensor dataset featuring multi-range detection capabilities. This dataset integrates short-range LiDAR data, medium-range W-band radar data, and long-range X-band radar data into a unified framework. Additionally, it includes object labels for oceanic object detection usage, derived from radar and stereo camera images. The dataset comprises seven sequences collected from diverse regions with varying levels of estimation difficulty, ranging from easy to challenging, and includes common locations suitable for global localization tasks. This dataset serves as a valuable resource for advancing research in place recognition, odometry estimation, SLAM, object detection, and dynamic object elimination within maritime environments. Dataset can be found in following link: https://sites.google.com/view/rpmmoana△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03887
Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models,['Computation and Language'],"['Swarnava Sinha Roy', 'Ayan Kundu']","Integrated Gradients is a well-known technique for explaining deep learning models. It calculates feature importance scores by employing a gradient based approach computing gradients of the model output with respect to input features and accumulating them along a linear path. While this works well for continuous features spaces, it may not be the most optimal way to deal with discrete spaces like word embeddings. For interpreting LLMs (Large Language Models), there exists a need for a non-linear path where intermediate points, whose gradients are to be computed, lie close to actual words in the embedding space. In this paper, we propose a method called Uniform Discretized Integrated Gradients (UDIG) based on a new interpolation strategy where we choose a favorable nonlinear path for computing attribution scores suitable for predictive language models. We evaluate our method on two types of NLP tasks- Sentiment Classification and Question Answering against three metrics viz Log odds, Comprehensiveness and Sufficiency. For sentiment classification, we have used the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for Question Answering, we have used the fine-tuned BERT model on SQuAD dataset. Our approach outperforms the existing methods in almost all the metrics.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03886
A Unified Framework for Evaluating the Effectiveness and Enhancing the Transparency of Explainable AI Methods in Real-World Applications,['Artificial Intelligence'],"['Md. Ariful Islam', 'M. F. Mridha', 'Md Abrar Jahin', 'Nilanjan Dey']","The rapid advancement of deep learning has resulted in substantial advancements in AI-driven applications; however, the ""black box"" characteristic of these models frequently constrains their interpretability, transparency, and reliability. Explainable artificial intelligence (XAI) seeks to elucidate AI decision-making processes, guaranteeing that explanations faithfully represent the model's rationale and correspond with human comprehension. Despite comprehensive research in XAI, a significant gap persists in standardized procedures for assessing the efficacy and transparency of XAI techniques across many real-world applications. This study presents a unified XAI evaluation framework incorporating extensive quantitative and qualitative criteria to systematically evaluate the correctness, interpretability, robustness, fairness, and completeness of explanations generated by AI models. The framework prioritizes user-centric and domain-specific adaptations, hence improving the usability and reliability of AI models in essential domains. To address deficiencies in existing evaluation processes, we suggest defined benchmarks and a systematic evaluation pipeline that includes data loading, explanation development, and thorough method assessment. The suggested framework's relevance and variety are evidenced by case studies in healthcare, finance, agriculture, and autonomous systems. These provide a solid basis for the equitable and dependable assessment of XAI methodologies. This paradigm enhances XAI research by offering a systematic, flexible, and pragmatic method to guarantee transparency and accountability in AI systems across many real-world contexts.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03884
DiffSign: AI-Assisted Generation of Customizable Sign Language Videos With Enhanced Realism,['Computer Vision and Pattern Recognition'],"['Sudha Krishnamurthy', 'Vimal Bhat', 'Abhinav Jain']","The proliferation of several streaming services in recent years has now made it possible for a diverse audience across the world to view the same media content, such as movies or TV shows. While translation and dubbing services are being added to make content accessible to the local audience, the support for making content accessible to people with different abilities, such as the Deaf and Hard of Hearing (DHH) community, is still lagging. Our goal is to make media content more accessible to the DHH community by generating sign language videos with synthetic signers that are realistic and expressive. Using the same signer for a given media content that is viewed globally may have limited appeal. Hence, our approach combines parametric modeling and generative modeling to generate realistic-looking synthetic signers and customize their appearance based on user preferences. We first retarget human sign language poses to 3D sign language avatars by optimizing a parametric model. The high-fidelity poses from the rendered avatars are then used to condition the poses of synthetic signers generated using a diffusion-based generative model. The appearance of the synthetic signer is controlled by an image prompt supplied through a visual adapter. Our results show that the sign language videos generated using our approach have better temporal consistency and realism than signing videos generated by a diffusion model conditioned only on text prompts. We also support multimodal prompts to allow users to further customize the appearance of the signer to accommodate diversity (e.g. skin tone, gender). Our approach is also useful for signer anonymization.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03878
AyutthayaAlpha: A Thai-Latin Script Transliteration Transformer,['Computation and Language'],"['Davor Lauc', 'Attapol Rutherford', 'Weerin Wongwarawipatr']","This study introduces AyutthayaAlpha, an advanced transformer-based machine learning model designed for the transliteration of Thai proper names into Latin script. Our system achieves state-of-the-art performance with 82.32% first-token accuracy and 95.24% first-three-token accuracy, while maintaining a low character error rate of 0.0047. The complexity of Thai phonology, including tonal features and vowel length distinctions, presents significant challenges for accurate transliteration, which we address through a novel two-model approach: AyutthayaAlpha-Small, based on the ByT5 architecture, and AyutthayaAlpha-VerySmall, a computationally efficient variant that unexpectedly outperforms its larger counterpart. Our research combines linguistic rules with deep learning, training on a carefully curated dataset of 1.2 million Thai-Latin name pairs, augmented through strategic upsampling to 2.7 million examples. Extensive evaluations against existing transliteration methods and human expert benchmarks demonstrate that AyutthayaAlpha not only achieves superior accuracy but also effectively captures personal and cultural preferences in name romanization. The system's practical applications extend to cross-lingual information retrieval, international data standardization, and identity verification systems, with particular relevance for government databases, academic institutions, and global business operations. This work represents a significant advance in bridging linguistic gaps between Thai and Latin scripts, while respecting the cultural and personal dimensions of name transliteration.△ Less","5 December, 2024;",https://arxiv.org/pdf/2412.03877
Versatile Optical Ground Station for Satellite-based Quantum Key Distribution in Abu Dhabi,['Quantum Physics'],"['Sana Amairi-Pyka', 'Christoph Fischer', 'Konstantin Kravtsov', 'Gianluca De Santis', 'Alessandro Grosso', 'Edgar Fischer', 'Klaus Kudielka', 'James A. Grieve']","With the growing number of satellite-based Quantum Key Distribution (QKD) payload launches, it becomes essential to ensure compatibility across different platforms for satellite tracking and quantum signal acquisition. In this paper, the Technology Innovation Institute (TII) presents the development of the Abu Dhabi Quantum Optical Ground Station (ADQOGS) for secure free-space optical communications. With the know-how of GA-Synopta's experienced engineering team, we have developed a versatile multi-wavelength quantum acquisition and tracking system tailored to support various upcoming space-based QKD missions, crucial for the practical implementation of global quantum communication networks. This system is capable of handling multiple wavelengths, ranging from 600 nm to 1560 nm for downlink beacons and 1530 nm to 1610 nm for uplink beacons. It includes a free-space quantum module adequate to detect QKD signals at $780\pm10$ nm and $850\pm3$ nm and offers spatial and spectral filtering capabilities along with a motorized polarization correction system.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03872
Isolated neutron stars as Science Validation for XMM2ATHENA: Ensuring robust data for future X-ray Astronomy,['High Energy Astrophysical Phenomena'],"['Adriana Mancini Pires', 'Christian Motch', 'Axel Schwope', 'Iris Traulsen', 'Jean Ballet', 'Sudip Chakraborty', 'David Homan', 'Jan Kurpas', 'Ada Nebot Gomez-Moran', 'Francois-Xavier Pineau', 'Hugo Tranin', 'Natalie Webb']","The discovery of radio-quiet, X-ray thermally emitting isolated neutron stars (XINSs) in the ROSAT All-Sky Survey revealed a previously overlooked component of the neutron star population. Advancements in X-ray instrumentation and the availability of deep, wide-area optical surveys now enable us to explore XINSs at fainter X-ray fluxes and greater distances. In this study, we investigated candidates selected from the 4XMM-DR9 catalogue using XMM-Newton, focusing on long-term flux stability, spectral characterisation, and astrometry. By leveraging resources from the XMM2ATHENA project -- including updated catalogues, multiwavelength characterisation and machine learning classification -- we refined our understanding of this sample of soft X-ray emitters. Our findings enhance the characterisation of XINS candidates, laying the groundwork for more targeted investigations and future catalogue searches.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03870
GP-FL: Model-Based Hessian Estimation for Second-Order Over-the-Air Federated Learning,['Machine Learning'],"['Shayan Mohajer Hamidi', 'Ali Bereyhi', 'Saba Asaad', 'H. Vincent Poor']","Second-order methods are widely adopted to improve the convergence rate of learning algorithms. In federated learning (FL), these methods require the clients to share their local Hessian matrices with the parameter server (PS), which comes at a prohibitive communication cost. A classical solution to this issue is to approximate the global Hessian matrix from the first-order information. Unlike in idealized networks, this solution does not perform effectively in over-the-air FL settings, where the PS receives noisy versions of the local gradients. This paper introduces a novel second-order FL framework tailored for wireless channels. The pivotal innovation lies in the PS's capability to directly estimate the global Hessian matrix from the received noisy local gradients via a non-parametric method: the PS models the unknown Hessian matrix as a Gaussian process, and then uses the temporal relation between the gradients and Hessian along with the channel model to find a stochastic estimator for the global Hessian matrix. We refer to this method as Gaussian process-based Hessian modeling for wireless FL (GP-FL) and show that it exhibits a linear-quadratic convergence rate. Numerical experiments on various datasets demonstrate that GP-FL outperforms all classical baseline first and second order FL approaches.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03867
Electric field control of the exchange field of a single spin impurity on a surface,['Mesoscale and Nanoscale Physics'],"['Xue Zhang', 'Jose Reina-Gálvez', ""Di'an Wu"", 'Jan Martinek', 'Andreas J. Heinrich', 'Taeyoung Choi', 'Christoph Wolf']","Electric control of spins offers faster switching and more localized manipulation compared to magnetic fields. In this work, we investigate static electric field effects on electron spin resonance of single molecules and atoms using scanning tunneling microscopy. We observe significant resonance frequency shifts when varying the applied DC voltage. These shifts cannot be adequately explained by g-factor changes or adsorbate displacement. Instead, we propose a model based on the control of the magnetic exchange field exerted to the spin impurity by the static electric field, which accurately reproduces the static electric field dependence of the resonance frequency. Our work provides crucial insights into electric field influence on surface spins, advancing fundamental understanding for many quantum technologies with efficient spin control via electric fields.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03866
Streamer discharge simulations in humid air: uncertainty in input data and sensitivity analysis,['Plasma Physics'],"['Baohong Guo', 'Hemaditya Malla', 'Alejandro Malagon-Romero', 'Jannis Teunissen']","We study how the choice of input data affects simulations of positive streamers in humid air, focusing on H2O cross sections, photoionization models, and chemistry sets. Simulations are performed in air with a mole fraction of 0%, 3% or 10% H2O using an axisymmetric fluid model. Five H2O cross section sets are considered, which lead to significant differences in the resulting electron attachment coefficient. As a result, the streamer velocity can vary by up to about 50% with 10% H2O. We compare results with three photoionization models: the Naidis model for humid air, the Aints model for humid air, and the standard Zheleznyak model for dry air. With the Naidis and in particular the Aints model, there is a significant reduction in photoionization with higher humidities. This results in higher streamer velocities and maximal electric fields, and it can also cause streamer branching in our axisymmetric simulations. Three humid air chemistry sets are considered. Differences between these sets, particularly in the formation of water clusters around positive ions, cause the streamer velocity to vary by up to about 50% with 10% H2O. A sensitivity analysis is performed to identify the most important chemical reactions in these chemistries.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03861
Combinatorial Selection with Costly Information,['Data Structures and Algorithms'],"['Shuchi Chawla', 'Dimitris Christou', 'Amit Harlev', 'Ziv Scully']","We consider a class of optimization problems over stochastic variables where the algorithm can learn information about the value of any variable through a series of costly steps; we model this information acquisition process as a Markov Decision Process (MDP). The algorithm's goal is to minimize the cost of its solution plus the cost of information acquisition, or alternately, maximize the value of its solution minus the cost of information acquisition. Such bandit superprocesses have been studied previously but solutions are known only for fairly restrictive special cases.
  We develop a framework for approximate optimization of bandit superprocesses that applies to arbitrary processes with a matroid (and in some cases, more general) feasibility constraint. Our framework establishes a bound on the optimal cost through a novel cost amortization; it then couples this bound with a notion of local approximation that allows approximate solutions for each component MDP in the superprocess to be composed without loss into a global approximation.
  We use this framework to obtain approximately optimal solutions for several variants of bandit superprocesses for both maximization and minimization. We obtain new approximations for combinatorial versions of the previously studied Pandora's Box with Optional Inspection and Pandora's Box with Partial Inspection; as well as approximation algorithms for a new problem that we call the Weighing Scale problem.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03860
Un-evaluated Solutions May Be Valuable in Expensive Optimization,['Neural and Evolutionary Computing'],"['Hao Hao', 'Xiaoqun Zhang', 'Aimin Zhou']","Expensive optimization problems (EOPs) are prevalent in real-world applications, where the evaluation of a single solution requires a significant amount of resources. In our study of surrogate-assisted evolutionary algorithms (SAEAs) in EOPs, we discovered an intriguing phenomenon. Because only a limited number of solutions are evaluated in each iteration, relying solely on these evaluated solutions for evolution can lead to reduced disparity in successive populations. This, in turn, hampers the reproduction operators' ability to generate superior solutions, thereby reducing the algorithm's convergence speed. To address this issue, we propose a strategic approach that incorporates high-quality, un-evaluated solutions predicted by surrogate models during the selection phase. This approach aims to improve the distribution of evaluated solutions, thereby generating a superior next generation of solutions. This work details specific implementations of this concept across various reproduction operators and validates its effectiveness using multiple surrogate models. Experimental results demonstrate that the proposed strategy significantly enhances the performance of surrogate-assisted evolutionary algorithms. Compared to mainstream SAEAs and Bayesian optimization algorithms, our approach incorporating the un-evaluated solution strategy shows a marked improvement.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03858
"What Do Machine Learning Researchers Mean by ""Reproducible""?",['Machine Learning'],"['Edward Raff', 'Michel Benaroch', 'Sagar Samtani', 'Andrew L. Farris']","The concern that Artificial Intelligence (AI) and Machine Learning (ML) are entering a ""reproducibility crisis"" has spurred significant research in the past few years. Yet with each paper, it is often unclear what someone means by ""reproducibility"". Our work attempts to clarify the scope of ""reproducibility"" as displayed by the community at large. In doing so, we propose to refine the research to eight general topic areas. In this light, we see that each of these areas contains many works that do not advertise themselves as being about ""reproducibility"", in part because they go back decades before the matter came to broader attention.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03854
Automated LaTeX Code Generation from Handwritten Math Expressions Using Vision Transformer,['Computer Vision and Pattern Recognition'],"['Jayaprakash Sundararaj', 'Akhil Vyas', 'Benjamin Gonzalez-Maldonado']","Converting mathematical expressions into LaTeX is challenging. In this paper, we explore using newer transformer based architectures for addressing the problem of converting handwritten/digital mathematical expression images into equivalent LaTeX code. We use the current state of the art CNN encoder and RNN decoder as a baseline for our experiments. We also investigate improvements to CNN-RNN architecture by replacing the CNN encoder with the ResNet50 model. Our experiments show that transformer architectures achieve a higher overall accuracy and BLEU scores along with lower Levenschtein scores compared to the baseline CNN/RNN architecture with room to achieve even better results with appropriate fine-tuning of model parameters.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03853
Development of decay energy spectroscopy for radio impurity analysis,['Instrumentation and Detectors'],"['J. S. Chung', 'O. Gileva', 'C. Ha', 'J. A Jeon', 'H. B. Kim', 'H. L. Kim', 'Y. H. Kim', 'H. J. Kim', 'M. B Kim', 'D. H. Kwon', 'D. S. Leonard', 'D. Y. Lee', 'Y. C. Lee', 'H. S. Lim', 'K. R. Woo', 'J. Y. Yang']","We present the development of a decay energy spectroscopy (DES) method for the analysis of radioactive impurities using magnetic microcalorimeters (MMCs). The DES system was designed to analyze radionuclides, such as Ra-226, Th-228, and their daughter nuclides, in materials like copper, commonly used in rare-event search experiments. We tested the DES system with a gold foil absorber measuring 20x20x0.05 mm^3, large enough to accommodate a significant drop of source solution. Using this large absorber and an MMC sensor, we conducted a long-term measurement over ten days of live time, requiring 11 ADR cooling cycles. The combined spectrum achieved an energy resolution of 45 keV FWHM, sufficient to identify most alpha and DES peaks of interest. Specific decay events from radionuclide contaminants in the absorber were identified. This experiment confirms the capability of the DES system to measure alpha decay chains of Ra-226 and Th-228, offering a promising method for radio-impurity evaluation in ultra-low background experiments.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03845
Using Cooperative Co-evolutionary Search to Generate Metamorphic Test Cases for Autonomous Driving Systems,['Software Engineering'],"['Hossein Yousefizadeh', 'Shenghui Gu', 'Lionel C. Briand', 'Ali Nasr']","Autonomous Driving Systems (ADSs) rely on Deep Neural Networks, allowing vehicles to navigate complex, open environments. However, the unpredictability of these scenarios highlights the need for rigorous system-level testing to ensure safety, a task usually performed with a simulator in the loop. Though one important goal of such testing is to detect safety violations, there are many undesirable system behaviors, that may not immediately lead to violations, that testing should also be focusing on, thus detecting more subtle problems and enabling a finer-grained analysis. This paper introduces Cooperative Co-evolutionary MEtamorphic test Generator for Autonomous systems (CoCoMEGA), a novel automated testing framework aimed at advancing system-level safety assessments of ADSs. CoCoMEGA combines Metamorphic Testing (MT) with a search-based approach utilizing Cooperative Co-Evolutionary Algorithms (CCEA) to efficiently generate a diverse set of test cases. CoCoMEGA emphasizes the identification of test scenarios that present undesirable system behavior, that may eventually lead to safety violations, captured by Metamorphic Relations (MRs). When evaluated within the CARLA simulation environment on the Interfuser ADS, CoCoMEGA consistently outperforms baseline methods, demonstrating enhanced effectiveness and efficiency in generating severe, diverse MR violations and achieving broader exploration of the test space. These results underscore CoCoMEGA as a promising, more scalable solution to the inherent challenges in ADS testing with a simulator in the loop. Future research directions may include extending the approach to additional simulation platforms, applying it to other complex systems, and exploring methods for further improving testing efficiency such as surrogate modeling.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03843
On the role of the unitary transformations in Bell inequalities,['Quantum Physics'],"['D. O. R. Azevedo', 'F. M. Guedes', 'M. S. Guimaraes', 'I. Roditi', 'S. P. Sorella', 'A. F. Vieira']","The role of the unitary transformations in the Bell-CHSH inequality is highlighted, in both Quantum Mechanics and relativistic Quantum Field Theory. In the former case, we discuss, through a few examples, how the violation of the Bell-CHSH inequality can be encoded into unitary transformations acting on a given reference set of Bell's observables. In the latter case, the Bell-CHSH inequality for a real massive scalar field in the vacuum state is considered. After introducing suitable bounded Hermitian operators, we show that using unitary transformations can significantly improve our previous numerical algorithm, leading to an increase in the size of the violation.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03840
Geometry and topology of closed geodesics complements in the 3-torus,['Geometric Topology'],['José Andrés Rodríguez Migueles'],"We show that for at most three closed geodesics with linearly independent directions, the homeomorphism type of its complement in the 3-torus is determine by the orbit of their direction vectors subspaces under the action of $\PSL_3(\mathbb{Z}).$ Moreover, we provide asymptotically sharp volume bounds for a family of closed geodesics complements. The bounds depend only on the distance in the Farey graph.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03838
"Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries",['Artificial Intelligence'],"['Abul Ehtesham', 'Saket Kumar', 'Aditi Singh', 'Tala Talaei Khoei']","Generative AI is reshaping the media landscape, enabling unprecedented capabilities in video creation, personalization, and scalability. This paper presents a comprehensive SWOT analysis of Metas Movie Gen, a cutting-edge generative AI foundation model designed to produce 1080p HD videos with synchronized audio from simple text prompts. We explore its strengths, including high-resolution video generation, precise editing, and seamless audio integration, which make it a transformative tool across industries such as filmmaking, advertising, and education. However, the analysis also addresses limitations, such as constraints on video length and potential biases in generated content, which pose challenges for broader adoption. In addition, we examine the evolving regulatory and ethical considerations surrounding generative AI, focusing on issues like content authenticity, cultural representation, and responsible use. Through comparative insights with leading models like DALL-E and Google Imagen, this paper highlights Movie Gens unique features, such as video personalization and multimodal synthesis, while identifying opportunities for innovation and areas requiring further research. Our findings provide actionable insights for stakeholders, emphasizing both the opportunities and challenges of deploying generative AI in media production. This work aims to guide future advancements in generative AI, ensuring scalability, quality, and ethical integrity in this rapidly evolving field.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03837
Information theoretic limits of robust sub-Gaussian mean estimation under star-shaped constraints,['Statistics Theory'],"['Akshay Prasadan', 'Matey Neykov']","We obtain the minimax rate for a mean location model with a bounded star-shaped set $K \subseteq \mathbb{R}^n$ constraint on the mean, in an adversarially corrupted data setting with Gaussian noise. We assume an unknown fraction $ε<1/2-κ$ for some fixed $κ\in(0,1/2]$ of the $N$ observations are arbitrarily corrupted. We obtain a minimax risk up to proportionality constants under the squared $\ell_2$ loss of $\max(η^{*2},σ^2ε^2)\wedge d^2$ with \begin{align*}
  η^* = \sup \bigg\{η: \frac{Nη^2}{σ^2} \leq \log M^{\operatorname{loc}}(η,c)\bigg\}, \end{align*} where $\log M^{\operatorname{loc}}(η,c)$ denotes the local entropy of the set $K$, $d$ is the diameter of $K$, $σ^2$ is the variance, and $c$ is some sufficiently large absolute constant. A variant of our algorithm achieves the same rate for settings with known or symmetric sub-Gaussian noise, with a smaller breakdown point, still of constant order. We further study the case of unknown sub-Gaussian noise and show that the rate is slightly slower: $\max(η^{*2},σ^2ε^2\log(1/ε))\wedge d^2$. We generalize our results to the case when $K$ is star-shaped but unbounded.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03832
Optimal Correlation for Bernoulli Trials with Covariates,['Methodology'],"['Tim Morrison', 'Art B. Owen']","Given covariates for $n$ units, each of which is to receive a treatment with probability $1/2$, we study the question of how best to correlate their treatment assignments to minimize the variance of the IPW estimator of the average treatment effect. Past work by \cite{bai2022} found that the optimal stratified experiment is a matched-pair design, where the matching depends on oracle knowledge of the distributions of potential outcomes given covariates. We show that, in the strictly broader class of all admissible correlation structures, the optimal design is to divide the units into two clusters and uniformly assign treatment to exactly one of the two clusters. This design can be computed by solving a 0-1 knapsack problem that uses the same oracle information and can result in an arbitrarily large variance improvement. A shift-invariant version can be constructed by ensuring that exactly half of the units are treated. A method with just two clusters is not robust to a bad proxy for the oracle, and we mitigate this with a hybrid that uses $O(n^α)$ clusters for $0<α<1$. Under certain assumptions, we also derive a CLT for the IPW estimator under our design and a consistent estimator of the variance. We compare our proposed designs to the optimal stratified design in simulated examples and find improved performance.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03827
Reconstruction of boosted and resolved multi-Higgs-boson events with symmetry-preserving attention networks,['High Energy Physics - Phenomenology'],"['Haoyang Li', 'Marko Stamenkovic', 'Alexander Shmakov', 'Michael Fenton', 'Darius Shih-Chieh Chao', 'Kaitlyn Maiya White', 'Caden Mikkelsen', 'Jovan Mitic', 'Cristina Mantilla Suarez', 'Melissa Quinnan', 'Greg Landsberg', 'Harvey Newman', 'Pierre Baldi', 'Daniel Whiteson', 'Javier Duarte']","The production of multiple Higgs bosons at the CERN LHC provides a direct way to measure the trilinear and quartic Higgs self-interaction strengths as well as potential access to beyond the standard model effects that can enhance production at large transverse momentum $p_{\mathrm{T}}$. The largest event fraction arises from the fully hadronic final state in which every Higgs boson decays to a bottom quark-antiquark pair ($b\bar{b}$). This introduces a combinatorial challenge known as the \emph{jet assignment problem}: assigning jets to sets representing Higgs boson candidates. Symmetry-preserving attention networks (SPA-Nets) have been been developed to address this challenge. However, the complexity of jet assignment increases when simultaneously considering both $H\rightarrow b\bar{b}$ reconstruction possibilities, i.e., two ""resolved"" small-radius jets each containing a shower initiated by a $b$-quark or one ""boosted"" large-radius jet containing a merged shower initiated by a $b\bar{b}$ pair. The latter improves the reconstruction efficiency at high $p_{\mathrm{T}}$. In this work, we introduce a generalization to the SPA-Net approach to simultaneously consider both boosted and resolved reconstruction possibilities and unambiguously interpret an event as ""fully resolved'', ""fully boosted"", or in between. We report the performance of baseline methods, the original SPA-Net approach, and our generalized version on nonresonant $HH$ and $HHH$ production at the LHC. Considering both boosted and resolved topologies, our SPA-Net approach increases the Higgs boson reconstruction purity by 57--62\% and the efficiency by 23--38\% compared to the baseline method depending on the final state.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03819
EditScout: Locating Forged Regions from Diffusion-based Edited Images with Multimodal LLM,['Computer Vision and Pattern Recognition'],"['Quang Nguyen', 'Truong Vu', 'Trong-Tung Nguyen', 'Yuxin Wen', 'Preston K Robinette', 'Taylor T Johnson', 'Tom Goldstein', 'Anh Tran', 'Khoi Nguyen']","Image editing technologies are tools used to transform, adjust, remove, or otherwise alter images. Recent research has significantly improved the capabilities of image editing tools, enabling the creation of photorealistic and semantically informed forged regions that are nearly indistinguishable from authentic imagery, presenting new challenges in digital forensics and media credibility. While current image forensic techniques are adept at localizing forged regions produced by traditional image manipulation methods, current capabilities struggle to localize regions created by diffusion-based techniques. To bridge this gap, we present a novel framework that integrates a multimodal Large Language Model (LLM) for enhanced reasoning capabilities to localize tampered regions in images produced by diffusion model-based editing methods. By leveraging the contextual and semantic strengths of LLMs, our framework achieves promising results on MagicBrush, AutoSplice, and PerfBrush (novel diffusion-based dataset) datasets, outperforming previous approaches in mIoU and F1-score metrics. Notably, our method excels on the PerfBrush dataset, a self-constructed test set featuring previously unseen types of edits. Here, where traditional methods typically falter, achieving markedly low scores, our approach demonstrates promising performance.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03809
Clifford-Deformed Compass Codes,['Quantum Physics'],"['Julie A. Campos', 'Kenneth R. Brown']","We can design efficient quantum error-correcting (QEC) codes by tailoring them to our choice of quantum architecture. Useful tools for constructing such codes include Clifford deformations and appropriate gauge fixings of compass codes. In this work, we find Clifford deformations that can be applied to elongated compass codes resulting in QEC codes with improved performance under noise models with errors biased towards dephasing commonly seen in quantum computing architectures. These Clifford deformations enhance decoder performance by introducing symmetries, while the stabilizers of compass codes can be selected to obtain more information on high-rate errors. As a result, the codes exhibit thresholds that increase with bias and display lower logical error rates. One of the Clifford deformations we explore yields QEC codes with better thresholds and logical error rates than those of the XZZX surface code at moderate biases.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03808
A mathematical language for linking fine-scale structure in spikes from hundreds to thousands of neurons with behaviour,['Quantitative Methods'],"['Alexandra N. Busch', 'Roberto C. Budzinski', 'Federico W. Pasini', 'Ján Mináč', 'Jonathan A. Michaels', 'Megan Roussy', 'Roberto A. Gulli', 'Ben C. Corrigan', 'J. Andrew Pruszynski', 'Julio Martinez-Trujillo', 'Lyle E. Muller']","Recent advances in neural recording technology allow simultaneously recording action potentials from hundreds to thousands of neurons in awake, behaving animals. However, characterizing spike patterns in the resulting data, and linking these patterns to behaviour, remains a challenging task. The lack of a rigorous mathematical language for variable numbers of events (spikes) emitted by multiple agents (neurons) is an important limiting factor. We introduce a new mathematical operation to decompose complex spike patterns into a set of simple, structured elements. This creates a mathematical language that allows comparing spike patterns across trials, detecting sub-patterns, and making links to behaviour via a clear distance measure. We apply the method to dual Utah array recordings from macaque prefrontal cortex, where this technique reveals previously unseen structure that can predict both memory-guided decisions and errors in a virtual-reality working memory task. These results demonstrate that this technique provides a powerful new approach to understand structure in the spike times of neural populations, at a scale that will continue to grow more and more rapidly in upcoming years.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03804
Towards an Autonomous Test Driver: High-Performance Driver Modeling via Reinforcement Learning,['Robotics'],"['John Subosits', 'Jenna Lee', 'Shawn Manuel', 'Paul Tylkin', 'Avinash Balachandran']","Success in racing requires a unique combination of vehicle setup, understanding of the racetrack, and human expertise. Since building and testing many different vehicle configurations in the real world is prohibitively expensive, high-fidelity simulation is a critical part of racecar development. However, testing different vehicle configurations still requires expert human input in order to evaluate their performance on different racetracks. In this work, we present the first steps towards an autonomous test driver, trained using deep reinforcement learning, capable of evaluating changes in vehicle setup on racing performance while driving at the level of the best human drivers. In addition, the autonomous driver model can be tuned to exhibit more human-like behavioral patterns by incorporating imitation learning into the RL training process. This extension permits the possibility of driver-specific vehicle setup optimization.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03803
"Prediction of polarization vortices, charge modulation, flat bands, and moiré magnetism in twisted oxide bilayers",['Materials Science'],"['Naafis Ahnaf Shahed', 'Kartik Samanta', 'Mohamed Elekhtiar', 'Kai Huang', 'Chang-Beom Eom', 'Mark S. Rzchowski', 'Kirill D. Belashchenko', 'Evgeny Y. Tsymbal']","The recent surge of interest in moiré superlattices of twisted van der Waals compounds has spotlighted the emergence of unconventional superconductivity and novel electronic phases. However, the range of moiré phenomena can be dramatically expanded by incorporating complex oxide materials into twisted heterostructures. In this study, motivated by the recent breakthroughs in synthesis of free-standing oxide membranes, we explore the emergent structural and electronic properties of twisted oxide bilayers. We focus on the classic perovskite oxide, SrTiO3, and design SrTiO3 bilayers with a relative twist between the individual layers. Using density functional theory calculations, we predict the appearance of vortex-antivortex polarization patterns at the interface of the SrTiO3 bilayers driven by twist. We also predict charge modulation of the interfacial Ti ions induced by varying local coordination which follow the moiré pattern. Furthermore, we forecast the emergence of flat bands at large twist angles and the associated localized electronic states with moiré-periodic charge density, originating from the interlayer bonding effects resulting in the formation of dangling bonds. Finally, we predict that hole doping induces unconventional d0 magnetism in otherwise nonmagnetic SrTiO3, driven by the exchange splitting of the high-density O-p bands and producing the spin density with moiré periodicity. These results demonstrate a broad landscape of emergent phenomena which may occur in moiré-engineered oxide heterostructures showing far-reaching perspectives of these material systems for further fundamental studies and potential applications.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03798
Automated Multi-Label Annotation for Mental Health Illnesses Using Large Language Models,['Artificial Intelligence'],"['Abdelrahaman A. Hassan', 'Radwa J. Hanafy', 'Mohammed E. Fouda']","The growing prevalence and complexity of mental health disorders present significant challenges for accurate diagnosis and treatment, particularly in understanding the interplay between co-occurring conditions. Mental health disorders, such as depression and Anxiety, often co-occur, yet current datasets derived from social media posts typically focus on single-disorder labels, limiting their utility in comprehensive diagnostic analyses. This paper addresses this critical gap by proposing a novel methodology for cleaning, sampling, labeling, and combining data to create versatile multi-label datasets. Our approach introduces a synthetic labeling technique to transform single-label datasets into multi-label annotations, capturing the complexity of overlapping mental health conditions. To achieve this, two single-label datasets are first merged into a foundational multi-label dataset, enabling realistic analyses of co-occurring diagnoses. We then design and evaluate various prompting strategies for large language models (LLMs), ranging from single-label predictions to unrestricted prompts capable of detecting any present disorders. After rigorously assessing multiple LLMs and prompt configurations, the optimal combinations are identified and applied to label six additional single-disorder datasets from RMHD. The result is SPAADE-DR, a robust, multi-label dataset encompassing diverse mental health conditions. This research demonstrates the transformative potential of LLM-driven synthetic labeling in advancing mental health diagnostics from social media data, paving the way for more nuanced, data-driven insights into mental health care.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03796
Samudra: An AI Global Ocean Emulator for Climate,['Atmospheric and Oceanic Physics'],"['Surya Dheeshjith', 'Adam Subel', 'Alistair Adcroft', 'Julius Busecke', 'Carlos Fernandez-Granda', 'Shubham Gupta', 'Laure Zanna']","AI emulators for forecasting have emerged as powerful tools that can outperform conventional numerical predictions. The next frontier is to build emulators for long-term climate projections with robust skill across a wide range of spatiotemporal scales, a particularly important goal for the ocean. Our work builds a skillful global emulator of the ocean component of a state-of-the-art climate model. We emulate key ocean variables, sea surface height, horizontal velocities, temperature, and salinity, across their full depth. We use a modified ConvNeXt UNet architecture trained on multidepth levels of ocean data. We show that the ocean emulator - Samudra - which exhibits no drift relative to the truth, can reproduce the depth structure of ocean variables and their interannual variability. Samudra is stable for centuries and 150 times faster than the original ocean model. Samudra struggles to capture the correct magnitude of the forcing trends and simultaneously remains stable, requiring further work.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03795
Safe Adaptive Cruise Control Under Perception Uncertainty: A Deep Ensemble and Conformal Tube Model Predictive Control Approach,['Robotics'],"['Xiao Li', 'Anouck Girard', 'Ilya Kolmanovsky']","Autonomous driving heavily relies on perception systems to interpret the environment for decision-making. To enhance robustness in these safety critical applications, this paper considers a Deep Ensemble of Deep Neural Network regressors integrated with Conformal Prediction to predict and quantify uncertainties. In the Adaptive Cruise Control setting, the proposed method performs state and uncertainty estimation from RGB images, informing the downstream controller of the DNN perception uncertainties. An adaptive cruise controller using Conformal Tube Model Predictive Control is designed to ensure probabilistic safety. Evaluations with a high-fidelity simulator demonstrate the algorithm's effectiveness in speed tracking and safe distance maintaining, including in Out-Of-Distribution scenarios.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03792
Coordinate In and Value Out: Training Flow Transformers in Ambient Space,['Machine Learning'],"['Yuyang Wang', 'Anurag Ranjan', 'Josh Susskind', 'Miguel Angel Bautista']","Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on unstructured data like 3D point clouds. These models are commonly trained in two stages: first, a data compressor (i.e., a variational auto-encoder) is trained, and in a subsequent training stage a flow matching generative model is trained in the low-dimensional latent space of the data compressor. This two stage paradigm adds complexity to the overall training recipe and sets obstacles for unifying models across data domains, as specific data compressors are used for different data modalities. To this end, we introduce Ambient Space Flow Transformers (ASFT), a domain-agnostic approach to learn flow matching transformers in ambient space, sidestepping the requirement of training compressors and simplifying the training process. We introduce a conditionally independent point-wise training objective that enables ASFT to make predictions continuously in coordinate space. Our empirical results demonstrate that using general purpose transformer blocks, ASFT effectively handles different data modalities such as images and 3D point clouds, achieving strong performance in both domains and outperforming comparable approaches. ASFT is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03791
"Evidence for environmental effects in the $z\,{=}\,4.3$ protocluster core SPT2349$-$56",['Astrophysics of Galaxies'],"['Chayce Hughes', 'Ryley Hill', 'Scott Chapman', 'Manuel Aravena', 'Melanie Archipley', 'Veronica J. Dike', 'Anthony Gonzalez', 'Thomas R. Greve', 'Gayathri Gururajan', 'Chris Hayward', 'Kedar Phadke', 'Cassie Reuter', 'Justin Spilker', 'Nikolaus Sulzenauer', 'Joaquin D. Vieira', 'David Vizgan', 'George Wang', 'Axel Weiss', 'Dazhi Zhou']","We present ALMA observations of the [CI] 492 and 806$\,$GHz fine-structure lines in 25 dusty star-forming galaxies (DSFGs) at $z\,{=}\,4.3$ in the core of the SPT2349$-$56 protocluster. The protocluster galaxies exhibit a median $L^\prime_{[\text{CI}](2-1)}/L^\prime_{[\text{CI}](1-0)}$ ratio of 0.94 with an interquartile range of 0.81-1.24. These ratios are markedly different to those observed in DSFGs in the field (across a comparable redshift and 850$\,μ$m flux density range), where the median is 0.55 with an interquartile range of 0.50-0.76, and we show that this difference is driven by an excess of [CI](2-1) in the protocluster galaxies for a given 850$\,μ$m flux density. We estimate gas excitation temperatures of $T_{\rm ex}\,{=}\,59.1^{+8.1}_{-6.8}\,$K for our protocluster sample and $T_{\rm ex}\,{=}\,33.9^{+2.4}_{-2.2}\,$K for the field sample. Our main interpretation of this result is that the protocluster galaxies have had their cold gas driven to their cores via close-by interactions within the dense environment, leading to an overall increase in the average gas density and excitation temperature, and an elevated [CI](2-1) luminosity-to-far-infrared luminosity ratio.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03790
Validation of NSFsim as a Grad-Shafranov Equilibrium Solver at DIII-D,['Plasma Physics'],"['Randall Clark', 'Maxim Nurgaliev', 'Eduard Khayrutdinov', 'Georgy Subbotin', 'Anders Welander', 'Dmitri M. Orlov']","Plasma shape is a significant factor that must be considered for any Fusion Pilot Plant (FPP) as it has significant consequences for plasma stability and core confinement. A new simulator, NSFsim, has been developed based on a historically successful code, DINA, offering tools to simulate both transport and plasma shape. Specifically, NSFsim is a free boundary equilibrium and transport solver and has been configured to match the properties of the DIII-D tokamak. This paper is focused on validating the Grad-Shafranov (GS) solver of NSFsim by analyzing its ability to recreate the plasma shape, the poloidal flux distribution, and the measurements of the simulated diagnostic signals originating from flux loops and magnetic probes in DIII-D. Five different plasma shapes are simulated to show the robustness of NSFsim to different plasma conditions; these shapes are Lower Single Null (LSN), Upper Single Null (USN), Double Null (DN), Inner Wall Limited (IWL), and Negative Triangularity (NT). The NSFsim results are compared against real measured signals, magnetic profile fits from EFIT, and another plasma equilibrium simulator, GSevolve. EFIT reconstructions of shots are readily available at DIII-D, but GSevolve was manually ran by us to provide simulation data to compare against.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03786
A Galactic Scale Magnetized Wind Around a Normal Star-Forming Galaxy,['Astrophysics of Galaxies'],"['A. M. Matthews', 'W. D. Cotton', 'W. M. Peters', 'L. Marchetti', 'T. H. Jarrett', 'J. J. Condon', 'J. M. van der Hulst', 'M. Moloko']","Galaxy formation theory identifies superwinds as a key regulator of star formation rates, galaxy growth, and chemical enrichment. Thermal and radiation pressure are known to drive galactic-scale winds in dusty starbursting galaxies (e.g. M82), but modern numerical simulations have recently highlighted that cosmic-ray (CR) driven winds may be especially important in normal galaxies with modest star formation rate surface densities. However, CR-driven winds have yet to be conclusively observed -- leaving significant uncertainty in their detailed microphysics. We present MeerKAT radio continuum and HI spectral-line observations of one such normal galaxy, NGC 1532; a nearby ($D\sim15\,\mathrm{Mpc}$) and edge-on ($i \gtrsim 80^{\circ}$) spiral galaxy tidally interacting with its smaller elliptical companion, NGC 1531. We find magnetized, highly-ordered radio continuum loops extending $\sim10$ kpc above and below the disk; visibly connecting discrete star-forming regions in the disk with the nucleus. The deep MeerKAT HI observations place an upper limit on the column density of neutral gas coincident with the outflow to $N_\mathrm{HI} \lesssim 3 \times 10^{19}\,\mathrm{cm}^{-2}$. Unlike previously observed outflows -- for which ejected gas and dust can be traced across multiple wavelengths -- the loops in NGC 1532 show no detectable signs of dust or gas coincident with the radio emission far from the disk. We explore multiple possible mechanisms for driving this magnetic wind and favor an explanation where cosmic-ray pressure plays a significant role in launching these outflows.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03785
The broader spectrum of in-context learning,['Computation and Language'],"['Andrew Kyle Lampinen', 'Stephanie C. Y. Chan', 'Aaditya K. Singh', 'Murray Shanahan']","The ability of language models to learn a task from a few examples in context has generated substantial interest. Here, we provide a perspective that situates this type of supervised few-shot learning within a much broader spectrum of meta-learned in-context learning. Indeed, we suggest that any distribution of sequences in which context non-trivially decreases loss on subsequent predictions can be interpreted as eliciting a kind of in-context learning. We suggest that this perspective helps to unify the broad set of in-context abilities that language models exhibit $\unicode{x2014}$ such as adapting to tasks from instructions or role play, or extrapolating time series. This perspective also sheds light on potential roots of in-context learning in lower-level processing of linguistic dependencies (e.g. coreference or parallel structures). Finally, taking this perspective highlights the importance of generalization, which we suggest can be studied along several dimensions: not only the ability to learn something novel, but also flexibility in learning from different presentations, and in applying what is learned. We discuss broader connections to past literature in meta-learning and goal-conditioned agents, and other perspectives on learning and adaptation. We close by suggesting that research on in-context learning should consider this broader spectrum of in-context capabilities and types of generalization.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03782
Signatures of Floquet Engineering in the proximal Kitaev Quantum Spin Liquid H$_3$LiIr$_2$O$_6$ by tr-RIXS,['Strongly Correlated Electrons'],"['Jungho Kim', 'Tae-Kyu Choi', 'Edward Mercer', 'Liam T. Schmidt', 'Jaeku Park', 'Sang-Youn Park', 'Dogeun Jang', 'Seo Hyoung Chang', 'Ayman Said', 'Sae Hwan Chun', 'Kyeong Jun Lee', 'Sang Wook Lee', 'Hyunjeong Jeong', 'Hyeonhui Jeong', 'Chanhyeon Lee', 'Kwang-Yong Choi', 'Faranak Bahrami', 'Fazel Tafti', 'Martin Claassen', 'Alberto de la Torre']","We present the first circularly polarized Floquet engineering time-resolved Resonant Inelastic X-ray Scattering (tr-RIXS) experiment in H$_3$LiIr$_2$O$_6$, an iridium-based Kitaev system. Our calculations and experimental results are consistent with the modification of the low energy magnetic excitations in H$_3$LiIr$_2$O$_6$ only during illumination by the laser pulse, consistent with the Floquet engineering of the exchange interactions. However, the penetration length mismatch between the X-ray probe and laser pump and the intrinsic complexity of Kitaev magnets prevented us from unequivocally extracting towards which ground H$_3$LiIr$_2$O$_6$ was driven. We outline possible solutions to these challenges for Floquet stabilization and observation of the Kitaev Quantum Spin Liquid limit by RIXS.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03777
Diffusion in Zero-Shot Learning for Environmental Audio,['Sound'],"['Ysobel Sims', 'Stephan Chalup', 'Alexandre Mendes']","Zero-shot learning enables models to generalize to unseen classes by leveraging semantic information, bridging the gap between training and testing sets with non-overlapping classes. While much research has focused on zero-shot learning in computer vision, the application of these methods to environmental audio remains underexplored, with poor performance in existing studies. Generative methods, which have demonstrated success in computer vision, are notably absent from environmental audio zero-shot learning, where classification-based approaches dominate.
  To address this gap, this work investigates generative methods for zero-shot learning in environmental audio. Two successful generative models from computer vision are adapted: a cross-aligned and distribution-aligned variational autoencoder (CADA-VAE) and a leveraging invariant side generative adversarial network (LisGAN). Additionally, a novel diffusion model conditioned on class auxiliary data is introduced. The diffusion model generates synthetic data for unseen classes, which is combined with seen-class data to train a classifier.
  Experiments are conducted on two environmental audio datasets, ESC-50 and FSC22. Results show that the diffusion model significantly outperforms all baseline methods, achieving more than 25% higher accuracy on the ESC-50 test partition.
  This work establishes the diffusion model as a promising generative approach for zero-shot learning and introduces the first benchmark of generative methods for environmental audio zero-shot learning, providing a foundation for future research in the field.
  Code is provided at https://github.com/ysims/ZeroDiffusion for the novel ZeroDiffusion method.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03771
Learning Networks from Wide-Sense Stationary Stochastic Processes,['Machine Learning'],"['Anirudh Rayas', 'Jiajun Cheng', 'Rajasekhar Anguluri', 'Deepjyoti Deka', 'Gautam Dasarathy']","Complex networked systems driven by latent inputs are common in fields like neuroscience, finance, and engineering. A key inference problem here is to learn edge connectivity from node outputs (potentials). We focus on systems governed by steady-state linear conservation laws: $X_t = {L^{\ast}}Y_{t}$, where $X_t, Y_t \in \mathbb{R}^p$ denote inputs and potentials, respectively, and the sparsity pattern of the $p \times p$ Laplacian $L^{\ast}$ encodes the edge structure. Assuming $X_t$ to be a wide-sense stationary stochastic process with a known spectral density matrix, we learn the support of $L^{\ast}$ from temporally correlated samples of $Y_t$ via an $\ell_1$-regularized Whittle's maximum likelihood estimator (MLE). The regularization is particularly useful for learning large-scale networks in the high-dimensional setting where the network size $p$ significantly exceeds the number of samples $n$.
  We show that the MLE problem is strictly convex, admitting a unique solution. Under a novel mutual incoherence condition and certain sufficient conditions on $(n, p, d)$, we show that the ML estimate recovers the sparsity pattern of $L^\ast$ with high probability, where $d$ is the maximum degree of the graph underlying $L^{\ast}$. We provide recovery guarantees for $L^\ast$ in element-wise maximum, Frobenius, and operator norms. Finally, we complement our theoretical results with several simulation studies on synthetic and benchmark datasets, including engineered systems (power and water networks), and real-world datasets from neural systems (such as the human brain).△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03768
Thermodynamic Fidelity of Generative Models for Ising System,['Statistical Mechanics'],"['Brian H. Lee', 'Kat Nykiel', 'Ava E. Hallberg', 'Brice Rider', 'Alejandro Strachan']","Machine learning has become a central technique for modeling in science and engineering, either complementing or as surrogates to physics-based models. Significant efforts have recently been devoted to models capable of predicting field quantities but the limitations of current state-of-the-art models in describing complex physics are not well understood. We characterize the ability of generative diffusion models and generative adversarial networks (GAN) to describe the Ising model. We find diffusion models trained using equilibrium configurations obtained using Metropolis Monte Carlo for a range of temperatures around the critical temperature can capture average thermodynamic variables across the phase transformation and extrapolate to higher and lower temperatures. The model also captures the overall trends of physical properties associated with fluctuations (specific heat and susceptibility) except at the non-ergodic low temperatures and non-trivial scale-free correlations at the critical temperature, albeit with some difference in the critical exponent compared to Monte Carlo simulations. GANs perform more poorly on thermodynamic properties and are susceptible to mode-collapse without careful training. This investigation highlights the potential and limitations of generative models in capturing the complex phenomena associated with certain physical systems.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03764
Synthetic graphs for link prediction benchmarking,['Social and Information Networks'],"['Alexey Vlaskin', 'Eduardo G. Altmann']","Predicting missing links in complex networks requires algorithms that are able to explore statistical regularities in the existing data. Here we investigate the interplay between algorithm efficiency and network structures through the introduction of suitably-designed synthetic graphs. We propose a family of random graphs that incorporates both micro-scale motifs and meso-scale communities, two ubiquitous structures in complex networks. A key contribution is the derivation of theoretical upper bounds for link prediction performance in our synthetic graphs, allowing us to estimate the predictability of the task and obtain an improved assessment of the performance of any method. Our results on the performance of classical methods (e.g., Stochastic Block Models, Node2Vec,GraphSage) show that the performance of all methods correlate with the theoretical predictability, that no single method is universally superior, and that each of the methods exploit different characteristics known to exist in large classes of networks. Our findings underline the need for careful consideration of graph structure when selecting a link prediction method and emphasize the value of comparing performance against synthetic benchmarks. We provide open-source code for generating these synthetic graphs, enabling further research on link prediction methods.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03757
Multi-view Image Diffusion via Coordinate Noise and Fourier Attention,['Computer Vision and Pattern Recognition'],"['Justin Theiss', 'Norman Müller', 'Daeil Kim', 'Aayush Prakash']","Recently, text-to-image generation with diffusion models has made significant advancements in both higher fidelity and generalization capabilities compared to previous baselines. However, generating holistic multi-view consistent images from prompts still remains an important and challenging task. To address this challenge, we propose a diffusion process that attends to time-dependent spatial frequencies of features with a novel attention mechanism as well as novel noise initialization technique and cross-attention loss. This Fourier-based attention block focuses on features from non-overlapping regions of the generated scene in order to better align the global appearance. Our noise initialization technique incorporates shared noise and low spatial frequency information derived from pixel coordinates and depth maps to induce noise correlations across views. The cross-attention loss further aligns features sharing the same prompt across the scene. Our technique improves SOTA on several quantitative metrics with qualitatively better results when compared to other state-of-the-art approaches for multi-view consistency.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03756
Impact of the exciton fine structure splitting and measurement orientations on the robustness of cryptographic keys generated via the quantum protocol E91,['Quantum Physics'],"['Adrián F. Hernández-Borda', 'María P. Rojas-Sepúlveda', 'Hanz Y. Ramírez-Gómez']","Photons and optical circuits are among the most promising platforms for implementation of quantum technologies, because of its potential use in quantum computing, quantum cryptography and long-distance quantum communication. One of the main requirements to achieve reliable quantum communications are on-demand sources of highly entangled photon pairs, and semiconductor quantum dots have emerged as prominent candidates to satisfy the necessary conditions of brightness and entanglement fidelity. However, in most cases the biexciton-exciton-vacuum cascade produces a pair of maximally polarization-entangled photons with a dephasing, due to the exciton fine structure splitting. This work focuses on the performance of the E91 quantum key distribution protocol under the variation of two elements: first, the phase in the input state when the protocol is implemented using entangled photons generated via the radiative cascade, and second, the relative directions of the polarization analyzers. We obtain analytical expressions for the protocol's secret key rate and Bell's parameter as functions of the studied phase and angles. Then, we validate those expressions by means of a quantum computational implementation with the IBM's API Qiskit. Our results show that the performance of the quantum transmission is highly impacted by the product between the exciton lifetime and the quantum dot's fine structure splitting and that such an impact may be modulated through the orientation of the polarizers. These findings provide important insight for the scalable implementation of quantum key distribution protocols with quantum dots as entanglement sources.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03753
A novel approach to differential expression analysis of co-occurrence networks for small-sampled microbiome data,['Quantitative Methods'],"['Nandini Gadhia', 'Michalis Smyrnakis', 'Po-Yu Liu', 'Damer Blake', 'Melanie Hay', 'Anh Nguyen', 'Dominic Richards', 'Dong Xia', 'Ritesh Krishna']","Graph-based machine learning methods are useful tools in the identification and prediction of variation in genetic data. In particular, the comprehension of phenotypic effects at the cellular level is an accelerating research area in pharmacogenomics. In this article, a novel graph theoretic approach is proposed to infer a co-occurrence network from 16S microbiome data. The approach is specialised to handle datasets containing a small number of samples. Small datasets exacerbate the significant challenges faced by biological data, which exhibit properties such as sparsity, compositionality, and complexity of interactions. Methodologies are also proposed to enrich and statistically filter the inferred networks. The utility of the proposed method lies in that it extracts an informative network from small sampled data that is not only feature-rich, but also biologically meaningful and statistically significant. Although specialised for small data sets, which are abundant, it can be generally applied to any small-sampled dataset, and can also be extended to integrate multi-omics data. The proposed methodology is tested on a data set of chickens vaccinated against and challenged by the protozoan parasite Eimeria tenella. The raw genetic reads are processed, and networks inferred to describe the ecosystems of the chicken intestines under three different stages of disease progression. Analysis of the expression of network features derive biologically intuitive conclusions from purely statistical methods. For example, there is a clear evolution in the distribution of node features in line with the progression of the disease. The distributions also reveal clusters of species interacting mutualistically and parasitically, as expected. Moreover, a specific sub-network is found to persist through all experimental conditions, representative of a persistent microbiome.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03744
A Hybrid Deep-Learning Model for El Niño Southern Oscillation in the Low-Data Regime,['Machine Learning'],"['Jakob Schlör', 'Matthew Newman', 'Jannik Thuemmel', 'Antonietta Capotondi', 'Bedartha Goswami']","While deep-learning models have demonstrated skillful El Niño Southern Oscillation (ENSO) forecasts up to one year in advance, they are predominantly trained on climate model simulations that provide thousands of years of training data at the expense of introducing climate model biases. Simpler Linear Inverse Models (LIMs) trained on the much shorter observational record also make skillful ENSO predictions but do not capture predictable nonlinear processes. This motivates a hybrid approach, combining the LIMs modest data needs with a deep-learning non-Markovian correction of the LIM. For O(100 yr) datasets, our resulting Hybrid model is more skillful than the LIM while also exceeding the skill of a full deep-learning model. Additionally, while the most predictable ENSO events are still identified in advance by the LIM, they are better predicted by the Hybrid model, especially in the western tropical Pacific for leads beyond about 9 months, by capturing the subsequent asymmetric (warm versus cold phases) evolution of ENSO.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03743
Impact of local structure on melt dynamics in Cu-Ti alloys: Insights from ab-initio molecular dynamics simulations,['Materials Science'],"['Lucas P. Kreuzer', 'Fan Yang', 'Andreas Mayer', 'Noel Jakse']","First-principle based molecular-dynamics simulations have been performed for binary Cu$_x$Ti$_{1-x}$ (x = 0.31, 0.50, and 0.76) alloys to investigate the relationship between local structure and dynamical properties in the liquid and undercooled melt. The undercooled melts show a pronounced short-range order, majorly a five-fold symmetry (FFS) around the Cu atoms, which competes with bcc ordering. This complex SRO is also reflected in the partial coordination numbers, where mainly a Z12 coordination is present around Cu, which corresponds to an icosahedral ordering. Higher coordination numbers were obtained for Ti compatible with Frank-Kasper polyhedra. The increasing Frank-Kasper polyhedra coordination scenario around Ti impacts on the interatomic distances of Ti atoms, which increase with increasing Ti content. The Cu$_{50}$Ti$_{50}$ composition exhibits the highest FFS ordering and amount of Frank-Kasper polyhedra, which explains the slowest melt dynamics, found experimentally and in simulations for this composition. Thus, our results suggest that the high undercooling degree and glass-forming ability of binary CuTi alloys, originates from the high complexity of the local structure rather than due to the preferred formation of Cu-Ti pairs, as Cu-Ti interactions were found to be weak.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03741
A Global Perspective with Updated Constraints on the Ultra-hot Jupiter WASP-19b: Atmospheric Properties and Stellar Activity,['Earth and Planetary Astrophysics'],"['Abigail A. Tumborang', 'Jessica J. Spake', 'Heather A. Knutson', 'Megan Weiner Mansfield', 'Kimberly Paragas', 'Billy Edwards', 'Tiffany Kataria', 'Thomas M. Evans-Soma', 'Nikole K. Lewis', 'Gilda E. Ballester']","We present a detailed reanalysis of the atmospheric properties of WASP-19b, an ultra-hot Jupiter (1.14 M Jup, 1.41 R Jup) orbiting an active Sun-like star every 0.79 day. We reanalyze a transit and secondary eclipse of WASP-19b observed by the Hubble Space Telescope's Wide Field Camera 3 spectrograph (1.1 - 1.7 microns). When combined with Spitzer photometry at longer wavelengths, our analyses indicate the presence of water absorption features in both the planet's transmission and emission spectra, consistent with results from previously published studies. We jointly fit WASP-19b's dayside emission and transmission spectra with a retrieval model in order to constrain its atmospheric composition, and explore the effect of stellar activity on its transmission spectrum in greater depth. We also compare our dayside emission spectrum to predictions from a general circulation model, and conclude that magnetic drag appears to be relatively unimportant in shaping WASP-19b's atmospheric circulation. Lastly, we compare the size of WASP-19b's dayside water absorption feature to the population of hot Jupiters with similar measurements, and show that it is located in the transitional irradiation regime where temperature inversions first begin to emerge. As in previous studies, we find that the current observations provide relatively weak constraints on this planet's atmospheric properties. These constraints could be significantly improved by the addition of spectroscopically resolved observations at longer wavelengths with JWST/NIRSpec PRISM.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03739
Characterising higher-order phase correlations in gain-switched laser sources with application to quantum key distribution,['Quantum Physics'],"['Alessandro Marcomini', 'Guillermo Currás-Lorenzo', 'Davide Rusca', 'Angel Valle', 'Kiyoshi Tamaki', 'Marcos Curty']","Multi-photon emissions in laser sources represent a serious threat for the security of quantum key distribution (QKD). While the decoy-state technique allows to solve this problem, it requires uniform phase randomisation of the emitted pulses. However, gain-switched lasers operating at high repetition rates do not fully satisfy this requirement, as residual photons in the laser cavity introduce correlations between the phases of consecutive pulses. Here, we introduce experimental schemes to characterise the phase probability distribution of the emitted pulses, and demonstrate that an optimisation task over interferometric measures suffices in determining the impact of arbitrary order correlations, which ultimately establishes the security level of the implementation according to recent security proofs. We expect that our findings may find usages beyond QKD as well.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03738
Utilizing Machine Learning Models to Predict Acute Kidney Injury in Septic Patients from MIMIC-III Database,['Machine Learning'],"['Aleyeh Roknaldin', 'Zehao Zhang', 'Jiayuan Xu', 'Kamiar Alaei', 'Maryam Pishgar']","Sepsis is a severe condition that causes the body to respond incorrectly to an infection. This reaction can subsequently cause organ failure, a major one being acute kidney injury (AKI). For septic patients, approximately 50% develop AKI, with a mortality rate above 40%. Creating models that can accurately predict AKI based on specific qualities of septic patients is crucial for early detection and intervention. Using medical data from septic patients during intensive care unit (ICU) admission from the Medical Information Mart for Intensive Care 3 (MIMIC-III) database, we extracted 3301 patients with sepsis, with 73% of patients developing AKI. The data was randomly divided into a training set (n = 1980, 40%), a test set (n = 661, 10%), and a validation set (n = 660, 50%). The proposed model was logistic regression, and it was compared against five baseline models: XGBoost, K Nearest Neighbors (KNN), Support Vector Machines (SVM), Random Forest (RF), and LightGBM. Area Under the Curve (AUC), Accuracy, F1-Score, and Recall were calculated for each model. After analysis, we were able to select 23 features to include in our model, the top features being urine output, maximum bilirubin, minimum bilirubin, weight, maximum blood urea nitrogen, and minimum estimated glomerular filtration rate. The logistic regression model performed the best, achieving an AUC score of 0.887 (95% CI: [0.861-0.915]), an accuracy of 0.817, an F1 score of 0.866, a recall score of 0.827, and a Brier score of 0.13. Compared to the best existing literature in this field, our model achieved an 8.57% improvement in AUC while using 13 fewer variables, showcasing its effectiveness in determining AKI in septic patients. While the features selected for predicting AKI in septic patients are similar to previous literature, the top features that influenced our model's performance differ.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03737
Domain-specific Question Answering with Hybrid Search,['Computation and Language'],"['Dewang Sultania', 'Zhaoyu Lu', 'Twisha Naik', 'Franck Dernoncourt', 'David Seunghyun Yoon', 'Sanat Sharma', 'Trung Bui', 'Ashok Gupta', 'Tushar Vatsa', 'Suhas Suresha', 'Ishita Verma', 'Vibha Belavadi', 'Cheng Chen', 'Michael Friedrich']","Domain specific question answering is an evolving field that requires specialized solutions to address unique challenges. In this paper, we show that a hybrid approach combining a fine-tuned dense retriever with keyword based sparse search methods significantly enhances performance. Our system leverages a linear combination of relevance signals, including cosine similarity from dense retrieval, BM25 scores, and URL host matching, each with tunable boost parameters. Experimental results indicate that this hybrid method outperforms our single-retriever system, achieving improved accuracy while maintaining robust contextual grounding. These findings suggest that integrating multiple retrieval methodologies with weighted scoring effectively addresses the complexities of domain specific question answering in enterprise settings.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03736
A Modified Bisecting K-Means for Approximating Transfer Operators: Application to the Lorenz Equations,['Computational Physics'],"['Andre N. Souza', 'Simone Silvestri']","We investigate the convergence behavior of the extended dynamic mode decomposition for constructing a discretization of the continuity equation associated with the Lorenz equations using a nonlinear dictionary of over 1,000,000 terms. The primary objective is to analyze the resulting operator by varying the number of terms in the dictionary and the timescale. We examine what happens when the number of terms of the nonlinear dictionary is varied with respect to its ability to represent the invariant measure, Koopman eigenfunctions, and temporal autocorrelations. The dictionary comprises piecewise constant functions through a modified bisecting k-means algorithm and can efficiently scale to higher-dimensional systems.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03734
A simple model to investigate jet quenching and correlated errors for centrality-dependent nuclear-modification factors in relativistic heavy-ion collisions,['Nuclear Theory'],"['Ron A Soltz', 'Dhanush A Hangal', 'Aaron Angerami']","We apply Bayesian techniques to compare a simple, empirical model for jet-quenching in heavy-ion collisions to centrality-dependent jet-$R_{AA}$ measured by ATLAS for Pb+Pb collisions at $\sqrt{s_{NN}}=5.02$~TeV. We find that the $R_{AA}$ values for central collisions are adequately described with a model for the mean $p_T$-dependent jet energy-loss using only 2-parameters. This model is extended by incorporating 2D initial geometry information from TRENTO and compared to centrality-dependent $R_{AA}$ values. We find that the results are sensitive to value of the jet-quenching formation time, $τ_f$, and that the optimal value of $τ_f$ varies with the assumed path-length dependence of the energy-loss. We construct a covariance error matrix for the data from the $p_T$ dependent contributions to the ATLAS systematic errors and perform Bayesian calibrations for several different assumptions for the systematic error correlations. We show that most-probable functions and $χ^2$ values are sensitive to assumptions made when fitting to correlated errors.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03724
Bayesian Perspective for Orientation Estimation in Cryo-EM and Cryo-ET,['Applications'],"['Sheng Xu', 'Amnon Balanov', 'Tamir Bendory']","Accurate orientation estimation is a crucial component of 3D molecular structure reconstruction, both in single-particle cryo-electron microscopy (cryo-EM) and in the increasingly popular field of cryo-electron tomography (cryo-ET). The dominant method, which involves searching for an orientation with maximum cross-correlation relative to given templates, falls short, particularly in low signal-to-noise environments. In this work, we propose a Bayesian framework to develop a more accurate and flexible orientation estimation approach, with the minimum mean square error (MMSE) estimator as a key example. This method effectively accommodates varying structural conformations and arbitrary rotational distributions. Through simulations, we demonstrate that our estimator consistently outperforms the cross-correlation-based method, especially in challenging conditions with low signal-to-noise ratios, and offer a theoretical framework to support these improvements. We further show that integrating our estimator into the iterative refinement in the 3D reconstruction pipeline markedly enhances overall accuracy, revealing substantial benefits across the algorithmic workflow. Finally, we show empirically that the proposed Bayesian approach enhances robustness against the ``Einstein from Noise'' phenomenon, reducing model bias and improving reconstruction reliability. These findings indicate that the proposed Bayesian framework could substantially advance cryo-EM and cryo-ET by enhancing the accuracy, robustness, and reliability of 3D molecular structure reconstruction, thereby facilitating deeper insights into complex biological systems.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03723
Optimal probabilistic feature shifts for reclassification in tree ensembles,['Optimization and Control'],"['Váctor Blanco', 'Alberto Japón', 'Justo Puerto', 'Peter Zhang']","In this paper we provide a novel mathematical optimization based methodology to perturb the features of a given observation to be re-classified, by a tree ensemble classification rule, to a certain desired class. The method is based on these facts: the most viable changes for an observation to reach the desired class do not always coincide with the closest distance point (in the feature space) of the target class; individuals put effort on a few number of features to reach the desired class; and each individual is endowed with a probability to change each of its features to a given value, which determines the overall probability of changing to the target class. Putting all together, we provide different methods to find the features where the individuals must exert effort to maximize the probability to reach the target class. Our method also allows us to rank the most important features in the tree-ensemble. The proposed methodology is tested on a real dataset, validating the proposal.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03722
PathletRL++: Optimizing Trajectory Pathlet Extraction and Dictionary Formation via Reinforcement Learning,['Machine Learning'],"['Gian Alix', 'Arian Haghparast', 'Manos Papagelis']","Advances in tracking technologies have spurred the rapid growth of large-scale trajectory data. Building a compact collection of pathlets, referred to as a trajectory pathlet dictionary, is essential for supporting mobility-related applications. Existing methods typically adopt a top-down approach, generating numerous candidate pathlets and selecting a subset, leading to high memory usage and redundant storage from overlapping pathlets. To overcome these limitations, we propose a bottom-up strategy that incrementally merges basic pathlets to build the dictionary, reducing memory requirements by up to 24,000 times compared to baseline methods. The approach begins with unit-length pathlets and iteratively merges them while optimizing utility, which is defined using newly introduced metrics of trajectory loss and representability. We develop a deep reinforcement learning framework, PathletRL, which utilizes Deep Q-Networks (DQN) to approximate the utility function, resulting in a compact and efficient pathlet dictionary. Experiments on both synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art techniques, reducing the size of the constructed dictionary by up to 65.8%. Additionally, our results show that only half of the dictionary pathlets are needed to reconstruct 85% of the original trajectory data. Building on PathletRL, we introduce PathletRL++, which extends the original model by incorporating a richer state representation and an improved reward function to optimize decision-making during pathlet merging. These enhancements enable the agent to gain a more nuanced understanding of the environment, leading to higher-quality pathlet dictionaries. PathletRL++ achieves even greater dictionary size reduction, surpassing the performance of PathletRL, while maintaining high trajectory representability.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03715
Relativistic dissipative fluids in the trace-fixed particle frame: strongly hyperbolic quasi-linear first-order evolution equations,['General Relativity and Quantum Cosmology'],"['J. Félix Salazar', 'Ana Laura García-Perciante', 'Olivier Sarbach']","In this paper we derive a new first-order theory of dissipative fluids by adopting the trace-fixed particle frame. Whereas in a companion paper we show that this theory is hyperbolic, causal and stable at global equilibrium states, here we prove that the full nonlinear system of equations can be cast into a first-order quasilinear system which is strongly hyperbolic. By rewriting the system in first-order form, auxiliary constraints are introduced. However, we show that these constraints propagate, and thus our theory leads to a well-posed Cauchy problem.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03713
"Relativistic dissipative fluids in the trace-fixed particle frame: hyperbolicity, causality and stability",['General Relativity and Quantum Cosmology'],"['J. Félix Salazar', 'Ana Laura García-Perciante', 'Olivier Sarbach']","We propose a first-order theory of dissipative fluids in the trace-fixed particle frame, which is similar to Eckart's frame except that the temperature is determined by fixing the trace of the stress-energy tensor. Our theory is hyperbolic and causal provided a single inequality holds. For low wave numbers, the expected damped modes in the shear, acoustic, and heat diffusion channels are recovered. Stability of global equilibria with respect to all wave numbers is also analyzed. The conditions for hyperbolicity, causality and stability are satisfied for a simple gas of hard spheres or disks.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03712
CIKAN: Constraint Informed Kolmogorov-Arnold Networks for Autonomous Spacecraft Rendezvous using Time Shift Governor,['Systems and Control'],"['Taehyeun Kim', 'Anouck Girard', 'Ilya Kolmanovsky']","The paper considers a Constrained-Informed Neural Network (CINN) approximation for the Time Shift Governor (TSG), which is an add-on scheme to the nominal closed-loop system used to enforce constraints by time-shifting the reference trajectory in spacecraft rendezvous applications. We incorporate Kolmogorov-Arnold Networks (KANs), an emerging architecture in the AI community, as a fundamental component of CINN and propose a Constrained-Informed Kolmogorov-Arnold Network (CIKAN)-based approximation for TSG. We demonstrate the effectiveness of the CIKAN-based TSG through simulations of constrained spacecraft rendezvous missions on highly elliptic orbits and present comparisons between CIKANs, MLP-based CINNs, and the conventional TSG.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03710
Rectified Control Barrier Functions for High-Order Safety Constraints,['Systems and Control'],"['Pio Ong', 'Max H. Cohen', 'Tamas G. Molnar', 'Aaron D. Ames']","This paper presents a novel approach for synthesizing control barrier functions (CBFs) from high relative degree safety constraints: Rectified CBFs (ReCBFs). We begin by discussing the limitations of existing High-Order CBF approaches and how these can be overcome by incorporating an activation function into the CBF construction. We then provide a comparative analysis of our approach with related methods, such as CBF backstepping. Our results are presented first for safety constraints with relative degree two, then for mixed-input relative degree constraints, and finally for higher relative degrees. The theoretical developments are illustrated through simple running examples and an aircraft control problem.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03708
Stellar-gas kinematic misalignments in EAGLE: lifetimes and longevity of misaligned galaxies,['Astrophysics of Galaxies'],"['Maximilian K. Baker', 'Timothy A. Davis', 'Freeke van de Voort', 'Ilaria Ruffa']","The dominant processes by which galaxies replenish their cold gas reservoirs remain disputed, especially in massive galaxies. Stellar-gas kinematic misalignments offer an opportunity to study these replenishment processes. However, observed distributions of these misalignments conflict with current models of gas replenishment in early-type galaxies (ETGs), with longer relaxation timescales suggested as a possible solution. We use the EAGLE simulation to explore the relaxation of unstable misaligned gas in galaxies with masses of $M_{*}\geqslant \mathrm{10^{9.5}}$ M$_\odot$ between $0<z<1$. We extract misalignments from formation to relaxation providing a sample of $\sim3200$ relaxations. We find relaxation timescales tend to be short-duration, with median lifetimes of $\sim0.5$ Gyr, though with a notable population of unstable misalignments lasting $\gtrsim1$ Gyr. Relaxation time distributions show a log-linear relationship, with $\approx20\%$ of unstable misalignments persisting for $\gtrsim3$ torquing times. Long-lived unstable misalignments are predominantly found in galaxies with higher stellar masses, lower star-forming gas fractions, higher ongoing gas inflow, and which reside in the centres of dense environments. Mergers only cause $\sim17\%$ of unstable misalignments in EAGLE. We conclude that, at least in EAGLE, unstable kinematic misalignments are not predominantly driven by gas-rich minor mergers. Additionally, processes that significantly extend relaxation times are not dominant in the galaxy population. Instead, we see a diverse formation pathway for misalignments such as through hot halo cooling.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03707
Long-lived population inversion in resonantly driven excitonic antiferromagnet,['Strongly Correlated Electrons'],"['Jacob A. Warshauer', 'Huyongqing Chen', 'Daniel Alejandro Bustamante Lopez', 'Qishuo Tan', 'Jing Tang', 'Xi Ling', 'Wanzheng Hu']","Van der Waals magnets are an emerging material family for investigating light-matter interactions and spin-correlated excitations. Here, we report the discovery of a photo-induced state with a lifetime of 17 ps in the van der Waals antiferromagnet NiPS$_3$, which appears exclusively with resonant pumping at 1.476 eV in the antiferromagnetic state. The long-lived state comes with a negative photoconductivity, a characteristic optical response of population inversion. Our findings demonstrate a promising pathway to potentially achieve long-lived lasing at terahertz frequencies in reduced dimensions.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03705
Density of states and differential entropy in the Dirac materials in crossed magnetic and in-plane electric fields,['Mesoscale and Nanoscale Physics'],"['Andrii A. Chaika', 'Yelizaveta Kulynych', 'D. O. Oriekhov', 'Sergei G. Sharapov']","The density of states and differential entropy per particle are analyzed for Dirac-like electrons in graphene subjected to a perpendicular magnetic field and an in-plane electric field. For comparison, the derived density of states is contrasted with the well-known case of nonrelativistic electrons in crossed magnetic and electric fields. The study considers ballistic electrons and also includes the effect of small impurity scattering. In the latter case, the limit of zero magnetic field and the so-called collapse of Landau levels in graphene are examined analytically. By comparing the results with numerical calculations on graphene ribbons, we demonstrate that the Landau state counting procedure must be modified for Dirac-like electrons, leading to a field-dependent Landau level degeneracy factor. Additionally, it is shown that peaks in the differential entropy arise from the dispersionless surface mode localized at the zigzag edges of the ribbon.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03703
Interpretable Hierarchical Attention Network for Medical Condition Identification,['Machine Learning'],"['Dongping Fang', 'Lian Duan', 'Xiaojing Yuan', 'Allyn Klunder', 'Kevin Tan', 'Suiting Cao', 'Yeqing Ji', 'Mike Xu']","Accurate prediction of medical conditions with straight past clinical evidence is a long-sought topic in the medical management and health insurance field. Although great progress has been made with machine learning algorithms, the medical community is still skeptical about the model accuracy and interpretability. This paper presents an innovative hierarchical attention deep learning model to achieve better prediction and clear interpretability that can be easily understood by medical professionals.
  This paper developed an Interpretable Hierarchical Attention Network (IHAN). IHAN uses a hierarchical attention structure that matches naturally with the medical history data structure and reflects patients encounter (date of service) sequence. The model attention structure consists of 3 levels: (1) attention on the medical code types (diagnosis codes, procedure codes, lab test results, and prescription drugs), (2) attention on the sequential medical encounters within a type, (3) attention on the individual medical codes within an encounter and type.
  This model is applied to predict the occurrence of stage 3 chronic kidney disease (CKD), using three years medical history of Medicare Advantage (MA) members from an American nationwide health insurance company. The model takes members medical events, both claims and Electronic Medical Records (EMR) data, as input, makes a prediction of stage 3 CKD and calculates contribution from individual events to the predicted outcome.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03701
Non-BCS behavior of the pairing susceptibility near the onset of superconductivity in a quantum-critical metal,['Superconductivity'],"['Artem Abanov', 'Shang-Shun Zhang', 'Andrey Chubukov']","We analyze the dynamical pairing susceptibility $χ_{pp} (ω_m)$ at $T=0$ in a quantum-critical metal, where superconductivity emerges out of a non-Fermi liquid ground state once the pairing interaction exceeds a certain threshold. We obtain $χ_{pp} (ω_m)$ as the ratio of the fully dressed dynamical pairing vertex $Φ(ω_m)$ and the bare $Φ_0 (ω_m)$ (both infinitesimally small). For superconductivity out of a Fermi liquid, the pairing susceptibility is positive above $T_c$, diverges at $T_c$, and becomes negative below it. For superconductivity out of a non-Fermi liquid, the behavior of $χ_{pp} (ω_m)$ is different in two aspects: (i) it diverges at the onset of pairing at $T=0$ only for a certain subclass of bare $Φ_0 (ω_m)$ and remains non-singular for other $Φ_0 (ω_m)$, and (ii) below the instability, it becomes a non-unique function of a continuous parameter $φ$ for an arbitrary $Φ_0 (ω_m)$. The susceptibility is negative in some range of $φ$ and diverges at the boundary of this range. We argue that this behavior of the susceptibility reflects a multi-critical nature of a superconducting transition in a quantum-critical metal when immediately below the instability an infinite number of superconducting states emerges simultaneously with different amplitudes of the order parameter down to an infinitesimally small one.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03698
Bidiagonal matrix factorisations related to multiple orthogonal polynomials,['Classical Analysis and ODEs'],"['Amílcar Branquinho', 'Juan E. F. Díaz', 'Ana Foulquié-Moreno', 'Hélder Lima', 'Manuel Mañas']","In this work, we develop two methods to obtain a factorisation as a product of bidiagonal matrices for the Hessenberg recurrence matrix of a system of multiple orthogonal polynomials. One method is based on the Gauss-Borel factorisation of the moment matrix of the system and of its Darboux transformations, and the other uses the connection of multiple orthogonal polynomials with production matrices and branched continued fractions. As a case study, we present an explicit bidiagonal factorisation for the Hessenberg recurrence matrix of the Jacobi-Piñeiro polynomials, using both methods, and of the multiple Laguerre polynomials of first kind, as a limiting case.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03694
Robustness of the proxy-SU(3) symmetry in atomic nuclei and the role of the next highest weight irreducible representation,['Nuclear Theory'],"['Dennis Bonatsos', 'Andriana Martinou', 'S. K. Peroulis', 'D. Petrellis', 'P. Vasileiou', 'T. J. Mertzimekis', 'N. Minkov']","The proxy-SU(3) symmetry predicts, in a parameter-free way, the collective deformation variables beta and gamma in even-even atomic nuclei away from closed shells based on the highest weight irreducible representations (irreps) of SU(3) in the relevant proton and neutron shells, which are the most symmetric irreps allowed by the Pauli principle and the short-range nature of the nucleon-nucleon interactions. The special cases in which the use of the next highest weight irrep of SU(3) becomes necessary are pointed out and numerical results are given for several regions of the nuclear chart, which can be used as input for irrep-mixing calculations.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03692
Ferroelectric domain walls for environmental sensors,['Materials Science'],"['L. Richarz', 'I. C. Skogvoll', 'E. Y. Tokle', 'K. A. Hunnestad', 'U. Ludacka', 'J. He', 'E. Bourret', 'Z. Yan', 'A. T. J. van Helvoort', 'J. Schultheiß', 'S. M. Selbach', 'D. Meier']","Domain walls in ferroelectric oxides provide fertile ground for the development of next-generation nanotechnology. Examples include domain-wall-based memory, memristors, and diodes, where the unusual electronic properties and the quasi-2D nature of the walls are leveraged to emulate the behavior of electronic components at ultra-small length scales. Here, we demonstrate atmosphere-related reversible changes in the electronic conduction at neutral ferroelectric domain walls in Er(Mn,Ti)O$_3$. By exposing the system to reducing and oxidizing conditions, we drive the domain walls from insulating to conducting, and vice versa, translating the environmental changes into current signals. Density functional theory calculations show that the effect is predominately caused by charge carrier density modulations, which arise as oxygen interstitials accumulate at the domain walls. The work introduces an innovative concept for domain-wall based environmental sensors, giving an additional dimension to the field of domain wall nanoelectronics and sensor technology in general.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03691
Constraining the link between the 2175Å dust absorption feature and PAHs in Nearby Star-Forming Galaxies using Swift/UVOT and JWST/MIRI,['Astrophysics of Galaxies'],"['A. J. Battisti', 'I. Shivaei', 'H. -J. Park', 'M. Decleir', 'D. Calzetti', 'J. Mathew', 'E. Wisnioski', 'Elisabete da Cunha']","The 2175Å bump is a prominent absorption feature at ultraviolet (UV) wavelengths in dust extinction and attenuation curves. Understanding the relative strength of this feature is important for accurate dust corrections at both low- and high-redshift. This feature is postulated to arise from polycyclic aromatic hydrocarbon (PAH) dust grains; however, the carrier has not been definitively established. We present results on the correlation between the 2175Å feature and PAH abundances in a spatially-resolved manner for 15 local galaxies in the PHANGS-JWST survey that have NUV and mid-IR imaging data from Swift/UVOT and JWST/MIRI, respectively. We find a moderate positive correlation between the 2175Å feature strength and PAH abundance, albeit with large intrinsic scatter. However, most of this trend can be attributed to a stronger negative correlation of both quantities with SFR surface density and specific-SFR (proxies of ionising radiation). The latter trends are consistent with previous findings that both the 2175Å carrier and PAHs are small grains that are easily destroyed by UV photons, although the proxy for PAH abundance could also be influenced by dust heating. When controlling for SFR surface density, we find weaker correlations between the 2175Å feature and PAH abundances, disfavouring a direct link. However, analyses based on spectroscopic measurements of the 2175Å feature and PAH features are required to verify our findings. No significant trends with gas-phase metallicity are found for the 2175Å feature and PAHs, however the metallicity range of our sample is limited. We provide prescriptions for the strength of the 2175Å feature and PAHs in local massive (metal-rich) galaxies with SFR surface density and specific-SFR, however the former should be used with caution since bump strengths measured from Swift/UVOT are expected to be underestimated.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03690
"Decomposition of matrices from $SL_ 2(K[x, y])$",['Group Theory'],"['Y. Chapovskyi', 'O. Kozachok', 'A. Petravchuk']","Let $\mathbb{K}$ be an algebraically closed field of characteristic zero and $\mathbb{K}[x,y]$ the polynomial ring. The group $\text{SL}_{2}\left(\mathbb{K}[x,y]\right)$ of all matrices with determinant equal to $1$ over $\mathbb{K}[x,y]$ can not be generated by elementary matrices. The known counterexample was pointed out by P.M. Cohn. Conversely, A.A.Suslin proved that the group $\text{SL}_{r}\left(\mathbb{K}[x_{1},\dots,x_{n}]\right)$ is generated by elementary matrices for $r\ge 3$ and arbitrary $n\geq 2$, the same is true for $n=1$ and arbitrary $r.$ It is proven that any matrix from $\text{SL}_{2}\left(\mathbb{K}[x,y]\right)$ with at least one entry of degree $\le 2$ is either a product of elementary matrices or a product of elementary matrices and of a matrix similar to the one pointed out by P. Cohn. For any matrix $\begin{pmatrix}\begin{array}{cc} f & g\\ -Q & P \end{array}\end{pmatrix}\in\text{SL}_{2}\left(\mathbb{K}[x,y]\right)$, we obtain formulas for the homogeneous components $P_i , Q_i$ for the unimodular row $(-Q, P) $ as combinations of homogeneous components of the polynomials $f, g, $ respectively, with the same coefficients.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03688
Assessing Changes in Thinking about Troubleshooting in Physical Computing: A Clinical Interview Protocol with Failure Artifacts Scenarios,['Computers and Society'],"['Luis Morales-Navarro', 'Deborah A. Fields', 'Yasmin B. Kafai', 'Deepali Barapatre']","Purpose: The purpose of this paper is to examine how a clinical interview protocol with failure artifact scenarios can capture changes in high school students' explanations of troubleshooting processes in physical computing activities. We focus on physical computing since finding and fixing hardware and software bugs is a highly contextual practice that involves multiple interconnected domains and skills. Approach: We developed and piloted a ""failure artifact scenarios"" clinical interview protocol. Youth were presented with buggy physical computing projects over video calls and asked for suggestions on how to fix them without having access to the actual project or its code. We applied this clinical interview protocol before and after an eight-week-long physical computing (more specifically, electronic textiles) unit. We analyzed matching pre- and post-interviews from 18 students at four different schools. Findings: Our findings demonstrate how the protocol can capture change in students' thinking about troubleshooting by eliciting students' explanations of specificity of domain knowledge of problems, multimodality of physical computing, iterative testing of failure artifact scenarios, and concreteness of troubleshooting and problem solving processes. Originality: Beyond tests and surveys used to assess debugging, which traditionally focus on correctness or student beliefs, our ""failure artifact scenarios"" clinical interview protocol reveals student troubleshooting-related thinking processes when encountering buggy projects. As an assessment tool, it may be useful to evaluate the change and development of students' abilities over time.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03687
The Role of Chalcogen Vacancies in Single Photon Emission from Monolayer Tungsten Dichalcogenides,['Materials Science'],"['S. Carin Gavin', 'Charles J. Zeman IV', 'Anushka Dasgupta', 'Yiying Liu', 'Wenjing Wu', 'Shengxi Huang', 'Tobin J. Marks', 'Mark C. Hersam', 'George C. Schatz', 'Nathaniel P. Stern']","Understanding the mechanism of single photon emission (SPE) in two-dimensional (2D) materials is an unsolved problem important for quantum optical materials and the development of quantum information applications. In 2D transition metal dichalcogenides (TMDs) such as tungsten diselenide (WSe$_2$), quantum emission has been broadly attributed to exciton localization from atomic point defects, yet the precise microscopic picture is not fully understood. This work presents a new framework, supported by both computational and experimental evidence, to explain both the origins of facile SPE in WSe2 and the relative scarcity of SPE in the related 2D TMD, tungsten disulfide (WS$_2$). A vertical divacancy configuration of selenium creates a defect-centered, direct energy gap in the band structure of WSe2, giving rise to highly localized, radiative transitions. This configuration is shown to be energetically preferred in the monolayer lattice, which is reflected in the abundant experimental observation of SPE in WSe2 both from the literature and this work. In contrast, the same vertical divacancy configuration in WS2 does not create direct localized transitions, consistent with scarce observations of SPE in that material. By revealing a single mechanism in tungsten-based TMDs that addresses the prevalence of SPE and is consistent between theory and experiment, these results provide a framework for better understanding the rules governing the atomic origins of single photon emission in TMDs.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03686
Sprite Sheet Diffusion: Generate Game Character for Animation,['Graphics'],"['Cheng-An Hsieh', 'Jing Zhang', 'Ava Yan']","In the game development process, creating character animations is a vital step that involves several stages. Typically for 2D games, illustrators begin by designing the main character image, which serves as the foundation for all subsequent animations. To create a smooth motion sequence, these subsequent animations involve drawing the character in different poses and actions, such as running, jumping, or attacking. This process requires significant manual effort from illustrators, as they must meticulously ensure consistency in design, proportions, and style across multiple motion frames. Each frame is drawn individually, making this a time-consuming and labor-intensive task. Generative models, such as diffusion models, have the potential to revolutionize this process by automating the creation of sprite sheets. Diffusion models, known for their ability to generate diverse images, can be adapted to create character animations. By leveraging the capabilities of diffusion models, we can significantly reduce the manual workload for illustrators, accelerate the animation creation process, and open up new creative possibilities in game development.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03685
LDPC-Coded Molecular Communications with Increased Diversity,['Signal Processing'],"['Alpar Türkoğlu', 'Berk Karabacakoğlu', 'Ali Emre Pusane']","This paper suggests achieving diversity gains while utilizing low-denisty parity check (LDPC) codes in molecular communications. Intersymbol interference (ISI) causes a significant disadvantage in error performance for molecular communications. Even though decoding LDPC codes with soft decoding yields a considerable enhancement in the bit error rate (BER) curves, this can be further improved by utilizing diversity gain. In order to achieve this, two different messenger molecule types are sent to transmit the message codeword and its interleaved version. The molecular communication channel is then modeled, and the error performance of the proposed method is estimated by Monte-Carlo simulations. This approach provides considerable improvement in the error performance in the scenario where few messenger molecules are transmitted per bit.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03684
Evidence of a CP broken deconfined phase in 4D SU(2) Yang-Mills theory at $θ=π$ from imaginary $θ$ simulations,['High Energy Physics - Theory'],"['Mistuaki Hirasawa', 'Masazumi Honda', 'Akira Matsumoto', 'Jun Nishimura', 'Atis Yosprakob']","The spontaneous breaking of CP symmetry in 4D SU($N$) pure Yang-Mills theory at $θ=π$ has recently attracted much attention in the context of the higher-form symmetry and the 't Hooft anomaly matching condition. Here we use Monte Carlo simulations to study the $N=2$ case, which is interesting since it is the case opposite to the large-$N$ limit, where explicit calculations are available. In order to circumvent the severe sign problem due to the $θ$ term for real $θ$, we first obtain results at imaginary $θ$, where the sign problem is absent, and make an analytic continuation to real $θ$. We use the stout smearing in defining the $θ$ term in the action to be used in our simulations. Thus we obtain the expectation value of the topological charge and the deconfining temperature at $θ=π$, and provide an evidence that the CP symmetry, which is spontaneously broken at low temperature, gets restored \emph{strictly above} the deconfining temperature. This conclusion is consistent with the anomaly matching condition and yet differs from the prediction in the large-$N$ limit.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03683
JWST-TST DREAMS: A Precise Water Abundance for Hot Jupiter WASP-17b from the NIRISS SOSS Transmission Spectrum,['Earth and Planetary Astrophysics'],"['Dana R. Louie', 'Elijah Mullens', 'Lili Alderson', 'Ana Glidden', 'Nikole K. Lewis', 'Hannah R. Wakeford', 'Natasha E. Batalha', 'Knicole D. Colón', 'Amélie Gressier', 'Douglas Long', 'Michael Radica', 'Néstor Espinoza', 'Jayesh Goyal', 'Ryan J. MacDonald', 'Erin M. May', 'Sara Seager', 'Kevin B. Stevenson', 'Jeff A. Valenti', 'Natalie H. Allen', 'Caleb I. Cañas', 'Ryan C. Challener', 'David Grant', 'Jingcheng Huang', 'Zifan Lin', 'Daniel Valentine']","Water has proven to be ubiquitously detected in near-infrared (NIR) transmission spectroscopy observations of hot Jupiter atmospheres, including WASP-17b. However, previous analyses of WASP-17b's atmosphere based upon Hubble Space Telescope (HST) and Spitzer data could not constrain the water abundance, finding that sub-solar, super-solar and bimodal posterior distributions were all statistically valid. In this work, we observe one transit of the hot Jupiter WASP-17b using JWST's Near Infrared Imager and Slitless Spectrograph Single Object Slitless Spectroscopy (NIRISS SOSS) mode. We analyze our data using three independent data analysis pipelines, finding excellent agreement between results. Our transmission spectrum shows multiple H$_2$O absorption features and a flatter slope towards the optical than seen in previous HST observations. We analyze our spectrum using both PICASO+Virga forward models and free retrievals. POSEIDON retrievals provide a well-constrained super-solar $\log$(H$_2$O) abundance (-2.96$^{+0.31}_{-0.24}$), breaking the degeneracy from the previous HST/Spitzer analysis. We verify our POSEIDON results with petitRADTRANS retrievals. Additionally, we constrain the abundance of $\log$(H$^-$), -10.19$^{+0.30}_{-0.23}$, finding that our model including H$^-$ is preferred over our model without H$^-$ to 5.1 $σ$. Furthermore, we constrain the $\log$(K) abundance (-8.07$^{+0.58}_{-0.52}$) in WASP-17b's atmosphere for the first time using space-based observations. Our abundance constraints demonstrate the power of NIRISS SOSS's increased resolution, precision, and wavelength range to improve upon previous NIR space-based results. This work is part of a series of studies by our JWST Telescope Scientist Team (JWST-TST), in which we use Guaranteed Time Observations to perform Deep Reconnaissance of Exoplanet Atmospheres through Multi-instrument Spectroscopy (DREAMS).△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03675
Interpreting Transformers for Jet Tagging,['High Energy Physics - Phenomenology'],"['Aaron Wang', 'Abijith Gandrakota', 'Jennifer Ngadiuba', 'Vivekanand Sahu', 'Priyansh Bhatnagar', 'Elham E Khoda', 'Javier Duarte']","Machine learning (ML) algorithms, particularly attention-based transformer models, have become indispensable for analyzing the vast data generated by particle physics experiments like ATLAS and CMS at the CERN LHC. Particle Transformer (ParT), a state-of-the-art model, leverages particle-level attention to improve jet-tagging tasks, which are critical for identifying particles resulting from proton collisions. This study focuses on interpreting ParT by analyzing attention heat maps and particle-pair correlations on the $η$-$φ$ plane, revealing a binary attention pattern where each particle attends to at most one other particle. At the same time, we observe that ParT shows varying focus on important particles and subjets depending on decay, indicating that the model learns traditional jet substructure observables. These insights enhance our understanding of the model's internal workings and learning process, offering potential avenues for improving the efficiency of transformer architectures in future high-energy physics applications.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03673
Hyperparameter Tuning Through Pessimistic Bilevel Optimization,['Machine Learning'],"['Meltem Apaydin Ustun', 'Liang Xu', 'Bo Zeng', 'Xiaoning Qian']","Automated hyperparameter search in machine learning, especially for deep learning models, is typically formulated as a bilevel optimization problem, with hyperparameter values determined by the upper level and the model learning achieved by the lower-level problem. Most of the existing bilevel optimization solutions either assume the uniqueness of the optimal training model given hyperparameters or adopt an optimistic view when the non-uniqueness issue emerges. Potential model uncertainty may arise when training complex models with limited data, especially when the uniqueness assumption is violated. Thus, the suitability of the optimistic view underlying current bilevel hyperparameter optimization solutions is questionable. In this paper, we propose pessimistic bilevel hyperparameter optimization to assure appropriate outer-level hyperparameters to better generalize the inner-level learned models, by explicitly incorporating potential uncertainty of the inner-level solution set. To solve the resulting computationally challenging pessimistic bilevel optimization problem, we develop a novel relaxation-based approximation method. It derives pessimistic solutions with more robust prediction models. In our empirical studies of automated hyperparameter search for binary linear classifiers, pessimistic solutions have demonstrated better prediction performances than optimistic counterparts when we have limited training data or perturbed testing data, showing the necessity of considering pessimistic solutions besides existing optimistic ones.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03666
Oxygen Isotope Ratios in Hydrogen-Deficient Carbon Stars: A Correlation with Effective Temperature and Implications for White Dwarf Merger Outcomes,['Solar and Stellar Astrophysics'],"['Advait Mehla', 'Mansi M. Kasliwal', 'Viraj Karambelkar', 'Patrick Tisserand', 'Courtney Crawford', 'Geoffrey Clayton', 'Jamie Soon', 'Varun Bhalerao']","Hydrogen-deficient Carbon (HdC) stars are a class of supergiants with anomalous chemical compositions, suggesting that they are remnants of CO-He white dwarf (WD) mergers. This class comprises two spectroscopically similar subclasses - dusty R Coronae Borealis (RCB) and dustless Hydrogen-deficient Carbon (dLHdC) stars. Both subclasses have a stark overabundance of $^{18}\textrm{O}$ in their atmospheres, but spectroscopic differences between them remain poorly studied. We present high-resolution ($R \approx 75000$) K-band spectra of six RCB and six dLHdC stars, including four newly discovered dLHdC stars, making this the largest sample to date. We develop a semi-automated fitting routine to measure $^{16}\textrm{O}/^{18}\textrm{O}$ ratios for this sample, tripling the number of dLHdC stars with oxygen isotope ratios measured from high resolution spectra. All six dLHdC stars have $^{16}\textrm{O}/^{18}\textrm{O}<1$, while the RCB stars have $^{16}\textrm{O}/^{18}\textrm{O}>4$. Additionally, for the first time, we find a trend of decreasing $^{16}\textrm{O}/^{18}\textrm{O}$ ratios with increasing effective temperature for HdC stars, consistent with predictions of theoretical WD merger models. However, we note that current models overpredict the low $^{16}\textrm{O}/^{18}\textrm{O}$ ratios of dLHdC stars by two orders of magnitude. We also measure abundances of C, N, O, Fe, S, Si, Mg, Na, and Ca for these stars. We observe a correlation between the abundances of $^{14}\textrm{N}$ and $^{18}\textrm{O}$ in our sample, suggesting that a fixed fraction of the $^{14}\textrm{N}$ is converted to $^{18}\textrm{O}$ in these stars via $α$-capture. Our results affirm the emerging picture that the mass ratio/total mass of the WD binary determine whether an RCB or dLHdC is formed post-merger.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03664
Energy cascades and condensation via coherent dynamics in Hamiltonian systems,['Mathematical Physics'],"['Anxo Biasi', 'Patrick Gérard']","This work makes analytic progress in the deterministic study of turbulence in Hamiltonian systems by identifying two types of energy cascade solutions and the corresponding large- and small-scale structures they generate. The first cascade represents condensate formation via a highly coherent process recently uncovered, while the second cascade, which has not been previously observed, leads to the formation of other large-scale structures. The concentration of energy at small scales is characterized in both cases by the development of a power-law spectrum in finite time, causing the blow-up of Sobolev norms and the formation of coherent structures at small scales. These structures approach two different types of singularities: a point discontinuity in one case and a cusp in the other. The results are fully analytic and explicit, based on two solvable families of Hamiltonian systems identified in this study.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03663
The Growth of Galaxy Stellar Haloes Over $0.2 \leq z \leq 1.1$,['Astrophysics of Galaxies'],"['Devin J. Williams', 'Ivana Damjanov', 'Marcin Sawicki', 'Harrison Souchereau', 'Lingjian Chen', 'Guillaume Desprez', 'Angelo George', 'Marianna Annunziatella', 'Stephen Gwyn']","Galaxies are predicted to assemble their stellar haloes through the accretion of stellar material from interactions with their cosmic environment. Observations that trace stellar halo buildup probe the processes that drive galaxy size and stellar mass growth. We investigate stellar halo assembly over $0.2 \leq z \leq 1.1$ in a mass-complete ($M_{\star} \geq 10^{9.5}M_{\odot}$) sample of 242,456 star-forming and 88,421 quiescent galaxies (SFGs and QGs) from the CLAUDS and HSC-SSP surveys. We extract galaxy rest-frame $g$-band surface brightness ($μ_g$) profiles to study faint, extended emission in galaxy outskirts. We examine trends in galaxy assembly by analyzing the median $μ_g$ profiles in different SFG and QG \msS ranges with decreasing redshift and connecting evolution in galaxy $μ_g$ profiles with the underlying stellar mass growth in galaxies. Since $z=1.1$, the majority of evolution in the median $μ_g$ profiles of galaxies ($\sim$64$\%$ in SFGs and $\sim$71$\%$ in QGs) occurs throughout their stellar halo regions (2-10$R_e$). More massive galaxies assemble stellar halo material more rapidly at $0.2 \leq z \leq 1.1$. Over this period, QGs grow a larger fraction of their stellar haloes than SFGs at fixed $M_{\star}$ (factor of $\sim$1.2). Although star formation can account for the stellar halo growth observed in low-mass SFGs ($10^{9.5}M_\odot \leq M_\star < 10^{10.5}M_\odot$), high-mass SFGs ($M_\star \geq 10^{10.5}M_\odot$) and both low- and high-mass QGs require an additional assembly mechanism. Our results suggest accretion via minor mergers drives additional stellar halo growth in these galaxies. The contribution from accretion is larger in more massive galaxies (over $M_{\star} \geq 10^{9.5}M_{\odot}$), and QGs exhibit larger fractional increases to their ex-situ fractions over $0.2 \leq z \leq 1.1$ than SFGs at fixed $M_{\star}$.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03662
Resilience and Criticality: Brothers in Arms for 6G,['Information Theory'],"['Robert-Jeron Reifert', 'Yasemin Karacora', 'Christina Chaccour', 'Aydin Sezgin', 'Walid Saad']","In this paper, we develop the first comprehensive tutorial on designing future 6G networks that synergistically integrate notions of resilience and criticality from the ground up. While resilience refers to the ability to absorb, adapt to, and recover from adversarial or challenging conditions, criticality indicates the degree of importance or urgency assigned to a particular service or component. Despite a spiking interest in designing resilient wireless networks, most prior works do not provide a unified resilience definition, nor harness the intricate interplay between resilience and criticality. In order to fill this gap, in this paper, we highlight the importance of a criticality-aware approach as a key enabler for providing reliable and resilient service functionality. Moreover, we delve into the unique challenges and opportunities of the envisioned 6G features pertaining to resilience and (mixed) criticality. After reviewing resilience definitions, we present a core resilience strategy, a unified resilience metric, different criteria for service criticality, and prioritization frameworks, that augment the 6G resilience prospects. Afterwards, we explore the opportunities presented by promising technologies that enable a resilient 6G network design from a radio access network protocol stack perspective. We briefly revisit state-of-the-art network architectures, establish a rough connection to the Open-RAN Alliance vision, and discuss opportunities, existing techniques, and promising enabling mechanisms for 6G at each layer. Finally, the article discusses important research directions and open problems concerning resilience and criticality in 6G.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03661
Stepping Up Superradiance Constraints on Axions,['High Energy Physics - Phenomenology'],"['Samuel J. Witte', 'Andrew Mummery']","Light feebly-coupled bosonic particles can efficiently extract the rotational energy of rapidly spinning black holes on sub-astrophysical timescales via a phenomenon known as black hole superradiance. In the case of light axions, the feeble self-interactions of these particles can lead to a non-linear coupled evolution of many superradiant quasi-bound states, dramatically altering the rate at which the black hole is spun down. In this work, we extend the study of axion superradiance to higher order states, solving for the first time the coupled evolution of all states with $n \leq 5$ in the fully relativistic limit (with $n$ being the principle quantum number). Using a Bayesian framework, we re-derive constraints on axions using the inferred spins of solar mass black holes, demonstrating that previously adopted limit-setting procedures have underestimated current sensitivity to the axion decay constant $f_a$ by around one order of magnitude, and that the inclusion to higher order states allows one to reasonably capture the evolution of typical high-spin black holes across a much wider range of parameter space, thereby allowing constraints to be extended to more massive axions. We conclude with an extensive discussion on the systematics associated with spin inference from x-ray observations.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03655
Image-Constrained Modeling with Hubble and Keck Images Reveals that OGLE-2012-BLG-0563Lb is a Jupiter-Mass planet Orbiting a K Dwarf,['Earth and Planetary Astrophysics'],"['David P. Bennett', 'Aparna Bhattacharya', 'Jean-Philippe Beaulieu', 'Naoki Koshimoto', 'Joshua W. Blackman', 'Ian A. Bond', 'Clement Ranc', 'Natalia Rektsini', 'Sean K. Terry', 'Aikaterini Vandorou']","We present high angular resolution imaging from the {\sl Hubble Space Telescope} combined with adaptive optics imaging results from the {\sl Keck}-II telescope to determine the mass of the OGLE-2012-BLG-0563L host star and planet to be $M_{\rm host} = 0.801\pm 0.033M_\odot$ and $M_{\rm planet} = 1.116 \pm 0.087 M_{\rm Jupiter}$, respectively, located at a distance of $D_L = 5.46\pm 0.56\,$kpc. There is a close-wide degeneracy in the light curve models that indicates star-planet projected separation of $1.50\pm 0.16\,$AU for the close model and $8.41\pm 0.87\,$AU for the wide model. We used the image-constrained modeling method to analyze the light curve data with constraints from this high angular resolution image analysis. This revealed systematic errors in some of the ground-based light curve photometry that led to an estimate of the angular Einstein Radius, $θ_E$, that was too large by a factor of $\sim 2$. The host star mass is a factor of 2.4 larger than the value presented in the \citet{fukui15} discovery paper. Although most systematic photometry errors seen in ground-based microlensing light curve photometry will not be repeated in data from the {\sl Roman Space Telescope}'s Galactic Bulge Time Domain Survey, we argue that image constrained modeling will be a valuable method to identify possible systematic errors in {\sl Roman} photometry.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03651
A Unified Model of Cosmic Ray Propagation and Radio Extreme Scattering Events from Intermittent Interstellar Structures,['High Energy Astrophysical Phenomena'],"['Philipp Kempski', 'Dongzi Li', 'Drummond B. Fielding', 'Eliot Quataert', 'E. Sterl Phinney', 'Matthew W. Kunz', 'Dylan L. Jow', 'Alexander A. Philippov']","Intermittent magnetic structures are a plausible candidate for explaining cosmic-ray (CR) diffusion rates derived from observed CR energy spectra. Independently, studies of extreme scattering events (ESEs) of radio quasars and pulsar scintillation have hinted that very straight, large-aspect-ratio, magnetic current sheets may be responsible for the localized large scattering of radio waves. The required shortest axis of the typical structures producing ESEs is of the same scale ($\sim$AU) as the gyroradii of $\sim$GeV CRs. In this paper, we propose that the same magnetic/density sheets can produce large scattering of both CRs and radio waves. We demonstrate that the geometry and volume filling factor of the sheets derived from quasar ESEs can explain the observed mean free path of GeV CRs without introducing free parameters. The model places constraints on the sheet geometry, such as straightness and large aspect ratio, and assumes the statistics of the sheets are similar throughout the Galactic volume. We, therefore, discuss observational tests of the sheet model, which includes observations of echoes in pulsars and fast radio bursts, gravitationally lensed quasars, the distribution of ESE durations, and spatial correlations between ESE events and rotation-measure fluctuations. Such tests will be enabled by upcoming wide-field radio instruments, including Canadian Hydrogen Observatory and Radio-transient Detector (CHORD) and Deep Synoptic Array 2000 Antennas (DSA-2000).△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03649
Phase Diagram of the Easy-Axis Triangular-Lattice $J_1\!-\!J_2$ Model,['Strongly Correlated Electrons'],"['Cesar A. Gallegos', 'Shengtao Jiang', 'Steven R. White', 'A. L. Chernyshev']","The phase diagram of the $S\!=\!1/2$ easy-axis triangular-lattice $J_1\!-\!J_2$ model is investigated using the density-matrix renormalization group and analytical insights. We find a significant spin-liquid region extending from the Heisenberg limit and residing between the Y phase-known as the magnetic analogue of the ""supersolid""-and collinear stripe phase. The order parameters of the supersolid are analyzed and an understanding of its lack of ferromagnetic moment is suggested.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03648
Direct Deflection of Millicharged Radiation,['High Energy Physics - Phenomenology'],"['Asher Berlin', 'Surjeet Rajendran', 'Harikrishnan Ramani', 'Erwin H. Tanin']","Millicharged particles are generic in theories of dark sectors. A cosmic or local abundance of them may be produced by the early universe, stellar environments, or the decay or annihilation of dark matter/dark energy. Furthermore, if such particles are light, these production channels result in a background of millicharged radiation. We show that light-shining-through-wall experiments employing superconducting RF cavities can also be used as ``direct deflection"" experiments to search for this relativistic background. The millicharged plasma is first subjected to an oscillating electromagnetic field of a driven cavity, which causes charge separation in the form of charge and current perturbations. In turn, these perturbations can propagate outwards and resonantly excite electromagnetic fields in a well-shielded cavity placed nearby, enabling detection. We estimate that future versions of the existing Dark SRF experiment can probe orders of magnitude of currently unexplored parameter space, including millicharges produced from the Sun, the cosmic neutrino background, or other mechanisms that generate a thermal abundance with energy density as small as $\sim 10^{-4}$ that of the cosmic microwave background.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03643
Gauge-invariant projector calculus for quantum state geometry and applications to observables in crystals,['Strongly Correlated Electrons'],"['Johannes Mitscherling', 'Alexander Avdoshkin', 'Joel E. Moore']","The importance of simple geometrical invariants, such as the Berry curvature and quantum metric, constructed from the Bloch states of a crystal has become well-established over four decades of research. More complex aspects of geometry emerge in properties linking multiple bands, such as optical responses. In the companion work [arXiv:2409.16358], we identified novel multi-state geometrical invariants using an explicitly gauge-invariant formalism based on projection operators, which we used to clarify the relation between the shift current and the theory of electronic polarization among other advancements for second-order non-linear optics. Here, we provide considerably more detail on the projector formalism and the geometrical invariants arising in the vicinity of a specific value of crystal momentum. We combine the introduction to multi-state quantum geometry with broadly relevant algebraic relationships and detailed example calculations, enabling extensions toward future applications to topological and geometrical properties of insulators and metals.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03637
Explainable Malware Detection through Integrated Graph Reduction and Learning Techniques,['Cryptography and Security'],"['Hesamodin Mohammadian', 'Griffin Higgins', 'Samuel Ansong', 'Roozbeh Razavi-Far', 'Ali A. Ghorbani']","Control Flow Graphs and Function Call Graphs have become pivotal in providing a detailed understanding of program execution and effectively characterizing the behavior of malware. These graph-based representations, when combined with Graph Neural Networks (GNN), have shown promise in developing high-performance malware detectors. However, challenges remain due to the large size of these graphs and the inherent opacity in the decision-making process of GNNs. This paper addresses these issues by developing several graph reduction techniques to reduce graph size and applying the state-of-the-art GNNExplainer to enhance the interpretability of GNN outputs. The analysis demonstrates that integrating our proposed graph reduction technique along with GNNExplainer in the malware detection framework significantly reduces graph size while preserving high performance, providing an effective balance between efficiency and transparency in malware detection.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03634
NBM: an Open Dataset for the Acoustic Monitoring of Nocturnal Migratory Birds in Europe,['Sound'],"['Louis Airale', 'Adrien Pajot', 'Juliette Linossier']","The persisting threats on migratory bird populations highlights the urgent need for effective monitoring techniques that could assist in their conservation. Among these, passive acoustic monitoring is an essential tool, particularly for nocturnal migratory species that are difficult to track otherwise. This work presents the Nocturnal Bird Migration (NBM) dataset, a collection of 13,359 annotated vocalizations from 117 species of the Western Palearctic. The dataset includes precise time and frequency annotations, gathered by dozens of bird enthusiasts across France, enabling novel downstream acoustic analysis. In particular, we demonstrate that a two-stage object detection model, tailored for the processing of audio data, can be trained on our dataset to retrieve localized bounding box coordinates around each signal of interest in a spectrogram. This object detection approach, which is largely overlooked in the bird sound recognition literature, allows important applications by potentially differentiating individual birds within audio windows. Further, we show that the accuracy of our recognition model on the 45 main species of the dataset competes with state-of-the-art systems trained on much larger datasets. This highlights the interest of fostering similar open-science initiatives to acquire costly but valuable fine-grained annotations of audio files. All data and code are made openly available.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03633
DiffuPT: Class Imbalance Mitigation for Glaucoma Detection via Diffusion Based Generation and Model Pretraining,['Image and Video Processing'],"['Youssof Nawar', 'Nouran Soliman', 'Moustafa Wassel', 'Mohamed ElHabebe', 'Noha Adly', 'Marwan Torki', 'Ahmed Elmassry', 'Islam Ahmed']","Glaucoma is a progressive optic neuropathy characterized by structural damage to the optic nerve head and functional changes in the visual field. Detecting glaucoma early is crucial to preventing loss of eyesight. However, medical datasets often suffer from class imbalances, making detection more difficult for deep-learning algorithms. We use a generative-based framework to enhance glaucoma diagnosis, specifically addressing class imbalance through synthetic data generation. In addition, we collected the largest national dataset for glaucoma detection to support our study. The imbalance between normal and glaucomatous cases leads to performance degradation of classifier models. By combining our proposed framework leveraging diffusion models with a pretraining approach, we created a more robust classifier training process. This training process results in a better-performing classifier. The proposed approach shows promising results in improving the harmonic mean (sensitivity and specificity) and AUC for the roc for the glaucoma classifier. We report an improvement in the harmonic mean metric from 89.09% to 92.59% on the test set of our national dataset. We examine our method against other methods to overcome imbalance through extensive experiments. We report similar improvements on the AIROGS dataset. This study highlights that diffusion-based generation can be of great importance in tackling class imbalances in medical datasets to improve diagnostic performance.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03629
Counting of lattices containing up to four comparable reducible elements and having nullity up to three,['Combinatorics'],"['B. P. Aware', 'A. N. Bhavale']","In 2020 Bhavale and Waphare introduced the concept of a nullity of a poset as nullity of its cover graph. According to Bhavale and Waphare, if a dismantlable lattice of nullity k contains r reducible elements then 2 $\leq$ r $\leq$ 2k. In 2003 Pawar and Waphare counted all non-isomorphic lattices with equal number of elements and edges, which are precisely the lattices of nullity one. Recently, Bhavale and Aware counted all non-isomorphic lattices on n elements having nullity up to two. Bhavale and Aware also counted all non-isomorphic lattices on n elements, containing up to three reducible elements, having nullity k $\geq$ 2. In this paper, we count up to isomorphism the class of all lattices on n elements containing four comparable reducible elements, and having nullity three.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03627
How to Correctly do Semantic Backpropagation on Language-based Agentic Systems,['Artificial Intelligence'],"['Wenyi Wang', 'Hisham A. Alyahya', 'Dylan R. Ashley', 'Oleg Serikov', 'Dmitrii Khizbullin', 'Francesco Faccio', 'Jürgen Schmidhuber']","Language-based agentic systems have shown great promise in recent years, transitioning from solving small-scale research problems to being deployed in challenging real-world tasks. However, optimizing these systems often requires substantial manual labor. Recent studies have demonstrated that these systems can be represented as computational graphs, enabling automatic optimization. Despite these advancements, most current efforts in Graph-based Agentic System Optimization (GASO) fail to properly assign feedback to the system's components given feedback on the system's output. To address this challenge, we formalize the concept of semantic backpropagation with semantic gradients -- a generalization that aligns several key optimization techniques, including reverse-mode automatic differentiation and the more recent TextGrad by exploiting the relationship among nodes with a common successor. This serves as a method for computing directional information about how changes to each component of an agentic system might improve the system's output. To use these gradients, we propose a method called semantic gradient descent which enables us to solve GASO effectively. Our results on both BIG-Bench Hard and GSM8K show that our approach outperforms existing state-of-the-art methods for solving GASO problems. A detailed ablation study on the LIAR dataset demonstrates the parsimonious nature of our method. A full copy of our implementation is publicly available at https://github.com/HishamAlyahya/semantic_backprop△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03624
Network-aided Efficient Large Language Model Services With Denoising-inspired Prompt Compression,['Networking and Internet Architecture'],"['Feiran You', 'Hongyang Du', 'Kaibin Huang', 'Abbas Jamalipour']","Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, leading to their increasing adoption in diverse services delivered through wireless networks. There is a growing trend toward longer prompts to better leverage LLMs' capabilities and address difficult tasks. However, longer prompts not only increase data transmission costs across wireless transmission but also require more computing resources and processing time, impacting the overall system efficiency and user experience. To address this challenge, we propose Joint Power and Prompt Optimization (JPPO), a framework that combines Small Language Model (SLM)-based prompt compression with wireless power allocation optimization. By deploying SLM at edge devices for prompt compression and employing Deep Reinforcement Learning (DRL) for joint optimization of compression ratio and transmission power, JPPO effectively balances service quality with resource efficiency. Furthermore, inspired by denoising diffusion models, we design a denoising-inspired prompt compression approach that iteratively compresses prompts by gradually removing non-critical information. Experimental results demonstrate that our framework achieves high service fidelity while optimizing power usage in wireless LLM services, reducing the total service response time. With our DRL-based JPPO, the framework maintains fidelity comparable to the no-compression baseline while still achieving a 17% service time reduction through adaptive compression. When prioritizing compression, our framework achieves up to 16x compression ratio while maintaining acceptable fidelity (within 30% reduction). Compared to no compression, baseline single-round compression with a 16x compression ratio reduces the system total response time by approximately 42.3%, while the denoising-inspired method achieves a 46.5% service time-saving.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03621
Recommender Systems for Sustainability: Overview and Research Issues,['Information Retrieval'],"['Alexander Felfernig', 'Manfred Wundara', 'Thi Ngoc Trang Tran', 'Seda Polat-Erdeniz', 'Sebastian Lubos', 'Merfat El-Mansi', 'Damian Garber', 'Viet-Man Le']","Sustainability development goals (SDGs) are regarded as a universal call to action with the overall objectives of planet protection, ending of poverty, and ensuring peace and prosperity for all people. In order to achieve these objectives, different AI technologies play a major role. Specifically, recommender systems can provide support for organizations and individuals to achieve the defined goals. Recommender systems integrate AI technologies such as machine learning, explainable AI (XAI), case-based reasoning, and constraint solving in order to find and explain user-relevant alternatives from a potentially large set of options. In this article, we summarize the state of the art in applying recommender systems to support the achievement of sustainability development goals. In this context, we discuss open issues for future research.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03620
Jet EEC aWAKEning: hydrodynamic response on the celestial sphere,['High Energy Physics - Phenomenology'],"['João Barata', 'Matvey V. Kuzmin', 'José Guilherme Milhano', 'Andrey V. Sadofyev']","The observation of the medium response generated by the propagation of high energy partons in the quark gluon plasma produced in heavy-ion collisions would provide a clear and unmistakable evidence for the hydrodynamic behavior of the bulk. Recently, it has been argued that the features of the medium's back-reaction to the jet could be cleanly imprinted in the correlations of asymptotic energy flows, in principle allowing to isolate this signal from other uncorrelated physical processes. Nonetheless, the current limited theoretical understanding of these jet observables in heavy-ion collisions constrains their applicability as probes of the medium (hydro)dynamics. In this work, we provide an analytic picture for the medium back-reaction's effect on the energy flux and two point energy correlator. We show that the medium response leads to the emergence of an universal classical scaling law, competing with the perturbative QCD contribution at large angles. Comparing the associated correlator to recent experimental measurements, we find that the observed large angle features can be qualitatively described by a purely hydrodynamically driven response and its interplay with the hard jet component.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03616
Chatting with Logs: An exploratory study on Finetuning LLMs for LogQL,['Databases'],"['Vishwanath Seshagiri', 'Siddharth Balyan', 'Vaastav Anand', 'Kaustubh Dhole', 'Ishan Sharma', 'Avani Wildani', 'José Cambronero', 'Andreas Züfle']","Logging is a critical function in modern distributed applications, but the lack of standardization in log query languages and formats creates significant challenges. Developers currently must write ad hoc queries in platform-specific languages, requiring expertise in both the query language and application-specific log details -- an impractical expectation given the variety of platforms and volume of logs and applications. While generating these queries with large language models (LLMs) seems intuitive, we show that current LLMs struggle with log-specific query generation due to the lack of exposure to domain-specific knowledge. We propose a novel natural language (NL) interface to address these inconsistencies and aide log query generation, enabling developers to create queries in a target log query language by providing NL inputs. We further introduce ~\textbf{NL2QL}, a manually annotated, real-world dataset of natural language questions paired with corresponding LogQL queries spread across three log formats, to promote the training and evaluation of NL-to-loq query systems. Using NL2QL, we subsequently fine-tune and evaluate several state of the art LLMs, and demonstrate their improved capability to generate accurate LogQL queries. We perform further ablation studies to demonstrate the effect of additional training data, and the transferability across different log formats. In our experiments, we find up to 75\% improvement of finetuned models to generate LogQL queries compared to non finetuned models.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03612
The Use of Artificial Intelligence in Military Intelligence: An Experimental Investigation of Added Value in the Analysis Process,['Artificial Intelligence'],"['Christian Nitzl', 'Achim Cyran', 'Sascha Krstanovic', 'Uwe M. Borghoff']","It is beyond dispute that the potential benefits of artificial intelligence (AI) in military intelligence are considerable. Nevertheless, it remains uncertain precisely how AI can enhance the analysis of military data. The aim of this study is to address this issue. To this end, the AI demonstrator deepCOM was developed in collaboration with the start-up Aleph Alpha.
  The AI functions include text search, automatic text summarization and Named Entity Recognition (NER). These are evaluated for their added value in military analysis. It is demonstrated that under time pressure, the utilization of AI functions results in assessments clearly superior to that of the control group. Nevertheless, despite the demonstrably superior analysis outcome in the experimental group, no increase in confidence in the accuracy of their own analyses was observed. Finally, the paper identifies the limitations of employing AI in military intelligence, particularly in the context of analyzing ambiguous and contradictory information.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03610
CBEval: A framework for evaluating and interpreting cognitive biases in LLMs,['Computation and Language'],"['Ammar Shaikh', 'Raj Abhijit Dandekar', 'Sreedath Panat', 'Rajat Dandekar']","Rapid advancements in Large Language models (LLMs) has significantly enhanced their reasoning capabilities. Despite improved performance on benchmarks, LLMs exhibit notable gaps in their cognitive processes. Additionally, as reflections of human-generated data, these models have the potential to inherit cognitive biases, raising concerns about their reasoning and decision making capabilities. In this paper we present a framework to interpret, understand and provide insights into a host of cognitive biases in LLMs. Conducting our research on frontier language models we're able to elucidate reasoning limitations and biases, and provide reasoning behind these biases by constructing influence graphs that identify phrases and words most responsible for biases manifested in LLMs. We further investigate biases such as round number bias and cognitive bias barrier revealed when noting framing effect in language models.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03605
HunyuanVideo: A Systematic Framework For Large Video Generative Models,['Computer Vision and Pattern Recognition'],"['Weijie Kong', 'Qi Tian', 'Zijian Zhang', 'Rox Min', 'Zuozhuo Dai', 'Jin Zhou', 'Jiangfeng Xiong', 'Xin Li', 'Bo Wu', 'Jianwei Zhang', 'Kathrina Wu', 'Qin Lin', 'Junkun Yuan', 'Yanxin Long', 'Aladdin Wang', 'Andong Wang', 'Changlin Li', 'Duojun Huang', 'Fang Yang', 'Hao Tan', 'Hongmei Wang', 'Jacob Song', 'Jiawang Bai', 'Jianbing Wu', 'Jinbao Xue']","Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.△ Less",v1,https://arxiv.org/pdf/2412.03603
Spectroscopy of $^{52}$K,['Nuclear Experiment'],"['M. Enciu', 'A. Obertelli', 'P. Doornenbal', 'M. Heinz', 'T. Miyagi', 'F. Nowacki', 'K. Ogata', 'A. Poves', 'A. Schwenk', 'K. Yoshida', 'N. L. Achouri', 'H. Baba', 'F. Browne', 'D. Calvet', 'F. Château', 'S. Chen', 'N. Chiga', 'A. Corsi', 'M. L. Cortés', 'A. Delbart', 'J. -M. Gheller', 'A. Giganon', 'A. Gillibert', 'C. Hilaire', 'T. Isobe']",The first spectroscopy of $^{52}$K was investigated via in-beam $γ$-ray spectroscopy at the RIKEN Radioactive Isotope Beam Factory after one-proton and one-neutron knockout from $^{53}$Ca and $^{53}$K beams impinging on a 15-cm liquid hydrogen target at $\approx$ 230~MeV/nucleon. The energy level scheme of $^{52}$K was built using single $γ$ and $γ$-$γ$ coincidence spectra. The spins and parities of the excited states were established based on momentum distributions of the fragment after the knockout reaction and based on exclusive cross sections. The results were compared to state-of-the-art shell model calculations with the SDPF-Umod interaction and ab initio IMSRG calculations with chiral effective field theory nucleon-nucleon and three-nucleon forces.△ Less,"3 December, 2024;",https://arxiv.org/pdf/2412.03602
CPTQuant -- A Novel Mixed Precision Post-Training Quantization Techniques for Large Language Models,['Computation and Language'],"['Amitash Nanda', 'Sree Bhargavi Balija', 'Debashis Sahoo']","Large language models have transformed the comprehension and generation of natural language tasks, but they come with substantial memory and computational requirements. Quantization techniques have emerged as a promising avenue for addressing these challenges while preserving accuracy and making energy efficient. We propose CPTQuant, a comprehensive strategy that introduces correlation-based (CMPQ), pruning-based (PMPQ), and Taylor decomposition-based (TDMPQ) mixed precision techniques. CMPQ adapts the precision level based on canonical correlation analysis of different layers. PMPQ optimizes precision layer-wise based on their sensitivity to sparsity. TDMPQ modifies precision using Taylor decomposition to assess each layer's sensitivity to input perturbation. These strategies allocate higher precision to more sensitive layers while diminishing precision to robust layers. CPTQuant assesses the performance across BERT, OPT-125M, OPT-350M, OPT-1.3B, and OPT-2.7B. We demonstrate up to 4x compression and a 2x-fold increase in efficiency with minimal accuracy drop compared to Hugging Face FP16. PMPQ stands out for achieving a considerably higher model compression. Sensitivity analyses across various LLMs show that the initial and final 30% of layers exhibit higher sensitivities than the remaining layers. PMPQ demonstrates an 11% higher compression ratio than other methods for classification tasks, while TDMPQ achieves a 30% greater compression ratio for language modeling tasks.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.03599
The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?,['Computation and Language'],"['Sourav Banerjee', 'Ayushi Agarwal', 'Eishkaran Singh']","The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and MMLU. These vulnerabilities manifest through benchmark exploitation, dataset contamination, and evaluation bias, creating a false perception of progress in language understanding capabilities. Through extensive review of contemporary evaluation approaches, we identify significant limitations in static benchmark designs, human evaluation protocols, and LLM-as-judge frameworks, all of which compromise the reliability of current performance assessments. As LLM capabilities evolve and existing benchmarks become redundant, we lay the groundwork for new evaluation methods that resist manipulation, minimize data contamination, and assess domain-specific tasks. This requires frameworks that are adapted dynamically, addressing current limitations and providing a more accurate reflection of LLM performance.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.03597
"PLUMED Tutorials: a collaborative, community-driven learning ecosystem",['Physics Education'],"['Gareth A. Tribello', 'Massimiliano Bonomi', 'Giovanni Bussi', 'Carlo Camilloni', 'Blake I. Armstrong', 'Andrea Arsiccio', 'Simone Aureli', 'Federico Ballabio', 'Mattia Bernetti', 'Luigi Bonati', 'Samuel G. H. Brookes', 'Z. Faidon Brotzakis', 'Riccardo Capelli', 'Michele Ceriotti', 'Kam-Tung Chan', 'Pilar Cossio', 'Siva Dasetty', 'Davide Donadio', 'Bernd Ensing', 'Andrew L. Ferguson', 'Guillaume Fraux', 'Julian D. Gale', 'Francesco Luigi Gervasio', 'Toni Giorgino', 'Nicholas S. M. Herringer']","In computational physics, chemistry, and biology, the implementation of new techniques in a shared and open source software lowers barriers to entry and promotes rapid scientific progress. However, effectively training new software users presents several challenges. Common methods like direct knowledge transfer and in-person workshops are limited in reach and comprehensiveness. Furthermore, while the COVID-19 pandemic highlighted the benefits of online training, traditional online tutorials can quickly become outdated and may not cover all the software's functionalities. To address these issues, here we introduce ``PLUMED Tutorials'', a collaborative model for developing, sharing, and updating online tutorials. This initiative utilizes repository management and continuous integration to ensure compatibility with software updates. Moreover, the tutorials are interconnected to form a structured learning path and are enriched with automatic annotations to provide broader context. This paper illustrates the development, features, and advantages of PLUMED Tutorials, aiming to foster an open community for creating and sharing educational resources.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.03595
Enhancing Document AI Data Generation Through Graph-Based Synthetic Layouts,['Computation and Language'],"['Amit Agarwal', 'Hitesh Patel', 'Priyaranjan Pattnayak', 'Srikant Panda', 'Bhargava Kumar', 'Tejaswini Kumar']","The development of robust Document AI models has been constrained by limited access to high-quality, labeled datasets, primarily due to data privacy concerns, scarcity, and the high cost of manual annotation. Traditional methods of synthetic data generation, such as text and image augmentation, have proven effective for increasing data diversity but often fail to capture the complex layout structures present in real world documents. This paper proposes a novel approach to synthetic document layout generation using Graph Neural Networks (GNNs). By representing document elements (e.g., text blocks, images, tables) as nodes in a graph and their spatial relationships as edges, GNNs are trained to generate realistic and diverse document layouts. This method leverages graph-based learning to ensure structural coherence and semantic consistency, addressing the limitations of traditional augmentation techniques. The proposed framework is evaluated on tasks such as document classification, named entity recognition (NER), and information extraction, demonstrating significant performance improvements. Furthermore, we address the computational challenges of GNN based synthetic data generation and propose solutions to mitigate domain adaptation issues between synthetic and real-world datasets. Our experimental results show that graph-augmented document layouts outperform existing augmentation techniques, offering a scalable and flexible solution for training Document AI models.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.03590
Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models,['Artificial Intelligence'],"['Valentina Anita Carriero', 'Antonia Azzini', 'Ilaria Baroni', 'Mario Scrocca', 'Irene Celino']","Procedural Knowledge is the know-how expressed in the form of sequences of steps needed to perform some tasks. Procedures are usually described by means of natural language texts, such as recipes or maintenance manuals, possibly spread across different documents and systems, and their interpretation and subsequent execution is often left to the reader. Representing such procedures in a Knowledge Graph (KG) can be the basis to build digital tools to support those users who need to apply or execute them. In this paper, we leverage Large Language Model (LLM) capabilities and propose a prompt engineering approach to extract steps, actions, objects, equipment and temporal information from a textual procedure, in order to populate a Procedural KG according to a pre-defined ontology. We evaluate the KG extraction results by means of a user study, in order to qualitatively and quantitatively assess the perceived quality and usefulness of the LLM-extracted procedural knowledge. We show that LLMs can produce outputs of acceptable quality and we assess the subjective perception of AI by human evaluators.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.03589
Spectral Networks: Bridging higher-rank Teichmüller theory and BPS states,['Mathematical Physics'],"['Clarence Kineider', 'Georgios Kydonakis', 'Eugen Rogozinnikov', 'Valdo Tatitscheff', 'Alexander Thomas']","This monograph is aiming to serve as a primary introduction to spectral networks, by presenting them and their many applications from the scope of geometry and mathematical physics in a unified way. We do not attempt to treat these two approaches separately but rather provide broad motivation and the necessary background to reach to the frontiers of this fast-evolving field of research, explaining simultaneously the relevant geometric and physical aspects. The targeted audience is researchers and advanced students from either a rather mathematical or physical oriented education who wish to enter the field.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.03588
A Survey on E-Commerce Learning to Rank,['Information Retrieval'],"['Md. Ahsanul Kabir', 'Mohammad Al Hasan', 'Aritra Mandal', 'Daniel Tunkelang', 'Zhe Wu']","In e-commerce, ranking the search results based on users' preference is the most important task. Commercial e-commerce platforms, such as, Amazon, Alibaba, eBay, Walmart, etc. perform extensive and relentless research to perfect their search result ranking algorithms because the quality of ranking drives a user's decision to purchase or not to purchase an item, directly affecting the profitability of the e-commerce platform. In such a commercial platforms, for optimizing search result ranking numerous features are considered, which emerge from relevance, personalization, seller's reputation and paid promotion. To maintain their competitive advantage in the market, the platforms do no publish their core ranking algorithms, so it is difficult to know which of the algorithms or which of the features is the most effective for finding the most optimal search result ranking in e-commerce. No extensive surveys of ranking to rank in the e-commerce domain is also not yet published. In this work, we survey the existing e-commerce learning to rank algorithms. Besides, we also compare these algorithms based on query relevance criterion on a large real-life e-commerce dataset and provide a quantitative analysis. To the best of our knowledge this is the first such survey which include an experimental comparison among various learning to rank algorithms.△ Less","18 November, 2024;",https://arxiv.org/pdf/2412.03581
PerfCodeGen: Improving Performance of LLM Generated Code with Execution Feedback,['Software Engineering'],"['Yun Peng', 'Akhilesh Deepak Gotmare', 'Michael Lyu', 'Caiming Xiong', 'Silvio Savarese', 'Doyen Sahoo']","Large Language Models (LLMs) are widely adopted for assisting in software development tasks, yet their performance evaluations have narrowly focused on the functional correctness of generated code. Human programmers, however, require LLM-generated code to be not only correct but also optimally efficient. We propose PerfCodeGen, a training-free framework that enhances the performance of LLM-generated code by incorporating feedback based on runtime during test case execution into the self-refinement iterations. With PerfCodeGen, we achieve speedups for a significantly higher proportion of problems compared to using the base LLM with sophisticated prompting techniques. Applied to open language models like Phi-3-mini, PerfCodeGen achieves runtime efficiency comparable to prompting powerful closed models like GPT-4. We achieve state-of-the-art runtime efficiency on benchmarks such as HumanEval, MBPP, and APPS, frequently surpassing the ground truth reference solutions with PerfCodeGen using GPT-3.5 and GPT-4. Additionally, we demonstrate the effectiveness of our approach in enhancing code quality across a range of open LLMs of varying sizes including Phi-3-mini, Llama 3 8B, Mixtral 8x7B, Command R, and Llama 3 70B.△ Less","18 November, 2024;",https://arxiv.org/pdf/2412.03578
Ethical Challenges and Evolving Strategies in the Integration of Artificial Intelligence into Clinical Practice,['Computers and Society'],"['Ellison B. Weiner', 'Irene Dankwa-Mullan', 'William A. Nelson', 'Saeed Hassanpour']","Artificial intelligence (AI) has rapidly transformed various sectors, including healthcare, where it holds the potential to revolutionize clinical practice and improve patient outcomes. However, its integration into medical settings brings significant ethical challenges that need careful consideration. This paper examines the current state of AI in healthcare, focusing on five critical ethical concerns: justice and fairness, transparency, patient consent and confidentiality, accountability, and patient-centered and equitable care. These concerns are particularly pressing as AI systems can perpetuate or even exacerbate existing biases, often resulting from non-representative datasets and opaque model development processes. The paper explores how bias, lack of transparency, and challenges in maintaining patient trust can undermine the effectiveness and fairness of AI applications in healthcare. In addition, we review existing frameworks for the regulation and deployment of AI, identifying gaps that limit the widespread adoption of these systems in a just and equitable manner. Our analysis provides recommendations to address these ethical challenges, emphasizing the need for fairness in algorithm design, transparency in model decision-making, and patient-centered approaches to consent and data privacy. By highlighting the importance of continuous ethical scrutiny and collaboration between AI developers, clinicians, and ethicists, we outline pathways for achieving more responsible and inclusive AI implementation in healthcare. These strategies, if adopted, could enhance both the clinical value of AI and the trustworthiness of AI systems among patients and healthcare professionals, ensuring that these technologies serve all populations equitably.△ Less","17 November, 2024;",https://arxiv.org/pdf/2412.03576
Back-filling Missing Data When Predicting Domestic Electricity Consumption From Smart Meter Data,['Computers and Society'],"['Xianjuan Chen', 'Shuxiang Cai', 'Alan F. Smeaton']","This study uses data from domestic electricity smart meters to estimate annual electricity bills for a whole year. We develop a method for back-filling data smart meter for up to six missing months for users who have less than one year of smart meter data, ensuring reliable estimates of annual consumption. We identify five distinct electricity consumption user profiles for homes based on day, night, and peak usage patterns, highlighting the economic advantages of Time-of-Use (ToU) tariffs over fixed tariffs for most users, especially those with higher nighttime consumption. Ultimately, the results of this study empowers consumers to manage their energy use effectively and to make informed choices regarding electricity tariff plans.△ Less","17 November, 2024;",https://arxiv.org/pdf/2412.03574
Navigation World Models,['Computer Vision and Pattern Recognition'],"['Amir Bar', 'Gaoyue Zhou', 'Danny Tran', 'Trevor Darrell', 'Yann LeCun']","Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03572
The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control,['Artificial Intelligence'],"['Ruili Feng', 'Han Zhang', 'Zhantao Yang', 'Jie Xiao', 'Zhilei Shu', 'Zhiheng Liu', 'Andy Zheng', 'Yukun Huang', 'Yu Liu', 'Hongyang Zhang']","We present The Matrix, the first foundational realistic world simulator capable of generating continuous 720p high-fidelity real-scene video streams with real-time, responsive control in both first- and third-person perspectives, enabling immersive exploration of richly dynamic environments. Trained on limited supervised data from AAA games like Forza Horizon 5 and Cyberpunk 2077, complemented by large-scale unsupervised footage from real-world settings like Tokyo streets, The Matrix allows users to traverse diverse terrains -- deserts, grasslands, water bodies, and urban landscapes -- in continuous, uncut hour-long sequences. Operating at 16 FPS, the system supports real-time interactivity and demonstrates zero-shot generalization, translating virtual game environments to real-world contexts where collecting continuous movement data is often infeasible. For example, The Matrix can simulate a BMW X3 driving through an office setting--an environment present in neither gaming data nor real-world sources. This approach showcases the potential of AAA game data to advance robust world models, bridging the gap between simulations and real-world applications in scenarios with limited data.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03568
Streaming Detection of Queried Event Start,['Computer Vision and Pattern Recognition'],"['Cristobal Eyzaguirre', 'Eric Tang', 'Shyamal Buch', 'Adrien Gaidon', 'Jiajun Wu', 'Juan Carlos Niebles']","Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding-Streaming Detection of Queried Event Start (SDQES). The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting. Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling. We evaluate three vision-language backbones and three adapter architectures on both short-clip and untrimmed video settings.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03567
Characterizing the Distinguishability of Product Distributions through Multicalibration,['Cryptography and Security'],"['Cassandra Marcussen', 'Aaron L. Putterman', 'Salil Vadhan']","Given a sequence of samples $x_1, \dots , x_k$ promised to be drawn from one of two distributions $X_0, X_1$, a well-studied problem in statistics is to decide $\textit{which}$ distribution the samples are from. Information theoretically, the maximum advantage in distinguishing the two distributions given $k$ samples is captured by the total variation distance between $X_0^{\otimes k}$ and $X_1^{\otimes k}$. However, when we restrict our attention to $\textit{efficient distinguishers}$ (i.e., small circuits) of these two distributions, exactly characterizing the ability to distinguish $X_0^{\otimes k}$ and $X_1^{\otimes k}$ is more involved and less understood.
  In this work, we give a general way to reduce bounds on the computational indistinguishability of $X_0$ and $X_1$ to bounds on the $\textit{information-theoretic}$ indistinguishability of some specific, related variables $\widetilde{X}_0$ and $\widetilde{X}_1$. As a consequence, we prove a new, tight characterization of the number of samples $k$ needed to efficiently distinguish $X_0^{\otimes k}$ and $X_1^{\otimes k}$ with constant advantage as
  \[
  k = Θ\left(d_H^{-2}\left(\widetilde{X}_0, \widetilde{X}_1\right)\right),
  \] which is the inverse of the squared Hellinger distance $d_H$ between two distributions $\widetilde{X}_0$ and $\widetilde{X}_1$ that are computationally indistinguishable from $X_0$ and $X_1$. Likewise, our framework can be used to re-derive a result of Geier (TCC 2022), proving nearly-tight bounds on how computational indistinguishability scales with the number of samples for arbitrary product distributions.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03562
Best-of-N Jailbreaking,['Computation and Language'],"['John Hughes', 'Sara Price', 'Aengus Lynch', 'Rylan Schaeffer', 'Fazl Barez', 'Sanmi Koyejo', 'Henry Sleight', 'Erik Jones', 'Ethan Perez', 'Mrinank Sharma']","We introduce Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that jailbreaks frontier AI systems across modalities. BoN Jailbreaking works by repeatedly sampling variations of a prompt with a combination of augmentations - such as random shuffling or capitalization for textual prompts - until a harmful response is elicited. We find that BoN Jailbreaking achieves high attack success rates (ASRs) on closed-source language models, such as 89% on GPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts. Further, it is similarly effective at circumventing state-of-the-art open-source defenses like circuit breakers. BoN also seamlessly extends to other modalities: it jailbreaks vision language models (VLMs) such as GPT-4o and audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific augmentations. BoN reliably improves when we sample more augmented prompts. Across all modalities, ASR, as a function of the number of samples (N), empirically follows power-law-like behavior for many orders of magnitude. BoN Jailbreaking can also be composed with other black-box algorithms for even more effective attacks - combining BoN with an optimized prefix attack achieves up to a 35% increase in ASR. Overall, our work indicates that, despite their capability, language models are sensitive to seemingly innocuous changes to inputs, which attackers can exploit across modalities.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03556
PaliGemma 2: A Family of Versatile VLMs for Transfer,['Computer Vision and Pattern Recognition'],"['Andreas Steiner', 'André Susano Pinto', 'Michael Tschannen', 'Daniel Keysers', 'Xiao Wang', 'Yonatan Bitton', 'Alexey Gritsenko', 'Matthias Minderer', 'Anthony Sherbondy', 'Shangbang Long', 'Siyang Qin', 'Reeve Ingle', 'Emanuele Bugliarello', 'Sahar Kazemzadeh', 'Thomas Mesnard', 'Ibrahim Alabdulmohsin', 'Lucas Beyer', 'Xiaohua Zhai']","PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03555
BinSparX: Sparsified Binary Neural Networks for Reduced Hardware Non-Idealities in Xbar Arrays,['Hardware Architecture'],"['Akul Malhotra', 'Sumeet Kumar Gupta']","Compute-in-memory (CiM)-based binary neural network (CiM-BNN) accelerators marry the benefits of CiM and ultra-low precision quantization, making them highly suitable for edge computing. However, CiM-enabled crossbar (Xbar) arrays are plagued with hardware non-idealities like parasitic resistances and device non-linearities that impair inference accuracy, especially in scaled technologies. In this work, we first analyze the impact of Xbar non-idealities on the inference accuracy of various CiM-BNNs, establishing that the unique properties of CiM-BNNs make them more prone to hardware non-idealities compared to higher precision deep neural networks (DNNs). To address this issue, we propose BinSparX, a training-free technique that mitigates non-idealities in CiM-BNNs. BinSparX utilizes the distinct attributes of BNNs to reduce the average current generated during the CiM operations in Xbar arrays. This is achieved by statically and dynamically sparsifying the BNN weights and activations, respectively (which, in the context of BNNs, is defined as reducing the number of +1 weights and activations). This minimizes the IR drops across the parasitic resistances, drastically mitigating their impact on inference accuracy. To evaluate our technique, we conduct experiments on ResNet-18 and VGG-small CiM-BNNs designed at the 7nm technology node using 8T-SRAM and 1T-1ReRAM. Our results show that BinSparX is highly effective in alleviating the impact of non-idealities, recouping the inference accuracy to near-ideal (software) levels in some cases and providing accuracy boost of up to 77.25%. These benefits are accompanied by energy reduction, albeit at the cost of mild latency/area increase.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03553
Teaching an Old Dog New Tricks: Verifiable FHE Using Commodity Hardware,['Cryptography and Security'],"['Jules Drean', 'Fisher Jepsen', 'Edward Suh', 'Srini Devadas', 'Aamer Jaleel', 'Gururaj Saileshwar']","We present Argos, a simple approach for adding verifiability to fully homomorphic encryption (FHE) schemes using trusted hardware. Traditional approaches to verifiable FHE require expensive cryptographic proofs, which incur an overhead of up to seven orders of magnitude on top of FHE, making them impractical.
  With Argos, we show that trusted hardware can be securely used to provide verifiability for FHE computations, with minimal overhead relative to the baseline FHE computation. An important contribution of Argos is showing that the major security pitfall associated with trusted hardware, microarchitectural side channels, can be completely mitigated by excluding any secrets from the CPU and the memory hierarchy. This is made possible by focusing on building a platform that only enforces program and data integrity and not confidentiality (which is sufficient for verifiable FHE, since all data remain encrypted at all times). All secrets related to the attestation mechanism are kept in a separate coprocessor (e.g., a TPM) inaccessible to any software-based attacker. Relying on a discrete TPM typically incurs significant performance overhead, which is why (insecure) software-based TPMs are used in practice. As a second contribution, we show that for FHE applications, the attestation protocol can be adapted to only incur a fixed cost.
  Argos requires no dedicated hardware extensions and is supported on commodity processors from 2008 onward. Our prototype implementation introduces 6% overhead to the FHE evaluation, and 8% for more complex protocols. In particular, we show that Argos can be adapted for real-world applications of FHE, such as PIR and PSI. By demonstrating how to combine cryptography with trusted hardware, Argos paves the way for widespread deployment of FHE-based protocols beyond the semi-honest setting, without the overhead of cryptographic proofs.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03550
Quantifying the Critical Micelle Concentration of Nonionic and Ionic Surfactants by Self-Consistent Field Theory,['Soft Condensed Matter'],"['Chao Duan', 'Mu Wang', 'Ahmad Ghobadi', 'David M. Eike', 'Rui Wang']","Quantifying the critical micelle concentration (CMC) and understanding its relationship with both the intrinsic molecular structures and environmental conditions are crucial for the rational design of surfactants. Here, we develop a self-consistent field theory which unifies the study of CMC, micellar structure and kinetic pathway of micellization in one framework. The long-range electrostatic interactions are accurately treated, which not only makes the theory applicable to both nonionic and ionic surfactants but also enables us to capture a variety of salt effects. The effectiveness and versatility of the theory is verified by applying it to three types of commonly used surfactants. For polyoxyethylene alkyl ethers (C$_m$E$_n$) surfactants, we predict a wide span of CMC from $10^{-6}$ to $10^{-2}$M as the composition parameters $m$ and $n$ are adjusted. For the ionic sodium dodecyl sulfate (SDS) surfactant, we show the decrease of CMC as salt concentration increases, and capture both the specific cation effect and the specific anion effect. Furthermore, for sodium lauryl ether sulfate (SLES) surfactants, we find a non-monotonic dependence of both the CMC and micelle size on the number of oxyethylene groups. Our theoretical predictions of CMC are in quantitative agreement with experimental data reported in literature for all the three types of surfactants.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03549
Kibble-Zurek Dynamics & Statistics of Topological Defects in Chiral Superfluid $^3$He Films,['Superconductivity'],"['Noble Gluscevich', 'J. A. Sauls']","In equilibrium, confined films of superfluid $^3$He-A have the chiral axis, $\hat{\ell}$, locked normal to the surface of the film. There are two degenerate ground states $\hat{\ell}\;||\pm\hat{z}$. However, for a temperature quench, i.e. cool down through the phase transition at a finite rate, causally disconnected regions of order parameter fluctuations develop and evolve into an inhomogeneous ordered phase that hosts both domain walls between time-reversed chiral phases as well as vortices with winding numbers $p\in\mathbb{Z}$. We present simulations based on a time-dependent generalization of Ginzburg-Landau theory for strong-coupling $^3$He that reveal both types of topological defects to be present following the temperature quench. Results for the dynamics of vortices interacting with anti-vortices as well as domain walls are presented. The vortex number density as a function of quench rate agrees well with the scaling predicted by Kibble and Zurek. We also present results for the number distribution and compare with other theoretical models for full counting statistics of the topological defect density. Finally, we present results for an asymmetry in the post-freeze-out populations of inequivalent vortex core structures that are characteristic of a chiral superfluid.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03544
Dielectric tensor of perovskite oxides at finite temperature using equivariant graph neural network potentials,['Materials Science'],"['Alex Kutana', 'Koki Yoshimochi', 'Ryoji Asahi']","Atomistic simulations of properties of materials at finite temperatures are computationally demanding and require models that are more efficient than the ab initio approaches. Machine learning (ML) and artificial intelligence (AI) address this issue by enabling accurate models with close to ab initio accuracy. Here, we demonstrate the utility of ML models in capturing properties of realistic materials by performing finite temperature molecular dynamics simulations of perovskite oxides using a force field based on equivariant graph neural networks. The models demonstrate efficient learning from a small training dataset of energies, forces, stresses, and tensors of Born effective charges. We qualitatively capture the temperature dependence of the dielectric tensor and structural phase transitions in calcium titanate.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03541
On unique continuation in measure for fractional heat equations,['Analysis of PDEs'],"['Agnid Banerjee', 'Nicola Garofalo']","We prove a theorem of unique continuation in measure for nonlocal equations of the type $(\partial_t - Δ)^s u= V(x,t) u$, for $0<s <1$. Our main result, Theorem 1.1, establishes a delicate nonlocal counterpart of the unique continuation in measure for the local case $s=1$.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03536
Angular Momentum Drain: Despinning Embedded Planetesimals,['Earth and Planetary Astrophysics'],"['Stephen Luniewski', 'Maggie Ju', 'A. C. Quillen', 'Adam E. Rubinstein']","Young and forming planetesimals experience impacts from particles present in a protostellar disk. Using crater scaling laws, we integrate ejecta distributions for oblique impacts. For impacts at 10 to 65 m/s, expected for impacts associated with a disk wind, we estimate the erosion rate and torque exerted on the planetesimal. We find that the mechanism for angular momentum drain proposed by Dobrovolskis and Burns (1984) for asteroids could operate in the low velocity regime of a disk wind. Though spin-down associated with impacts can facilitate planetesimal collapse, we find that the process is inefficient. We find that angular momentum drain via impacts operates in the gravitational focusing regime, though even less efficiently than for lower mass planetesimals. The angular momentum transfer is most effective when the wind speed is low, the projectile density is high compared to the bulk planetesimal density, and the planetesimal is composed of low-strength material. Due to its inefficiency, we find that angular momentum drain due to impacts within a pebble cloud does not by itself facilitate collapse of single planetesimals.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03533
Universal projection theorems with applications to multifractal analysis and the dimension of every ergodic measure on self-conformal sets simultaneously,['Dynamical Systems'],"['Balázs Bárány', 'Károly Simon', 'Adam Śpiewak']","We prove a universal projection theorem, giving conditions on a parametrized family of maps $Π_λ: X \to \mathbb{R}^d$ and a collection $M$ of measures on $X$ under which for almost every $λ$ equality $\mathrm{dim}_H Π_λμ= \min\{d, \mathrm{dim}_H μ\}$ holds for all measures $μ\in M$ simultaneously (i.e. on a full-measure set of $λ$'s independent of $μ$). We require $Π_λ$ to satisfy a transversality condition and $M$ to satisfy a new condition called relative dimension separability. We also prove that if the Assouad dimension of $X$ is smaller than d, then for almost every $λ$, projection $Π_λ$ is nearly bi-Lipschitz at $μ$-a.e. $x$, for all measures $μ\in M$ simultaneously. Our setting can include families of orthogonal projections, natural projections for conformal, non-autonomous or random iterated functions systems.
  As an application, we prove that for a parametrized family of contracting conformal IFS with natural projections $Π_λ$ satisfying the transversality condition, for almost every parameter $λ$ one has $\mathrm{dim}_H Π_λμ= \min\{ d, \frac{h(μ)}{χ(λ, μ)} \}$ for all ergodic shift-invariant measures simultaneously.
  We also prove that for self-similar systems on the line with similarity dimension smaller than one, for Lebesgue almost every choice of translations the multifractal formalism holds simultaneously on the full spectrum interval $\left[\min \frac{\log p_i}{\log|λ_i|},\max \frac{\log p_i}{\log|λ_i|}\right]$ for every self-similar measure.
  We prove that the dimension part of the Marstrand-Mattila projection theorem holds simultaneously for the collection of all ergodic measures on a self-conformal set with the strong separation condition and without any separation for the collection of all Gibbs measures.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03529
The R.O.A.D. to clinical trial emulation,['Applications'],"['Dimitris Bertsimas', 'Angelos G. Koulouras', 'Hiroshi Nagata', 'Carol Gao', 'Junki Mizusawa', 'Yukihide Kanemitsu', 'Georgios Antonios Margonis']","Observational studies provide the only evidence on the effectiveness of interventions when randomized controlled trials (RCTs) are impractical due to cost, ethical concerns, or time constraints. While many methodologies aim to draw causal inferences from observational data, there is a growing trend to model observational study designs after RCTs, a strategy known as ""target trial emulation."" Despite its potential, causal inference through target trial emulation cannot fully address the confounding bias in real-world data due to the lack of randomization. In this work, we present a novel framework for target trial emulation that aims to overcome several key limitations, including confounding bias. The framework proceeds as follows: First, we apply the eligibility criteria of a specific trial to an observational cohort. We then ""correct"" this cohort by extracting a subset that matches both the distribution of covariates and the baseline prognosis of the control group in the target RCT. Next, we address unmeasured confounding by adjusting the prognosis estimates of the treated group to align with those observed in the trial. Following trial emulation, we go a step further by leveraging the emulated cohort to train optimal decision trees, to identify subgroups of patients with heterogeneity in treatment effects (HTE). The absence of confounding is verified using two external models, and the validity of the treatment recommendations is independently confirmed by the team responsible for the original trial we emulate. To our knowledge, this is the first framework to successfully address both observed and unobserved confounding, a challenge that has historically limited the use of randomized trial emulation and causal inference. Additionally, our framework holds promise in advancing precision medicine by identifying patient subgroups that benefit most from specific treatments.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03528
Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos,['Computer Vision and Pattern Recognition'],"['Hanxue Liang', 'Jiawei Ren', 'Ashkan Mirzaei', 'Antonio Torralba', 'Ziwei Liu', 'Igor Gilitschenski', 'Sanja Fidler', 'Cengiz Oztireli', 'Huan Ling', 'Zan Gojcic', 'Jiahui Huang']","Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis. However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content. We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes. Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames. Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets. Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets, even compared with optimization-based approaches.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03526
Gapped commensurate antiferromagnetic response in a strongly underdoped model cuprate superconductor,['Superconductivity'],"['Zachary W. Anderson', 'Yang Tang', 'Vikram Nagarajan', 'Mun K. Chan', 'Chelsey J. Dorow', 'Guichuan Yu', 'Douglas L. Abernathy', 'Andrew D. Christianson', 'Lucile Mangin-Thro', 'Paul Steffens', 'Tyler Sterling', 'Dmitry Reznik', 'Dalila Bounoua', 'Yvan Sidis', 'Philippe Bourges', 'Martin Greven']","It is a distinct possibility that spin fluctuations are the pairing interactions in a wide range of unconventional superconductors. In the case of the high-transition-temperature (high-$T_c$) cuprates, in which superconductivity emerges upon doping an antiferromagnetic Mott-insulating state, spin correlations might furthermore drive unusual pseudogap phenomena. Here we use polarized and unpolarized magnetic neutron scattering to study the simple tetragonal cuprate $\mathrm{HgBa}_{2}\mathrm{CuO}_{4+δ}$ at very low doping ($T_c \approx 55$ K, hole concentration $p \approx 0.064$). In stark contrast to prior results for other underdoped cuprates, we find no evidence of incommensurate spin-density-wave, charge-spin stripe, or $q = 0$ magnetic order. Instead, the antiferromagnetic response in both the superconducting and pseudogap states is gapped below $Δ_\mathrm{AF} \approx 6$ meV, commensurate over a wide energy range, and disperses above about 55 meV. Given the documented model nature of $\mathrm{HgBa}_{2}\mathrm{CuO}_{4+δ}$, which exhibits high structural symmetry and minimal point disorder effects, we conclude that the observed behavior signifies the unmasked response of the quintessential $\mathrm{CuO}_{2}$ planes near the Mott-insulating state. These results for $\mathrm{HgBa}_{2}\mathrm{CuO}_{4+δ}$ can therefore be expected to serve as a benchmark for a refined theoretical understanding of the cuprates.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03524
"Microwave Remote Sensing of Soil Moisture, Above Ground Biomass and Freeze-Thaw Dynamic: Modeling and Empirical Approaches",['Numerical Analysis'],"['Laura Angeloni', 'Domenico Daniele Bloisi', 'Paolo Burghignoli', 'Davide Comite', 'Danilo Costarelli', 'Michele Piconi', 'Anna Rita Sambucini', 'Alessio Troiani', 'Alessandro Veneri']","Human actions have accelerated changes in global temperature, precipitation patterns, and other critical Earth systems. Key markers of these changes can be linked to the dynamic of Essential Climate Variables (ECVs) and related quantities, such as Soil Moisture (SM), Above Ground Biomass (AGB), and Freeze-Thaw (FT) Dynamics. These variables are crucial for understanding global climate changes, hydrological and carbon cycles included. Monitoring these variables helps to validate climate models and inform policy decisions. Technologies like microwave remote sensing provide critical tools for monitoring the effects of human activities on these variables at a global scale. Other than proper tachenological developments, the study of ECVs requires suitable theoretical retrieval tools, which leads to the solutions of inverse problems. In this brief survey, we analyze and summarize the main retrieval techniques available in the literature for SM, AGB, and FT, performed on data collected with microwave remote sensing sensors. Such methods will be some of the fundamental algorithms that can find applications in the research activities of the interdisciplinary, curiosity-driven, project {\it REmote sensing daTa INversion with multivariate functional modeling for essential climAte variables characterization (RETINA)}, recently funded by the European Union under the Italian National Recovery and Resilience Plan of NextGenerationEU, under the Italian Ministry of University and Research. The main goal of RETINA, in which three research units from three different italian universities are involved, is to create innovative techniques for analyzing data generated by the interaction of electromagnetic waves with the Earth's surface, applying theoretical insights to address real-world challenges.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03523
"Rusanov-type Schemes for Hyperbolic Equations: Wave-Speed Estimates, Monotonicity and Stability",['Numerical Analysis'],"['Eleuterio F. Toro', 'Svetlana A. Tokareva']","HLL-type schemes constitute a large hierarchy of numerical methods, in the finite volume and discontinuous Galerkin finite element frameworks, for solving hyperbolic equations. The hierarchy of fluxes includes Rusanov schemes, HLL schemes, HLLC schemes, and other variations. All of these schemes rely on wave speed estimates. Recent work has shown that most wave speed estimates in current use underestimate the true wave speeds. In the present paper we carry out a theoretical study of the consequences arising from errors in the wave speed estimates, on the monotonicity and stability properties of the derived schemes. For the simplest case of the hierarchy, that is Rusanov-type schemes, we carry out a detailed analysis in terms of the linear advection equation in one and two space dimensions. It is found that errors from underestimates of the wave speed could cause loss of monotonicity, severe reduction of the stability limit, and even loss of stability. Errors from overestimates, though preserving monotonicity, will cause a reduction of the stability limit. We find that overestimation is preferable to underestimation, for two reasons. First, schemes from overestimation are monotone, and second, their stability regions are larger than those from underestimation, for equivalent displacements from the exact speed. The findings of this paper may prove useful in raising awareness of the potential pitfalls of a seemingly simple practical computational task, that of providing wave speed estimates. Our reported findings may also motivate subsequent studies for complex non-linear hyperbolic systems, requiring estimates for two or more waves, such as in HLL and HLLC schemes.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03522
Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion,['Computer Vision and Pattern Recognition'],"['Shengyuan Zhang', 'An Zhao', 'Ling Yang', 'Zejian Li', 'Chenye Meng', 'Haoran Xu', 'Tianrun Chen', 'AnYang Wei', 'Perry Pengyun GU', 'Lingyun Sun']","Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed $\textbf{ScoreLiDAR}$, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel $\textbf{Structural Loss}$, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame ($>$5$\times$) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03515
Adaptive Personalized Over-the-Air Federated Learning with Reflecting Intelligent Surfaces,['Information Theory'],"['Jiayu Mao', 'Aylin Yener']","Over-the-air federated learning (OTA-FL) unifies communication and model aggregation by leveraging the inherent superposition property of the wireless medium. This strategy can enable scalable and bandwidth-efficient learning via simultaneous transmission of model updates using the same frequency resources, if care is exercised to design the physical layer jointly with learning. In this paper, a federated learning system facilitated by a heterogeneous edge-intelligent network is considered. The edge users (clients) have differing user resources and non-i.i.d. local dataset distributions. A general non-convex learning objective is considered for the model training task(s) at hand. We augment the network with Reconfigurable Intelligent Surfaces (RIS) in order to enhance the learning system. We propose a cross-layer algorithm that jointly assigns communication, computation and learning resources. In particular, we adaptively adjust the number of local steps in conjunction with RIS configuration to boost the learning performance. Our system model considers channel noise and channel estimation errors in both the uplink (model updates) and downlink (global model broadcast), employing dynamic power control for both. We provide the convergence analysis for the proposed algorithms and extend the frameworks to personalized learning. Our experimental results demonstrate that the proposed algorithms outperform the state-of-the-art joint communication and learning baselines.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03514
Near-optimal shattering in the Ising pure p-spin and rarity of solutions returned by stable algorithms,['Probability'],['Ahmed El Alaoui'],"We show that in the Ising pure $p$-spin model of spin glasses, shattering takes place at all inverse temperatures $β\in (\sqrt{(2 \log p)/p}, \sqrt{2\log 2})$ when $p$ is sufficiently large as a function of $β$. Of special interest is the lower boundary of this interval which matches the large $p$ asymptotics of the inverse temperature marking the hypothetical dynamical transition predicted in statistical physics. We show this as a consequence of a `soft' version of the overlap gap property which asserts the existence of a distance gap of points of typical energy from a typical sample from the Gibbs measure. We further show that this latter property implies that stable algorithms seeking to return a point of at least typical energy are confined to an exponentially rare subset of that super-level set, provided that their success probability is not vanishingly small.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03511
Stagnation points at grain contacts generate an elastic flow instability in 3D porous media,['Fluid Dynamics'],"['Emily Y. Chen', 'Christopher A. Browne', 'Simon J. Haward', 'Amy Q. Shen', 'Sujit S. Datta']","Many environmental, energy, and industrial processes involve the flow of polymer solutions in three-dimensional (3D) porous media where fluid is confined to navigate through complex pore space geometries. As polymers are transported through the tortuous pore space, elastic stresses accumulate, leading to the onset of unsteady flow fluctuations above a threshold flow rate. How does pore space geometry influence the development and features of this elastic instability? Here, we address this question by directly imaging polymer solution flow in microfabricated 3D ordered porous media with precisely controlled geometries consisting of simple-cubic (SC) or body-centered cuboid (BC) arrays of spherical grains. In both cases, we find that the flow instability is generated at stagnation points arising at the contacts between grains rather than at the polar upstream/downstream grain surfaces, as is the case for flow around a single grain. The characteristics of the flow instability are strongly dependent on the unit cell geometry: in SC packings, the instability manifests through the formation of time-dependent, fluctuating 3D eddies, whereas in BC packings, it manifests as continual fluctuating 'wobbles' and crossing in the flow pathlines. Despite this difference, we find that characteristics of the transition from steady to unsteady flow with increasing flow rate have commonalities across geometries. Moreover, for both packing geometries, our data indicate that extensional flow-induced polymeric stresses generated by contact-associated stagnation points are the primary contributor to the macroscopic resistance to flow across the entire medium. Altogether, our work highlights the pivotal role of inter-grain contacts -- which are typically idealized as discrete points and therefore overlooked, but are inherent in most natural and engineered media -- in shaping elastic instabilities in porous media.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03510
Signatures of the Shock Interaction as an Additional Power Source in the Nebular Spectra of SN 2023ixf,['High Energy Astrophysical Phenomena'],"['Amit Kumar', 'Raya Dastidar', 'Justyn R. Maund', 'Adam J. Singleton', 'Ning-Chen Sun']","Red supergiants may lose significant mass through steady winds and episodic eruptions in the final 100-1000 years before the core collapses, shaping their circumstellar environment. Interaction between supernova (SN) ejecta and distant circumstellar material (CSM) can generate shocks, which can energize the ejecta and serve as a key power source during the nebular phase of the SN. In the present work, we investigate the nebular spectrum of SN 2023ixf, observed one year post-explosion (at +363 d) with the recently commissioned WEAVE instrument on the 4.2m William Herschel Telescope. This marks the first supernova spectrum captured with WEAVE. In this spectrum, H$α$ exhibits a peculiar evolution, flanked by blueward and redward broad components centred at $\sim\pm 5650\,\mathrm{km\,s^{-1}}$ from the rest velocity of H$α$, which are seen for only a few SNe to date. These features indicate energy deposition from shocks generated by the interaction of ejecta with a CSM expelled nearly 350 $-$ 640 years pre-explosion. Comparisons of the +363 d spectrum with model spectra from the literature, that include varying shock powers, suggest a shock power of at least $\sim 5 \times 10 ^{40}\,\mathrm{erg\,s^{-1}}$ at this epoch. Additionally, analysis of the [O I] doublet, along with other prominent emission lines, provides evidence for clumpiness, dust formation, and asymmetry within the ejecta and/or the surrounding CSM. These emission lines also helped to constrain the oxygen mass ($\approx0.19^{\scriptscriptstyle +0.08}_{\scriptscriptstyle -0.04} M_\odot$), He-core mass ($<3 M_\odot$) and the zero-age main sequence mass ($\lesssim 12 M_\odot$) of the progenitor of SN 2023ixf. The comparison with other Type II SNe highlights SN 2023ixf's unique shock interaction signatures and evidence of dust formation, setting it apart in terms of evolution and dynamics.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03509
On multiplicative recurrence along linear patterns,['Number Theory'],"['Dimitrios Charamaras', 'Andreas Mountakis', 'Konstantinos Tsinas']","In a recent article, Donoso, Le, Moreira and Sun studied sets of recurrence for actions of the multiplicative semigroup $(\mathbb{N}, \times)$ and provided some sufficient conditions for sets of the form $S=\{(an+b)/(cn+d) \colon n \in \mathbb{N} \}$ to be sets of recurrence for such actions. A necessary condition for $S$ to be a set of multiplicative recurrence is that for every completely multiplicative function $f$ taking values on the unit circle, we have that $\liminf_{n \to \infty} |f(an+b)-f(cn+d)|=0.$ In this article, we fully characterize the integer quadruples $(a,b,c,d)$ which satisfy the latter property. Our result generalizes a result of Klurman and Mangerel concerning the pair $(n,n+1)$, as well as some results of Donoso, Le, Moreira and Sun. In addition, we prove that, under the same conditions on $(a,b,c,d)$, the set $S$ is a set of recurrence for finitely generated actions of $(\mathbb{N}, \times)$.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03504
A Space-Time Discontinuous Petrov-Galerkin Finite Element Formulation for a Modified Schrödinger Equation for Laser Pulse Propagation in Waveguides,['Numerical Analysis'],"['Ankit Chakraborty', 'Judit Munoz-Matute', 'Leszek Demkowicz', 'Jake Grosek']","In this article, we propose a modified nonlinear Schrödinger equation for modeling pulse propagation in optical waveguides. The proposed model bifurcates into a system of elliptic and hyperbolic equations depending on waveguide parameters. The proposed model leads to a stable first-order system of equations, distinguishing itself from the canonical nonlinear Schrödinger equation. We have employed the space-time discontinuous Petrov-Galerkin finite element method to discretize the first-order system of equations. We present a stability analysis for both the elliptic and hyperbolic systems of equations and demonstrate the stability of the proposed model through several numerical examples on space-time meshes.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03502
Non-stoichiometric and Subnano-heterogeneous Ln-incorporated UO2: its defect chemistry and thermal oxidation,['Materials Science'],"['Juejing Liu', 'Shinhyo Bang', 'Natalie S. Yaw', 'Sam Karcher', 'Emma C. Kindall', 'Arjen van Veelen', 'Steven Conradson', 'Nicolas Clavier', 'John McCloy', 'Nicolas Dacheux', 'Xiaofeng Guo']","The defect chemistry and thermal oxidation of lanthanide (Ln) incorporated-UO2 are critical for understanding and predicting their behavior as enhanced fuels, mixed oxide (MOX) fuels, spent nuclear fuels (SNF), and particles for safeguard purposes. In this study, we independently controlled the Ln type (Ce4+, Nd3+, and Gd3+) and the preparation condition (reduced and nonreduced) to investigate their correlations to the generated non-equilibrated defects correspondingly. From early to late lanthanides: Ce and U formed close-to-ideal solid solutions in Fm-3m and oxidized to (Ce, U)4O9, Nd and U mixing under the reducing condition formed solid solutions with oxygen vacancies aggregating near Nd, and the mixing of smaller Gd with U resulted in short-range subnano-domain segregations with Ia-3 region embedded in the global Fm-3m matrix. Both trivalent Ln-incorporated UO2 oxidized to a mixture of (Ln, U)4O9 and (Ln, U)3O8. From these signature defect structures resulting from both Ln type and preparation condition, we proposed kinetic model and thermodynamic hypothesis for explaining the oxidation resistance of (Ln, U)O2. Although originated from f-block oxides, the discovery of long-range disorder short-range ordering may be not uncommon in other metal oxide systems, which can strongly influence their functionalities and properties.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03501
A Bidirectional Siamese Recurrent Neural Network for Accurate Gait Recognition Using Body Landmarks,['Computer Vision and Pattern Recognition'],"['Proma Hossain Progga', 'Md. Jobayer Rahman', 'Swapnil Biswas', 'Md. Shakil Ahmed', 'Arif Reza Anwary', 'Swakkhar Shatabda']","Gait recognition is a significant biometric technique for person identification, particularly in scenarios where other physiological biometrics are impractical or ineffective. In this paper, we address the challenges associated with gait recognition and present a novel approach to improve its accuracy and reliability. The proposed method leverages advanced techniques, including sequential gait landmarks obtained through the Mediapipe pose estimation model, Procrustes analysis for alignment, and a Siamese biGRU-dualStack Neural Network architecture for capturing temporal dependencies. Extensive experiments were conducted on large-scale cross-view datasets to demonstrate the effectiveness of the approach, achieving high recognition accuracy compared to other models. The model demonstrated accuracies of 95.7%, 94.44%, 87.71%, and 86.6% on CASIA-B, SZU RGB-D, OU-MVLP, and Gait3D datasets respectively. The results highlight the potential applications of the proposed method in various practical domains, indicating its significant contribution to the field of gait recognition.△ Less",v1,https://arxiv.org/pdf/2412.03498
Few-fermion resonant tunneling and underbarrier trapping in asymmetric potentials,['Quantum Physics'],"['Elvira Bilokon', 'Valeriia Bilokon', 'Dusty R. Lindberg', 'Andrii Sotnikov', 'Lev Kaplan', 'Denys I. Bondar']","We investigate the tunneling dynamics of few-fermion systems in lattices under asymmetric external potentials - a setup realizable in experiments with ultracold atoms in optical lattices. We first prove that noninteracting fermions exhibit symmetric tunneling probabilities regardless of the barrier's orientation. Then, we demonstrate that inter-particle interactions break this symmetry and lead to pronounced asymmetric tunneling. Remarkably, such a simple system exhibits an unexpectedly diverse range of dynamical behaviors, offering insights into the interplay among fermion-fermion interactions, barrier asymmetry, and spin configurations. We explore the dependence of tunneling behavior on the initial spin configurations: spin-singlet states preserve tunneling symmetry, while spin-triplet states show strong asymmetry. We identify regimes where interactions mediate tunneling through under-barrier resonant trapping and enhance tunneling via many-body resonant tunneling - a phenomenon arising solely from inter-particle interactions and fundamentally different from traditional single-particle resonant tunneling. Our results may be applied to the design of nanoscale devices with tailored transport properties, such as diodes and memristors.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03495
Bayesian approach to equipartition estimation of magnetic field strength,['Instrumentation and Methods for Astrophysics'],"['Adam A. Zychowicz', 'Krzysztof T. Chyży']","Magnetic fields, together with cosmic rays (CRs), play an important role in the dynamics and evolution of galaxies, but are difficult to estimate. Energy equipartition between magnetic fields and CRs provides a convenient way to approximate magnetic field strength from radio observations. We present a new approach for calculating the equipartition magnetic field strength based on Bayesian methods. In this approach, the magnetic field is a random variable that is distributed according to a posterior distribution conditional on synchrotron emission and the size of the emitting region. It allows the direct application of the general formulas for total and polarized synchrotron radiation without the need to invert these formulas, which has limited the equipartition method to highly simplified cases. We have derived the equipartition condition for the case of different low-energy breaks, slopes, and high-energy cutoffs of power law spectra of the CR proton and electron distributions. The derived formalism was applied in the general case of a magnetic field consisting of both uniform and randomly oriented field components. The applied Bayesian approach naturally provides the uncertainties in the estimated magnetic field strengths resulting from the uncertainties in the observables and the assumed values of the unknown physical parameters. In the examples presented, we used two different Markov Chain Monte Carlo methods to generate the posterior distribution of the magnetic field. We have also developed a web application called BMAG that implements the described approach for different models and observational parameters of real sources.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03494
Power of simultaneous X-ray and UV high-resolution spectroscopy for probing AGN outflows,['High Energy Astrophysical Phenomena'],"['Missagh Mehdipour', 'Laura W. Brenneman', 'Jon M. Miller', 'Elisa Costantini', 'Ehud Behar', 'Luigi C. Gallo', 'Jelle S. Kaastra', 'Sibasish Laha', 'Michael A. Nowak']","Black hole accretion in active galactic nuclei (AGN) is coupled to the evolution of their host galaxies. Outflowing winds in AGN can play an important role in this evolution through the resulting feedback mechanism. Multi-wavelength spectroscopy is key for probing the intertwined physics of inflows and outflows in AGN. However, with the current spectrometers, crucial properties of the ionized outflows are poorly understood, such as their coupling to the accretion rate, their launching mechanism, and their kinetic power. In this paper we discuss the need for simultaneous X-ray and UV high-resolution spectroscopy for tackling outstanding questions on these outflows in AGN. The instrumental requirements for achieving the scientific objectives are addressed. We demonstrate that these requirements would be facilitated by the proposed Arcus Probe mission concept. The multi-wavelength spectroscopy and timing by Arcus would enable us to establish the kinematics and ionization structure of the entire ionized outflow, extending from the vicinity of the accretion disk to the outskirts of the host galaxy. Arcus would provide key diagnostics on the origin, driving mechanism, and the energetics of the outflows, which are useful benchmarks for testing various theoretical models of outflows and understanding their impact in AGN.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03493
Direct Evidence of a Highest Wave-Driven Energetic Electron Flux at the Earth's Magnetopause,['Space Physics'],"['Shubhangi Lagad', 'Amar Kakad', 'Bharati Kakad']","Spacecraft observations of high-energy electron flux enhancement up to 125 keV at Earth's magnetopause are typically linked to the magnetic reconnection. Here, we report the first ever observation of prolonged electron flux enhancement reaching very high energy up to 650 keV at magnetopause near the subsolar point, notably without reconnection signatures. The observation reveals that the high-energy electron flux enhancement near the magnetopause is associated with the simultaneous occurrence of electron cyclotron wave harmonics, whistler waves, and Langmuir waves. Theoretical modeling confirms the wave generation by electron temperature anisotropy and the electron loss cone distribution. This direct, in-situ observation of high-energy electron flux linked to cascaded wave-particle interactions will help us advance our understanding about the tiniest electron-scale intricacies affecting near-Earth space weather.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03492
Beyond algorithm hyperparameters: on preprocessing hyperparameters and associated pitfalls in machine learning applications,['Machine Learning'],"['Christina Sauer', 'Anne-Laure Boulesteix', 'Luzia Hanßum', 'Farina Hodiamont', 'Claudia Bausewein', 'Theresa Ullmann']","Adequately generating and evaluating prediction models based on supervised machine learning (ML) is often challenging, especially for less experienced users in applied research areas. Special attention is required in settings where the model generation process involves hyperparameter tuning, i.e. data-driven optimization of different types of hyperparameters to improve the predictive performance of the resulting model. Discussions about tuning typically focus on the hyperparameters of the ML algorithm (e.g., the minimum number of observations in each terminal node for a tree-based algorithm). In this context, it is often neglected that hyperparameters also exist for the preprocessing steps that are applied to the data before it is provided to the algorithm (e.g., how to handle missing feature values in the data). As a consequence, users experimenting with different preprocessing options to improve model performance may be unaware that this constitutes a form of hyperparameter tuning - albeit informal and unsystematic - and thus may fail to report or account for this optimization. To illuminate this issue, this paper reviews and empirically illustrates different procedures for generating and evaluating prediction models, explicitly addressing the different ways algorithm and preprocessing hyperparameters are typically handled by applied ML users. By highlighting potential pitfalls, especially those that may lead to exaggerated performance claims, this review aims to further improve the quality of predictive modeling in ML applications.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03491
Data Fusion of Semantic and Depth Information in the Context of Object Detection,['Computer Vision and Pattern Recognition'],"['Md Abu Yusuf', 'Md Rezaul Karim Khan', 'Partha Pratim Saha', 'Mohammed Mahbubur Rahaman']","Considerable study has already been conducted regarding autonomous driving in modern era. An autonomous driving system must be extremely good at detecting objects surrounding the car to ensure safety. In this paper, classification, and estimation of an object's (pedestrian) position (concerning an ego 3D coordinate system) are studied and the distance between the ego vehicle and the object in the context of autonomous driving is measured. To classify the object, faster Region-based Convolution Neural Network (R-CNN) with inception v2 is utilized. First, a network is trained with customized dataset to estimate the reference position of objects as well as the distance from the vehicle. From camera calibration to computing the distance, cutting-edge technologies of computer vision algorithms in a series of processes are applied to generate a 3D reference point of the region of interest. The foremost step in this process is generating a disparity map using the concept of stereo vision.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03490
Galvanic Body-Coupled Powering for Wireless Implanted Neurostimulators,['Signal Processing'],"['Asif Iftekhar Omi', 'Emma Farina', 'Anyu Jiang', 'Adam Khalifa', 'Shriya Srinivasan', 'Baibhab Chatterjee']","Body-coupled powering (BCP) is an innovative wireless power transfer (WPT) technique, recently explored for its potential to deliver power to cutting-edge biomedical implants such as nerve and muscle stimulators. This paper demonstrates the efficient technique of designing WPT systems embedding BCP via galvanic coupling (G-BCP). The G-BCP configuration utilizes two metal circular rings surrounding the body area of interest as the transmitter (TX) electrodes required for galvanic (differential) excitation and a wireless implant as the receiver (RX) equipped with two electrodes for differential power reception accordingly. By focusing on the unique advantages of this approach - such as enhanced targeting accuracy, improved power transfer efficiency (PTE), and favorable tissue penetration characteristics, G-BCP emerges as a superior alternative to traditional WPT methods. A comprehensive analysis is conducted to obtain the optimized device parameters while simultaneously allowing flexible placement of implants at different depths and alignments. To substantiate the proposed design concept, a prototype was simulated in Ansys HFSS, employing a multi-layered tissue medium of 10mm radius and targeting the sciatic nerve of a rat. Impressively, this prototype achieves > 20% PTE at 1.25 GHz, with the implant (radius of RX electrodes = 1 mm) located 2 mm deep inside the tissue model having complex load impedance of Rload = 1000 Ohm and Cload = 5pF. Therefore, the G-BCP-based wirelessly powered microdevices are envisaged to be a key enabler in neural recording and stimulation, specifically for the peripheral nervous system, enhancing therapeutic outcomes and patient experiences.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03488
Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective,['Machine Learning'],"['Neta Shaul', 'Itai Gat', 'Marton Havasi', 'Daniel Severo', 'Anuroop Sriram', 'Peter Holderrieth', 'Brian Karrer', 'Yaron Lipman', 'Ricky T. Q. Chen']","The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction. In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case. We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03487
Tight PAC-Bayesian Risk Certificates for Contrastive Learning,['Machine Learning'],"['Anna Van Elst', 'Debarghya Ghoshdastidar']","Contrastive representation learning is a modern paradigm for learning representations of unlabeled data via augmentations -- precisely, contrastive models learn to embed semantically similar pairs of samples (positive pairs) closer than independently drawn samples (negative samples). In spite of its empirical success and widespread use in foundation models, statistical theory for contrastive learning remains less explored. Recent works have developed generalization error bounds for contrastive losses, but the resulting risk certificates are either vacuous (certificates based on Rademacher complexity or $f$-divergence) or require strong assumptions about samples that are unreasonable in practice. The present paper develops non-vacuous PAC-Bayesian risk certificates for contrastive representation learning, considering the practical considerations of the popular SimCLR framework. Notably, we take into account that SimCLR reuses positive pairs of augmented data as negative samples for other data, thereby inducing strong dependence and making classical PAC or PAC-Bayesian bounds inapplicable. We further refine existing bounds on the downstream classification loss by incorporating SimCLR-specific factors, including data augmentation and temperature scaling, and derive risk certificates for the contrastive zero-one risk. The resulting bounds for contrastive loss and downstream prediction are much tighter than those of previous risk certificates, as demonstrated by experiments on CIFAR-10.△ Less",v1,https://arxiv.org/pdf/2412.03486
Accelerating HI density predictions during the Epoch of Reionization using a GPR-based emulator on N-body simulations,['Cosmology and Nongalactic Astrophysics'],"['Gaurav Pundir', 'Aseem Paranjape', 'Tirthankar Roy Choudhury']","Building fast and accurate ways to model the distribution of neutral hydrogen during the Epoch of Reionization (EoR) is essential for interpreting upcoming 21 cm observations. A key component of semi-numerical models of reionization is the collapse fraction field $f_{\text{coll}}(\mathbf{x})$, which represents the fraction of mass within dark matter halos at each location. Using high-dynamic range N-body simulations to obtain this is computationally prohibitive and semi-analytical approaches, while being fast, end up compromising on accuracy. In this work, we bridge the gap by developing a machine learning model that can generate $f_{\text{coll}}$ maps by sampling from the full distribution of $f_{\text{coll}}$ conditioned on the dark matter density contrast $δ$. The conditional distribution functions and the input density field to the model are taken from low-dynamic range N-body simulations that are more efficient to run. We evaluate the performance of our ML model by comparing its predictions to a high-dynamic range N-body simulation. Using these $f_{\text{coll}}$ maps, we compute the HI and HII maps through a semi-numerical code for reionization. We are able to recover the large-scale HI density field power spectra $(k \lesssim 1\ h\,{\rm Mpc}^{-1})$ at the $\lesssim10\%$ level, while the HII density field is reproduced with errors well below 10% across all scales. Compared to existing semi-analytical prescriptions, our approach offers significantly improved accuracy in generating the collapse fraction field, providing a robust and efficient alternative for modeling reionization.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03485
The morphology and interface structure of titanium on graphene,['Materials Science'],"['Joachim Dahl Thomsen', 'Wissam A. Saidi', 'Kate Reidy', 'Jatin J. Patil', 'Serin Lee', 'Frances M. Ross', 'Prineha Narang']","Titanium (Ti) is an adhesion and contact metal commonly used in nanoelectronics and two-dimensional (2D) materials research. However, when Ti is deposited on graphene (Gr), we obtain dramatically different film morphology depending on the experimental conditions. Through a combination of transmission electron microscopy, Raman spectroscopy, and ab initio density functional theory calculations, we show that the most critical parameters are the number of Gr layers, the nature of the Gr support, and the deposition temperature. Particularly distinctive is the island morphology and large defect density of Ti on monolayer Gr, compared to bilayer or thicker Gr. We propose that this results from structural and mechanical differences between monolayer and thicker Gr flakes, where monolayer Gr is more flexible, exhibits larger surface roughness and therefore lower Ti diffusivity, and is more easily damaged. Our results highlight the extreme sensitivity of Ti morphology on Gr to processing and substrate conditions, allowing us to propose design rules for controlling Ti-Gr interface properties and morphology and to discuss the implications for other technologically relevant metal deposition processes.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03480
Small kissing polytopes,['Combinatorics'],"['Antoine Deza', 'Zhongyuan Liu', 'Lionel Pournin']","A lattice $(d,k)$-polytope is the convex hull of a set of points in $\mathbb{R}^d$ whose coordinates are integers ranging between $0$ and $k$. We consider the smallest possible distance $\varepsilon(d,k)$ between two disjoint lattice $(d,k)$-polytopes. We propose an algebraic model for this distance and derive from it an explicit formula for $\varepsilon(2,k)$. Our model also allows for the computation of previously intractable values of $\varepsilon(d,k)$. In particular, we compute $\varepsilon(3,k)$ when $4\leq{k}\leq8$, $\varepsilon(4,k)$ when $2\leq{k}\leq3$, and $\varepsilon(6,1)$.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03479
Toric sheaves and polyhedra,['Algebraic Geometry'],"['Klaus Altmann', 'Andreas Hochenegger', 'Frederik Witt']","Over a smooth projective toric variety we study toric sheaves, that is, reflexive sheaves equivariant with respect to the acting torus, from a polyhedral point of view. One application is the explicit construction of the torus invariant universal extension of two nef line bundles via polyhedral inclusion/exclusion sequences.
  Second, we link the cohomology of toric sheaves to the cohomology of certain constructible sheaves explicitly built out of the associated polyhedra. For the latter we define a concrete double complex and a spectral sequence which computes the cohomology of toric sheaves from the reduced cohomology of polyhedral subsets living in the realification of the character lattice of the toric variety.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03476
On the long-time limit of the mean curvature flow in closed manifolds,['Differential Geometry'],"['Alexander Mramor', 'Ao Sun']","In this article we show that generally almost regular flows, introduced by Bamler and Kleiner, in closed 3-manifolds will either go extinct in finite time or flow to a collection of smooth embedded minimal surfaces, possibly with multiplicity. Using a perturbative argument then we construct piecewise almost regular flows which either go extinct in finite time or flow to a stable minimal surface, possibly with multiplicity. We apply these results to construct minimal surfaces in 3-manifolds in a variety of circumstances, mainly novel from the point of the view that the arguments are via parabolic methods.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03475
The Dirac Vacuum in Discrete Spacetime,['Quantum Physics'],"['Chaitanya Gupta', 'Anthony J. Short']","We consider introducing the Dirac sea in a discrete spacetime model of fermions which approximates the Dirac equation in the continuum limit. However, if we attempt to fill up the `negative' energy states, we run into a problem. A new boundary is created between positive and negative energy states, at which pair creation seems energetically favourable. This happens because of the modular nature of energy in discrete time models. We then suggest a possible remedy by amending the model, in order to pull states away from the new boundary.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03466
Validity and efficiency of the conformal CUSUM procedure,['Statistics Theory'],"['Vladimir Vovk', 'Ilia Nouretdinov', 'Alex Gammerman']",In this paper we study the validity and efficiency of a conformal version of the CUSUM procedure for change detection both experimentally and theoretically.△ Less,"4 December, 2024;",https://arxiv.org/pdf/2412.03464
Multi-Momentum Observer Contact Estimation for Bipedal Robots,['Robotics'],"['J. Joe Payne', 'Daniel A. Hagen', 'Denis Garagić', 'Aaron M. Johnson']","As bipedal robots become more and more popular in commercial and industrial settings, the ability to control them with a high degree of reliability is critical. To that end, this paper considers how to accurately estimate which feet are currently in contact with the ground so as to avoid improper control actions that could jeopardize the stability of the robot. Additionally, modern algorithms for estimating the position and orientation of a robot's base frame rely heavily on such contact mode estimates. Dedicated contact sensors on the feet can be used to estimate this contact mode, but these sensors are prone to noise, time delays, damage/yielding from repeated impacts with the ground, and are not available on every robot. To overcome these limitations, we propose a momentum observer based method for contact mode estimation that does not rely on such contact sensors. Often, momentum observers assume that the robot's base frame can be treated as an inertial frame. However, since many humanoids' legs represent a significant portion of the overall mass, the proposed method instead utilizes multiple simultaneous dynamic models. Each of these models assumes a different contact condition. A given contact assumption is then used to constrain the full dynamics in order to avoid assuming that either the body is an inertial frame or that a fully accurate estimate of body velocity is known. The (dis)agreement between each model's estimates and measurements is used to determine which contact mode is most likely using a Markov-style fusion method. The proposed method produces contact detection accuracy of up to 98.44% with a low noise simulation and 77.12% when utilizing data collect on the Sarcos Guardian XO robot (a hybrid humanoid/exoskeleton).△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03462
"CUBES, the Cassegrain U-Band Efficient Spectrograph: towards final design review",['Instrumentation and Methods for Astrophysics'],"['Matteo Genoni', 'Hans Dekker', 'Stefano Covino', 'Roberto Cirami', 'Marcello Agostino Scalera', 'Lawrence Bissel', 'Walter Seifert', 'Ariadna Calcines', 'Gerardo Avila', 'Julian Stuermer', 'Christopher Ritz', 'David Lunney', 'Chris Miller', 'Stephen Watson', 'Chris Waring', 'Bruno Vaz Castilho', 'Marcio De Arruda', 'Orlando Verducci', 'Igor Coretti', 'Luca Oggioni', 'Giorgio Pariani', 'Edoardo Alberto Maria Redaelli', ""Matteo D'Ambrogio"", 'Giorgio Calderone', 'Matteo Porru']","In the era of Extremely Large Telescopes, the current generation of 8-10m facilities are likely to remain competitive at ground-UV wavelengths for the foreseeable future. The Cassegrain U-Band Efficient Spectrograph (CUBES) has been designed to provide high instrumental efficiency ( $>$ 37\%) observations in the near UV (305-400 nm requirement, 300-420 nm goal) at a spectral resolving power of R $>$ 20, 000 (with a lower-resolution, sky-limited mode of R $\sim$ 7, 000). With the design focusing on maximizing the instrument throughput (ensuring a Signal to Noise Ratio -SNR- $\sim$ 20 per spectral resolution element at 313 nm for U $\sim$ 17.5 mag objects in 1h of observations), it will offer new possibilities in many fields of astrophysics: i) access to key lines of stellar spectra (e.g. lighter elements, in particular Beryllium), extragalactic studies (e.g. circumgalactic medium of distant galaxies, cosmic UV background) and follow-up of explosive transients. We present the CUBES instrument design, currently in Phase-C and approaching the final design review, summarizing the hardware architecture and interfaces between the different subsystems as well as the relevant technical requirements. We describe the optical, mechanical, electrical design of the different subsystems (from the telescope adapter and support structure, through the main opto-mechanical path, including calibration unit, detector devices and cryostat control, main control electronics), detailing peculiar instrument functions like the Active Flexure Compensation (AFC). Furthermore, we outline the AITV concept and the main instrument operations giving an overview of its software ecosystem. Installation at the VLT is planned for 2028-2029 and first science operations in late 2029.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03460
Thermodynamic Theory of Linear Optical and Electro-Optic Properties of Ferroelectrics,['Materials Science'],"['Aiden Ross', 'Mohamed S. M. M. Ali', 'Akash Saha', 'Rui Zu', 'Venkatraman Gopalan', 'Ismaila Dabo', 'Long-Qing Chen']","Ferroelectric materials underlie key optical technologies in optical communications, integrated optics and quantum computing. Yet, there is a lack of a consistent thermodynamic framework to predict the optical properties of ferroelectrics and the mutual connections among ferroelectric polarization, optical properties, and optical dispersion. For example, there is no existing thermodynamic model for establishing the relationship between the ferroelectric polarization and the optical properties in the visible spectrum. Here we present a thermodynamic theory of the linear optical and electro-optic properties of ferroelectrics by separating the lattice and electronic contributions to the total polarization. We introduce a biquadratic coupling between the lattice and electronic contributions validated by both first-principles calculations and experimental measurements. As an example, we derive the temperature and wavelength-dependent anisotropic optical properties of BaTiO3, including the full linear optical dielectric tensor and the linear electro-optic (Pockels) effect through multiple ferroelectric phase transitions, which are in excellent agreement with existing experimental data and first principles calculations. This general framework incorporates essentially all optical properties of materials, including coupling between the ionic and electronic order parameters, as well as their dispersion and temperature dependence, and thus offers a powerful theoretical tool for analyzing light-matter interactions in ferroelectrics-based optical devices.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03459
Modeling of pattern formation of the ordered intermediate phases during co-deposition of binary thin film,['Materials Science'],"['Serhii Abakumov', 'Andriy Gusak']","Formation of the intermediate phase patterns in the thin-film co-deposition process is simulated using the Stochastic Kinetic Mean-Field method and Monte Carlo. Three basic morphologies of the 2D sections are distinguished: (1) spots (rod-like in 3D), (2) layered structures-lamellae, zigzags, and labyrinths (plate-like in 3D), and (3) net-like structures (inverse to spot-like structures, when spots become majority and the surrounding matrix becomes a minority). They are characterized and distinguished with the help of only one special topological parameter.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03457
Gesture Classification in Artworks Using Contextual Image Features,['Computer Vision and Pattern Recognition'],"['Azhar Hussian', 'Mathias Zinnen', 'Thi My Hang Tran', 'Andreas Maier', 'Vincent Christlein']",Recognizing gestures in artworks can add a valuable dimension to art understanding and help to acknowledge the role of the sense of smell in cultural heritage. We propose a method to recognize smell gestures in historical artworks. We show that combining local features with global image context improves classification performance notably on different backbones.△ Less,"4 December, 2024;",https://arxiv.org/pdf/2412.03456
Pre-trained Multiple Latent Variable Generative Models are good defenders against Adversarial Attacks,['Computer Vision and Pattern Recognition'],"['Dario Serez', 'Marco Cristani', 'Alessio Del Bue', 'Vittorio Murino', 'Pietro Morerio']","Attackers can deliberately perturb classifiers' input with subtle noise, altering final predictions. Among proposed countermeasures, adversarial purification employs generative networks to preprocess input images, filtering out adversarial noise. In this study, we propose specific generators, defined Multiple Latent Variable Generative Models (MLVGMs), for adversarial purification. These models possess multiple latent variables that naturally disentangle coarse from fine features. Taking advantage of these properties, we autoencode images to maintain class-relevant information, while discarding and re-sampling any detail, including adversarial noise. The procedure is completely training-free, exploring the generalization abilities of pre-trained MLVGMs on the adversarial purification downstream task. Despite the lack of large models, trained on billions of samples, we show that smaller MLVGMs are already competitive with traditional methods, and can be used as foundation models. Official code released at https://github.com/SerezD/gen_adversarial.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03453
On intermediate levels of nested occupancy scheme in random environment generated by stick-breaking: the case of heavy tails,['Probability'],"['Oksana Braganets', 'Alexander Iksanov']","We investigate a nested balls-in-boxes scheme in a random environment. The boxes follow a nested hierarchy, with infinitely many boxes in each level, and the hitting probabilities of boxes are random and obtained by iterated fragmentation of a unit mass. The hitting probabilities of the first-level boxes are given by a stick-breaking model $P_k = W_1 W_2\cdot \ldots\cdot W_{k-1}(1- W_k)$ for $k \in \mathbb{N}$, where $W_1$, $W_2,\ldots$ are independent copies of a random variable $W$ taking values in $(0,1)$. The infinite balls-in-boxes scheme in the first level is known as a Bernoulli sieve. We assume that the mean of $|\log W|$ is infinite and the distribution tail of $|\log W|$ is regularly varying at $\infty$. Denote by $K_n(j)$ the number of occupied boxes in the $j$th level provided that there are $n$ balls and call the level $j$ intermediate, if $j = j_n \to \infty$ and $j_n = o((\log n)^a)$ as $n \to \infty$ for appropriate $a>0$. We prove that, for some intermediate levels $j$, finite-dimensional distributions of the process $(K_n(\lfloor j_n u\rfloor))_{u>0}$, properly normalized, converge weakly as $n\to\infty$ to those of a pathwise Lebesgue-Stieltjes integral, with the integrand being an exponential function and the integrator being an inverse stable subordinator. The present paper continues the line of investigation initiated in the articles Buraczewski, Dovgay and Iksanov (2020) and Iksanov, Marynych and Samoilenko (2022) in which the random variable $|\log W|$ has a finite second moment, and Iksanov, Marynych and Rashytov (2022) in which $|\log W|$ has a finite mean and an infinite second moment.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03450
Consequences of the failure of equipartition for the p-V behavior of liquid water and the hydration free energy components of a small protein,['Soft Condensed Matter'],"['Dilipkumar N. Asthagiri', 'Arjun Valiya Parambathu', 'Thomas L. Beck']","Earlier we showed that in the molecular dynamics simulation of a rigid model of water it is necessary to use an integration time-step $δt \leq 0.5$ fs to ensure equipartition between translational and rotational modes. Here we extend that study in the $NVT$ ensemble to $NpT$ conditions and to an aqueous protein. We study neat liquid water with the rigid, SPC/E model and the protein BBA (PDB ID: 1FME) solvated in the rigid, TIP3P model. We examine integration time-steps ranging from $0.5$ fs to $4.0$ fs for various thermostat plus barostat combinations. We find that a small $δt$ is necessary to ensure consistent prediction of the simulation volume. Hydrogen mass repartitioning alleviates the problem somewhat, but is ineffective for the typical time-step used with this approach. The compressibility, a measure of volume fluctuations, and the dielectric constant, a measure of dipole moment fluctuations, are also seen to be sensitive to $δt$. Using the mean volume estimated from the $NpT$ simulation, we examine the electrostatic and van der Waals contribution to the hydration free energy of the protein in the $NVT$ ensemble. These contributions are also sensitive to $δt$. In going from $δt = 2$ fs to $δt = 0.5$ fs, the change in the net electrostatic plus van der Waals contribution to the hydration of BBA is already in excess of the folding free energy reported for this protein.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03448
State Frequency Estimation for Anomaly Detection,['Machine Learning'],"['Clinton Cao', 'Agathe Blaise', 'Annibale Panichella', 'Sicco Verwer']","Many works have studied the efficacy of state machines for detecting anomalies within NetFlows. These works typically learn a model from unlabeled data and compute anomaly scores for arbitrary traces based on their likelihood of occurrence or how well they fit within the model. However, these methods do not dynamically adapt their scores based on the traces seen at test time. This becomes a problem when an adversary produces seemingly common traces in their attack, causing the model to miss the detection by assigning low anomaly scores. We propose SEQUENT, a new approach that uses the state visit frequency to adapt its scoring for anomaly detection dynamically. SEQUENT subsequently uses the scores to generate root causes for anomalies. These allow the grouping of alarms and simplify the analysis of anomalies. Our evaluation of SEQUENT on three NetFlow datasets indicates that our approach outperforms existing methods, demonstrating its effectiveness in detecting anomalies.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03442
CleanDIFT: Diffusion Features without Noise,['Computer Vision and Pattern Recognition'],"['Nick Stracke', 'Stefan Andreas Baumann', 'Kolja Bauer', 'Frank Fundel', 'Björn Ommer']","Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03439
The Thurston norm of graph manifolds,['Geometric Topology'],['Alessandro V. Cigna'],"The Thurston norm of a closed oriented graph manifold is a sum of absolute values of linear functionals, and either each or none of the top-dimensional faces of its unit ball are fibered. We show that, conversely, every norm that can be written as a sum of absolute values of linear functionals with rational coefficients is the nonvanishing Thurston norm of some graph manifold, with respect to a rational basis on its second real homology. Moreover, we can choose such graph manifold either to fiber over the circle or not. In particular, every symmetric polygon with rational vertices is the unit polygon of the nonvanishing Thurston norm of a graph manifold fibering over the circle. In dimension $\ge 3$ many symmetric polyhedra with rational vertices are not realizable as nonvanishing Thurston norm ball of any graph manifold. However, given such a polyhedron, we show that there is always a graph manifold whose nonvanishing Thurston norm ball induces a finer partition into cones over the faces.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03437
The impact of initial conditions on quasi-normal modes,['General Relativity and Quantum Cosmology'],"['Ameya Chavda', 'Macarena Lagos', 'Lam Hui']","This study investigates the influence of initial conditions on the evolution and properties of linear quasi-normal modes (QNMs). Using a toy model in which the quasi-normal mode can be unambiguously identified, we highlight an aspect of QNMs that is long known yet often ignored: the amplitude of a QNM (after factoring out the corresponding exponential with a complex frequency) is not constant but instead varies with time. We stress that this is true even within the regime of validity of linear perturbation theory. The precise time variation depends on the initial conditions. In particular, it is possible to find initial conditions for which the QNM fails to materialize; it is also possible to find those for which the QNM amplitude grows indefinitely. Focusing on cases where the QNM amplitude does stabilize at late times, we explore how the timescale for amplitude stabilization depends on the shape and location of the initial perturbation profile. Our findings underscore the need for care in fitting linear QNMs to ringdown data. They also suggest recent computations of quadratic QNMs, sourced purely by {\it stabilized} linear QNMs, do not fully capture what determines the amplitude of the quadratic QNMs, even at late times. Our results motivate a detailed investigation of the initial perturbations generated in the aftermath of a binary merger.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03435
BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement,['Robotics'],"['Miguel Arturo Vega Torres', 'Anna Ribic', 'Borja García de Soto', 'André Borrmann']","This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse LiDAR data and camera measurements with pre-existing building information models (BIMs), enhancing fast and accurate indoor mapping with affordable sensors. BIMCaP refines sensor poses by leveraging a 3D BIM and employing a bundle adjustment technique to align real-world measurements with the model. Experiments using real-world open-access data show that BIMCaP achieves superior accuracy, reducing translational error by over 4 cm compared to current state-of-the-art methods. This advancement enhances the accuracy and cost-effectiveness of 3D mapping methodologies like SLAM. BIMCaP's improvements benefit various fields, including construction site management and emergency response, by providing up-to-date, aligned digital maps for better decision-making and productivity. Link to the repository: https://github.com/MigVega/BIMCaP△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03434
Genetic Algorithm Based System for Path Planning with Unmanned Aerial Vehicles Swarms in Cell-Grid Environments,['Robotics'],"['Alejandro Puente-Castro', 'Enrique Fernandez-Blanco', 'Daniel Rivero']","Path Planning methods for autonomously controlling swarms of unmanned aerial vehicles (UAVs) are gaining momentum due to their operational advantages. An increasing number of scenarios now require autonomous control of multiple UAVs, as autonomous operation can significantly reduce labor costs. Additionally, obtaining optimal flight paths can lower energy consumption, thereby extending battery life for other critical operations. Many of these scenarios, however, involve obstacles such as power lines and trees, which complicate Path Planning. This paper presents an evolutionary computation-based system employing genetic algorithms to address this problem in environments with obstacles. The proposed approach aims to ensure complete coverage of areas with fixed obstacles, such as in field exploration tasks, while minimizing flight time regardless of map size or the number of UAVs in the swarm. No specific goal points or prior information beyond the provided map is required. The experiments conducted in this study used five maps of varying sizes and obstacle densities, as well as a control map without obstacles, with different numbers of UAVs. The results demonstrate that this method can determine optimal paths for all UAVs during full map traversal, thus minimizing resource consumption. A comparative analysis with other state-of-the-art approach is presented to highlight the advantages and potential limitations of the proposed method.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03433
Efficient GHz electro-optical modulation with a nonlocal lithium niobate metasurface in the linear and nonlinear regime,['Optics'],"['Agostino Di Francescantonio', 'Alessandra Sabatti', 'Helena Weigand', 'Elise Bailly', 'Maria Antonietta Vincenti', 'Luca Carletti', 'Jost Kellner', 'Attilio Zilli', 'Marco Finazzi', 'Michele Celebrano', 'Rachel Grange']","Electro-optical modulation is widely employed for optical signal processing and in laser technology. To date, it is efficiently realized in integrated photonic systems as well as in bulk optics devices. Yet, the achievement of modulators exploiting Pockels effect in flat optics, essential to scale down the electric radiation-optical control in free space, currently lag behind bulk and on-chip integrated platforms in terms efficiency and speed. We bridge this gap realizing a metasurface based on lithium niobate (LiNbO3) on insulator that leverages on resonances with quality-factor as high as 8e3 to achieve fast electrical modulation of both linear and nonlinear optical properties. LiNbO3, well known for its high nonlinear susceptibility and wide transparency window across the infrared and visible spectrum, is employed to realize an asymmetric, one-dimensional array of nanowires, exhibiting resonances with linewidth < 0.2 nm. By applying a CMOS-compatible electrical bias, the metasurface imparts a relative reflectivity modulation around 0.1, with a modulation efficiency, defined as relative modulation per applied Volt, larger than 0.01 V^-1 on a bandwidth of about 1 GHz. We also demonstrated more than one order of magnitude intensity modulation of the second harmonic seeded by a continuous-wave laser, with a modulation efficiency of about 0.12 V^-1. This dual modulation capability, rooted in the interplay between optical resonances and electric field manipulation, holds significant potential for cutting-edge applications in high-speed photonics, nonlinear optics, and reconfigurable communication systems. Our findings highlight the transformative potential of LiNbO3-based metasurfaces for integration into next-generation optical technologies that demand rapid, efficient electrical control of light.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03422
"Governance as a complex, networked, democratic, satisfiability problem",['Physics and Society'],"['Laurent Hébert-Dufresne', 'Nicholas W. Landry', 'Juniper Lovato', 'Jonathan St-Onge', 'Jean-Gabriel Young', 'Marie-Ève Couture-Ménard', 'Stéphane Bernatchez', 'Catherine Choquette', 'Alan A. Cohen']","Democratic governments comprise a subset of a population whose goal is to produce coherent decisions that solve societal challenges while respecting the will of the people they represent. New governance frameworks represent this problem as a social network rather than as a hierarchical pyramid with centralized authority. But how should this network be structured? To investigate this question, we model the set of decisions a population must make as a satisfiability problem and the structure of information flow involved in decision-making as a social hypergraph. This allows us to consider the benefits of different governance structures, from dictatorships to direct democracy. In between these extremes, we find a regime of effective governance where decision groups are formed as needed by key stakeholders to discuss and make specific decisions. This regime of effective governance allows even incoherent or polarized populations to make coherent decisions at low coordination costs. More broadly, we present not just simulation results, but a modeling framework that can be used to explore the costs and benefits of a wide range of governance strategies using bottom-up approaches and their ability to tackle decision problems that challenge standard governments.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03421
Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic,['Software Engineering'],"['Clinton Cao', 'Annibale Panichella', 'Sicco Verwer']","The rising popularity of the microservice architectural style has led to a growing demand for automated testing approaches tailored to these systems. EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to automatically generate test cases for microservices' REST APIs. One limitation of these EAs is the use of unit-level search heuristics, such as branch distances, which focus on fine-grained code coverage and may not effectively capture the complex, interconnected behaviors characteristic of system-level testing. To address this limitation, we propose a new search heuristic (MISH) that uses real-time automaton learning to guide the test case generation process. We capture the sequential call patterns exhibited by a test case by learning an automaton from the stream of log events outputted by different microservices within the same system. Therefore, MISH learns a representation of the systemwide behavior, allowing us to define the fitness of a test case based on the path it traverses within the inferred automaton. We empirically evaluate MISH's effectiveness on six real-world benchmark microservice applications and compare it against a state-of-the-art technique, MOSA, for testing REST APIs. Our evaluation shows promising results for using MISH to guide the automated test case generation within EvoMaster.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03420
X-ray study of WR 48-6: A possible colliding wind binary,['High Energy Astrophysical Phenomena'],"['Vishal Jadoliya', 'Jeewan C Pandey', 'Anandmayee Tej']","This paper presents an investigation of the X-ray emission associated with the Wolf-Rayet star, WR 48-6, using observations from the XMM Newton and Chandra X-ray telescopes covering two epochs separated by eleven months. The X-ray spectrum of WR 48-6 is well explained by a two-temperature plasma model, with cool and hot plasma temperatures of $0.8_{-0.2}^{\,+0.1}$ and $2.86_{-0.66}^{\,+1.01}$ keV. No significant X-ray variability is observed during these two epochs of observations. However, an increase in the local hydrogen column density accompanied by a decrease in the intrinsic X-ray flux between two epochs of observations is seen. Additionally, the intrinsic X-ray luminosity is found to be more than $10^{33} \rm\,erg\,s^{-1}$ during both epochs of observations. Based on the analysis presented, WR 48-6 is a promising colliding wind binary candidate with a possible companion of spectral type O5-O6.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03419
Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion,['Computer Vision and Pattern Recognition'],"['Andrea Asperti', 'Ali Aydogdu', 'Emanuela Clementi', 'Angelo Greco', 'Lorenzo Mentaschi', 'Fabio Merizzi', 'Pietro Miraglio', 'Paolo Oddo', 'Nadia Pinardi', 'Alessandro Testa']","Sea Surface Temperature (SST) is crucial for understanding Earth's oceans and climate, significantly influencing weather patterns, ocean currents, marine ecosystem health, and the global energy balance. Large-scale SST monitoring relies on satellite infrared radiation detection, but cloud cover presents a major challenge, creating extensive observational gaps and hampering our ability to fully capture large-scale ocean temperature patterns. Efforts to address these gaps in existing L4 datasets have been made, but they often exhibit notable local and seasonal biases, compromising data reliability and accuracy. To tackle this challenge, we employed deep neural networks to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas, using MODIS satellite derived observations of SST. Our best-performing architecture showed significant skill improvements over established methodologies, achieving substantial reductions in error metrics when benchmarked against widely used approaches and datasets. These results underscore the potential of advanced AI techniques to enhance the completeness of satellite observations in Earth-science remote sensing, providing more accurate and reliable datasets for environmental assessments, data-driven model training, climate research, and seamless integration into model data assimilation workflows.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03413
"A dark, bare rock for TOI-1685 b from a JWST NIRSpec G395H phase curve",['Earth and Planetary Astrophysics'],"['Rafael Luque', 'Brandon Park Coy', 'Qiao Xue', 'Adina D. Feinstein', 'Eva-Maria Ahrer', 'Quentin Changeat', 'Michael Zhang', 'Sarah E. Moran', 'Jacob L. Bean', 'Edwin Kite', 'Megan Weiner Mansfield', 'Enric Pallé']","We report JWST NIRSpec/G395H observations of TOI-1685 b, a hot rocky super-Earth orbiting an M2.5V star, during a full orbit. We obtain transmission and emission spectra of the planet and characterize the properties of the phase curve, including its amplitude and offset. The transmission spectrum rules out clear H$_2$-dominated atmospheres, while secondary atmospheres (made of water, methane, or carbon dioxide) cannot be statistically distinguished from a flat line. The emission spectrum is featureless and consistent with a blackbody-like brightness temperature, helping rule out thick atmospheres with high mean molecular weight. Collecting all evidence, the properties of TOI-1685 b are consistent with a blackbody with no heat redistribution and a low albedo, with a dayside brightness temperature 0.98$\pm$0.07 times that of a perfect blackbody in the NIRSpec NRS2 wavelength range (3.823-5.172 um). Our results add to the growing number of seemingly airless M-star rocky planets, thus constraining the location of the ""Cosmic Shoreline"".
  Three independent data reductions have been carried out, all showing a high-amplitude correlated noise component in the white and spectroscopic light curves. The correlated noise properties are different between the NRS1 and NRS2 detectors - importantly the timescales of the strongest components (4.5 hours and 2.5 hours, respectively) - suggesting the noise is from instrumental rather than astrophysical origins. We encourage the community to look into the systematics of NIRSpec for long time-series observations.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03411
PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation,['Computer Vision and Pattern Recognition'],"['Ao Wang', 'Hui Chen', 'Jianchao Tan', 'Kefeng Zhang', 'Xunliang Cai', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding']","Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at \url{https://github.com/THU-MIG/PrefixKV}.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03409
Skel3D: Skeleton Guided Novel View Synthesis,['Computer Vision and Pattern Recognition'],"['Aron Fóthi', 'Bence Fazekas', 'Natabara Máté Gyöngyössy', 'Kristian Fenech']","In this paper, we present an approach for monocular open-set novel view synthesis (NVS) that leverages object skeletons to guide the underlying diffusion model. Building upon a baseline that utilizes a pre-trained 2D image generator, our method takes advantage of the Objaverse dataset, which includes animated objects with bone structures. By introducing a skeleton guide layer following the existing ray conditioning normalization (RCN) layer, our approach enhances pose accuracy and multi-view consistency. The skeleton guide layer provides detailed structural information for the generative model, improving the quality of synthesized views. Experimental results demonstrate that our skeleton-guided method significantly enhances consistency and accuracy across diverse object categories within the Objaverse dataset. Our method outperforms existing state-of-the-art NVS techniques both quantitatively and qualitatively, without relying on explicit 3D representations.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03407
The Outskirt Stellar Mass of Low-Redshift Massive Galaxies is an Excellent Halo Mass Proxy in Illustris/IllustrisTNG Simulations,['Astrophysics of Galaxies'],"['Shuo Xu', 'Song Huang', 'Alexie Leauthaud', 'Benedikt Diemer', 'Katya Leidig', 'Carlo Cannarozzo', 'Conghao Zhou']","Recent observations suggest that the extended stellar halos of low-redshift massive galaxies are tightly connected to the assembly of their dark matter halos. In this paper, we use the Illustris, IllustrisTNG100, and IllustrisTNG300 simulations to compare how different stellar aperture masses trace halo mass. For massive central galaxies ($M_\star\geq 10^{11.2}M_\odot$), we find that a 2D outskirt stellar mass measured between 50 to 100 kpc ($M_{\star,[50,100]}$) consistently outperforms other aperture-based stellar masses. We further show that $M_{\star,[50,100]}$ correlates better with halo mass than the total amount of accreted stars (the ex situ mass), which suggests that not all accreted stars connect to halo assembly equally. While the galaxy formation recipes are different between Illustris and IllustrisTNG100, the two simulations yield consistent ex situ outskirt fractions for massive galaxies (about 70% in $M_{\star,[50,100]}$). These results demonstrate the potential of using the outskirt stellar mass to deepen our understanding of galaxy-halo connection in massive dark matter halos and trace dark matter halos better.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03406
Sensing and Control of Single Trapped Electrons Above 1 Kelvin,['Quantum Physics'],"['K. E. Castoria', 'N. R. Beysengulov', 'G. Koolstra', 'H. Byeon', 'E. O. Glen', 'M. Sammon', 'S. A. Lyon', 'J. Pollanen', 'D. G. Rees']","Electrons trapped on the surface of cryogenic substrates (liquid helium, solid neon or hydrogen) are an emerging platform for quantum information processing made attractive by the inherent purity of the electron environment, the scalability of trapping devices and the predicted long lifetime of electron spin states. Here we demonstrate the spatial control and detection of single electrons above the surface of liquid helium at temperatures above 1 K. A superconducting coplanar waveguide resonator is used to read out the charge state of an electron trap defined by gate electrodes beneath the helium surface. Dispersive frequency shifts are observed as the trap is loaded with electrons, from several tens down to single electrons. These frequency shifts are in good agreement with our theoretical model that treats each electron as a classical oscillator coupled to the cavity field. This sensitive charge readout scheme can aid efforts to develop large-scale quantum processors that require the high cooling powers available in cryostats operating above 1 K.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03404
An asymptotic characterisation of the Kerr spacetime,['General Relativity and Quantum Cosmology'],"['Robert Sansom', 'Juan A. Valiente Kroon']","We provide a characterisation of the Kerr spacetime close to future null infinity using the asymptotic characteristic initial value problem in a conformally compactified spacetime. Stewart's gauge is used to set up the past-oriented characteristic initial value problem. By a theorem of M. Mars characterising the Kerr spacetime, we provide conditions for the existence of an asymptotically timelike Killing vector on the development of the initial data by demanding that the spacetime is endowed with a Killing spinor. The conditions on the characteristic initial data ensuring the existence of a Killing spinor are, in turn, analysed. Finally, we write the conditions on the initial data in terms of the free data in the characteristic initial value problem. As a result, we characterise the Kerr spacetime using only a section of future null infinity and its intersection with an outgoing null hypersurface.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03402
Benchmarking Pretrained Attention-based Models for Real-Time Recognition in Robot-Assisted Esophagectomy,['Computer Vision and Pattern Recognition'],"['Ronald L. P. D. de Jong', 'Yasmina al Khalil', 'Tim J. M. Jaspers', 'Romy C. van Jaarsveld', 'Gino M. Kuiper', 'Yiping Li', 'Richard van Hillegersberg', 'Jelle P. Ruurda', 'Marcel Breeuwer', 'Fons van der Sommen']","Esophageal cancer is among the most common types of cancer worldwide. It is traditionally treated using open esophagectomy, but in recent years, robot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a promising alternative. However, robot-assisted surgery can be challenging for novice surgeons, as they often suffer from a loss of spatial orientation. Computer-aided anatomy recognition holds promise for improving surgical navigation, but research in this area remains limited. In this study, we developed a comprehensive dataset for semantic segmentation in RAMIE, featuring the largest collection of vital anatomical structures and surgical instruments to date. Handling this diverse set of classes presents challenges, including class imbalance and the recognition of complex structures such as nerves. This study aims to understand the challenges and limitations of current state-of-the-art algorithms on this novel dataset and problem. Therefore, we benchmarked eight real-time deep learning models using two pretraining datasets. We assessed both traditional and attention-based networks, hypothesizing that attention-based networks better capture global patterns and address challenges such as occlusion caused by blood or other tissues. The benchmark includes our RAMIE dataset and the publicly available CholecSeg8k dataset, enabling a thorough assessment of surgical segmentation tasks. Our findings indicate that pretraining on ADE20k, a dataset for semantic segmentation, is more effective than pretraining on ImageNet. Furthermore, attention-based models outperform traditional convolutional neural networks, with SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former additionally excelling in average symmetric surface distance.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03401
Berry Phase Dynamics of Sliding Electron Crystals,['Mesoscale and Nanoscale Physics'],"['Yongxin Zeng', 'Andrew J. Millis']","Systems such as Wigner crystals and incommensurate charge density waves that spontaneously break a continuous translation symmetry have unusual transport properties arising from their ability to slide coherently in space. Recent experimental and theoretical studies suggest that spontaneous translation symmetry breaking in some two-dimensional materials with nontrivial quantum geometry (e.g., rhombohedral pentalayer graphene) leads to a topologically nontrivial electron crystal state called the anomalous Hall crystal and characterized by a vanishing linear-response dc longitudinal conductivity and a non-vanishing Hall conductivity. In this work we present a theoretical investigation of the sliding dynamics of this new type of electron crystal, taking into account the system's nontrivial quantum geometry. We find that when accelerated by an external electric field, the crystal acquires a transverse anomalous velocity that stems from not only the Berry curvature of the parent band but also the Galilean non-invariance of the crystal state (i.e., crystal states with different momenta are not related by simple momentum boosts). We further show that acceleration of the crystal modifies its internal current from the static crystal value that is determined by the Chern number of the crystal state. The net Hall conductance including contributions from center-of-mass motion and internal current is in general not quantized. As an experimentally relevant example, we present numerical results in rhombohedral pentalayer graphene and discuss possible experimental implications.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03399
An even simpler hard variant of Not-All-Equal 3-SAT,['Computational Complexity'],"['Andreas Darmann', 'Janosch Döcker', 'Britta Dorn']","We show that Not-All-Equal 3-Sat remains NP-complete when restricted to instances that simultaneously satisfy the following properties: (i) The clauses are given as the disjoint union of k partitions, for any fixed $k \geq 4$, of the variable set into subsets of size 3, and (ii) each pair of distinct clauses shares at most one variable. Property (i) implies that each variable appears in exactly $k$ clauses and each clause consists of exactly 3 unnegated variables. Therewith, we improve upon our earlier result (Darmann and Döcker, 2020). Complementing the hardness result for at least $4$ partitions, we show that for $k\leq 3$ the corresponding decision problem is in P. In particular, for $k\in \{1,2\}$, all instances that satisfy Property (i) are nae-satisfiable.
  By the well-known correspondence between Not-All-Equal 3-Sat and hypergraph coloring, we obtain the following corollary of our results: For $k\geq 4$, Bicolorability is NP-complete for linear 3-uniform $k$-regular hypergraphs even if the edges are given as a decomposition into $k$ perfect matchings; with the same restrictions, for $k \leq 3$ Bicolorability is in P, and for $k \in \{1,2\}$ all such hypergraphs are bicolorable.
  Finally, we deduce from a construction in the work by Pilz (Pilz, 2019) that every instance of Positive Planar Not-All-Equal Sat with at least three distinct variables per clause is nae-satisfiable. Hence, when restricted to instances with a planar incidence graph, each of the above variants of Not-All-Equal 3-Sat turns into a trivial decision problem.△ Less",v1,https://arxiv.org/pdf/2412.03395
Enhancing Supply Chain Visibility with Generative AI: An Exploratory Case Study on Relationship Prediction in Knowledge Graphs,"['Computational Engineering, Finance, and Science']","['Ge Zheng', 'Alexandra Brintrup']","A key stumbling block in effective supply chain risk management for companies and policymakers is a lack of visibility on interdependent supply network relationships. Relationship prediction, also called link prediction is an emergent area of supply chain surveillance research that aims to increase the visibility of supply chains using data-driven techniques. Existing methods have been successful for predicting relationships but struggle to extract the context in which these relationships are embedded - such as the products being supplied or locations they are supplied from. Lack of context prevents practitioners from distinguishing transactional relations from established supply chain relations, hindering accurate estimations of risk. In this work, we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine learning framework that leverages pre-trained language models as embedding models combined with machine learning models to predict supply chain relationships within knowledge graphs. By integrating Generative AI techniques, our approach captures the nuanced semantic relationships between entities, thereby improving supply chain visibility and facilitating more precise risk management. Using data from a real case study, we show that GenAI-enhanced link prediction surpasses all benchmarks, and demonstrate how GenAI models can be explored and effectively used in supply chain risk management.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03390
Diamond-defect engineering of NV- centers using ion beam irradiation,['Materials Science'],"['J. L. Sánchez Toural', 'J. García-Pérez', 'R. Bernardo-Gavito', 'D. Granados', 'A. Andrino-Gómez', 'G. García', 'J. L. Pau', 'M. A. Ramos', 'N. Gordillo']","The interplay between ion beam modification techniques in the MeV range and the controlled generation of negatively charged nitrogen-vacancy (NV-) centers in nitrogen-doped synthetic diamond crystals is explored. An experimental approach employing both light (H+) and heavy (Br+6) ions was followed to assess their respective impacts on the creation of NV- centers, using different ion energies or fluences to generate varying amounts of vacancies. Photoluminescence spectroscopy was applied to characterize NV- and neutral NV0 centers. Initially, no NV centers were detected post-irradiation, despite the presence of substitutional nitrogen and vacancies. However, after annealing at 800C (and in some cases at 900C), most samples exhibited a high density of NV0 and especially NV- centers. This demonstrates that thermal treatment is essential for vacancy-nitrogen recombination and NV- formation, often through electron capture from nearby nitrogen atoms. Notably, we achieved high NV- densities without graphitization, which is essential for preserving the material's properties for quantum applications. This study underscores and quantifies the effectiveness of MeV-range ions in controlling vacancy distributions and highlights their potential for optimizing NV- center formation to enhance the sensitivity of diamond-based quantum magnetic sensors.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03386
Reactive Orchestration for Hierarchical Federated Learning Under a Communication Cost Budget,"['Distributed, Parallel, and Cluster Computing']","['Ivan Čilić', 'Anna Lackinger', 'Pantelis Frangoudis', 'Ivana Podnar Žarko', 'Alireza Furutanpey', 'Ilir Murturi', 'Schahram Dustdar']","Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server. This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC. In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy. Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost). Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria. By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03385
Mapping using Transformers for Volumes -- Network for Super-Resolution with Long-Range Interactions,['Computer Vision and Pattern Recognition'],"['August Leander Høeg', 'Sophia W. Bardenfleth', 'Hans Martin Kjer', 'Tim B. Dyrby', 'Vedrana Andersen Dahl', 'Anders Dahl']","Until now, it has been difficult for volumetric super-resolution to utilize the recent advances in transformer-based models seen in 2D super-resolution. The memory required for self-attention in 3D volumes limits the receptive field. Therefore, long-range interactions are not used in 3D to the extent done in 2D and the strength of transformers is not realized. We propose a multi-scale transformer-based model based on hierarchical attention blocks combined with carrier tokens at multiple scales to overcome this. Here information from larger regions at coarse resolution is sequentially carried on to finer-resolution regions to predict the super-resolved image. Using transformer layers at each resolution, our coarse-to-fine modeling limits the number of tokens at each scale and enables attention over larger regions than what has previously been possible. We experimentally compare our method, MTVNet, against state-of-the-art volumetric super-resolution models on five 3D datasets demonstrating the advantage of an increased receptive field. This advantage is especially pronounced for images that are larger than what is seen in popularly used 3D datasets. Our code is available at https://github.com/AugustHoeg/MTVNet△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03379
Granular Ball Twin Support Vector Machine with Universum Data,['Machine Learning'],"['M. A. Ganaie', 'Vrushank Ahire']","Classification with support vector machines (SVM) often suffers from limited performance when relying solely on labeled data from target classes and is sensitive to noise and outliers. Incorporating prior knowledge from Universum data and more robust data representations can enhance accuracy and efficiency. Motivated by these findings, we propose a novel Granular Ball Twin Support Vector Machine with Universum Data (GBU-TSVM) that extends the TSVM framework to leverage both Universum samples and granular ball computing during model training. Unlike existing TSVM methods, the proposed GBU-TSVM represents data instances as hyper-balls rather than points in the feature space. This innovative approach improves the model's robustness and efficiency, particularly in handling noisy and large datasets. By grouping data points into granular balls, the model achieves superior computational efficiency, increased noise resistance, and enhanced interpretability. Additionally, the inclusion of Universum data, which consists of samples that are not strictly from the target classes, further refines the classification boundaries. This integration enriches the model with contextual information, refining classification boundaries and boosting overall accuracy. Experimental results on UCI benchmark datasets demonstrate that the GBU-TSVM outperforms existing TSVM models in both accuracy and computational efficiency. These findings highlight the potential of the GBU-TSVM model in setting a new standard in data representation and classification.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03375
Exploring trends in audio mixes and masters: Insights from a dataset analysis,['Sound'],"['Angeliki Mourgela', 'Elio Quinton', 'Spyridon Bissas', 'Joshua D. Reiss', 'David Ronan']","We present an analysis of a dataset of audio metrics and aesthetic considerations about mixes and masters provided by the web platform MixCheck studio. The platform is designed for educational purposes, primarily targeting amateur music producers, and aimed at analysing their recordings prior to them being released. The analysis focuses on the following data points: integrated loudness, mono compatibility, presence of clipping and phase issues, compression and tonal profile across 30 user-specified genres. Both mixed (mixes) and mastered audio (masters) are included in the analysis, where mixes refer to the initial combination and balance of individual tracks, and masters refer to the final refined version optimized for distribution. Results show that loudness-related issues along with dynamics issues are the most prevalent, particularly in mastered audio. However mastered audio presents better results in compression than just mixed audio. Additionally, results show that mastered audio has a lower percentage of stereo field and phase issues.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03373
Hard diagrams of split links,['Geometric Topology'],"['Corentin Lunel', 'Arnaud de Mesmay', 'Jonathan Spreer']","Deformations of knots and links in ambient space can be studied combinatorially on their diagrams via local modifications called Reidemeister moves. While it is well-known that, in order to move between equivalent diagrams with Reidemeister moves, one sometimes needs to insert excess crossings, there are significant gaps between the best known lower and upper bounds on the required number of these added crossings. In this article, we study the problem of turning a diagram of a split link into a split diagram, and we show that there exist split links with diagrams requiring an arbitrarily large number of such additional crossings. More precisely, we provide a family of diagrams of split links, so that any sequence of Reidemeister moves transforming a diagram with $c$ crossings into a split diagram requires going through a diagram with $Ω(\sqrt{c})$ extra crossings. Our proof relies on the framework of bubble tangles, as introduced by Lunel and de Mesmay, and a technique of Chambers and Liokumovitch to turn homotopies into isotopies in the context of Riemannian geometry.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03372
Light structuring via nonlinear total angular momentum addition with flat optics,['Optics'],"['Evgenii Menshikov', 'Paolo Franceschini', 'Kristina Frizyuk', 'Ivan Fernandez-Corbaton', 'Andrea Tognazzi', 'Alfonso Carmelo Cino', 'Denis Garoli', 'Mihail Petrov', 'Domenico de Ceglia', 'Costantino De Angelis']","Shaping the structure of light with flat optical devices has driven significant advancements in our fundamental understanding of light and light-matter interactions, and enabled a broad range of applications, from image processing and microscopy to optical communication, quantum information processing, and the manipulation of microparticles. Yet, pushing the boundaries of structured light beyond the linear optical regime remains an open challenge. Nonlinear optical interactions, such as wave mixing in nonlinear flat optics, offer a powerful platform to unlock new degrees of freedom and functionalities for generating and detecting structured light. In this study, we experimentally demonstrate the non-trivial structuring of third-harmonic light enabled by the addition of total angular momentum projection in a nonlinear, isotropic flat optics element -- a single thin film of amorphous silicon. We identify the total angular momentum projection and helicity as the most critical properties for analyzing the experimental results. The theoretical model we propose, supported by numerical simulations, offers quantitative predictions for light structuring through nonlinear wave mixing under various pumping conditions, including vectorial and non-paraxial pump light. Notably, we reveal that the shape of third-harmonic light is highly sensitive to the polarization state of the pump. Our findings demonstrate that harnessing the addition of total angular momentum projection in nonlinear wave mixing can be a powerful strategy for generating and detecting precisely controlled structured light.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03367
When can we see micromotion? Experimental and theoretical analysis of the ChiSCAT scheme,['Optics'],"['Andrii Trelin', 'Jette Abel', 'Christian Rimmbach', 'Robert David', 'Andreas Hermann', 'Friedemann Reinhard']","We present an in-depth analysis of ChiSCAT, a recently introduced interferometric microscopy scheme to detect recurring micromotion events in cells. Experimentally, we demonstrate that illumination with low-coherence sources can greatly improve the robustness of the scheme to vibrations. Theoretically, we analyze the performance of ChiSCAT under various noise models, in particular photon shot noise and noise dominated by cellular motions other than the signal. We finally propose ways to improve performance, especially in a setting dominated by cell motions, and conclude with an outlook on potential future directions.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03365
Distortions in Charged-Particle Images of Laser Direct-Drive Inertial Confinement Fusion Implosions,['Plasma Physics'],"['P. V. Heuer', 'J. L. Peebles', 'J. Kunimune', 'H. G. Rinderknecht', 'J. R. Davies', 'V. Gopalaswamy', 'J. Frelier', 'M. Scott', 'J. Roberts', 'B. Brannon', 'H. McClow', 'R. Fairbanks', 'S. P. Regan', 'J. A. Frenje', 'M. Gatu Johnson', 'F. H. Séguin', 'A. J. Crilly', 'B. D. Appelbe', 'M. Farrell', 'J. Stutz']","Energetic charged particles generated by inertial confinement fusion (ICF) implosions encode information about the spatial morphology of the hot-spot and dense fuel during the time of peak fusion reactions. The knock-on deuteron imager (KoDI) was developed at the Omega Laser Facility to image these particles in order to diagnose low-mode asymmetries in the hot-spot and dense fuel layer of cryogenic deuterium--tritium ICF implosions. However, the images collected are distorted in several ways that prevent reconstruction of the deuteron source. In this paper we describe these distortions and a series of attempts to mitigate or compensate for them. We present several potential mechanisms for the distortions, including a new model for scattering of charged particles in filamentary electric or magnetic fields surrounding the implosion. A novel particle-tracing methodology is developed and utilized to create synthetic KoDI data based on the filamentary field model that reproduces the main experimentally observed image distortions. We conclude with a discussion of the outlook for KoDI, and potential considerations for other charged-particle diagnostics.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03362
Measurement of electron beam induced sample heating in SEM experiments,['Materials Science'],"['Christina Koenig', 'Alice Bastos da Silva Fanta', 'Joerg R. Jinschek']","Scanning Electron Microscopy (SEM) experiments provide detailed insights into material microstructures, enabling high-resolution imaging as well as crystallographic analysis through advanced techniques like Electron Backscatter Diffraction (EBSD). However, the interaction of the high-energy electron beam with the material can lead to localized heating, which may significantly impact specimen integrity, especially in applications requiring prolonged beam exposure, for instance when mapping the crystal structure using EBSD. This study examines electron-beam-induced heating effects on a model metal sample (iron), directly measuring the locally deposited electron beam energy with a MEMS-based heating device and validating these measurements through simulations, including Monte Carlo and Finite Element methods. The analysis focuses on the effects of various experimental parameters such as acceleration voltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time (from 1$μ$s to 1ms) and sample tilt (0° to 70°). The findings reveal that local sample temperatures can increase by up to 70 °C during EBSD experiments, primarily affected by the choice in beam current and acceleration voltage, with beam current having the most significant impact.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03361
Fairer Analysis and Demographically Balanced Face Generation for Fairer Face Verification,['Computer Vision and Pattern Recognition'],"['Alexandre Fournier-Montgieux', 'Michael Soumm', 'Adrian Popescu', 'Bertrand Luvison', 'Hervé Le Borgne']","Face recognition and verification are two computer vision tasks whose performances have advanced with the introduction of deep representations. However, ethical, legal, and technical challenges due to the sensitive nature of face data and biases in real-world training datasets hinder their development. Generative AI addresses privacy by creating fictitious identities, but fairness problems remain. Using the existing DCFace SOTA framework, we introduce a new controlled generation pipeline that improves fairness. Through classical fairness metrics and a proposed in-depth statistical analysis based on logit models and ANOVA, we show that our generation pipeline improves fairness more than other bias mitigation approaches while slightly improving raw performance.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03349
Impact of hyperons on structural properties of neutron stars and hybrid stars within four-dimensional Einstein-Gauss-Bonnet gravity,['Nuclear Theory'],"['Ishfaq Ahmad Rather', 'Grigoris Panotopoulos']","We investigate the impact of hyperons and phase transition to quark matter on the structural properties of neutron stars within the four-dimensional Einstein-Gauss-Bonnet gravity (EGB). We employ the density-dependent relativistic mean-field model (DDME2) for the hadronic phase and the density-dependent quark mass (DDQM) model for the quark phase to construct hadronic and hybrid equations of state (EoSs) that are consistent with the astrophysical constraints. The presence of hyperons softens the EoS and with a phase transition, the EoS further softens, and the speed of sound squared drops to around 0.2 for the maximum mass configuration which lies in the pure quark phase. Adjusting the Gaussian-Bonnet coupling constant $α$ within its allowed range results in a decrease in the mass-radius relationship for negative $α$, and an increase for positive $α$. In addition, functions are fitted to the maximum mass and its associated radius as a function of constant $α$ to observe its impact on these properties.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03348
Nonlocal effective action and particle creation in $D$ dimensions,['High Energy Physics - Theory'],"['Andrés Boasso', 'Sebastián Franchino-Viñas', 'Francisco D. Mazzitelli']","We compute the particle creation rate in the context of quantum fields in curved spacetimes, by evaluating the imaginary part of the effective action up to second order in the curvatures. For arbitrary metrics in dimensions $D\geq4$, we express the vacuum persistence amplitude in terms of the Ricci scalar and the Weyl tensor, showing that, up to their second power, no particle creation occurs for conformal fields in conformally flat spacetimes. We pinpoint an analogy with the electromagnetic pair creation, by writing the squared Weyl tensor invariant in terms of its electric and magnetic parts. In addition, we present an alternative expression for the imaginary part of the effective action employing the Cotton tensor. This is particularly useful in $D=3$, where the Weyl tensor trivially vanishes and the Cotton tensor is related to conformal flatness. Finally, we highlight the importance of the threshold for particle creation, a point that has been overlooked in some recent studies.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03340
Spontaneous Torque on an Inhomogeneous Chiral Body out of Thermal Equilibrium,['Quantum Physics'],"['Kimball A. Milton', 'Nima Pourtolami', 'Gerard Kennedy']","In a previous paper we showed that an inhomogeneous body in vacuum will experience a spontaneous force if it is not in thermal equilibrium with its environment. This is due to the asymmetric asymptotic radiation pattern such an object emits. We demonstrated this self-propulsive force by considering an expansion in powers of the electric susceptibility: A torque arises in first order, but only if the material constituting the body is nonreciprocal. No force arises in first order. A force does occur for bodies made of ordinary (reciprocal) materials in second order. Here we extend these considerations to the torque. As one would expect, a spontaneous torque will also appear on an inhomogeneous chiral object if it is out of thermal equilibrium with its environment. Once a chiral body starts to rotate, it will experience a small quantum frictional torque, but much more important, unless a mechanism is provided to maintain the nonequilibrium state, is thermalization: The body will rapidly reach thermal equilibrium with the vacuum, and the angular acceleration will essentially become zero. For a small, or even a large, inhomogeneous chiral body, a terminal angular velocity will result, which seems to be in the realm of observability.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03336
Specific heat at low temperatures in quasiplanar molecular crystals: Where do glassy anomalies in minimally disordered crystals come from?,['Disordered Systems and Neural Networks'],"['Daria Szewczyk', 'Manuel Moratalla', 'Grzegorz Chajewski', 'Jonathan F. Gebbia', 'Andrzej Jeżowski', 'Alexander I. Krivchikov', 'María Barrio', 'Josep Ll. Tamarit', 'Miguel A. Ramos']","We present low-temperature specific heat (Cp) measurements of a monoclinic P2_{1}/c crystal formed by quasiplanar molecules of tetrachloro-m-xylene. The dynamic disorder frozen at low-temperature of the asymmetric unit (formed by a half molecule) consists of reorientation around a three-fold-like axis perpendicular to the benzene ring. Such a minimal disorder gives rise to typical glassy anomalies, as a linear in contribution in Cp ascribed to two-level systems and a broad maximum around 6.6 K in Cp/T^3 (the boson peak). We discuss these results in the framework of other quasiplanar molecular crystals with different accountable number of in-plane molecular orientations We find that the density of two-level systems does not correlate with the degree of orientational disorder. Rather, it is the molecular asymmetry that seems to play a relevant role in the thermal anomalies. Furthermore, we discuss the suggested correlation between the boson peak and Debye temperatures. We find that a linear correlation between the boson peak and Debye temperatures holds for many -- but not all -- structural glasses and strikingly holds even better for some disordered crystals, including our studied quasiplanar molecular crystals.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03335
Quasi-invariant states with uniformly bounded cocycles,['Operator Algebras'],"['Ameur Dhahri', 'Eric Ricard']","We investigate the notion of quasi-invariant states introduced in [2, 3] from an analytic viewpoint.We give the structures of quasi-invariant states with uniformly bounded cocycles. As a consequence, we can apply a Theorem of Kovacs and Szucs to get a conditional expectation on fixed points and another of Stormer to get an invariant semifinite trace under extra assumptions.△ Less",v1,https://arxiv.org/pdf/2412.03328
Generation of Tunable Correlated Frequency Comb via Four-Wave-Mixing in Optical fibers,['Quantum Physics'],"['Aryan Bhardwaj', 'Debanuj Chatterjee', 'Ashutosh Kumar Singh', 'Anil Prabhakar']","We report an all-fiber-based experimental setup to generate a correlated photon-pair comb using Four Wave Mixing (FWM) in Highly Non-Linear Fiber (HNLF). Temporal correlations of the generated photons were confirmed through coincidence measurements. We observed a maximum of 32 kcps, with a coincidence to accidental ratio of 17$\pm$1. To further understand the underlying processes, we also simulated a generalized FWM event involving the interaction between an arbitrary frequency comb and a Continuous Wave (CW) pump. Non-linear dynamics through the HNLF were modelled using Schrödinger propagation equations, with numerical predictions agreeing with our experimental results.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03323
Geometry-guided Cross-view Diffusion for One-to-many Cross-view Image Synthesis,['Computer Vision and Pattern Recognition'],"['Tao Jun Lin', 'Wenqing Wang', 'Yujiao Shi', 'Akhil Perincherry', 'Ankit Vora', 'Hongdong Li']","This paper presents a novel approach for cross-view synthesis aimed at generating plausible ground-level images from corresponding satellite imagery or vice versa. We refer to these tasks as satellite-to-ground (Sat2Grd) and ground-to-satellite (Grd2Sat) synthesis, respectively. Unlike previous works that typically focus on one-to-one generation, producing a single output image from a single input image, our approach acknowledges the inherent one-to-many nature of the problem. This recognition stems from the challenges posed by differences in illumination, weather conditions, and occlusions between the two views. To effectively model this uncertainty, we leverage recent advancements in diffusion models. Specifically, we exploit random Gaussian noise to represent the diverse possibilities learnt from the target view data. We introduce a Geometry-guided Cross-view Condition (GCC) strategy to establish explicit geometric correspondences between satellite and street-view features. This enables us to resolve the geometry ambiguity introduced by camera pose between image pairs, boosting the performance of cross-view image synthesis. Through extensive quantitative and qualitative analyses on three benchmark cross-view datasets, we demonstrate the superiority of our proposed geometry-guided cross-view condition over baseline methods, including recent state-of-the-art approaches in cross-view image synthesis. Our method generates images of higher quality, fidelity, and diversity than other state-of-the-art approaches.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03315
Scalar sector of Type 2 Seesaw model explorations with multi-lepton final states,['High Energy Physics - Phenomenology'],"['Călin Alexa', 'Otilia A. Ducu', 'Ana E. Dumitriu', 'Adam Jinaru', 'Emmanuel Monnier', 'Gilbert Moultaka', 'Alexandra Tudorache', 'Hanlin Xu']","Originally motivated for the generation of (Majorana) neutrino masses, the Type 2 Seesaw Model has also a rich extended Higgs sector with, if accessible at the LHC, a distinctive phenomenology of neutral, charged and doubly-charged states. The goal of the work is to present an exhaustive phenomenological study of the most promising production and decay channels of pair or associated scalars, decaying directly or in cascades to Standard Model particles at the LHC. The study is complementary to the literature in that it highlights a previously unnoticed important sensitivity to a mixing angle. The ensuing uncertainty calls for a comprehensive experimental search strategy for the various processes. These processes can be studied within LHC energies reach, by comparing cutflow results for different final states. We carried out prospective search analyses with multi-lepton, jets and missing energy configurations, assuming an ATLAS-like detector at LHC and HL-LHC, for charged, doubly-charged, and for the first time neutral scalar productions. The work is a collaboration between ATLAS experimentalists and theoreticians in continuation of an endeavor that lead to previous published ATLAS analyses for the search of (doubly)charged Higgs bosons, aiming at proposals for future experimental searches.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03311
Contextual Data Integration for Bike-sharing Demand Prediction with Graph Neural Networks in Degraded Weather Conditions,['Artificial Intelligence'],"['Romain Rochas', 'Angelo Furno', 'Nour-Eddin El Faouzi']","Demand for bike sharing is impacted by various factors, such as weather conditions, events, and the availability of other transportation modes. This impact remains elusive due to the complex interdependence of these factors or locationrelated user behavior variations. It is also not clear which factor is additional information which are not already contained in the historical demand. Intermodal dependencies between bike-sharing and other modes are also underexplored, and the value of this information has not been studied in degraded situations. The proposed study analyzes the impact of adding contextual data, such as weather, time embedding, and road traffic flow, to predict bike-sharing Origin-Destination (OD) flows in atypical weather situations Our study highlights a mild relationship between prediction quality of bike-sharing demand and road traffic flow, while the introduced time embedding allows outperforming state-of-the-art results, particularly in the case of degraded weather conditions. Including weather data as an additional input further improves our model with respect to the basic ST-ED-RMGC prediction model by reducing of more than 20% the prediction error in degraded weather condition.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03307
Turnover of investment portfolio via covariance matrix of returns,['Portfolio Management'],"['A. V. Kuliga', 'I. N. Shnurnikov']","An investment portfolio consists of $n$ algorithmic trading strategies, which generate vectors of positions in trading assets. Sign opposite trades (buy/sell) cross each other as strategies are combined in a portfolio. Then portfolio turnover becomes a non linear function of strategies turnover. It rises a problem of effective (quick and precise) portfolio turnover estimation. Kakushadze and Liew (2014) shows how to estimate turnover via covariance matrix of returns. We build a mathematical model for such estimations; prove a theorem which gives a necessary condition for model applicability; suggest new turnover estimations; check numerically the preciseness of turnover estimations for algorithmic strategies on USA equity market.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03305
Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation,['Computation and Language'],"['Shivalika Singh', 'Angelika Romanou', 'Clémentine Fourrier', 'David I. Adelani', 'Jian Gang Ngui', 'Daniel Vila-Suero', 'Peerat Limkonchotiwat', 'Kelly Marchisio', 'Wei Qi Leong', 'Yosephine Susanto', 'Raymond Ng', 'Shayne Longpre', 'Wei-Yin Ko', 'Madeline Smith', 'Antoine Bosselut', 'Alice Oh', 'Andre F. T. Martins', 'Leshem Choshen', 'Daphne Ippolito', 'Enzo Ferrante', 'Marzieh Fadaee', 'Beyza Ermis', 'Sara Hooker']","Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artifacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global-MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global-MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03304
Simultaneous ponderomotive squeezing of light by two mechanical modes in an optomechanical system,['Quantum Physics'],"['Peyman Malekzadeh', 'Emil Zeuthen', 'Eric Langman', 'Albert Schliesser', 'Eugene Polzik']","We experimentally demonstrate a source of squeezed light featuring simultaneous ponderomotive squeezing from two mechanical modes of an optomechanical system. We use ultra-coherent vibrational modes ($Q$ factors on the order of $10^{8}$) of a soft-clamped membrane placed in a Fabry-Pérot optical cavity at cryogenic conditions ($T=11\,\mathrm{K}$) and driven by quantum fluctuations in the intensity of light to create correlations between amplitude and phase quadratures of the intra-cavity light field. Continuous optical monitoring was conducted on two different mechanical modes with a frequency separation of around $1\,\mathrm{MHz}$. As a result of the interaction between the membrane position and the light, we generated ponderomotive squeezing of $4.8\,\mathrm{dB}$ for the first localized mechanical mode at $1.32\,\mathrm{MHz}$ and $4.2\,\mathrm{dB}$ for the second localized mode at $2.43\,\mathrm{MHz}$, as observed in direct detection when correcting for the detection inefficiency. Thus, we have demonstrated how squeezed light generation can be extended beyond a single octave in an optomechanical system by leveraging more than one mechanical mode. Utilizing homodyne detection to detect squeezing in an optimal quadrature would lead to squeezing levels at the output of the cavity of $7.3\,\mathrm{dB}$ and $6.8\,\mathrm{dB}$, in the two modes respectively. Squeezing of light demonstrated here for near-infrared light can be achieved in a broad range of wavelengths due to the relative insensitivity of optomechanical interaction with SiN membranes to the wavelength.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03303
Gaussian Processes for Probabilistic Estimates of Earthquake Ground Shaking: A 1-D Proof-of-Concept,['Geophysics'],"['Sam A. Scivier', 'Tarje Nissen-Meyer', 'Paula Koelemeijer', 'Atılım Güneş Baydin']","Estimates of seismic wave speeds in the Earth (seismic velocity models) are key input parameters to earthquake simulations for ground motion prediction. Owing to the non-uniqueness of the seismic inverse problem, typically many velocity models exist for any given region. The arbitrary choice of which velocity model to use in earthquake simulations impacts ground motion predictions. However, current hazard analysis methods do not account for this source of uncertainty. We present a proof-of-concept ground motion prediction workflow for incorporating uncertainties arising from inconsistencies between existing seismic velocity models. Our analysis is based on the probabilistic fusion of overlapping seismic velocity models using scalable Gaussian process (GP) regression. Specifically, we fit a GP to two synthetic 1-D velocity profiles simultaneously, and show that the predictive uncertainty accounts for the differences between the models. We subsequently draw velocity model samples from the predictive distribution and estimate peak ground displacement using acoustic wave propagation through the velocity models. The resulting distribution of possible ground motion amplitudes is much wider than would be predicted by simulating shaking using only the two input velocity models. This proof-of-concept illustrates the importance of probabilistic methods for physics-based seismic hazard analysis.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03299
Straightforward Phase I Dose-Finding Design for Healthy Volunteers Accounting for Surrogate Activity Biomarkers,['Applications'],"['Sandrine Boulet', 'Emmanuelle Comets', 'Antoine Guillon', 'Linda B. S. Aulin', 'Robin Michelet', 'Charlotte Kloft', 'Sarah Zohar', 'Moreno Ursino']","Conventionally, a first-in-human phase I trial in healthy volunteers aims to confirm the safety of a drug in humans. In such situations, volunteers should not suffer from any safety issues and simple algorithm-based dose-escalation schemes are often used. However, to avoid too many clinical trials in the future, it might be appealing to design these trials to accumulate information on the link between dose and efficacy/activity under strict safety constraints. Furthermore, an increasing number of molecules for which the increasing dose-activity curve reaches a plateau are emerging.In a phase I dose-finding trial context, our objective is to determine, under safety constraints, among a set of doses, the lowest dose whose probability of activity is closest to a given target. For this purpose, we propose a two-stage dose-finding design. The first stage is a typical algorithm dose escalation phase that can both check the safety of the doses and accumulate activity information. The second stage is a model-based dose-finding phase that involves selecting the best dose-activity model according to the plateau location.Our simulation study shows that our proposed method performs better than the common Bayesian logistic regression model in selecting the optimal dose.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03298
Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models,['Cryptography and Security'],"['Andreas Müller', 'Denis Lukovnikov', 'Jonas Thietke', 'Asja Fischer', 'Erwin Quiring']","Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03283
Electrically controlled laser generation in a photonic crystal - liquid crystal - metal microcavity,['Optics'],"['Daniil S. Buzin', 'Pavel S. Pankin', 'Dmitrii N. Maksimov', 'Vitaly S. Sutormin', 'Gavriil A. Romanenko', 'Rashid G. Bikbaev', 'Sergey V. Nedelin', 'Nikita A. Zolotovskii', 'Igor A. Tambasov', 'Stepan Ya. Vetrov', 'Kuo-Ping Chen', 'Ivan V. Timofeev']","A comprehensive approach for simulating lasing dynamics in a liquid crystal based laser is presented. The approach takes into account the transformation of the liquid crystal structure caused by applied voltage. In particular, it allows us to explicitly account for a resonant mode frequency shift in the laser equations. The laser dynamic is described by a set of coupled non-linear differential equations for dye polarizations, population densities and the electromagnetic fields. The proposed model is applied to a photonic crystal$-$metal microcavity filled with a resonant nematic liquid crystal layer doped with a dye. The calculated lasing spectra governed by external electric field are verified in comparison with measured spectra.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03282
Rotograb: Combining Biomimetic Hands with Industrial Grippers using a Rotating Thumb,['Robotics'],"['Arnaud Bersier', 'Matteo Leonforte', 'Alessio Vanetta', 'Sarah Lia Andrea Wotke', 'Andrea Nappi', 'Yifan Zhou', 'Sebastiano Oliani', 'Alexander M. Kübler', 'Robert K. Katzschmann']","The development of robotic grippers and hands for automation aims to emulate human dexterity without sacrificing the efficiency of industrial grippers. This study introduces Rotograb, a tendon-actuated robotic hand featuring a novel rotating thumb. The aim is to combine the dexterity of human hands with the efficiency of industrial grippers. The rotating thumb enlarges the workspace and allows in-hand manipulation. A novel joint design minimizes movement interference and simplifies kinematics, using a cutout for tendon routing. We integrate teleoperation, using a depth camera for real-time tracking and autonomous manipulation powered by reinforcement learning with proximal policy optimization. Experimental evaluations demonstrate that Rotograb's rotating thumb greatly improves both operational versatility and workspace. It can handle various grasping and manipulation tasks with objects from the YCB dataset, with particularly good results when rotating objects within its grasp. Rotograb represents a notable step towards bridging the capability gap between human hands and industrial grippers. The tendon-routing and thumb-rotating mechanisms allow for a new level of control and dexterity. Integrating teleoperation and autonomous learning underscores Rotograb's adaptability and sophistication, promising substantial advancements in both robotics research and practical applications.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03279
Generating Synthetic Genotypes using Diffusion Models,"['Computational Engineering, Finance, and Science']","['Philip Kenneweg', 'Raghuram Dandinasivara', 'Xiao Luo', 'Barbara Hammer', 'Alexander Schönhuth']","In this paper, we introduce the first diffusion model designed to generate complete synthetic human genotypes, which, by standard protocols, one can straightforwardly expand into full-length, DNA-level genomes. The synthetic genotypes mimic real human genotypes without just reproducing known genotypes, in terms of approved metrics. When training biomedically relevant classifiers with synthetic genotypes, accuracy is near-identical to the accuracy achieved when training classifiers with real data. We further demonstrate that augmenting small amounts of real with synthetically generated genotypes drastically improves performance rates. This addresses a significant challenge in translational human genetics: real human genotypes, although emerging in large volumes from genome wide association studies, are sensitive private data, which limits their public availability. Therefore, the integration of additional, insensitive data when striving for rapid sharing of biomedical knowledge of public interest appears imperative.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03278
EAP-FIDO: A Novel EAP Method for Using FIDO2 Credentials for Network Authentication,['Cryptography and Security'],"['Martiño Rivera-Dourado', 'Christos Xenakis', 'Alejandro Pazos', 'Jose Vázquez-Naya']","The adoption of FIDO2 authentication by major tech companies in web applications has grown significantly in recent years. However, we argue FIDO2 has broader potential applications. In this paper, we introduce EAP-FIDO, a novel Extensible Authentication Protocol (EAP) method for use in IEEE 802.1X-protected networks. This allows organisations with WPA2/3-Enterprise wireless networks or MACSec-enabled wired networks to leverage FIDO2's passwordless authentication in compliance with existing standards. Additionally, we provide a comprehensive security and performance analysis to support the feasibility of this approach.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03277
The bcc coating of Lennard-Jones crystal nuclei vanishes with a change of local structure detection algorithm,['Soft Condensed Matter'],"['Willem Gispen', 'Alberto Pérez de Alba Ortíz', 'Marjolein Dijkstra']","Since the influential work of ten Wolde, Ruiz-Montero, and Frenkel [Phys. Rev. Lett. 75, 2714 (1995)], crystal nucleation from a Lennard-Jones fluid has been regarded as a paradigmatic example of metastable crystal ordering at the surface of a critical nucleus. We apply seven commonly used local structure detection algorithms to characterize crystal nuclei obtained from transition path sampling simulations. The polymorph composition of these nuclei varies significantly depending on the algorithm used. Our results indicate that one should be very careful when characterizing the local structure near solid-solid and solid-fluid interfaces. Particles near such interfaces exhibit a local structure distinct from that of bulk fluid or bulk crystal phases. We argue that incorporating outlier detection into the local structure detection method is beneficial, leading to greater confidence in the classification results. Interestingly, the bcc coating nearly disappears when adopting a machine learning method with outlier detection.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03276
The strong vertex span of trees,['Combinatorics'],"['Mateja Grašič', 'Chris Mouron', 'Andrej Taranenko']","The strong vertex (edge) span of a given graph $G$ is the maximum distance that two players can maintain at all times while visiting all vertices (edges) of $G$ and moving either to an adjacent vertex or staying in the current position independently of each other. We introduce the notions of switching walks and triod size of a tree, which are used to determine the strong vertex and the strong edge span of an arbitrary tree. The obtained results are used in an algorithm that computes the strong vertex (edge) span of the input tree in linear time.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03266
NeRF and Gaussian Splatting SLAM in the Wild,['Robotics'],"['Fabian Schmidt', 'Markus Enzweiler', 'Abhinav Valada']","Navigating outdoor environments with visual Simultaneous Localization and Mapping (SLAM) systems poses significant challenges due to dynamic scenes, lighting variations, and seasonal changes, requiring robust solutions. While traditional SLAM methods struggle with adaptability, deep learning-based approaches and emerging neural radiance fields as well as Gaussian Splatting-based SLAM methods, offer promising alternatives. However, these methods have primarily been evaluated in controlled indoor environments with stable conditions, leaving a gap in understanding their performance in unstructured and variable outdoor settings. This study addresses this gap by evaluating these methods in natural outdoor environments, focusing on camera tracking accuracy, robustness to environmental factors, and computational efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate that neural SLAM methods achieve superior robustness, particularly under challenging conditions such as low light, but at a high computational cost. At the same time, traditional methods perform the best across seasons but are highly sensitive to variations in lighting conditions. The code of the benchmark is publicly available at https://github.com/iis-esslingen/nerf-3dgs-benchmark.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03263
Is JPEG AI going to change image forensics?,['Image and Video Processing'],"['Edoardo Daniele Cannas', 'Sara Mandelli', 'Natasa Popovic', 'Ayman Alkhateeb', 'Alessandro Gnutti', 'Paolo Bestagini', 'Stefano Tubaro']","In this paper, we investigate the counter-forensic effects of the forthcoming JPEG AI standard based on neural image compression, focusing on two critical areas: deepfake image detection and image splicing localization. Neural image compression leverages advanced neural network algorithms to achieve higher compression rates while maintaining image quality. However, it introduces artifacts that closely resemble those generated by image synthesis techniques and image splicing pipelines, complicating the work of researchers when discriminating pristine from manipulated content. We comprehensively analyze JPEG AI's counter-forensic effects through extensive experiments on several state-of-the-art detectors and datasets. Our results demonstrate that an increase in false alarms impairs the performance of leading forensic detectors when analyzing genuine content processed through JPEG AI. By exposing the vulnerabilities of the available forensic tools we aim to raise the urgent need for multimedia forensics researchers to include JPEG AI images in their experimental setups and develop robust forensic techniques to distinguish between neural compression artifacts and actual manipulations.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03261
Remote Manipulation of Multiple Objects with Airflow Field Using Model-Based Learning Control,['Robotics'],"['Artur Kopitca', 'Shahriar Haeri', 'Quan Zhou']","Non-contact manipulation is an emerging and highly promising methodology in robotics, offering a wide range of scientific and industrial applications. Among the proposed approaches, airflow stands out for its ability to project across considerable distances and its flexibility in actuating objects of varying materials, sizes, and shapes. However, predicting airflow fields at a distance, as well as the motion of objects within them, remains notoriously challenging due to their nonlinear and stochastic nature. Here, we propose a model-based learning approach using a jet-induced airflow field for remote multi-object manipulation on a surface. Our approach incorporates an analytical model of the field, learned object dynamics, and a model-based controller. The model predicts an air velocity field over an infinite surface for a specified jet orientation, while the object dynamics are learned through a robust system identification algorithm. Using the model-based controller, we can automatically and remotely, at meter-scale distances, control the motion of single and multiple objects for different tasks, such as path-following, aggregating, and sorting.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03254
"A Cut-free, Sound and Complete Russellian Theory of Definite Descriptions",['Logic in Computer Science'],"['Andrzej Indrzejczak', 'Nils Kürbis']","We present a sequent calculus for first-order logic with lambda terms and definite descriptions. The theory formalised by this calculus is essentially Russellian, but avoids some of its well known drawbacks and treats definite description as genuine terms. A constructive proof of the cut elimination theorem and a Henkin-style proof of completeness are the main results of this contribution.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03251
Controlling the Mutation in Large Language Models for the Efficient Evolution of Algorithms,['Neural and Evolutionary Computing'],"['Haoran Yin', 'Anna V. Kononova', 'Thomas Bäck', 'Niki van Stein']","The integration of Large Language Models (LLMs) with evolutionary computation (EC) has introduced a promising paradigm for automating the design of metaheuristic algorithms. However, existing frameworks, such as the Large Language Model Evolutionary Algorithm (LLaMEA), often lack precise control over mutation mechanisms, leading to inefficiencies in solution space exploration and potentially suboptimal convergence. This paper introduces a novel approach to mutation control within LLM-driven evolutionary frameworks, inspired by theory of genetic algorithms. Specifically, we propose dynamic mutation prompts that adaptively regulate mutation rates, leveraging a heavy-tailed power-law distribution to balance exploration and exploitation. Experiments using GPT-3.5-turbo and GPT-4o models demonstrate that GPT-3.5-turbo fails to adhere to the specific mutation instructions, while GPT-4o is able to adapt its mutation based on the prompt engineered dynamic prompts. Further experiments show that the introduction of these dynamic rates can improve the convergence speed and adaptability of LLaMEA, when using GPT-4o. This work sets the starting point for better controlled LLM-based mutations in code optimization tasks, paving the way for further advancements in automated metaheuristic design.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03250
Nonparametric estimation of the Patient Weighted While-Alive Estimand,['Methodology'],"['Alessandra Ragni', 'Torben Martinussen', 'Thomas Scheike']","In clinical trials with recurrent events, such as repeated hospitalizations terminating with death, it is important to consider the patient events overall history for a thorough assessment of treatment effects. The occurrence of fewer events due to early deaths can lead to misinterpretation, emphasizing the importance of a while-alive strategy as suggested in Schmidli et al. (2023). We focus in this paper on the patient weighted while-alive estimand represented as the expected number of events divided by the time alive within a target window and develop efficient estimation for this estimand. We derive its efficient influence function and develop a one-step estimator, initially applied to the irreversible illness-death model. For the broader context of recurrent events, due to the increased complexity, the one-step estimator is practically intractable. We therefore suggest an alternative estimator that is also expected to have high efficiency focusing on the randomized treatment setting. We compare the efficiency of these two estimators in the illness-death setting. Additionally, we apply our proposed estimator to a real-world case study involving metastatic colorectal cancer patients, demonstrating the practical applicability and benefits of the while-alive approach.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03246
Impact of nuclear masses on r-process nucleosynthesis: bulk properties versus shell effects,['Nuclear Theory'],"['Samuel A. Giuliani', 'Gabriel Martínez-Pinedo', 'Andreas Bauswein', 'Vimal Vijayan']","Decomposing theoretical nuclear mass predictions into a liquid-drop parametrization and local shell effects shows that r-process abundances are virtually insensitive to large variations of the masses which originate from nuclear bulk properties of the model, such as the symmetry energy. Therefore, experimental and theoretical studies of masses devoted to r-process applications, such as the nucleosynthesis in the ejecta of neutron star mergers, should focus on the physical origin of local changes in mass trends without necessarily providing highly accurate mass determinations of individual nuclei.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03243
Benchmarking terminology building capabilities of ChatGPT on an English-Russian Fashion Corpus,['Computation and Language'],"['Anastasiia Bezobrazova', 'Miriam Seghiri', 'Constantin Orasan']","This paper compares the accuracy of the terms extracted using SketchEngine, TBXTools and ChatGPT. In addition, it evaluates the quality of the definitions produced by ChatGPT for these terms. The research is carried out on a comparable corpus of fashion magazines written in English and Russian collected from the web. A gold standard for the fashion terminology was also developed by identifying web pages that can be harvested automatically and contain definitions of terms from the fashion domain in English and Russian. This gold standard was used to evaluate the quality of the extracted terms and of the definitions produced. Our evaluation shows that TBXTools and SketchEngine, while capable of high recall, suffer from reduced precision as the number of terms increases, which affects their overall performance. Conversely, ChatGPT demonstrates superior performance, maintaining or improving precision as more terms are considered. Analysis of the definitions produced by ChatGPT for 60 commonly used terms in English and Russian shows that ChatGPT maintains a reasonable level of accuracy and fidelity across languages, but sometimes the definitions in both languages miss crucial specifics and include unnecessary deviations. Our research reveals that no single tool excels universally; each has strengths suited to particular aspects of terminology extraction and application.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03242
Dynamic Consistent $k$-Center Clustering with Optimal Recourse,['Data Structures and Algorithms'],"['Sebastian Forster', 'Antonis Skarlatos']","Given points from an arbitrary metric space and a sequence of point updates sent by an adversary, what is the minimum recourse per update (i.e., the minimum number of changes needed to the set of centers after an update), in order to maintain a constant-factor approximation to a $k$-clustering problem? This question has received attention in recent years under the name consistent clustering.
  Previous works by Lattanzi and Vassilvitskii [ICLM '17] and Fichtenberger, Lattanzi, Norouzi-Fard, and Svensson [SODA '21] studied $k$-clustering objectives, including the $k$-center and the $k$-median objectives, under only point insertions. In this paper we study the $k$-center objective in the fully dynamic setting, where the update is either a point insertion or a point deletion. Before our work, Łącki, Haeupler, Grunau, Rozhoň, and Jayaram [SODA '24] gave a deterministic fully dynamic constant-factor approximation algorithm for the $k$-center objective with worst-case recourse of $2$ per update.
  In this work, we prove that the $k$-center clustering problem admits optimal recourse bounds by developing a deterministic fully dynamic constant-factor approximation algorithm with worst-case recourse of $1$ per update. Moreover our algorithm performs simple choices based on light data structures, and thus is arguably more direct and faster than the previous one which uses a sophisticated combinatorial structure. Additionally, we develop a new deterministic decremental algorithm and a new deterministic incremental algorithm, both of which maintain a $6$-approximate $k$-center solution with worst-case recourse of $1$ per update. Our incremental algorithm improves over the $8$-approximation algorithm by Charikar, Chekuri, Feder, and Motwani [STOC '97]. Finally, we remark that since all three of our algorithms are deterministic, they work against an adaptive adversary.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03238
Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?,['Computation and Language'],"['Sravanti Addepalli', 'Yerram Varun', 'Arun Suggala', 'Karthikeyan Shanmugam', 'Prateek Jain']","Large Language Models (LLMs) are known to be susceptible to crafted adversarial attacks or jailbreaks that lead to the generation of objectionable content despite being aligned to human preferences using safety fine-tuning methods. While the large dimensionality of input token space makes it inevitable to find adversarial prompts that can jailbreak these models, we aim to evaluate whether safety fine-tuned LLMs are safe against natural prompts which are semantically related to toxic seed prompts that elicit safe responses after alignment. We surprisingly find that popular aligned LLMs such as GPT-4 can be compromised using naive prompts that are NOT even crafted with an objective of jailbreaking the model. Furthermore, we empirically show that given a seed prompt that elicits a toxic response from an unaligned model, one can systematically generate several semantically related natural prompts that can jailbreak aligned LLMs. Towards this, we propose a method of Response Guided Question Augmentation (ReG-QA) to evaluate the generalization of safety aligned LLMs to natural prompts, that first generates several toxic answers given a seed question using an unaligned LLM (Q to A), and further leverages an LLM to generate questions that are likely to produce these answers (A to Q). We interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to producing natural jailbreak questions from unsafe content (without denial) and can thus be used for the latter (A to Q) step. We obtain attack success rates that are comparable to/ better than leading adversarial attack methods on the JailbreakBench leaderboard, while being significantly more stable against defenses such as Smooth-LLM and Synonym Substitution, which are effective against existing all attacks on the leaderboard.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03235
Numerical Study of Disordered Noninteracting Chains Coupled to a Local Lindblad Bath,['Disordered Systems and Neural Networks'],"['Viktor Berger', 'Andrea Nava', 'Jens H. Bardarson', 'Claudia Artiaco']","Disorder can prevent many-body quantum systems from reaching thermal equilibrium, leading to a many-body localized phase. Recent works suggest that nonperturbative effects caused by rare regions of low disorder may destabilize the localized phase. However, numerical simulations of interacting systems are generically possible only for small system sizes, where finite-size effects might dominate. Here we perform a numerical investigation of noninteracting disordered spin chains coupled to a local Lindblad bath at the boundary. Our results reveal strong finite-size effects in the Lindbladian gap in both bath-coupled Anderson and Aubry-André-Harper models, leading to a non-monotonic behavior with the system size. We discuss the relaxation properties of a simple toy model coupled to local Lindblad baths, connecting its features to those of noninteracting localized chains. We comment on the implications of our findings for many-body systems.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03233
Achieving Beamfocusing via Two Separated Uniform Linear Arrays,['Signal Processing'],"['Alva Kosasih', 'Özlem Tugfe Demir', 'Emil Björnson']","This paper investigates coordinated beamforming using a modular linear array (MLA), composed of a pair of physically separated uniform linear arrays (ULAs), treated as sub-arrays. We focus on how such setups can give rise to near-field effects in 6G networks without requiring many antennas. Unlike conventional far-field beamforming, near-field beamforming enables simultaneous data service to multiple users at different distances in the same angular direction, offering significant multiplexing gains. We present a detailed analysis, including analytical expressions of the beamwidth and beamdepth for the MLA. Our findings reveal that using the MLA approach, we can remove approximately 36% of the antennas in the ULA while achieving the same level of beamfocusing.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03232
Information thermodynamics for Markov jump processes coupled to underdamped diffusion: Application to nanoelectromechanics,['Statistical Mechanics'],"['Ashwin Gopal', 'Nahuel Freitas', 'Massimiliano Esposito']","We extend the principles of information thermodynamics to study energy and information exchanges between coupled systems composed of one part undergoing a Markov jump process and another underdamped diffusion. We derive integral fluctuation theorems for the partial entropy production of each subsystem and analyze two distinct regimes. First, when the inertial dynamics is slow compared to the discrete-state transitions, we show that the steady-state energy and information flows vanish at the leading order in an adiabatic approximation, if the underdamped subsystem is governed purely by conservative forces. To capture the non-zero contributions, we consistently derive dynamical equations valid to higher order. Second, in the limit of infinite mass, the underdamped dynamics becomes a deterministic Hamiltonian dynamics driving the jump processes, we capture the next-order correction beyond this limit. We apply our framework to study self-oscillations in the single-electron shuttle - a nanoelectromechanical system (NEMS) - from a measurement-feedback perspective. We find that energy flows dominate over information flows in the self-oscillating regime, and study the efficiency with which this NEMS converts electrical work into mechanical oscillations.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03226
Building Europe's first space-based Quantum Key Distribution system -- The German Aerospace Center's role in the EAGLE-1 mission,['Quantum Physics'],"['Gabriela Calistro Rivera', 'Oliver Heirich', 'Amita Shrestha', 'Agnes Ferenczi', 'Alexandru Duliu', 'Jakob Eppinger', 'Bruno Femenia Castella', 'Christian Fuchs', 'Elisa Garbagnati', 'Douglas Laidlaw', 'Pia Lützen', 'Innocenzo De Marco', 'Florian Moll', 'Johannes Prell', 'Andrew Reeves', 'Jorge Rosano Nonay', 'Christian Roubal', 'Joana S. Torres', 'Matthias Wagner']","The EAGLE-1 mission aims to develop Europe's first sovereign, end-to-end space-based quantum key distribution (QKD) system. The mission is led by the European Space Agency (ESA) and SES in collaboration with several European National Space Agencies and private partners. The state-of-the-art QKD system will consist of a payload on board the EAGLE-1 low Earth orbit (LEO) satellite, optical ground stations, quantum operational networks, and key management system. The EAGLE-1 mission represents a major step for next-generation quantum communication infrastructures, delivering valuable technical results and mission data. The Institute of Communications and Navigation (IKN) of the German Aerospace Center (DLR) is a key partner in the EAGLE-1 mission and is involved in the research and development of elements in both space and ground segments. Here we report on the development of the QKD transmitter, a vital part of the QKD payload, and the customization of the Optical Ground Station Oberpfaffenhofen (OGS-OP) to conduct the IOT phase of EAGLE-1. For the space segment, DLR-IKN is in charge of the design of the QKD transmitter, including the development of the software and firmware. This transmitter generates quantum states which are used to implement a QKD protocol based on an optical signal, that will be transmitted to ground. For the ground segment, The OGS-OP will serve as the in-orbit testing ground station for EAGLE-1. Building upon the expertise with a range of satellites for quantum communication, as well as new implementations, OGS-OP will validate the performance of the payload, optical link and QKD system for the first time. We present the main developments of OGS-OP for the mission, which includes the implementation of an upgraded adaptive optics system to correct for atmospheric distortions and optimize the coupling of the incoming light into a single mode optical fiber.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03222
"Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges",['Machine Learning'],"['Minghao Shao', 'Abdul Basit', 'Ramesh Karri', 'Muhammad Shafique']","Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03220
Revisiting Atomic Patterns for Elliptic Curve Scalar Multiplication Revealing Inherent Vulnerability to Simple SCA,['Cryptography and Security'],"['Alkistis Aikaterini Sigourou', 'Zoya Dyka', 'Sze Hei Li', 'Peter Langendoerfer', 'Ievgen Kabin']","Elliptic Curve Scalar Multiplication denoted as kP operation is the basic operation in all Elliptic Curve based cryptographic protocols. The atomicity principle and different atomic patterns for kP algorithms were proposed in the past as countermeasures against simple side-channel analysis. In this work, we investigated the resistance of a kP algorithm implemented in hardware using Longa's atomic patterns. We analysed its simulated power trace. We show in the example of our kP implementation for the NIST EC P-256 that the field squaring operations are distinguishable from the field multiplications even if they are performed by the same field multiplier, due to the addressing of the second multiplicand. This inherent vulnerability of atomic patterns can be successfully exploited for revealing the scalar k.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03218
Social media and suicide: empirical evidence from the quasi-exogenous geographical adoption of Twitter,['General Economics'],"['Alexis Du', 'Thomas Renault']","Social media usage is often cited as a potential driver behind the rising suicide rates. However, distinguishing the causal effect - whether social media increases the risk of suicide - from reverse causality, where individuals already at higher risk of suicide are more likely to use social media, remains a significant challenge. In this paper, we use an instrumental variable approach to study the quasi-exogenous geographical adoption of Twitter and its causal relationship with suicide rates. Our analysis first demonstrates that Twitter's geographical adoption was driven by the presence of certain users at the 2007 SXSW festival, which led to long-term disparities in adoption rates across counties in the United States. Then, using a two-stage least squares (2SLS) regression and controlling for a wide range of geographic, socioeconomic and demographic factors, we find no significant relationship between Twitter adoption and suicide rates.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03217
Continual Low-Rank Scaled Dot-product Attention,['Computer Vision and Pattern Recognition'],"['Ginés Carreto Picón', 'Illia Oleksiienko', 'Lukas Hedegaard', 'Arian Bakhtiarnia', 'Alexandros Iosifidis']","Transformers are widely used for their ability to capture data relations in sequence processing, with great success for a wide range of static tasks. However, the computational and memory footprint of their main component, i.e., the Scaled Dot-product Attention, is commonly overlooked. This makes their adoption in applications involving stream data processing with constraints in response latency, computational and memory resources infeasible. Some works have proposed methods to lower the computational cost of transformers, i.e. low-rank approximations, sparsity in attention, and efficient formulations for Continual Inference. In this paper, we introduce a new formulation of the Scaled Dot-product Attention based on the Nyström approximation that is suitable for Continual Inference. In experiments on Online Audio Classification and Online Action Detection tasks, the proposed Continual Scaled Dot-product Attention can lower the number of operations by up to three orders of magnitude compared to the original Transformers while retaining the predictive performance of competing models.△ Less",v1,https://arxiv.org/pdf/2412.03214
Magnetic Topology of quiet-Sun Ellerman bombs and associated Ultraviolet brightenings,['Solar and Stellar Astrophysics'],"['Aditi Bhatnagar', 'Avijeet Prasad', 'Luc Rouppe van der Voort', 'Daniel Nóbrega-Siverio', 'Jayant Joshi']","Quiet-Sun Ellerman bombs (QSEBs) are small-scale magnetic reconnection events in the lower atmosphere of the quiet Sun. Recent work has shown that a small percentage of them can occur co-spatially and co-temporally to ultraviolet (UV) brightenings in the transition region. We aim to understand how the magnetic topologies associated with closely occurring QSEBs and UV brightenings can facilitate energy transport and connect these events. We used high-resolution H-beta observations from the Swedish 1-m Solar Telescope (SST) and detected QSEBs using k-means clustering. We obtained the magnetic field topology from potential field extrapolations using spectro-polarimetric data in the photospheric Fe I 6173 A line. To detect UV brightenings, we used coordinated and co-aligned data from the Interface Region Imaging Spectrograph (IRIS) and imposed a threshold of 5 sigma above the median background on the (IRIS) 1400 A slit-jaw image channel. We identify four distinct magnetic configurations that associate QSEBs with UV brightenings, including a simple dipole configuration and more complex fan-spine topologies with a three-dimensional (3D) magnetic null point. In the fan-spine topology, the UV brightenings occur near the 3D null point, while QSEBs can be found close to the footpoints of the outer spine, the inner spine, and the fan surface. We find that the height of the 3D null varies between 0.2 Mm to 2.6 Mm, depending on the magnetic field strength in the region. We note that some QSEBs and UV brightenings, though occurring close to each other, are not topologically connected with the same reconnection process. We find that the energy released during QSEBs falls in the range of 10^23 to 10^24 ergs. This study shows that magnetic connectivity and topological features, like 3D null points, are crucial in linking QSEBs in the lower atmosphere with UV brightenings in the transition region.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03211
Integrated InP-based transmitter for Continuous-Variable Quantum Key Distribution,['Quantum Physics'],"['Jennifer Aldama', 'Samael Sarmiento', 'Luis Trigo Vidarte', 'Sebastian Etcheverry', 'Ignacio López Grande', 'Lorenzo Castelvero', 'Alberto Hinojosa', 'Tobias Beckerwerth', 'Yoann Piétri', 'Amine Rhouni', 'Eleni Diamanti', 'Valerio Pruneri']","Developing quantum key distribution (QKD) systems using monolithic photonic integrated circuits (PICs) can accelerate their adoption by a wide range of markets, thanks to the potential reduction in size, complexity of the overall system, power consumption, and production cost. In this work, we design, fabricate and characterize an InP-based PIC transmitter for continuous-variable (CV) QKD applications. In a proof-of-principle experiment implementing a pulsed Gaussian-modulated coherent state (GMCS) CV-QKD protocol over an optical fiber channel of 11 km, the system showed a performance compatible with a secret key rate of 78 kbps in the asymptotic regime. These results show the potential of InP technologies to integrate CV-QKD systems onto a monolithic platform.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03208
Experimental reservoir computing with diffractively coupled VCSELs,['Emerging Technologies'],"['Moritz Pflüger', 'Daniel Brunner', 'Tobias Heuser', 'James A. Lott', 'Stephan Reitzenstein', 'Ingo Fischer']","We present experiments on reservoir computing (RC) using a network of vertical-cavity surface-emitting lasers (VCSELs) that we diffractively couple via an external cavity. Our optical reservoir computer consists of 24 physical VCSEL nodes. We evaluate the system's memory and solve the 2-bit XOR task and the 3-bit header recognition (HR) task with bit error ratios (BERs) below 1\,\% and the 2-bit digital-to-analog conversion (DAC) task with a root-mean-square error (RMSE) of 0.067.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03206
U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs,['Computation and Language'],"['Konstantin Chernyshev', 'Vitaliy Polshkov', 'Ekaterina Artemova', 'Alex Myasnikov', 'Vlad Stepanov', 'Alexei Miasnikov', 'Sergei Tilga']","The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored.
  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release $μ$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.
  The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on $μ$-MATH.△ Less",v1,https://arxiv.org/pdf/2412.03205
Quantum Optical Binding of Nanoscale Particles,['Quantum Physics'],"['Henning Rudolph', 'Uroš Delić', 'Klaus Hornberger', 'Benjamin A. Stickler']","Optical binding refers to the light-induced interaction between two or more objects illuminated by laser fields. The high tunability of the strength, sign, and reciprocity of this interaction renders it highly attractive for controlling nanoscale mechanical motion. Here, we discuss the quantum theory of optical binding and identify unique signatures of this interaction in the quantum regime. We show that these signatures are observable in near-future experiments with levitated nanoparticles. In addition, we prove the impossibility of entanglement induced by far-field optical binding in free space and identify strategies to circumvent this no-go theorem.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03204
Null device-independent prepare-and-prepare bipartite dimension test with a single joint measurement,['Quantum Physics'],"['Josep Batle', 'Tomasz Białecki', 'Adam Bednorz']","We propose a device-independent dimensionality test with bipartite measurements and input from two separate parties, based on a null witness. The dimension is determined from the rank of the matrix of measurements for pairs of states prepared by the parties. We have applied the test to various IBM Quantum devices. The results demonstrate extreme precision of the test, which is able to detect disagreements with the qubit (two-level) space of bipartite measurement even in the presence of technical imperfections. The deviations beyond 6 standard deviations have no simple origin and need urgent explanations to unblock progress in quantum computing.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03197
Soft Adaptive Feet for Legged Robots: An Open-Source Model for Locomotion Simulation,['Robotics'],"['Matteo Crotti', 'Luca Rossini', 'Anna Pace', 'Giorgio Grioli', 'Antonio Bicchi', 'Manuel G. Catalano']","In recent years, artificial feet based on soft robotics and under-actuation principles emerged to improve mobility on challenging terrains. This paper presents the application of the MuJoCo physics engine to realize a digital twin of an adaptive soft foot developed for use with legged robots. We release the MuJoCo soft foot digital twin as open source to allow users and researchers to explore new approaches to locomotion. The work includes the system modeling techniques along with the kinematic and dynamic attributes involved. Validation is conducted through a rigorous comparison with bench tests on a physical prototype, replicating these experiments in simulation. Results are evaluated based on sole deformation and contact forces during foot-obstacle interaction. The foot model is subsequently integrated into simulations of the humanoid robot COMAN+, replacing its original flat feet. Results show an improvement in the robot's ability to negotiate small obstacles without altering its control strategy. Ultimately, this study offers a comprehensive modeling approach for adaptive soft feet, supported by qualitative comparisons of bipedal locomotion with state of the art robotic feet.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03191
First Measurement of the Muon Neutrino Interaction Cross Section and Flux as a Function of Energy at the LHC with FASER,['High Energy Physics - Experiment'],"['FASER Collaboration', 'Roshan Mammen Abraham', 'Xiaocong Ai', 'John Anders', 'Claire Antel', 'Akitaka Ariga', 'Tomoko Ariga', 'Jeremy Atkinson', 'Florian U. Bernlochner', 'Tobias Boeckh', 'Jamie Boyd', 'Lydia Brenner', 'Angela Burger', 'Franck Cadoux', 'Roberto Cardella', 'David W. Casper', 'Charlotte Cavanagh', 'Xin Chen', 'Dhruv Chouhan', 'Andrea Coccaro', 'Stephane Débieux', ""Monica D'Onofrio"", 'Ansh Desai', 'Sergey Dmitrievsky', 'Radu Dobre']","This letter presents the measurement of the energy-dependent neutrino-nucleon cross section in tungsten and the differential flux of muon neutrinos and anti-neutrinos. The analysis is performed using proton-proton collision data at a center-of-mass energy of $13.6 \, {\rm TeV}$ and corresponding to an integrated luminosity of $(65.6 \pm 1.4) \, \mathrm{fb^{-1}}$. Using the active electronic components of the FASER detector, $338.1 \pm 21.0$ charged current muon neutrino interaction events are identified, with backgrounds from other processes subtracted. We unfold the neutrino events into a fiducial volume corresponding to the sensitive regions of the FASER detector and interpret the results in two ways: We use the expected neutrino flux to measure the cross section, and we use the predicted cross section to measure the neutrino flux. Both results are presented in six bins of neutrino energy, achieving the first differential measurement in the TeV range. The observed distributions align with Standard Model predictions. Using this differential data, we extract the contributions of neutrinos from pion and kaon decays.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03186
Information borrowing in Bayesian clinical trials: choice of tuning parameters for the robust mixture prior,['Methodology'],"['Vivienn Weru', 'Annette Kopp-Schneider', 'Manuel Wiesenfarth', 'Sebastian Weber', 'Silvia Calderazzo']","Borrowing historical data for use in clinical trials has increased in recent years. This is accomplished in the Bayesian framework by specification of informative prior distributions. One such approach is the robust mixture prior arising as a weighted mixture of an informative prior and a robust prior inducing dynamic borrowing that allows to borrow most when the current and external data are observed to be similar. The robust mixture prior requires the choice of three additional quantities: the mixture weight, and the mean and dispersion of the robust component. Some general guidance is available, but a case-by-case study of the impact of these quantities on specific operating characteristics seems lacking. We focus on evaluating the impact of parameter choices for the robust component of the mixture prior in one-arm and hybrid-control trials. The results show that all three quantities can strongly impact the operating characteristics. In particular, as already known, variance of the robust component is linked to robustness. Less known, however, is that its location can have a strong impact on Type I error rate and MSE which can even become unbounded. Further, the impact of the weight choice is strongly linked with the robust component's location and variance. Recommendations are provided for the choice of the robust component parameters, prior weight, alternative functional form for this component as well as considerations to keep in mind when evaluating operating characteristics.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03185
K-varieties and Galois representations,['Number Theory'],"['Enric Florit', 'Ariel Pacetti']","In a remarkable article Ribet showed how to attach rational $2$-dimensional representations to elliptic ${\mathbb Q}$-curves. An abelian variety $A$ is a (weak) $K$-variety if it is isogenous to all of its $\text{Gal}_K$-conjugates. In this article we study the problem of attaching an absolutely irreducible $\ell$-adic representation of $\text{Gal}_K$ to an abelian $K$-variety, which sometimes has smaller dimension than expected. When possible, we also construct a Galois-equivariant pairing, which restricts the image of this representation. As an application of our construction, we prove modularity of abelian surfaces over ${\mathbb Q}$ with potential quaternionic multiplication.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03184
Quantitative convergence of trained quantum neural networks to a Gaussian process,['Quantum Physics'],"['Anderson Melchor Hernandez', 'Filippo Girardi', 'Davide Pastorello', 'Giacomo De Palma']","We study quantum neural networks where the generated function is the expectation value of the sum of single-qubit observables across all qubits. In [Girardi \emph{et al.}, arXiv:2402.08726], it is proven that the probability distributions of such generated functions converge in distribution to a Gaussian process in the limit of infinite width for both untrained networks with randomly initialized parameters and trained networks. In this paper, we provide a quantitative proof of this convergence in terms of the Wasserstein distance of order $1$. First, we establish an upper bound on the distance between the probability distribution of the function generated by any untrained network with finite width and the Gaussian process with the same covariance. This proof utilizes Stein's method to estimate the Wasserstein distance of order $1$. Next, we analyze the training dynamics of the network via gradient flow, proving an upper bound on the distance between the probability distribution of the function generated by the trained network and the corresponding Gaussian process. This proof is based on a quantitative upper bound on the maximum variation of a parameter during training. This bound implies that for sufficiently large widths, training occurs in the lazy regime, \emph{i.e.}, each parameter changes only by a small amount. While the convergence result of [Girardi \emph{et al.}, arXiv:2402.08726] holds at a fixed training time, our upper bounds are uniform in time and hold even as $t \to \infty$.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03182
Evidence of spin reorientation transition below 150 K from magnetic force microscopy in a ferromagnetic BiFeO$_3$ thin film,['Materials Science'],"['Sudipta Goswami', 'Shubhankar Mishra', 'Kishor Kumar Sahoo', 'Kumar Brajesh', 'Mihir Ranjan Sahoo', 'Subhashree Chatterjee', 'Devajyoti Mukherjee', 'Kalpataru Pradhan', 'Ashish Garg', 'Chandan Kumar Ghosh', 'Dipten Bhattacharya']","We investigated the magnetic transitions in BiFeO$_3$ at low temperature (5-300 K) and observed nearly 90$^o$ rotation of magnetic domains (imaged by vertical magnetic force microscopy) across 150 K in an epitaxial thin film of thickness $\sim$36 nm. It offers a clear evidence of spin reorientation transition. It also corroborates the transition observed below $\sim$150 K in the zero-field-cooled and field-cooled magnetization versus temperature data. The field-driven 180$^o$ domain switching at room temperature, on the other hand, signifies presence of ferromagnetism. Since bulk antiferromagnetic BiFeO$_3$ does not exhibit such a transition, this observation in ferromagnetic thin film of BiFeO$_3$ indicates a radical effect because of epitaxial strain. Density functional theory based first-principles calculations too reveal that combined in- and out-of-plane epitaxial strain induces magnetic transition from G- to C-type structure in BiFeO$_3$.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03180
Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation,['Artificial Intelligence'],"['Gianni Franchi', 'Dat Nguyen Trong', 'Nacim Belkhir', 'Guoxuan Xia', 'Andrea Pilzer']","Uncertainty quantification in text-to-image (T2I) generative models is crucial for understanding model behavior and improving output reliability. In this paper, we are the first to quantify and evaluate the uncertainty of T2I models with respect to the prompt. Alongside adapting existing approaches designed to measure uncertainty in the image space, we also introduce Prompt-based UNCertainty Estimation for T2I models (PUNC), a novel method leveraging Large Vision-Language Models (LVLMs) to better address uncertainties arising from the semantics of the prompt and generated images. PUNC utilizes a LVLM to caption a generated image, and then compares the caption with the original prompt in the more semantically meaningful text space. PUNC also enables the disentanglement of both aleatoric and epistemic uncertainties via precision and recall, which image-space approaches are unable to do. Extensive experiments demonstrate that PUNC outperforms state-of-the-art uncertainty estimation techniques across various settings. Uncertainty quantification in text-to-image generation models can be used on various applications including bias detection, copyright protection, and OOD detection. We also introduce a comprehensive dataset of text prompts and generation pairs to foster further research in uncertainty quantification for generative models. Our findings illustrate that PUNC not only achieves competitive performance but also enables novel applications in evaluating and improving the trustworthiness of text-to-image models.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03178
Resilient Timed Elastic Band Planner for Collision-Free Navigation in Unknown Environments,['Robotics'],"['Geesara Kulathunga', 'Abdurrahman Yilmaz', 'Zhuoling Huang', 'Ibrahim Hroob', 'Hariharan Arunachalam', 'Leonardo Guevara', 'Alexandr Klimchik', 'Grzegorz Cielniak', 'Marc Hanheide']","In autonomous navigation, trajectory replanning, refinement, and control command generation are essential for effective motion planning. This paper presents a resilient approach to trajectory replanning addressing scenarios where the initial planner's solution becomes infeasible. The proposed method incorporates a hybrid A* algorithm to generate feasible trajectories when the primary planner fails and applies a soft constraints-based smoothing technique to refine these trajectories, ensuring continuity, obstacle avoidance, and kinematic feasibility. Obstacle constraints are modelled using a dynamic Voronoi map to improve navigation through narrow passages. This approach enhances the consistency of trajectory planning, speeds up convergence, and meets real-time computational requirements. In environments with around 30\% or higher obstacle density, the ratio of free space before and after placing new obstacles, the Resilient Timed Elastic Band (RTEB) planner achieves approximately 20\% reduction in traverse distance, traverse time, and control effort compared to the Timed Elastic Band (TEB) planner and Nonlinear Model Predictive Control (NMPC) planner. These improvements demonstrate the RTEB planner's potential for application in field robotics, particularly in agricultural and industrial environments, where navigating unstructured terrain is crucial for ensuring efficiency and operational resilience.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03174
IRisPath: Enhancing Off-Road Navigation with Robust IR-RGB Fusion for Improved Day and Night Traversability,['Robotics'],"['Saksham Sharma', 'Akshit Raizada', 'Suresh Sundaram']","Autonomous off-road navigation is required for applications in agriculture, construction, search and rescue and defence. Traditional on-road autonomous methods struggle with dynamic terrains, leading to poor vehicle control on off-road. Recent deep-learning models have used perception sensors along with kinesthetic feedback for navigation on such terrains. However, this approach has out-of-domain uncertainty. Factors like change in weather and time of day impacts the performance of the model. We propose a multi modal fusion network FuseIsPath capable of using LWIR and RGB images to provide robustness against dynamic weather and light conditions. To aid further works in this domain, we also open-source a day-night dataset with LWIR and RGB images along with pseudo-labels for traversability. In order to co-register the two images we developed a novel method for targetless extrinsic calibration of LWIR, LiDAR and RGB cameras with translation accuracy of 1.7cm and rotation accuracy of 0.827degree.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03173
Numin: Weighted-Majority Ensembles for Intraday Trading,"['Computational Engineering, Finance, and Science']","['Aniruddha Mukherjee', 'Rekha Singhal', 'Gautam Shroff']","We consider the application of machine learning models for short-term intra-day trading in equities. We envisage a scenario wherein machine learning models are submitted by independent data scientists to predict discretised ten-candle returns every five minutes, in response to five-minute candlestick data provided to them in near real-time. An ensemble model combines these multiple models via a weighted-majority algorithm. The weights of each model are dynamically updated based on the performance of each model, and can also be used to reward model owners. Each model's performance is evaluated according to two different metrics over a recent time window: In addition to accuracy, we also consider a `utility' metric that is a proxy for a model's potential profitability under a particular trading strategy. We present experimental results on real intra-day data that show that our weighted-majority ensemble techniques show improved accuracy as well as utility over any of the individual models, especially using the utility metric to dynamically re-weight models over shorter time-windows.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03167
Are Explanations Helpful? A Comparative Analysis of Explainability Methods in Skin Lesion Classifiers,['Computer Vision and Pattern Recognition'],"['Rosa Y. G. Paccotacya-Yanque', 'Alceu Bissoto', 'Sandra Avila']","Deep Learning has shown outstanding results in computer vision tasks; healthcare is no exception. However, there is no straightforward way to expose the decision-making process of DL models. Good accuracy is not enough for skin cancer predictions. Understanding the model's behavior is crucial for clinical application and reliable outcomes. In this work, we identify desiderata for explanations in skin-lesion models. We analyzed seven methods, four based on pixel-attribution (Grad-CAM, Score-CAM, LIME, SHAP) and three on high-level concepts (ACE, ICE, CME), for a deep neural network trained on the International Skin Imaging Collaboration Archive. Our findings indicate that while these techniques reveal biases, there is room for improving the comprehensiveness of explanations to achieve transparency in skin-lesion models.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03166
LEP-QNN: Loan Eligibility Prediction Using Quantum Neural Networks,['Quantum Physics'],"['Nouhaila Innan', 'Alberto Marchisio', 'Mohamed Bennai', 'Muhammad Shafique']","Predicting loan eligibility with high accuracy remains a significant challenge in the finance sector. Accurate predictions enable financial institutions to make informed decisions, mitigate risks, and effectively adapt services to meet customer needs. However, the complexity and the high-dimensional nature of financial data have always posed significant challenges to achieving this level of precision. To overcome these issues, we propose a novel approach that employs Quantum Machine Learning (QML) for Loan Eligibility Prediction using Quantum Neural Networks (LEP-QNN).Our innovative approach achieves an accuracy of 98% in predicting loan eligibility from a single, comprehensive dataset. This performance boost is attributed to the strategic implementation of a dropout mechanism within the quantum circuit, aimed at minimizing overfitting and thereby improving the model's predictive reliability. In addition, our exploration of various optimizers leads to identifying the most efficient setup for our LEP-QNN framework, optimizing its performance. We also rigorously evaluate the resilience of LEP-QNN under different quantum noise scenarios, ensuring its robustness and dependability for quantum computing environments. This research showcases the potential of QML in financial predictions and establishes a foundational guide for advancing QML technologies, marking a step towards developing advanced, quantum-driven financial decision-making tools.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03158
Higher order potential in the modified convective-viscous Cahn-Hilliard equation,['Materials Science'],"['P. O. Mchedlov-Petrosian', 'L. N. Davydov', 'O. A. Osmaev']","To describe highly heterogeneous systems using the Cahn-Hilliard equation, the standard form of the thermodynamic potential with a constant coefficient in the gradient term and a polynomial of the fourth degree may not be sufficient. The modification of the form of the thermodynamic potential with a polynomial of the sixth degree and the quadratic dependence of the coefficient at the gradient term is considered. Exact solutions in the form of a moving static wave and the conditions of their existence depending on the symmetry of the potential are obtained.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03156
Testing Neural Network Verifiers: A Soundness Benchmark with Hidden Counterexamples,['Machine Learning'],"['Xingjian Zhou', 'Hongji Xu', 'Andy Xu', 'Zhouxing Shi', 'Cho-Jui Hsieh', 'Huan Zhang']","In recent years, many neural network (NN) verifiers have been developed to formally verify certain properties of neural networks such as robustness. Although many benchmarks have been constructed to evaluate the performance of NN verifiers, they typically lack a ground-truth for hard instances where no current verifier can verify and no counterexample can be found, which makes it difficult to check the soundness of a new verifier if it claims to verify hard instances which no other verifier can do. We propose to develop a soundness benchmark for NN verification. Our benchmark contains instances with deliberately inserted counterexamples while we also try to hide the counterexamples from regular adversarial attacks which can be used for finding counterexamples. We design a training method to produce neural networks with such hidden counterexamples. Our benchmark aims to be used for testing the soundness of NN verifiers and identifying falsely claimed verifiability when it is known that hidden counterexamples exist. We systematically construct our benchmark and generate instances across diverse model architectures, activation functions, input sizes, and perturbation radii. We demonstrate that our benchmark successfully identifies bugs in state-of-the-art NN verifiers, as well as synthetic bugs, providing a crucial step toward enhancing the reliability of testing NN verifiers. Our code is available at https://github.com/MVP-Harry/SoundnessBench and our benchmark is available at https://huggingface.co/datasets/SoundnessBench/SoundnessBench.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03154
Mapping delocalization of impurity bands across archetypal Mott-Anderson transition,['Disordered Systems and Neural Networks'],"['M. Parzer', 'F. Garmroudi', 'A. Riss', 'T. Mori', 'A. Pustogow', 'E. Bauer']","Tailoring charge transport in solids on demand is the overarching goal of condensed-matter research as it is crucial for electronic applications. Yet, often the proper tuning knob is missing and extrinsic factors such as impurities and disorder impede coherent conduction. Here we control the very buildup of an electronic band from impurity states within the pseudogap of ternary Fe$_{2-x}$V$_{1+x}$Al Heusler compounds via reducing the Fe content. Our density functional theory calculations combined with specific heat and electrical resistivity experiments reveal that, initially, these states are Andersonlocalized at low V concentrations $0 < x < 0.1$. As x increases, we monitor the formation of mobility edges upon the archetypal Mott-Anderson transition and map the increasing bandwidth of conducting states by thermoelectric measurements. Ultimately, delocalization of charge carriers in fully disordered V$_3$Al results in a resistivity exactly at the Mott-Ioffe-Regel limit that is perfectly temperature-independent up to 700 K - more constant than constantan.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03147
"The total mass of the close, double degenerate (DA+DQ) system NLTT~16249",['Solar and Stellar Astrophysics'],"['Stephane Vennes', 'Adela Kawka']","We revisit the binary and stellar properties of the double-degenerate system NLTT 16249. An analysis of new echelle spectra, supported by a joint study of a DQZ velocity template NLTT 44303, confirms the orbital period and constrains the mass ratio revealing a carbon-polluted DQ white dwarf that is up to ~6 percent more massive than its hydrogen-rich DA companion. Our new model atmosphere analysis of the DA and DQ components, constrained by an accurate Gaia parallax measurement that places the binary at a distance of 57.8 pc, reveals lower mass and temperature than previously estimated for both components, but with higher carbon and nitrogen abundances in the DQ atmosphere. The two components are nearly coeval and could have been generated following a single common envelope event.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03144
Low-Frequency Turnover Star Forming Galaxies I: Radio Continuum Observations and Global Properties,['Astrophysics of Galaxies'],"['J. A. Grundy', 'N. Seymour', 'O. I. Wong', 'K. Lee-Waddell', 'T. J. Galvin', 'M. Cluver']","The broad-band radio spectral energy distribution (SED) of star-forming galaxies (SFGs) contains a wealth of complex physics. We aim to determine the physical emission and loss processes causing radio SED curvature and steepening to see which observed global astrophysical properties are correlated with radio SED complexity. We have acquired radio continuum data between 70 MHz and 17 GHz for a sample of 19 southern local (z < 0.04) SFGs. Of this sample 11 are selected to contain low-frequency (< 300 MHz) turnovers (LFTOs) in their SEDs and eight are control galaxies with similar global properties. We model the radio SEDs for our sample using a Bayesian framework whereby radio emission (synchrotron and free-free) and absorption or loss processes are included modularly. We find that without the inclusion of higher frequency data, single synchrotron power-law based models are always preferred for our sample; however, additional processes including free-free absorption (FFA) and synchrotron losses are often required to accurately model radio SED complexity in SFGs. The fitted synchrotron spectral indices range from -0.45 to -1.07 and are strongly anticorrelated with stellar mass suggesting that synchrotron losses are the dominant mechanism acting to steepen the spectral index in larger nearby SFGs. We find that LFTOs in the radio SED are independent from the inclination. The merging systems in our SFG sample have elevated specific star formation rates and flatter fitted spectral indices with unconstrained LFTOs. Lastly, we find no significant separation in global properties between SFGs with or without modelled LFTOs. Overall LFTOs are likely caused by a combination of FFA and ionisation losses in individual recent starburst regions with specific orientations and interstellar medium properties that, when averaged over the entire galaxy, do not correlate with global astrophysical properties.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03143
Short-reach Optical Communications: A Real-world Task for Neuromorphic Hardware,['Neural and Evolutionary Computing'],"['Elias Arnold', 'Eike-Manuel Edelmann', 'Alexander von Bank', 'Eric Müller', 'Laurent Schmalen', 'Johannes Schemmel']","Spiking neural networks (SNNs) emulated on dedicated neuromorphic accelerators promise to offer energy-efficient signal processing. However, the neuromorphic advantage over traditional algorithms still remains to be demonstrated in real-world applications. Here, we describe an intensity-modulation, direct-detection (IM/DD) task that is relevant to high-speed optical communication systems used in data centers. Compared to other machine learning-inspired benchmarks, the task offers several advantages. First, the dataset is inherently time-dependent, i.e., there is a time dimension that can be natively mapped to the dynamic evolution of SNNs. Second, small-scale SNNs can achieve the target accuracy required by technical communication standards. Third, due to the small scale and the defined target accuracy, the task facilitates the optimization for real-world aspects, such as energy efficiency, resource requirements, and system complexity.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03129
ObjectFinder: Open-Vocabulary Assistive System for Interactive Object Search by Blind People,['Human-Computer Interaction'],"['Ruiping Liu', 'Jiaming Zhang', 'Angela Schön', 'Karin Müller', 'Junwei Zheng', 'Kailun Yang', 'Kathrin Gerling', 'Rainer Stiefelhagen']","Assistive technology can be leveraged by blind people when searching for objects in their daily lives. We created ObjectFinder, an open-vocabulary interactive object-search prototype, which combines object detection with scene description and navigation. It enables blind persons to detect and navigate to objects of their choice. Our approach used co-design for the development of the prototype. We further conducted need-finding interviews to better understand challenges in object search, followed by a study with the ObjectFinder prototype in a laboratory setting simulating a living room and an office, with eight blind users. Additionally, we compared the prototype with BeMyEyes and Lookout for object search. We found that most participants felt more independent with ObjectFinder and preferred it over the baselines when deployed on more efficient hardware, as it enhances mental mapping and allows for active target definition. Moreover, we identified factors for future directions for the development of object-search systems.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03118
Matrix valued orthogonal polynomials arising from hexagon tilings with 3x3-periodic weightings,['Classical Analysis and ODEs'],['Arno B. J. Kuijlaars'],"Matrix valued orthogonal polynomials (MVOP) appear in the study of doubly periodic tiling models. Of particular interest is their limiting behavior as the degree tends to infinity. In recent years, MVOP associated with doubly periodic domino tilings of the Aztec diamond have been successfully analyzed. The MVOP related to doubly periodic lozenge tilings of a hexagon are more complicated. In this paper we focus on a special subclass of hexagon tilings with 3x3 periodicity. The special subclass leads to a genus one spectral curve with additional symmetries that allow us to find an equilibrium measure in an external field explicitly. The equilibrium measure gives the asymptotic distribution for the zeros of the determinant of the MVOP. The associated g-functions appear in the strong asymptotic formula for the MVOP that we obtain from a steepest descent analysis of the Riemann-Hilbert problem for MVOP.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03115
Non-linear shattered pellet injection modelling in ASDEX Upgrade,['Plasma Physics'],"['W. Tang', 'M. Hoelzl', 'M. Lehnen', 'D. Hu', 'F. J. Artola', 'P. Halldestam', 'P. Heinrich', 'S. Jachmich', 'E. Nardon', 'G. Papp', 'A. Patel', 'the ASDEX Upgrade Team', 'the EUROfusion Tokamak Exploitation Team', 'the JOREK Team']","Shattered pellet injection (SPI) is selected for the disruption mitigation system in ITER, due to deeper penetration, expected assimilation efficiency and prompt material delivery. This article describes non-linear simulations of SPI in the ASDEX Upgrade tokamak to test the mitigation efficiency of different injection parameters for neon-doped deuterium pellets using the JOREK code. The simulations are executed as fluid simulations. Additional marker particles are used to evolve the charge state distribution of impurities based on OpenADAS atomic data, i.e., no coronal equilibrium assumption is made. Regarding the pellet composition, neon fraction scans from 0 - 10% are performed. Numerical results show that the thermal quench (TQ) occurs in two stages. In the first stage, approximately half of the thermal energy is lost abruptly, primarily through convective and conductive transport in the stochastic fields. This stage is relatively independent of the injection parameters. In the second stage, where the majority of the remaining thermal energy is lost, radiation plays a dominant role. In cases of very low neon content, this second stage may not occur at all. A larger fraction ($\sim $20%) of the total material in the pellet is assimilated in the plasma for low neon fraction pellets (0.12%) since the full thermal collapse of the plasma occurs later than in high neon fraction scenarios. Nevertheless, the total number of assimilated neon atoms increases with increasing neon fraction. The effects of fragment size and penetration speed are further studied. Slower and smaller fragments promote edge cooling and the formation of a cold front. Faster fragments result in shorter TQ duration and higher assimilation as they reach the hotter plasma regions quicker. Using synthetic diagnostics, comparisons of general trend between simulations and experiments are conducted.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03112
Formation of Transitional cE/UCD Galaxies through Massive/Dwarf Disc Galaxy Mergers,['Astrophysics of Galaxies'],"['Alexander V. Khoperskov', 'Sergey S. Khrapov', 'Danila S. Sirotin']","The dynamics of the merger of a dwarf disc galaxy with a massive spiral galaxy of the Milky Way type have been studied in detail. The remnant of such interaction after numerous crossings of the satellite through the disc of the main galaxy is a compact stellar core, the characteristics of which are close to small compact elliptical galaxies (cEs) or large ultra-compact dwarfs (UCDs). Such transitional cE/UCD objects with an effective radius of 100-200 pc arise as a result of stripping the outer layers of the stellar core during the destruction of a disc dwarf galaxy. Numerical models of the satellite before interaction include baryonic matter (stars and gas) and dark mass. We use N-body to describe the dynamics of stars and dark matter and Smoothed-Particle Hydrodynamics to model the gas components of both galaxies. The direct method of calculating the gravitational force between all particles provides a qualitative resolution of spatial structures up to 10 pc. The simulated cE/UCD galaxies contain very little gas and dark matter at the end of their evolution.△ Less",v1,https://arxiv.org/pdf/2412.03100
A surprisal oracle for when every layer counts,['Computation and Language'],"['Xudong Hong', 'Sharid Loáiciga', 'Asad Sayeed']","Active Curriculum Language Modeling (ACLM; Hong et al., 2023) is a learner directed approach to training a language model. We proposed the original version of this process in our submission to the BabyLM 2023 task, and now we propose an updated ACLM process for the BabyLM 2024 task. ACLM involves an iteratively- and dynamically-constructed curriculum informed over the training process by a model of uncertainty; other training items that are similarly uncertain to a least certain candidate item are prioritized. Our new process improves the similarity model so that it is more dynamic, and we run ACLM over the most successful model from the BabyLM 2023 task: ELC-BERT (Charpentier and Samuel, 2023). We find that while our models underperform on fine-grained grammatical inferences, they outperform the BabyLM 2024 official base-lines on common-sense and world-knowledge tasks. We make our code available at https: //github.com/asayeed/ActiveBaby.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03098
Decentralized Mobile Target Tracking Using Consensus-Based Estimation with Nearly-Constant-Velocity Modeling,['Multiagent Systems'],"['Amir Ahmad Ghods', 'Mohammadreza Doostmohammadian']","Mobile target tracking is crucial in various applications such as surveillance and autonomous navigation. This study presents a decentralized tracking framework utilizing a Consensus-Based Estimation Filter (CBEF) integrated with the Nearly-Constant-Velocity (NCV) model to predict a moving target's state. The framework facilitates agents in a network to collaboratively estimate the target's position by sharing local observations and achieving consensus despite communication constraints and measurement noise. A saturation-based filtering technique is employed to enhance robustness by mitigating the impact of noisy sensor data. Simulation results demonstrate that the proposed method effectively reduces the Mean Squared Estimation Error (MSEE) over time, indicating improved estimation accuracy and reliability. The findings underscore the effectiveness of the CBEF in decentralized environments, highlighting its scalability and resilience in the presence of uncertainties.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03095
Parity results concerning the generalized divisor function involving small prime factors of integers,['Number Theory'],"['Krishnaswami Alladi', 'Ankush Goswami']","Let $ν_y(n)$ denote the number of distinct prime factors of $n$ that are
  $<y$. For $k$ a positive integer, and for $k+2\leq y\leq x$, let $S_{-k}(x,y)$ denote the sum \begin{eqnarray*} S_{-k}(x,y):=\sum_{n\leq x}(-k)^{ν_y(n)}. \end{eqnarray*} In this paper, we describe our recent results on the asymptotic behavior of $S_{-k}(x,y)$ for $k+2\leq y\leq x$, and $x$ sufficiently large. There is a crucial difference in the asymptotic behavior of $S_{-k}(x,y)$ when $k+1$ is a prime and $k+1$ is composite, and this makes the problem particularly interesting. The results are derived utilizing a combination of the Buchstab-de Bruijn recurrence, the Perron contour integral method, and certain difference-differential equations. We present a summary of our results against the background of earlier work of the first author on sums of the Möbius function over integers with restricted prime factors and on a multiplicative generalization of the sieve.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03088
Proposal for Superconducting Photodiode,['Superconductivity'],"['A. V. Parafilo', 'Meng Sun', 'K. Sonowal', 'V. M. Kovalev', 'I. G. Savenko']","We propose a concept of a superconducting photodiode - a device that transforms the energy and `spin' of an external electromagnetic field into the rectified steady-state supercurrent and develop a microscopic theory describing its properties. For this, we consider a two-dimensional thin film cooled down below the temperature of superconducting transition with the injected dc supercurrent and exposed to an external electromagnetic field with a frequency smaller than the superconducting gap. As a result, we predict the emergence of a photoexcited quasiparticle current, and, as a consequence, oppositely oriented stationary flow of Cooper pairs. The strength and direction of this photoinduced supercurrent depend on (i) such material properties as the effective impurity scattering time and the nonequilibrium quasiparticles' energy relaxation time and (ii) such electromagnetic field properties as its frequency and polarization.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03087
Complete homogeneous symmetric polynomials with repeating variables,['Combinatorics'],"['Luis Angel González-Serrano', 'Egor A. Maximenko']","We consider polynomials of the form $\operatorname{h}_m(y_1^{[\varkappa_1]},\ldots,y_n^{[\varkappa_n]})$, where $\operatorname{h}_m$ is the complete homogeneous polynomial of degree $m$ and $y_j^{[\varkappa_j]}$ denotes $y_j$ repeated $\varkappa_j$ times. Using the decomposition of the generating function into partial fractions we represent such polynomials in the form \[ \operatorname{h}_m(y_1^{[\varkappa_1]},\ldots,y_n^{[\varkappa_n]}) =\sum_{j=1}^n \sum_{r=1}^{\varkappa_j} \binom{r+m-1}{r-1} A_{y,\varkappa,j,r} y_j^m, \] where $A_{y,\varkappa,j,r}$ are some coefficients that do not depend on $m$. We also provide an alternative proof using the inverse of the confluent Vandermonde matrix.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03086
Hybrid deep learning-based strategy for the hepatocellular carcinoma cancer grade classification of H&E stained liver histopathology images,['Image and Video Processing'],"['Ajinkya Deshpande', 'Deep Gupta', 'Ankit Bhurane', 'Nisha Meshram', 'Sneha Singh', 'Petia Radeva']","Hepatocellular carcinoma (HCC) is a common type of liver cancer whose early-stage diagnosis is a common challenge, mainly due to the manual assessment of hematoxylin and eosin-stained whole slide images, which is a time-consuming process and may lead to variability in decision-making. For accurate detection of HCC, we propose a hybrid deep learning-based architecture that uses transfer learning to extract the features from pre-trained convolutional neural network (CNN) models and a classifier made up of a sequence of fully connected layers. This study uses a publicly available The Cancer Genome Atlas Hepatocellular Carcinoma (TCGA-LIHC)database (n=491) for model development and database of Kasturba Gandhi Medical College (KMC), India for validation. The pre-processing step involves patch extraction, colour normalization, and augmentation that results in 3920 patches for the TCGA dataset. The developed hybrid deep neural network consisting of a CNN-based pre-trained feature extractor and a customized artificial neural network-based classifier is trained using five-fold cross-validation. For this study, eight different state-of-the-art models are trained and tested as feature extractors for the proposed hybrid model. The proposed hybrid model with ResNet50-based feature extractor provided the sensitivity, specificity, F1-score, accuracy, and AUC of 100.00%, 100.00%, 100.00%, 100.00%, and 1.00, respectively on the TCGA database. On the KMC database, EfficientNetb3 resulted in the optimal choice of the feature extractor giving sensitivity, specificity, F1-score, accuracy, and AUC of 96.97, 98.85, 96.71, 96.71, and 0.99, respectively. The proposed hybrid models showed improvement in accuracy of 2% and 4% over the pre-trained models in TCGA-LIHC and KMC databases.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03084
CLAP: Unsupervised 3D Representation Learning for Fusion 3D Perception via Curvature Sampling and Prototype Learning,['Computer Vision and Pattern Recognition'],"['Runjian Chen', 'Hang Zhang', 'Avinash Ravichandran', 'Wenqi Shao', 'Alex Wong', 'Ping Luo']","Unsupervised 3D representation learning via masked-and-reconstruction with differentiable rendering is promising to reduce the labeling burden for fusion 3D perception. However, previous literature conduct pre-training for different modalities separately because of the hight GPU memory consumption. Consequently, the interaction between the two modalities (images and point clouds) is neglected during pre-training. In this paper, we explore joint unsupervised pre-training for fusion 3D perception via differentiable rendering and propose CLAP, short for Curvature sampLing and swApping Prototype assignment prediction. The contributions are three-fold. 1) To overcome the GPU memory consumption problem, we propose Curvature Sampling to sample the more informative points/pixels for pre-training. 2) We propose to use learnable prototypes to represent parts of the scenes in a common feature space and bring the idea of swapping prototype assignment prediction to learn the interaction between the two modalities. 3) To further optimize learnable prototypes, we propose an Expectation-Maximization training scheme to maximize the similarity between embeddings and prototypes, followed by a Gram Matrix Regularization Loss to avoid collapse. Experiment results on NuScenes show that CLAP achieves 300% more performance gain as compared to previous SOTA 3D pre-training method via differentiable rendering. Codes and models will be released.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03059
Point-GN: A Non-Parametric Network Using Gaussian Positional Encoding for Point Cloud Classification,['Computer Vision and Pattern Recognition'],"['Marzieh Mohammadi', 'Amir Salarpour']","This paper introduces Point-GN, a novel non-parametric network for efficient and accurate 3D point cloud classification. Unlike conventional deep learning models that rely on a large number of trainable parameters, Point-GN leverages non-learnable components-specifically, Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN), and Gaussian Positional Encoding (GPE)-to extract both local and global geometric features. This design eliminates the need for additional training while maintaining high performance, making Point-GN particularly suited for real-time, resource-constrained applications. We evaluate Point-GN on two benchmark datasets, ModelNet40 and ScanObjectNN, achieving classification accuracies of 85.29% and 85.89%, respectively, while significantly reducing computational complexity. Point-GN outperforms existing non-parametric methods and matches the performance of fully trained models, all with zero learnable parameters. Our results demonstrate that Point-GN is a promising solution for 3D point cloud classification in practical, real-time environments.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03056
TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception,['Computer Vision and Pattern Recognition'],"['Runjian Chen', 'Hyoungseob Park', 'Bo Zhang', 'Wenqi Shao', 'Ping Luo', 'Alex Wong']","Labeling LiDAR point clouds is notoriously time-and-energy-consuming, which spurs recent unsupervised 3D representation learning methods to alleviate the labeling burden in LiDAR perception via pretrained weights. Almost all existing work focus on a single frame of LiDAR point cloud and neglect the temporal LiDAR sequence, which naturally accounts for object motion (and their semantics). Instead, we propose TREND, namely Temporal REndering with Neural fielD, to learn 3D representation via forecasting the future observation in an unsupervised manner. Unlike existing work that follows conventional contrastive learning or masked auto encoding paradigms, TREND integrates forecasting for 3D pre-training through a Recurrent Embedding scheme to generate 3D embedding across time and a Temporal Neural Field to represent the 3D scene, through which we compute the loss using differentiable rendering. To our best knowledge, TREND is the first work on temporal forecasting for unsupervised 3D representation learning. We evaluate TREND on downstream 3D object detection tasks on popular datasets, including NuScenes, Once and Waymo. Experiment results show that TREND brings up to 90% more improvement as compared to previous SOTA unsupervised 3D pre-training methods and generally improve different downstream models across datasets, demonstrating that indeed temporal forecasting brings improvement for LiDAR perception. Codes and models will be released.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03054
Point-GR: Graph Residual Point Cloud Network for 3D Object Classification and Segmentation,['Computer Vision and Pattern Recognition'],"['Md Meraz', 'Md Afzal Ansari', 'Mohammed Javed', 'Pavan Chakraborty']","In recent years, the challenge of 3D shape analysis within point cloud data has gathered significant attention in computer vision. Addressing the complexities of effective 3D information representation and meaningful feature extraction for classification tasks remains crucial. This paper presents Point-GR, a novel deep learning architecture designed explicitly to transform unordered raw point clouds into higher dimensions while preserving local geometric features. It introduces residual-based learning within the network to mitigate the point permutation issues in point cloud data. The proposed Point-GR network significantly reduced the number of network parameters in Classification and Part-Segmentation compared to baseline graph-based networks. Notably, the Point-GR model achieves a state-of-the-art scene segmentation mean IoU of 73.47% on the S3DIS benchmark dataset, showcasing its effectiveness. Furthermore, the model shows competitive results in Classification and Part-Segmentation tasks.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03052
Coupling hotspots: distinguishing between positive and negative land-atmosphere interaction,['Atmospheric and Oceanic Physics'],"['Jun Yin', 'Amilcare Porporato']","Understanding the complex interactions between land surface and atmosphere is essential to improve weather and climate predictions. Various numerical experiments have suggested that regions of strong coupling strength (hotspots) are located in the transitional climate zones. However, atmospheric processes in these hotspots are found to have different responses to the perturbation of surface properties. Here we establish analytical relationships to identify key role of soil moisture variances in controlling the coupling hotspots. Using the most recent numerical experiments, we find different signs of feedback in two such hotspots, suggesting the coupling can either reinforce or attenuate persistent extreme climates. We further uncover new coupling hotspots in regions where precipitation is highly sensitive to soil moisture perturbation. Our results highlight the importance of both signs and magnitudes of land-atmosphere interactions over extensive regions, where the ecosystems and communities are particularly vulnerable to the extreme climate events.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03049
Real-time Dynamics of Soft Manipulators with Cross-section Inflation: Application to the Octopus Muscular Hydrostat,['Robotics'],"['Yuchen Sun', 'Anup Teejo Mathew', 'Imran Afgan', 'Federico Renda', 'Cecilia Laschi']","Inspired by the embodied intelligence of biological creatures like the octopus, the soft robotic arm utilizes its highly flexible structure to perform various tasks in the complex environment. While the classic Cosserat rod theory investigates the bending, twisting, shearing, and stretching of the soft arm, it fails to capture the in-plane deformation that occurs during certain tasks, particularly those involving active lateral traction. This paper introduces an extended Cosserat rod theory addressing these limitations by incorporating an extra strain variable reflecting the in-plane inflation ratio. To accurately describe the viscoelasticity effect of the soft body in dynamics, the proposed model enhances the constitutive law by integrating the Saint-Venant Kirchhoff hyperelastic and Kelvin-Voigt viscous models. The active and environmental loads are accounted for the equations of motion, which are numerically solved by adapting the Geometric Variable Strain (GVS) approach to balance the accuracy and computational efficiency. Our contributions include the derivation of the extended Cosserat rod theory in dynamic context, and the development of a reduced-order numerical method that enables rapid and precise solutions. We demonstrate applications of the model in stiffness tuning of a soft robotic arm and the study of complex octopus' arm motions.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03046
On a penalised likelihood approach for joint modelling of longitudinal covariates and partly interval-censored data -- an application to the Anti-PD1 brain collaboration trial,['Methodology'],"['Annabel Webb', 'Nan Zou', 'Serigne Lo', 'Jun Ma']","This article considers the joint modeling of longitudinal covariates and partly-interval censored time-to-event data. Longitudinal time-varying covariates play a crucial role in obtaining accurate clinically relevant predictions using a survival regression model. However, these covariates are often measured at limited time points and may be subject to measurement error. Further methodological challenges arise from the fact that, in many clinical studies, the event times of interest are interval-censored. A model that simultaneously accounts for all these factors is expected to improve the accuracy of survival model estimations and predictions. In this article, we consider joint models that combine longitudinal time-varying covariates with the Cox model for time-to-event data which is subject to interval censoring. The proposed model employs a novel penalised likelihood approach for estimating all parameters, including the random effects. The covariance matrix of the estimated parameters can be obtained from the penalised log-likelihood. The performance of the model is compared to an existing method under various scenarios. The simulation results demonstrated that our new method can provide reliable inferences when dealing with interval-censored data. Data from the Anti-PD1 brain collaboration clinical trial in advanced melanoma is used to illustrate the application of the new method.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03042
"Cost-Performance Evaluation of General Compute Instances: AWS, Azure, GCP, and OCI","['Distributed, Parallel, and Cluster Computing']","['Jay Tharwani', 'Arnab A Purkayastha']","Cloud computing has become the cornerstone of modern IT infrastructure, offering a wide range of general-purpose instances optimized for diverse workloads. This paper provides a comparative analysis of cost and performance for general-purpose compute instances across four major cloud providers: AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI). Using standardized configurations of 4 vCPUs and 16 GiB of RAM, the study evaluates instances based on processor architecture (Intel, AMD, ARM), pricing models, and performance benchmarks. Key findings reveal that ARM-based instances deliver superior price-performance ratios for cost-sensitive workloads, while Intel-based instances excel in enterprise-grade applications requiring versatility and reliability. The results aim to guide organizations in selecting the most cost-effective and performance-efficient cloud resources for their specific needs.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03037
A Granger-Causal Perspective on Gradient Descent with Application to Pruning,['Machine Learning'],"['Aditya Shah', 'Aditya Challa', 'Sravan Danda', 'Archana Mathur', 'Snehanshu Saha']","Stochastic Gradient Descent (SGD) is the main approach to optimizing neural networks. Several generalization properties of deep networks, such as convergence to a flatter minima, are believed to arise from SGD. This article explores the causality aspect of gradient descent. Specifically, we show that the gradient descent procedure has an implicit granger-causal relationship between the reduction in loss and a change in parameters. By suitable modifications, we make this causal relationship explicit. A causal approach to gradient descent has many significant applications which allow greater control. In this article, we illustrate the significance of the causal approach using the application of Pruning. The causal approach to pruning has several interesting properties - (i) We observe a phase shift as the percentage of pruned parameters increase. Such phase shift is indicative of an optimal pruning strategy. (ii) After pruning, we see that minima becomes ""flatter"", explaining the increase in accuracy after pruning weights.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03035
Low-Lying Zeros of $L$-functions of Adélic Hilbert Modular Forms and their Convolutions,['Number Theory'],"['Alia Hamieh', 'Peng-Jie Wong']","In this article, we study the density conjecture of Katz and Sarnak for $L$-functions of adélic Hilbert modular forms and their convolutions. In particular, under the generalised Riemann hypothesis, we establish several instances supporting the conjecture and extending the works of Iwaniec-Luo-Sarnak and many others. For applications, we obtain an upper bound for the average order of $L$-functions of Hilbert modular forms at $s=\frac{1}{2}$ as well as a positive proportion of non-vanishing of certain Rankin-Selberg $L$-functions.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03034
Edge System Design Using Containers and Unikernels for IoT Applications,"['Distributed, Parallel, and Cluster Computing']","['Shahidullah Kaiser', 'Ali Saman Tosun', 'Turgay Korkmaz']","Edge computing is emerging as a key enabler of low-latency, high-efficiency processing for the Internet of Things (IoT) and other real-time applications. To support these demands, containerization has gained traction in edge computing due to its lightweight virtualization and efficient resource management. However, there is currently no established framework to leverage both containers and unikernels on edge devices for optimized IoT deployments. This paper proposes a hybrid edge system design that leverages container and unikernel technologies to optimize resource utilization based on application complexity. Containers are employed for resource-intensive applications, e.g., computer vision, providing faster processing, flexibility, and ease of deployment. In contrast, unikernels are used for lightweight applications, offering enhanced resource performance with minimal overhead. Our system design also incorporates container orchestration to efficiently manage multiple instances across the edge efficiently, ensuring scalability and reliability. We demonstrate our hybrid approach's performance and efficiency advantages through real-world computer vision and data science applications on ARM-powered edge device. Our results demonstrate that this hybrid approach improves resource utilization and reduces latency compared to traditional virtualized solutions. This work provides insights into optimizing edge infrastructures, enabling more efficient and specialized deployment strategies for diverse application workloads.△ Less","4 December, 2024;",https://arxiv.org/pdf/2412.03032
Exploring the Viability of Unikernels for ARM-powered Edge Computing,"['Distributed, Parallel, and Cluster Computing']","['Shahidullah Kaiser', 'Ali Saman Tosun', 'Turgay Korkmaz']","The rapid expansion of IoT devices and their real-time applications have driven a growing need for edge computing. To meet this need, efficient and secure solutions are required for running such applications on resource-constrained devices with limited power, CPU, and memory. Unikernel, with its minimalistic design and application-specific approach, offers a promising alternative to traditional virtualization and container technologies in these environments. The existing research does not thoroughly examine the feasibility of using unikernel for edge computing. This paper investigates the potential of unikernel for ARM-powered edge computing by evaluating the performance and efficiency of three prominent unikernel systems such as OSv, Nanos, and Unikraft against Docker container. We experiment with real-world edge computing applications and utilize key metrics such as boot time, execution time, memory usage, CPU overhead, and network performance to determine how unikernel performs under the constraints of edge devices. Our findings reveal the potential advantages of unikernel in terms of reduced resource consumption and faster startup times while highlighting areas where they may need further optimization for edge deployment. This study provides valuable insights for researchers and practitioners considering unikernel as a lightweight, efficient solution for edge computing on ARM architectures.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.03030
Universal distributed blind quantum computing with solid-state qubits,['Quantum Physics'],"['Yan-Cheng Wei', 'Pieter-Jan Stas', 'Aziza Suleymanzade', 'Gefen Baranes', 'Francisco Machado', 'Yan Qi Huan', 'Can M. Knaut', 'Weiyi Sophie Ding', 'Moritz Merz', 'Erik N Knall', 'Umut Yazlar', 'Maxim Sirotin', 'Iria W. Wang', 'Bart Machielse', 'Susanne F. Yelin', 'Johannes Borregaard', 'Hongkun Park', 'Marko Loncar', 'Mikhail D. lukin']","Blind quantum computing (BQC) is a promising application of distributed quantum systems, where a client can perform computations on a remote server without revealing any details of the applied circuit. While the most promising realizations of quantum computers are based on various matter qubit platforms, implementing BQC on matter qubits remains an outstanding challenge. Using silicon-vacancy (SiV) centers in nanophotonic diamond cavities with an efficient optical interface, we experimentally demonstrate a universal quantum gate set consisting of single- and two-qubit blind gates over a distributed two-node network. Using these ingredients, we perform a distributed algorithm with blind operations across our two-node network, paving the way towards blind quantum computation with matter qubits in distributed, modular architectures.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.03020
Hamiltonian-based neural networks for systems under nonholonomic constraints,['Classical Physics'],"['Ignacio Puiggros T.', 'A. Srikantha Phani']","There has been increasing interest in methodologies that incorporate physics priors into neural network architectures to enhance their modeling capabilities. A family of these methodologies that has gained traction are Hamiltonian neural networks (HNN) and their variations. These architectures explicitly encode Hamiltonian mechanics both in their structure and loss function. Although Hamiltonian systems under nonholonomic constraints are in general not Hamiltonian, it is possible to formulate them in pseudo-Hamiltonian form, equipped with a Lie bracket which is almost Poisson. This opens the possibility of using some principles of HNNs in systems under nonholonomic constraints. The goal of the present work is to develop a modified Hamiltonian neural network architecture capable of modeling Hamiltonian systems under holonomic and nonholonomic constraints. A three-network parallel architecture is proposed to simultaneously learn the Hamiltonian of the system, the constraints, and their associated multipliers. A rolling disk and a ball on a spinning table are considered as canonical examples to assess the performance of the proposed Hamiltonian architecture. The experiments are then repeated with a noisy training set to study modeling performance under more realistic conditions.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.03018
NinjaSat: Astronomical X-ray CubeSat Observatory,['Instrumentation and Methods for Astrophysics'],"['Toru Tamagawa', 'Teruaki Enoto', 'Takao Kitaguchi', 'Wataru Iwakiri', 'Yo Kato', 'Masaki Numazawa', 'Tatehiro Mihara', 'Tomoshi Takeda', 'Naoyuki Ota', 'Sota Watanabe', 'Amira Aoyama', 'Satoko Iwata', 'Takuya Takahashi', 'Kaede Yamasaki', 'Chin-Ping Hu', 'Hiromitsu Takahashi', 'Yuto Yoshida', 'Hiroki Sato', 'Shoki Hayashi', 'Yuanhui Zhou', 'Keisuke Uchiyama', 'Arata Jujo', 'Hirokazu Odaka', 'Tsubasa Tamba', 'Kentaro Taniguchi']","NinjaSat is an X-ray CubeSat designed for agile, long-term continuous observations of bright X-ray sources, with the size of 6U ($100\times200\times300$ mm$^3$) and a mass of 8 kg. NinjaSat is capable of pointing at X-ray sources with an accuracy of less than $0^{\circ}\hspace{-1.0mm}.1$ (2$σ$ confidence level) with 3-axis attitude control. The satellite bus is a commercially available NanoAvionics M6P, equipped with two non-imaging gas X-ray detectors covering an energy range of 2-50 keV. A total effective area of 32 cm$^2$ at 6 keV is capable of observing X-ray sources with a flux of approximately 10$^{-10}$ erg cm$^{-2}$ s$^{-1}$. The arrival time of each photon can be tagged with a time resolution of 61 $μ$s. The two radiation belt monitors continuously measure the fluxes of protons above 5 MeV and electrons above 200 keV trapped in the geomagnetic field, alerting the X-ray detectors when the flux exceeds a threshold. The NinjaSat project started in 2020. Fabrication of the scientific payloads was completed in August 2022, and satellite integration and tests were completed in July 2023. NinjaSat was launched into a Sun-synchronous polar orbit at an altitude of about 530 km on 2023 November 11 by the SpaceX Transporter-9 mission. After about three months of satellite commissioning and payload verification, we observed the Crab Nebula on February 9, 2024, and successfully detected the 33.8262 ms pulsation from the neutron star. With this observation, NinjaSat met the minimum success criterion and stepped forward to scientific observations as initially planned. By the end of November 2024, we successfully observed 21 X-ray sources using NinjaSat. This achievement demonstrates that, with careful target selection, we can conduct scientific observations effectively using CubeSats, contributing to time-domain astronomy.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.03016
Neutrino portals to MeV WIMPs with s-channel mediators,['High Energy Physics - Phenomenology'],"['Nicole F. Bell', 'Matthew J. Dolan', 'Avirup Ghosh', 'Michael Virgato']","Large-scale neutrino detectors currently under construction will have the unique ability to probe the annihilation of low-mass thermal-relic dark matter to neutrinos. This represents an essential test of the thermal freezeout paradigm. This raises the question: what viable UV-complete models are there in which dark matter annihilates dominantly to neutrinos? We discuss models that fulfill this criteria, and are invariant under the Standard Model gauge group, for both scalar and fermionic dark matter. Specifically, we construct new models in which annihilation via the $s$-channel exchange of a scalar or pseudoscalar mediator achieves the correct relic density. In these models, dark matter is stabilised by an exact or a softly-broken lepton-number symmetry. The parameter space of such models will be probed, almost entirely, by the combination of JUNO, Hyper-Kamiokande and CMB-S4.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02994
Preference-based Pure Exploration,['Machine Learning'],"['Apurv Shukla', 'Debabrota Basu']","We study the preference-based pure exploration problem for bandits with vector-valued rewards. The rewards are ordered using a (given) preference cone $\mathcal{C}$ and our the goal is to identify the set of Pareto optimal arms. First, to quantify the impact of preferences, we derive a novel lower bound on the sample complexity for identifying the most preferred policy with confidence level $1-δ$. Our lower bound elicits the role played by the geometry of the preference cone and punctuates the difference in hardness compared to existing best-arm identification variants of the problem. We further explicate this geometry when rewards follow Gaussian distributions. We then provide a convex relaxation of the lower bound. and leverage it to design Preference-based Track and Stop (PreTS) algorithm that identifies the most preferred policy. Finally, we show that sample complexity of PreTS is asymptotically tight by deriving a new concentration inequality for vector-valued rewards.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02988
Birthmarks: Ergodicity Breaking Beyond Quantum Scars,['Quantum Physics'],"['Anton M. Graf', 'Joonas Keski-Rahkonen', 'Mingxuan Xiao', 'Saul Atwood', 'Zhongling Lu', 'Siyuan Chen', 'Eric J. Heller']","One manifestation of classical ergodicity is a complete loss of memory of the initial conditions due to the eventual uniform exploration of phase space. In quantum versions of the same systems, classical ergodic traits can be broken. Here, we extend the concept of quantum scars in new directions, more focused on ergodicity and infinite time averages than individual eigenstates. We specifically establish a union of short and long-term enhancements in terms of a \emph{quantum birthmark} (QB). Subsequently, we show (1) that the birth and early evolution of a nonstationary state is remembered forever in infinite time averages, and (2) that early recurrences in the autocorrelation function inevitably lead to nonergodic flow over infinite times. We recount here that phase space cannot be explored ergodically if there are early recurrences (well before the Heisenberg time) in the autocorrelation of the initial nonstationary quantum state. Employing random matrix theory, we show that QB extends beyond individual states to entire subspaces or ``{\it birthplaces}"" in Hilbert space. Finally, we visualize scar-amplified QBs unveiled within the time-averaged probability density of a wavepacket in a stadium system. By transcending the quantum scarring, QB delivers a new paradigm for understanding the elusive quantum nature of ergodicity.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02982
"Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models",['Machine Learning'],"['Alex Havrilla', 'Andrew Dai', ""Laura O'Mahony"", 'Koen Oostermeijer', 'Vera Zisler', 'Alon Albalak', 'Fabrizio Milo', 'Sharath Chandra Raparthy', 'Kanishk Gandhi', 'Baber Abbasi', 'Duy Phung', 'Maia Iyer', 'Dakota Mahan', 'Chase Blagden', 'Srishti Gureja', 'Mohammed Hamdy', 'Wen-Ding Li', 'Giovanni Paolini', 'Pawan Sasanka Ammanamanchi', 'Elliot Meyerson']","Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02980
Supporting Gig Worker Needs and Advancing Policy Through Worker-Centered Data-Sharing,['Computers and Society'],"['Jane Hsieh', 'Angie Zhang', 'Mialy Rasetarinera', 'Erik Chou', 'Daniel Ngo', 'Karen Lightman', 'Min Kyung Lee', 'Haiyi Zhu']","The proliferating adoption of platform-based gig work increasingly raises concerns for worker conditions. Past studies documented how platforms leveraged design to exploit labor, withheld information to generate power asymmetries, and left workers alone to manage logistical overheads as well as social isolation. However, researchers also called attention to the potential of helping workers overcome such costs via worker-led datasharing, which can enable collective actions and mutual aid among workers, while offering advocates, lawmakers and regulatory bodies insights for improving work conditions. To understand stakeholders' desiderata for a data-sharing system (i.e. functionality and policy initiatives that it can serve), we interviewed 11 policy domain experts in the U.S. and conducted co-design workshops with 14 active gig workers across four domains. Our results outline policymakers' prioritized initiatives, information needs, and (mis)alignments with workers' concerns and desires around data collectives. We offer design recommendations for data-sharing systems that support worker needs while bringing us closer to legislation that promote more thriving and equitable gig work futures.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02973
MedAutoCorrect: Image-Conditioned Autocorrection in Medical Reporting,['Computer Vision and Pattern Recognition'],"['Arnold Caleb Asiimwe', 'Dídac Surís', 'Pranav Rajpurkar', 'Carl Vondrick']","In medical reporting, the accuracy of radiological reports, whether generated by humans or machine learning algorithms, is critical. We tackle a new task in this paper: image-conditioned autocorrection of inaccuracies within these reports. Using the MIMIC-CXR dataset, we first intentionally introduce a diverse range of errors into reports. Subsequently, we propose a two-stage framework capable of pinpointing these errors and then making corrections, simulating an \textit{autocorrection} process. This method aims to address the shortcomings of existing automated medical reporting systems, like factual errors and incorrect conclusions, enhancing report reliability in vital healthcare applications. Importantly, our approach could serve as a guardrail, ensuring the accuracy and trustworthiness of automated report generation. Experiments on established datasets and state of the art report generation models validate this method's potential in correcting medical reporting errors.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02971
A Graph Neural Network Approach to Dispersed Systems,['Computational Physics'],"['Aref Hashemi', 'Aliakbar Izadkhah']","We present a Graph Neural Network (GNN) that accurately simulates a multidisperse suspension of interacting spherical particles. Our machine learning framework is build upon the recent work of Sanchez-Gonzalez et al. ICML, PMLR, 119, 8459-8468 (2020) on graph network simulators, and efficiently learns the intricate dynamics of the interacting particles. Nodes and edges of the GNN correspond to, respectively, the particles with their individual properties/data (e.g., radius, position, velocity) and the pair interactions between the particles (e.g., electrostatics, hydrodynamics). A key contribution of our work is to account for the finite dimensions of the particles and their impact on the system dynamics. We test our GNN against an exemplary case study of a multidisperse mixture of two-dimensional spheres sedimenting under gravity in a liquid and interacting with each other by a Lennard-Jones potential. The present GNN framework offers a fast and accurate method for the theoretical study of complex physical systems such as field-induced behavior of colloidal suspensions and ionic liquids. Our implementation of the GNN is available on GitHub at github.com/rfjd/GNS-DispersedSystems.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02967
Invariant Reduction for Partial Differential Equations. I: Conservation Laws and Systems with Two Independent Variables,['Exactly Solvable and Integrable Systems'],"['Kostya Druzhkov', 'Alexei Cheviakov']","For a system of partial differential equations that has an extended Kovalevskaya form, a reduction procedure is presented that allows one to use a local (point, contact, or higher) symmetry of a system and a symmetry-invariant conservation law to algorithmically calculate constants of motion holding for symmetry-invariant solutions. Several examples including cases of point and higher symmetry invariance are presented and discussed. An implementation of the algorithm in Maple is provided.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02965
Partitioning Theorems for Sets of Semi-Pfaffian Sets with Applications,['Logic'],"['Martin Lotz', 'Abhiram Natarajan', 'Nicolai Vorobjov']","We generalize the seminal polynomial partitioning theorems of Guth and Katz to a set of semi-Pfaffian sets. Specifically, given a set $Γ\subseteq \mathbb{R}^n$ of $k$-dimensional semi-Pfaffian sets, where each $γ\in Γ$ is defined by a fixed number of Pfaffian functions, and each Pfaffian function is in turn defined with respect to a Pfaffian chain $\vec{q}$ of length $r$, for any $D \ge 1$, we prove the existence of a polynomial $P \in \mathbb{R}[X_1, \ldots, X_n]$ of degree at most $D$ such that each connected component of $\mathbb{R}^n \setminus Z(P)$ intersects at most $\sim \frac{|Γ|}{D^{n - k - r}}$ elements of $Γ$. Also, for any $D \ge 1$, we prove the existence of a Pfaffian function $P'$ of degree at most $D$ defined with respect to $\vec{q}$, such that each connected component of $\mathbb{R}^n \setminus Z(P')$ intersects at most $\sim \frac{|Γ|}{D^{n-k}}$ elements of $Γ$. To do so, given a $k$-dimensional semi-Pfaffian set $\mathcal{X} \subseteq \mathbb{R}^n$, and a polynomial $P \in \mathbb{R}[X_1, \ldots, X_n]$ of degree at most $D$, we establish a uniform bound on the number of connected components of $\mathbb{R}^n \setminus Z(P)$ that $\mathcal{X}$ intersects; i.e. we prove that the number of connected components of $(\mathbb{R}^n \setminus Z(P)) \cap \mathcal{X}$ is at most $\sim D^{k+r}$. Finally as applications, we derive Pfaffian versions of Szemerédi-Trotter type theorems, and also prove bounds on the number of joints between Pfaffian curves.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02961
Generalized coherent states for the harmonic oscillator by the J-matrix method with an extension to the Morse potential,['Quantum Physics'],"['Hashim A. Yamani', 'Zouhaïr Mouayn']",While dealing with the J-Matrix method for the harmonic oscillator to write down its tridiagonal matrix representation in an orthonormal basis of L2(R); we rederive a set of generalized coherent states (GCS) of Perelomov type labeled by points z of the complex plane C and depending on a positive integer number m The number states expansion of these GCS gives rise to coefficients that are complex Hermite polynomials whose linear superpositions provide eigenfunctions for the two-dimensional magnetic Laplacian associated with the mth Landau level. We extend this procedure to the Morse oscillator by constructing a new set of GCS of Glauber type.△ Less,v1,https://arxiv.org/pdf/2412.02958
Variational quantum classifiers via a programmable photonic microprocessor,['Quantum Physics'],"['Hexiang Lin', 'Huihui Zhu', 'Zan Tang', 'Wei Luo', 'Wei Wang', 'Man-Wai Mak', 'Xudong Jiang', 'Lip Ket Chin', 'Leong Chuan Kwek', 'Ai Qun Liu']","Quantum computing holds promise across various fields, particularly with the advent of Noisy Intermediate-Scale Quantum (NISQ) devices, which can outperform classical supercomputers in specific tasks. However, challenges such as noise and limited qubit capabilities hinder its practical applications. Variational Quantum Algorithms (VQAs) offer a viable strategy to achieve quantum advantage by combining quantum and classical computing. Leveraging on VQAs, the performance of Variational Quantum Classifiers (VQCs) is competitive with many classical classifiers. This work implements a VQC using a silicon-based quantum photonic microprocessor and a classical computer, demonstrating its effectiveness in nonlinear binary and multi-classification tasks. An efficient gradient free genetic algorithm is employed for training. The VQC's performance was evaluated on three synthetic binary classification tasks with square-, circular-, and sine-shape decision boundaries and a real-world multiclass Iris dataset. The accuracies on the three binary classification tasks were 87.5%, 92.5%, and 85.0%, respectively, and 98.8% on the real world Iris dataset, highlighting the platform's potential to handle complex data patterns.△ Less",v1,https://arxiv.org/pdf/2412.02955
An indoor DSO-based ceiling-vision odometry system for indoor industrial environments,['Robotics'],"['Abdelhak Bougouffa', 'Emmanuel Seignez', 'Samir Bouaziz', 'Florian Gardes']","Autonomous Mobile Robots operating in indoor industrial environments require a localization system that is reliable and robust. While Visual Odometry (VO) can offer a reasonable estimation of the robot's state, traditional VO methods encounter challenges when confronted with dynamic objects in the scene. Alternatively, an upward-facing camera can be utilized to track the robot's movement relative to the ceiling, which represents a static and consistent space. We introduce in this paper Ceiling-DSO, a ceiling-vision system based on Direct Sparse Odometry (DSO). Unlike other ceiling-vision systems, Ceiling-DSO takes advantage of the versatile formulation of DSO, avoiding assumptions about observable shapes or landmarks on the ceiling. This approach ensures the method's applicability to various ceiling types. Since no publicly available dataset for ceiling-vision exists, we created a custom dataset in a real-world scenario and employed it to evaluate our approach. By adjusting DSO parameters, we identified the optimal fit for online pose estimation, resulting in acceptable error rates compared to ground truth. We provide in this paper a qualitative and quantitative analysis of the obtained results.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02950
Extracting Dual Solutions via Primal Optimizers,['Optimization and Control'],"['Yair Carmon', 'Arun Jambulapati', ""Liam O'Carroll"", 'Aaron Sidford']","We provide a general method to convert a ""primal"" black-box algorithm for solving regularized convex-concave minimax optimization problems into an algorithm for solving the associated dual maximin optimization problem. Our method adds recursive regularization over a logarithmic number of rounds where each round consists of an approximate regularized primal optimization followed by the computation of a dual best response. We apply this result to obtain new state-of-the-art runtimes for solving matrix games in specific parameter regimes, obtain improved query complexity for solving the dual of the CVaR distributionally robust optimization (DRO) problem, and recover the optimal query complexity for finding a stationary point of a convex function.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02949
Detection of Multiple Influential Observations on Model Selection,['Methodology'],"['Dongliang Zhang', 'Masoud Asgharian', 'Martin A. Lindquist']","Outlying observations are frequently encountered in a wide spectrum of scientific domains, posing significant challenges for the generalizability of statistical models and the reproducibility of downstream analysis. These observations can be identified through influential diagnosis, which refers to the detection of observations that are unduly influential on diverse facets of statistical inference. To date, methods for identifying observations influencing the choice of a stochastically selected submodel have been underdeveloped, especially in the high-dimensional setting where the number of predictors p exceeds the sample size n. Recently we proposed an improved diagnostic measure to handle this setting. However, its distributional properties and approximations have not yet been explored. To address this shortcoming, the notion of exchangeability is revived, and used to determine the exact finite- and large-sample distributions of our assessment metric. This forms the foundation for the introduction of both parametric and non-parametric approaches for its approximation and the establishment of thresholds for diagnosis. The resulting framework is extended to logistic regression models, followed by a simulation study conducted to assess the performance of various detection procedures. Finally the framework is applied to data from an fMRI study of thermal pain, with the goal of identifying outlying subjects that could distort the formulation of statistical models using functional brain activity in predicting physical pain ratings. Both linear and logistic regression models are used to demonstrate the benefits of detection and compare the performances of different detection procedures. In particular, two additional influential observations are identified, which are not discovered by previous studies.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02945
Passive polarization-encoded BB84 protocol using a heralded single-photon source,['Quantum Physics'],"['Anju Rani', 'Vardaan Mongia', 'Parvatesh Parvatikar', 'Rutuj Gharate', 'Tanya Sharma', 'Jayanth Ramakrishnan', 'Pooja Chandravanshi', 'R. P. Singh']","The BB84 quantum key distribution protocol set the foundation for achieving secure quantum communication. Since its inception, significant advancements have aimed to overcome experimental challenges and enhance security. In this paper, we report the implementation of a passive polarization-encoded BB84 protocol using a heralded single-photon source. By passively and randomly encoding polarization states with beam splitters and half-wave plates, the setup avoids active modulation, simplifying design and enhancing security against side-channel attacks. The heralded single-photon source ensures a low probability of multi-photon emissions, eliminating the need for decoy states and mitigating photon number splitting vulnerabilities. The quality of the single-photon source is certified by measuring the second-order correlation function at zero delay, $g^{2}(0)=0.0408\pm0.0008$, confirming a very low probability of multi-photon events. Compared to conventional BB84 or BBM92 protocols, our protocol provides optimized resource trade-offs, with fewer detectors (compared to BBM92) and no reliance on external quantum random number generators (compared to typical BB84) to drive Alice's encoding scheme. Our implementation achieved a quantum bit error rate of 7% and a secure key rate of 5 kbps. These results underscore the practical, secure, and resource-efficient framework our protocol offers for scalable quantum communication technologies.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02944
Nonlinear Valley and Spin Valves in Bilayer Graphene,['Mesoscale and Nanoscale Physics'],"['Xin Liao', 'Xing-Yu Liu', 'An-Qi Wang', 'Qing Yin', 'Tong-Yang Zhao', 'Zhi-Min Liao']","Nonlinear transport plays a vital role in probing the quantum geometry of Bloch electrons, valley chirality, and carrier scattering mechanisms. The nonlinear Hall effect, characterized by a nonlinear scaling of Hall voltage with longitudinal current, has been explored to reveal the Berry curvature and quantum metric related physics. In this work, we extend the study of nonlinear transport to spin and valley degrees of freedom. Using bilayer graphene devices with Fe3GeTe2 contacts, we observe a second-order nonlinear spin current exhibiting spin valve-like behaviors. By tracking magnetic moment precession under an in-plane magnetic field, we identify a significantly enhanced critical magnetic field required for in-plane rotation, suggesting out-of-plane valley polarization induced by ferromagnetic proximity. These findings offer deep insights into the interplay of valley and spin in second-order nonlinear transport, opening avenues for promising device applications.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02939
Orbital anomalous Hall effect in the few-layer Weyl semimetal TaIrTe4,['Mesoscale and Nanoscale Physics'],"['An-Qi Wang', 'Dong Li', 'Tong-Yang Zhao', 'Xing-Yu Liu', 'Jiantian Zhang', 'Xin Liao', 'Qing Yin', 'Zhen-Cun Pan', 'Peng Yu', 'Zhi-Min Liao']","We report on the observation of the linear anomalous Hall effect (AHE) in the nonmagnetic Weyl semimetal TaIrTe4. This is achieved by applying a direct current Idc and an alternating current Iac (Iac<<Idc) in TaIrTe4, where the former induces time-reversal symmetry breaking and the latter probes the triggered AHE. The anomalous Hall resistance VacH/Iac shows a linear dependence on Idc and changes sign with the polarity of Idc. In temperature-dependent measurements, VacH/Iac also experiences a sign reversal at 100 K, consistent with the temperature-dependent nonlinear Hall effect (NLHE). Furthermore, in measurements involving only dc transport, the dc Hall voltage exhibits a quadratic relationship with Idc. When the Idc direction is reversed, the Hall resistance changes sign, demonstrating a colossal nonreciprocal Hall effect (NRHE). Our theoretical calculations suggest that the observed linear AHE, NLHE, and NRHE all dominantly originate from the current-induced orbital magnetization compared to the minor spin contribution. This work provides deep insights into the orbital magnetoelectric effect and nonlinear Hall response, promising precise electric control of out-of-plane polarized orbit flow.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02937
Secondary staircase complexes on isotropic Grassmannians,['Algebraic Geometry'],['Alexander Novikov'],"We introduce a class of equivariant vector bundles on isotropic symplectic Grassmannians $\mathrm{IGr}(k,2n)$ defined as appropriate truncations of staircase complexes and show that these bundles can be assembled into a number of complexes quasi-isomorphic to the symplectic wedge powers of the symplectic bundle on $\mathrm{IGr}(k,2n)$. We are planning to use these secondary staircase complexes to study fullness of exceptional collections in the derived categories of isotropic Grassmannians and Lefschetz exceptional collections on $\mathrm{IGr}(3,2n)$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02914
Some characterizations of Riemannian manifolds endowed with a conformal vector fields,['Differential Geometry'],"['A. Barros', 'I. Evangelista', 'E. Viana']","The aim of this article is to investigate the presence of a conformal vector $ξ$ with conformal factor $ρ$ on a compact Riemannian manifold $M$ with or without boundary $\partial M$. We firstly prove that a compact Riemannian manifold $(M^n, g)\,,n \geq 3,$ with constant scalar curvature, with boundary $\partial M$ totally geodesic, in such way that the traceless Ricci curvature is zero in the direction of $\nabla ρ,$ is isometric to a standard hemisphere. In the $4$-dimensional case, under the condition $\displaystyle\int_M|\mathring{Ric}|^2\langle ξ,\nabla ρ\rangle \,dM\leq0$, we show that, either $M$ is isometric to a standard sphere, or $M$ is isometric to a standard hemisphere. Finally, we give a partial answer for the cosmic no-hair conjecture.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02910
Loss tolerant cross-Kerr enhancement via modulated squeezing,['Quantum Physics'],"['Ankit Tiwari', 'Daniel Burgarth', 'Linran Fan', 'Saikat Guha', 'Christian Arenz']","We develop squeezing protocols to enhance cross-Kerr interactions. We show that through alternating between squeezing along different quadratures of a single photonic mode, the cross-Kerr interaction strength can be generically amplified. As an application of the squeezing protocols we discuss speeding up the deterministic implementation of controlled phase gates in photonic quantum computing architectures. We develop bounds that characterize how fast and strong single-mode squeezing has to be applied to achieve a desired gate error and show that the protocols can overcome photon losses. Finally, we discuss experimental realizations of the squeezing strategies in optical fibers and nanophotonic waveguides.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02909
Predicting post-release defects with knowledge units (KUs) of programming languages: an empirical study,['Software Engineering'],"['Md Ahasanuzzaman', 'Gustavo A. Oliva', 'Ahmed E. Hassan', 'Zhen Ming', 'Jiang']","Traditional code metrics (product and process metrics) have been widely used in defect prediction. However, these metrics have an inherent limitation: they do not reveal system traits that are tied to certain building blocks of a given programming language. Taking these building blocks of a programming language into account can lead to further insights about a software system and improve defect prediction. To fill this gap, this paper reports an empirical study on the usage of knowledge units (KUs) of the Java programming language. A KU is a cohesive set of key capabilities that are offered by one or more building blocks of a given programming language. This study aims to understand whether we can obtain richer results in defect prediction when using KUs in combination with traditional code metrics. Using a defect dataset covering 28 releases of 8 Java systems, we analyze source code to extract both traditional code metrics and KU incidences. We find empirical evidence that KUs are different and complementary to traditional metrics, thus indeed offering a new lens through which software systems can be analyzed. We build a defect prediction model called KUCLS, which leverages the KU-based features. Our KUCLS achieves a median AUC of 0.82 and significantly outperforms the CC_PROD (model built with product metrics). The normalized AUC improvement of the KUCLS over CC_PROD ranges from 5.1% to 28.9% across the studied releases. Combining KUs with traditional metrics in KUCLS_CC further improves performance, with AUC gains of 4.9% to 33.3% over CC and 5.6% to 59.9% over KUCLS. Finally, we develop a cost-effective model that significantly outperforms the CC. These encouraging results can be helpful to researchers who wish to further study the aspect of feature engineering and building models for defect prediction.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02907
SuperLoc: The Key to Robust LiDAR-Inertial Localization Lies in Predicting Alignment Risks,['Robotics'],"['Shibo Zhao', 'Honghao Zhu', 'Yuanjun Gao', 'Beomsoo Kim', 'Yuheng Qiu', 'Aaron M. Johnson', 'Sebastian Scherer']","Map-based LiDAR localization, while widely used in autonomous systems, faces significant challenges in degraded environments due to lacking distinct geometric features. This paper introduces SuperLoc, a robust LiDAR localization package that addresses key limitations in existing methods. SuperLoc features a novel predictive alignment risk assessment technique, enabling early detection and mitigation of potential failures before optimization. This approach significantly improves performance in challenging scenarios such as corridors, tunnels, and caves. Unlike existing degeneracy mitigation algorithms that rely on post-optimization analysis and heuristic thresholds, SuperLoc evaluates the localizability of raw sensor measurements. Experimental results demonstrate significant performance improvements over state-of-the-art methods across various degraded environments. Our approach achieves a 54% increase in accuracy and exhibits the highest robustness. To facilitate further research, we release our implementation along with datasets from eight challenging scenarios△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02901
GUESS: Generative Uncertainty Ensemble for Self Supervision,['Machine Learning'],"['Salman Mohamadi', 'Gianfranco Doretto', 'Donald A. Adjeroh']","Self-supervised learning (SSL) frameworks consist of pretext task, and loss function aiming to learn useful general features from unlabeled data. The basic idea of most SSL baselines revolves around enforcing the invariance to a variety of data augmentations via the loss function. However, one main issue is that, inattentive or deterministic enforcement of the invariance to any kind of data augmentation is generally not only inefficient, but also potentially detrimental to performance on the downstream tasks. In this work, we investigate the issue from the viewpoint of uncertainty in invariance representation. Uncertainty representation is fairly under-explored in the design of SSL architectures as well as loss functions. We incorporate uncertainty representation in both loss function as well as architecture design aiming for more data-dependent invariance enforcement. The former is represented in the form of data-derived uncertainty in SSL loss function resulting in a generative-discriminative loss function. The latter is achieved by feeding slightly different distorted versions of samples to the ensemble aiming for learning better and more robust representation. Specifically, building upon the recent methods that use hard and soft whitening (a.k.a redundancy reduction), we introduce a new approach GUESS, a pseudo-whitening framework, composed of controlled uncertainty injection, a new architecture, and a new loss function. We include detailed results and ablation analysis establishing GUESS as a new baseline.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02896
Reconstruction of dynamic systems using genetic algorithms with dynamic search limits,['Neural and Evolutionary Computing'],"['Omar Rodríguez-Abreo', 'José Luis Aragón', 'Mario Alan Quiroz-Juárez']","Mathematical modeling is a powerful tool for describing, predicting, and understanding complex phenomena exhibited by real-world systems. However, identifying the equations that govern a system's dynamics from experimental data remains a significant challenge without a definitive solution. In this study, evolutionary computing techniques are presented to estimate the governing equations of a dynamical system using time-series data. The main approach is to propose polynomial equations with unknown coefficients, and subsequently perform a parametric estimation using genetic algorithms. Some of the main contributions of the present study are an adequate modification of the genetic algorithm to remove terms with minimal contributions, and a mechanism to escape local optima during the search. To evaluate the proposed method, we applied it to three dynamical systems: a linear model, a nonlinear model, and the Lorenz system. Our results demonstrate a reconstruction with an Integral Square Error below 0.22 and a coefficient of determination R-squared of 0.99 for all systems, indicating successful reconstruction of the governing dynamic equations.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02894
EvRT-DETR: The Surprising Effectiveness of DETR-based Detection for Event Cameras,['Computer Vision and Pattern Recognition'],"['Dmitrii Torbunov', 'Yihui Ren', 'Animesh Ghose', 'Odera Dim', 'Yonggang Cui']","Event-based cameras (EBCs) have emerged as a bio-inspired alternative to traditional cameras, offering advantages in power efficiency, temporal resolution, and high dynamic range. However, the development of image analysis methods for EBCs is challenging due to the sparse and asynchronous nature of the data. This work addresses the problem of object detection for the EBC cameras. The current approaches to EBC object detection focus on constructing complex data representations and rely on specialized architectures. Here, we demonstrate that the combination of a Real-Time DEtection TRansformer, or RT-DETR, a state-of-the-art natural image detector, with a simple image-like representation of the EBC data achieves remarkable performance, surpassing current state-of-the-art results. Specifically, we show that a properly trained RT-DETR model on the EBC data achieves performance comparable to the most advanced EBC object detection methods. Next, we propose a low-rank adaptation (LoRA)-inspired way to augment the RT-DETR model to handle temporal dynamics of the data. The designed EvRT-DETR model outperforms the current, most advanced results on standard benchmark datasets Gen1 (mAP $+2.3$) and Gen4 (mAP $+1.4$) while only using standard modules from natural image and video analysis. These results demonstrate that effective EBC object detection can be achieved through careful adaptation of mainstream object detection architectures without requiring specialized architectural engineering. The code is available at: https://github.com/realtime-intelligence/evrt-detr△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02890
Deep-Learning Based Docking Methods: Fair Comparisons to Conventional Docking Workflows,['Artificial Intelligence'],"['Ajay N. Jain', 'Ann E. Cleves', 'W. Patrick Walters']","The diffusion learning method, DiffDock, for docking small-molecule ligands into protein binding sites was recently introduced. Results included comparisons to more conventional docking approaches, with DiffDock showing superior performance. Here, we employ a fully automatic workflow using the Surflex-Dock methods to generate a fair baseline for conventional docking approaches. Results were generated for the common and expected situation where a binding site location is known and also for the condition of an unknown binding site. For the known binding site condition, Surflex-Dock success rates at 2.0 Angstroms RMSD far exceeded those for DiffDock (Top-1/Top-5 success rates, respectively, were 68/81% compared with 45/51%). Glide performed with similar success rates (67/73%) to Surflex-Dock for the known binding site condition, and results for AutoDock Vina and Gnina followed this pattern. For the unknown binding site condition, using an automated method to identify multiple binding pockets, Surflex-Dock success rates again exceeded those of DiffDock, but by a somewhat lesser margin. DiffDock made use of roughly 17,000 co-crystal structures for learning (98% of PDBBind version 2020, pre-2019 structures) for a training set in order to predict on 363 test cases (2% of PDBBind 2020) from 2019 forward. DiffDock's performance was inextricably linked with the presence of near-neighbor cases of close to identical protein-ligand complexes in the training set for over half of the test set cases. DiffDock exhibited a 40 percentage point difference on near-neighbor cases (two-thirds of all test cases) compared with cases with no near-neighbor training case. DiffDock has apparently encoded a type of table-lookup during its learning process, rendering meaningful applications beyond its reach. Further, it does not perform even close to competitively with a competently run modern docking workflow.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02889
Robust Optimal Contribution Selection,['Optimization and Control'],"['Josh Fogg', 'Jaime Ortiz', 'Ivan Pocrnić', 'J. A. Julian Hall', 'Gregor Gorjanc']","Optimal contribution selection (OCS) is a selective breeding method that manages the conversion of genetic variation into genetic gain to facilitate short-term competitiveness and long-term sustainability in breeding programmes. Traditional approaches to OCS do not account for uncertainty in input data, which is always present and challenges optimization and practical decision making. Here we use concepts from robust optimization to derive a robust OCS problem and develop two ways to solve the problem using either conic optimization or sequential quadratic programming. We have developed the open-source Python package 'robustocs' that leverages the Gurobi and HiGHS solvers to carry out these methods. Our testing shows favourable performance when solving the robust OCS problem using sequential quadratic programming and the HiGHS solver.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02888
Quantum sensitivity of parametric oscillators,['Quantum Physics'],"['Alex Gu', 'Jamison Sloan', 'Charles Roques-Carmes', 'Seou Choi', 'Eric I. Rosenthal', 'Michael Horodynski', 'Yannick Salamin', 'Jelena Vučković', 'Marin Soljačić']","Many quantum systems exhibit high sensitivity to their initial conditions, where microscopic quantum fluctuations can significantly influence macroscopic observables. Understanding how quantum states may influence the behavior of nonlinear dynamic systems may open new avenues in controlling light-matter interactions. To explore this issue, we analyze the sensitivity of a fundamental quantum optical process - parametric oscillation - to quantum initializations. Focusing on optical parametric oscillators (OPOs), we demonstrate that the quantum statistics of arbitrary initial states are imprinted in the early-stage dynamics and can persist in the steady-state probabilities. We derive the ""quantum sensitivity"" of parametric oscillators, linking the initial quantum state to the system's steady-state outcomes, highlighting how losses and parametric gain govern the system's quantum sensitivity. Moreover, we show that these findings extend beyond OPOs to a broader class of nonlinear systems, including Josephson junction based superconducting circuits. Our work opens the way to a new class of experiments that can test the sensitivity of macroscopic systems to quantum initial conditions and offers a pathway for controlling systems with quantum degrees of freedom.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02887
SymBreak: Mitigating Quantum Degeneracy Issues in QLDPC Code Decoders by Breaking Symmetry,['Quantum Physics'],"['Keyi Yin', 'Xiang Fang', 'Jixuan Ruan', 'Hezi Zhang', 'Dean Tullsen', 'Andrew Sornborger', 'Chenxu Liu', 'Ang Li', 'Travis Humble', 'Yufei Ding']","Quantum error correction (QEC) is critical for scalable and reliable quantum computing, but existing solutions, such as surface codes, incur significant qubit overhead. Quantum low-density parity check (qLDPC) codes have recently emerged as a promising alternative, requiring fewer qubits. However, the lack of efficient decoders remains a major barrier to their practical implementation. In this work, we introduce SymBreak, a novel decoder for qLDPC codes that adaptively modifies the decoding graph to improve the performance of state-of-the-art belief propagation (BP) decoders. Our key contribution is identifying quantum degeneracy as a root cause of the convergence issues often encountered in BP decoding of quantum LDPC codes. We propose a solution that mitigates this issue at the decoding graph level, achieving both fast and accurate decoding. Our results demonstrate that SymBreak outperforms BP and BP+OSD-a more complex variant of BP-with a $16.17\times$ reduction in logical error rate compared to BP and $3.23\times$ compared to BP+OSD across various qLDPC code families. With only an $18.97$% time overhead compared to BP, SymBreak provides significantly faster decoding times than BP+OSD, representing a major advancement in efficient and accurate decoding for qLDPC-based QEC architectures.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02885
TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get Resolved?,['Software Engineering'],"['Toufique Ahmed', 'Martin Hirzel', 'Rangeet Pan', 'Avraham Shinnar', 'Saurabh Sinha']","Test-driven development (TDD) is the practice of writing tests first and coding later, and the proponents of TDD expound its numerous benefits. For instance, given an issue on a source code repository, tests can clarify the desired behavior among stake-holders before anyone writes code for the agreed-upon fix. Although there has been a lot of work on automated test generation for the practice ""write code first, test later"", there has been little such automation for TDD. Ideally, tests for TDD should be fail-to-pass (i.e., fail before the issue is resolved and pass after) and have good adequacy with respect to covering the code changed during issue resolution. This paper introduces TDD-Bench Verified, a high-quality benchmark suite of 449 issues mined from real-world GitHub code repositories. The benchmark's evaluation harness runs only relevant tests in isolation for simple yet accurate coverage measurements, and the benchmark's dataset is filtered both by human judges and by execution in the harness. This paper also presents Auto-TDD, an LLM-based solution that takes as input an issue description and a codebase (prior to issue resolution) and returns as output a test that can be used to validate the changes made for resolving the issue. Our evaluation shows that Auto-TDD yields a better fail-to-pass rate than the strongest prior work while also yielding high coverage adequacy. Overall, we hope that this work helps make developers more productive at resolving issues while simultaneously leading to more robust fixes.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02883
iSEEtree: interactive explorer for hierarchical data,['Mathematical Software'],"['Giulio Benedetti', 'Ely Seraidarian', 'Theotime Pralas', 'Akewak Jeba', 'Tuomas Borman', 'Leo Lahti']","$\textbf{Motivation:}$ Hierarchical data structures are prevalent across several fields of research, as they represent an organised and efficient approach to study complex interconnected systems. Their significance is particularly evident in microbiome analysis, where microbial communities are classified at various taxonomic levels along the phylogenetic tree. In light of this trend, the R/Bioconductor community has established a reproducible analytical framework for hierarchical data, which relies on the highly generic and optimised TreeSummarizedExperiment data container. However, using this framework requires basic proficiency in programming.
  $\textbf{Results:}$ To reduce the entry requirements, we developed iSEEtree, an R shiny app which provides a visual interface for the analysis and exploration of TreeSummarizedExperiment objects, thereby expanding the interactive graphics capabilities of related work to hierarchical structures. This way, users can interactively explore several aspects of their data without the need for extensive knowledge of R programming. We describe how iSEEtree enables the exploration of hierarchical multi-table data and demonstrate its functionality with applications to microbiome analysis.
  $\textbf{Availability and Implementation:}$ iSEEtree was implemented in the R programming language and is available on Bioconductor at https://bioconductor.org/packages/iSEEtree under an Artistic 2.0 license.
  $\textbf{Contact:}$ giulio.benedetti@utu.fi or leo.lahti@utu.fi.△ Less",v1,https://arxiv.org/pdf/2412.02882
A Minimalistic 3D Self-Organized UAV Flocking Approach for Desert Exploration,['Robotics'],"['Thulio Amorim', 'Tiago Nascimento', 'Akash Chaudhary', 'Eliseo Ferrante', 'Martin Saska']","In this work, we propose a minimalistic swarm flocking approach for multirotor unmanned aerial vehicles (UAVs). Our approach allows the swarm to achieve cohesively and aligned flocking (collective motion), in a random direction, without externally provided directional information exchange (alignment control). The method relies on minimalistic sensory requirements as it uses only the relative range and bearing of swarm agents in local proximity obtained through onboard sensors on the UAV. Thus, our method is able to stabilize and control the flock of a general shape above a steep terrain without any explicit communication between swarm members. To implement proximal control in a three-dimensional manner, the Lennard-Jones potential function is used to maintain cohesiveness and avoid collisions between robots. The performance of the proposed approach was tested in real-world conditions by experiments with a team of nine UAVs. Experiments also present the usage of our approach on UAVs that are independent of external positioning systems such as the Global Navigation Satellite System (GNSS). Relying only on a relative visual localization through the ultraviolet direction and ranging (UVDAR) system, previously proposed by our group, the experiments verify that our system can be applied in GNSS-denied environments. The degree achieved of alignment and cohesiveness was evaluated using the metrics of order and steady-state value.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02881
Pairwise Spatiotemporal Partial Trajectory Matching for Co-movement Analysis,['Computer Vision and Pattern Recognition'],"['Maria Cardei', 'Sabit Ahmed', 'Gretchen Chapman', 'Afsaneh Doryab']","Spatiotemporal pairwise movement analysis involves identifying shared geographic-based behaviors between individuals within specific time frames. Traditionally, this task relies on sequence modeling and behavior analysis techniques applied to tabular or video-based data, but these methods often lack interpretability and struggle to capture partial matching. In this paper, we propose a novel method for pairwise spatiotemporal partial trajectory matching that transforms tabular spatiotemporal data into interpretable trajectory images based on specified time windows, allowing for partial trajectory analysis. This approach includes localization of trajectories, checking for spatial overlap, and pairwise matching using a Siamese Neural Network. We evaluate our method on a co-walking classification task, demonstrating its effectiveness in a novel co-behavior identification application. Our model surpasses established methods, achieving an F1-score up to 0.73. Additionally, we explore the method's utility for pair routine pattern analysis in real-world scenarios, providing insights into the frequency, timing, and duration of shared behaviors. This approach offers a powerful, interpretable framework for spatiotemporal behavior analysis, with potential applications in social behavior research, urban planning, and healthcare.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02879
Modeling and Discovering Direct Causes for Predictive Models,['Machine Learning'],"['Yizuo Chen', 'Amit Bhatia']","We introduce a causal modeling framework that captures the input-output behavior of predictive models (e.g., machine learning models) by representing it using causal graphs. The framework enables us to define and identify features that directly cause the predictions, which has broad implications for data collection and model evaluation. We show two assumptions under which the direct causes can be discovered from data, one of which further simplifies the discovery process. In addition to providing sound and complete algorithms, we propose an optimization technique based on an independence rule that can be integrated with the algorithms to speed up the discovery process both theoretically and empirically.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02878
Machine Learned Potential for High-Throughput Phonon Calculations of Metal-Organic Frameworks,['Materials Science'],"['Alin Marin Elena', 'Prathami Divakar Kamath', 'Théo Jaffrelot Inizan', 'Andrew S. Rosen', 'Federica Zanca', 'Kristin A. Persson']","Metal-organic frameworks (MOFs) are highly porous and versatile materials studied extensively for applications such as carbon capture and water harvesting. However, computing phonon-mediated properties in MOFs, like thermal expansion and mechanical stability, remains challenging due to the large number of atoms per unit cell, making traditional Density Functional Theory (DFT) methods impractical for high-throughput screening. Recent advances in machine learning potentials have led to foundation atomistic models, such as MACE-MP-0, that accurately predict equilibrium structures but struggle with phonon properties of MOFs. In this work, we developed a workflow for computing phonons in MOFs within the quasi-harmonic approximation with a fine-tuned MACE model, MACE-MP-MOF0. The model was trained on a curated dataset of 127 representative and diverse MOFs. The fine-tuned MACE-MP-MOF0 improves the accuracy of phonon density of states and corrects the imaginary phonon modes of MACE-MP-0, enabling high-throughput phonon calculations with state-of-the-art precision. The model successfully predicts thermal expansion and bulk moduli in agreement with DFT and experimental data for several well-known MOFs. These results highlight the potential of MACE-MP-MOF0 in guiding MOF design for applications in energy storage and thermoelectrics.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02877
Out-of-Distribution Detection for Neurosymbolic Autonomous Cyber Agents,['Machine Learning'],"['Ankita Samaddar', 'Nicholas Potteiger', 'Xenofon Koutsoukos']","Autonomous agents for cyber applications take advantage of modern defense techniques by adopting intelligent agents with conventional and learning-enabled components. These intelligent agents are trained via reinforcement learning (RL) algorithms, and can learn, adapt to, reason about and deploy security rules to defend networked computer systems while maintaining critical operational workflows. However, the knowledge available during training about the state of the operational network and its environment may be limited. The agents should be trustworthy so that they can reliably detect situations they cannot handle, and hand them over to cyber experts. In this work, we develop an out-of-distribution (OOD) Monitoring algorithm that uses a Probabilistic Neural Network (PNN) to detect anomalous or OOD situations of RL-based agents with discrete states and discrete actions. To demonstrate the effectiveness of the proposed approach, we integrate the OOD monitoring algorithm with a neurosymbolic autonomous cyber agent that uses behavior trees with learning-enabled components. We evaluate the proposed approach in a simulated cyber environment under different adversarial strategies. Experimental results over a large number of episodes illustrate the overall efficiency of our proposed approach.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02875
Unveiling the Dance of Molecules: Ro-Vibrational Dynamics of Molecules under Intense Illumination at Complex Plasmonic Interfaces,['Chemical Physics'],"['Maxim Sukharev', 'Joseph E. Subotnik', 'Abraham Nitzan']","Understanding the quantum dynamics of strongly coupled molecule-cavity systems remains a significant challenge in molecular polaritonics. This work develops a comprehensive self-consistent model simulating electromagnetic interactions of diatomic molecules with quantum ro-vibrational degrees of freedom in resonant optical cavities. The approach employs an efficient numerical methodology to solve coupled Schrodinger-Maxwell equations in real space-time, enabling three-dimensional simulations through a novel molecular mapping technique. The study investigates relaxation dynamics of an ensemble of molecules following intense resonant pump excitation in Fabry-Perot cavities and at three-dimensional plasmonic metasurfaces. The simulations reveal dramatically modified relaxation pathways inside cavities compared to free space, characterized by persistent molecular alignment arising from cavity-induced rotational pumping. They also indicate the presence of a previously unreported relaxation stabilization mechanism driven by dephasing of the collective molecular-cavity mode. Additionally, the study demonstrates that strong molecular coupling significantly modifies the circular dichroism spectra of chiral metasurfaces, suggesting new opportunities for controlling light-matter interactions in quantum optical systems.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02873
MAGMA: Manifold Regularization for MAEs,['Computer Vision and Pattern Recognition'],"['Alin Dondera', 'Anuj Singh', 'Hadi Jamali-Rad']","Masked Autoencoders (MAEs) are an important divide in self-supervised learning (SSL) due to their independence from augmentation techniques for generating positive (and/or negative) pairs as in contrastive frameworks. Their masking and reconstruction strategy also nicely aligns with SSL approaches in natural language processing. Most MAEs are built upon Transformer-based architectures where visual features are not regularized as opposed to their convolutional neural network (CNN) based counterparts, which can potentially hinder their performance. To address this, we introduce MAGMA, a novel batch-wide layer-wise regularization loss applied to representations of different Transformer layers. We demonstrate that by plugging in the proposed regularization loss, one can significantly improve the performance of MAE-based models. We further demonstrate the impact of the proposed loss on optimizing other generic SSL approaches (such as VICReg and SimCLR), broadening the impact of the proposed approach. Our code base can be found at https://github.com/adondera/magma.△ Less",v1,https://arxiv.org/pdf/2412.02871
Constrained Identifiability of Causal Effects,['Artificial Intelligence'],"['Yizuo Chen', 'Adnan Darwiche']","We study the identification of causal effects in the presence of different types of constraints (e.g., logical constraints) in addition to the causal graph. These constraints impose restrictions on the models (parameterizations) induced by the causal graph, reducing the set of models considered by the identifiability problem. We formalize the notion of constrained identifiability, which takes a set of constraints as another input to the classical definition of identifiability. We then introduce a framework for testing constrained identifiability by employing tractable Arithmetic Circuits (ACs), which enables us to accommodate constraints systematically. We show that this AC-based approach is at least as complete as existing algorithms (e.g., do-calculus) for testing classical identifiability, which only assumes the constraint of strict positivity. We use examples to demonstrate the effectiveness of this AC-based approach by showing that unidentifiable causal effects may become identifiable under different types of constraints.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02869
A note on the no-$(d+2)$-on-a-sphere problem,['Combinatorics'],"['Andrew Suk', 'Ethan Patrick White']","For fixed $d\geq 3$, we construct subsets of the $d$-dimensional lattice cube $[n]^d$ of size $n^{\frac{3}{d + 1} - o(1)}$ with no $d+2$ points on a sphere or a hyperplane. This improves the previously best known bound of $Ω(n^{\frac{1}{d-1}})$ due to Thiele from 1995.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02866
Memory-efficient Continual Learning with Neural Collapse Contrastive,['Computer Vision and Pattern Recognition'],"['Trung-Anh Dang', 'Vincent Nguyen', 'Ngoc-Son Vu', 'Christel Vrain']","Contrastive learning has significantly improved representation quality, enhancing knowledge transfer across tasks in continual learning (CL). However, catastrophic forgetting remains a key challenge, as contrastive based methods primarily focus on ""soft relationships"" or ""softness"" between samples, which shift with changing data distributions and lead to representation overlap across tasks. Recently, the newly identified Neural Collapse phenomenon has shown promise in CL by focusing on ""hard relationships"" or ""hardness"" between samples and fixed prototypes. However, this approach overlooks ""softness"", crucial for capturing intra-class variability, and this rigid focus can also pull old class representations toward current ones, increasing forgetting. Building on these insights, we propose Focal Neural Collapse Contrastive (FNC^2), a novel representation learning loss that effectively balances both soft and hard relationships. Additionally, we introduce the Hardness-Softness Distillation (HSD) loss to progressively preserve the knowledge gained from these relationships across tasks. Our method outperforms state-of-the-art approaches, particularly in minimizing memory reliance. Remarkably, even without the use of memory, our approach rivals rehearsal-based methods, offering a compelling solution for data privacy concerns.△ Less",v1,https://arxiv.org/pdf/2412.02865
Learning constitutive relations from experiments: 1. PDE constrained optimization,['Materials Science'],"['Andrew Akerson', 'Aakila Rajan', 'Kaushik Bhattacharya']","We propose a method to accurately and efficiently identify the constitutive behavior of complex materials through full-field observations. We formulate the problem of inferring constitutive relations from experiments as an indirect inverse problem that is constrained by the balance laws. Specifically, we seek to find a constitutive behavior that minimizes the difference between the experimental observation and the corresponding quantities computed with the model, while enforcing the balance laws. We formulate the forward problem as a boundary value problem corresponding to the experiment, and compute the sensitivity of the objective with respect to model using the adjoint method. The resulting method is robust and can be applied to constitutive models with arbitrary complexity. We focus on elasto-viscoplasticity, but the approach can be extended to other settings. In this part one, we formulate the method and demonstrate it using synthetic data on two problems, one quasistatic and the other dynamic.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02864
Powerful nuclear outflows and circumgalactic medium shocks driven by the most luminous quasar in the Universe,['Astrophysics of Galaxies'],"['Andrey Vayner', 'Tanio Díaz-Santos', 'Peter R. M. Eisenhardt', 'Daniel Stern', 'Lee Armus', 'Daniel Anglés-Alcázar', 'Roberto J. Assef', 'Román Fernández Aranda', 'Andrew W. Blain', 'Hyunsung D. Jun', 'Chao-Wei Tsai', 'Niranjan Chandra Roy', 'Drew Brisbin', 'Carl D. Ferkinhoff', 'Manuel Aravena', 'Jorge González-López', 'Guodong Li', 'Mai Liao', 'Devika Shobhana', 'Jingwen Wu', 'Dejene Zewdie']","We report integral field spectroscopy observations with the Near-Infrared Spectrograph on board JWST targeting the 60 kpc environment surrounding the most luminous quasar known at $z=4.6$. We detect ionized gas filaments on 40 kpc scales connecting a network of merging galaxies likely to form a cluster. We find regions of low ionization consistent with large-scale shock excitation surrounding the central dust-obscured quasar, out to distances nearly eight times the effective stellar radius of the quasar host galaxy. In the nuclear region, we find an ionized outflow driven by the quasar with velocities reaching 13,000 km s$^{-1}$, one of the fastest discovered to date with an outflow rate of 2000 M$_\odot$ yr$^{-1}$ and a kinetic luminosity of 6$\times10^{46}$ erg s$^{-1}$ resulting in coupling efficiency between the bolometric luminosity of the quasar and the outflow of 5%. The kinetic luminosity of the outflow is sufficient to power the turbulent motion of the gas on galactic and circumgalactic scales and is likely the primary driver of the radiative shocks on interstellar medium and circumgalactic medium scales. This provides compelling evidence supporting long-standing theoretical predictions that powerful quasar outflows are a main driver in regulating the heating and accretion rate of gas onto massive central cluster galaxies.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02862
An Information-Theoretic Analysis of Thompson Sampling for Logistic Bandits,['Machine Learning'],"['Amaury Gouverneur', 'Borja Rodríguez-Gálvez', 'Tobias J. Oechtering', 'Mikael Skoglund']","We study the performance of the Thompson Sampling algorithm for logistic bandit problems, where the agent receives binary rewards with probabilities determined by a logistic function $\exp(β\langle a, θ\rangle)/(1+\exp(β\langle a, θ\rangle))$. We focus on the setting where the action $a$ and parameter $θ$ lie within the $d$-dimensional unit ball with the action space encompassing the parameter space. Adopting the information-theoretic framework introduced by (Russo $\&$ Van Roy, 2015), we analyze the information ratio, which is defined as the ratio of the expected squared difference between the optimal and actual rewards to the mutual information between the optimal action and the reward. Improving upon previous results, we establish that the information ratio is bounded by $\tfrac{9}{2}d$. Notably, we obtain a regret bound in $O(d\sqrt{T \log(βT/d)})$ that depends only logarithmically on the parameter $β$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02861
Novel Magnetic Actuation Strategies for Precise Ferrofluid Marble Manipulation in Magnetic Digital Microfluidics: Position Control and Applications,['Robotics'],"['Mohammad Hossein Sarkhosh', 'Mohammad Hassan Dabirzadeh', 'Mohamad Ali Bijarchi', 'Hossein Nejat Pishkenari']","Precise manipulation of liquid marbles has significant potential in various applications such as lab-on-a-chip systems, drug delivery, and biotechnology and has been a challenge for researchers. Ferrofluid marble (FM) is a marble with a ferrofluid core that can easily be manipulated by a magnetic field. Although FMs have great potential for accurate positioning and manipulation, these marbles have not been precisely controlled in magnetic digital microfluidics, so far. In this study for the first time, a novel method of magnetic actuation is proposed using a pair of Helmholtz coils and permanent magnets. The governing equations for controlling the FM position are investigated, and it is shown that there are three different strategies for adjusting the applied magnetic force. Then, experiments are conducted to demonstrate the capability of the proposed method. To this aim, different magnetic setups are proposed for manipulating FMs. These setups are compared in terms of energy consumption and tracking ability across various frequencies. The study showcases several applications of precise FM position control, including controllable reciprocal positioning, simultaneous position control of two FMs, the transport of non-magnetic liquid marbles using the FMs, and sample extraction method from the liquid core of the FM.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02859
Unpaired Modality Translation for Pseudo Labeling of Histology Images,['Image and Video Processing'],"['Arthur Boschet', 'Armand Collin', 'Nishka Katoch', 'Julien Cohen-Adad']","The segmentation of histological images is critical for various biomedical applications, yet the lack of annotated data presents a significant challenge. We propose a microscopy pseudo labeling pipeline utilizing unsupervised image translation to address this issue. Our method generates pseudo labels by translating between labeled and unlabeled domains without requiring prior annotation in the target domain. We evaluate two pseudo labeling strategies across three image domains increasingly dissimilar from the labeled data, demonstrating their effectiveness. Notably, our method achieves a mean Dice score of $0.736 \pm 0.005$ on a SEM dataset using the tutoring path, which involves training a segmentation model on synthetic data created by translating the labeled dataset (TEM) to the target modality (SEM). This approach aims to accelerate the annotation process by providing high-quality pseudo labels as a starting point for manual refinement.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02858
Is Large-Scale Pretraining the Secret to Good Domain Generalization?,['Computer Vision and Pattern Recognition'],"['Piotr Teterwak', 'Kuniaki Saito', 'Theodoros Tsiligkaridis', 'Bryan A. Plummer', 'Kate Saenko']","Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains. Recent methods combine robust features from web-scale pretrained backbones with new features learned from source data, and this has dramatically improved benchmark results. However, it remains unclear if DG finetuning methods are becoming better over time, or if improved benchmark performance is simply an artifact of stronger pre-training. Prior studies have shown that perceptual similarity to pre-training data correlates with zero-shot performance, but we find the effect limited in the DG setting. Instead, we posit that having perceptually similar data in pretraining is not enough; and that it is how well these data were learned that determines performance. This leads us to introduce the Alignment Hypothesis, which states that the final DG performance will be high if and only if alignment of image and class label text embeddings is high. Our experiments confirm the Alignment Hypothesis is true, and we use it as an analysis tool of existing DG methods evaluated on DomainBed datasets by splitting evaluation data into In-pretraining (IP) and Out-of-pretraining (OOP). We show that all evaluated DG methods struggle on DomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our findings highlight the need for DG methods which can generalize beyond pretraining alignment.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02856
Effortless Efficiency: Low-Cost Pruning of Diffusion Models,['Computer Vision and Pattern Recognition'],"['Yang Zhang', 'Er Jin', 'Yanfei Dong', 'Ashkan Khakzar', 'Philip Torr', 'Johannes Stegmaier', 'Kenji Kawaguchi']","Diffusion models have achieved impressive advancements in various vision tasks. However, these gains often rely on increasing model size, which escalates computational complexity and memory demands, complicating deployment, raising inference costs, and causing environmental impact. While some studies have explored pruning techniques to improve the memory efficiency of diffusion models, most existing methods require extensive retraining to retain the model performance. Retraining a modern large diffusion model is extremely costly and resource-intensive, which limits the practicality of these methods. In this work, we achieve low-cost diffusion pruning without retraining by proposing a model-agnostic structural pruning framework for diffusion models that learns a differentiable mask to sparsify the model. To ensure effective pruning that preserves the quality of the final denoised latent, we design a novel end-to-end pruning objective that spans the entire diffusion process. As end-to-end pruning is memory-intensive, we further propose time step gradient checkpointing, a technique that significantly reduces memory usage during optimization, enabling end-to-end pruning within a limited memory budget. Results on state-of-the-art U-Net diffusion models SDXL and diffusion transformers (FLUX) demonstrate that our method can effectively prune up to 20% parameters with minimal perceptible performance degradation, and notably, without the need for model retraining. We also showcase that our method can still prune on top of time step distilled diffusion models.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02852
A Lower Mass Estimate for PSR J0348+0432 Based on CHIME/Pulsar Precision Timing,['High Energy Astrophysical Phenomena'],"['Alexander Saffer', 'Emmanuel Fonseca', 'Scott Ransom', 'Ingrid Stairs', 'Ryan Lynch', 'Deborah Good', 'Kiyoshi W. Masui', 'James W. McKee', 'Bradley W. Meyers', 'Swarali Shivraj Patil', 'Chia Min Tan']","The binary pulsar J0348+0432 was previously shown to have a mass of approximately 2\,${\rm M_\odot}$, based on the combination of radial-velocity and model-dependent mass parameters derived from high-resolution optical spectroscopy of its white-dwarf companion. We present follow-up timing observations that combine archival observations with data acquired by the Canadian Hydrogen Intensity Mapping Experiment (CHIME) pulsar instrument. We find that the inclusion of CHIME/Pulsar data yields an improved measurement of general-relativistic orbital decay in the system that falls within 1.2 $σ$ of the original values published by Antoniadis et al. (2013) while being roughly 6 times more precise due to the extended baseline. When we combine this new orbital evolution rate with the mass ratio determined from optical spectroscopy, we determine a pulsar mass of 1.806(37)\,${\rm M_\odot}$. For the first time for this pulsar, timing alone significantly constrains the pulsar mass. We explain why the new mass for the pulsar is $10\%$ lower and discuss how the mis-modeling of the initial observations of the white dwarf companion likely led to an inaccurate determination of the pulsar mass.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02850
Environmental Evidence for Overly Massive Black Holes in Low Mass Galaxies and a Black Hole - Halo Mass Relation at $z \sim 5$,['Astrophysics of Galaxies'],"['Jorryt Matthee', 'Rohan P. Naidu', 'Gauri Kotiwale', 'Lukas J. Furtak', 'Ivan Kramarenko', 'Ruari Mackenzie', 'Jenny Greene', 'Angela Adamo', 'Rychard J. Bouwens', 'Claudia Di Cesare', 'Anna-Christina Eilers', 'Anna de Graaff', 'Kasper E. Heintz', 'Daichi Kashino', 'Michael V. Maseda', 'Sandro Tacchella', 'Alberto Torralba']","JWST observations have unveiled faint active galactic nuclei (AGN) at high-redshift that provide insights on the formation of supermassive black holes (SMBHs) and their coevolution with galaxies. However, disentangling stellar from AGN light in these sources is challenging. Here, we use an empirical approach to infer the average stellar mass of 6 faint broad line (BL) Halpha emitters at z = 4 - 5 with BH masses ~ 6 (4 - 15)x10^6 Msun, with a method independent of their spectral energy distribution (SED). We use the deep JWST/NIRcam grism survey ALT to measure the over-densities around BL-Halpha emitters and around a spectroscopic reference sample of ~300 galaxies. In our reference sample, we find that Mpc-scale over-density correlates with stellar mass, while pair counts are flat below ~50 kpc due to satellites. Their large-scale environments suggest that BL-Halpha emitters are hosted by galaxies with stellar masses ~5x10^7 Msun, ~40 times lower than those inferred from galaxy-only SED fits. Adding measurements around more luminous z~6 AGNs, we find tentative correlations between line width, BH mass and the over-density, suggestive of a steep BH to halo mass relation. The main implications are (1) when BH masses are taken at face value, we confirm extremely high BH to stellar mass ratios of ~10 %, (2) the low stellar mass galaxies hosting growing SMBHs are in tension with typical hydrodynamical simulations, except those without feedback, (3) a 1 % duty cycle implied by the host mass hints at super-Eddington accretion, which may imply over-estimated SMBH masses, (4) the masses are at odds with a high stellar density interpretation of the line broadening, (5) our results imply a diversity of galaxy masses, environments and SEDs among AGN samples, depending on their luminosity.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02846
Optimized IoT Intrusion Detection using Machine Learning Technique,['Cryptography and Security'],"['Muhammad Zawad Mahmud', 'Samiha Islam', 'Shahran Rahman Alve', 'Al Jubayer Pial']","An application of software known as an Intrusion Detection System (IDS) employs machine algorithms to identify network intrusions. Selective logging, safeguarding privacy, reputation-based defense against numerous attacks, and dynamic response to threats are a few of the problems that intrusion identification is used to solve. The biological system known as IoT has seen a rapid increase in high dimensionality and information traffic. Self-protective mechanisms like intrusion detection systems (IDSs) are essential for defending against a variety of attacks. On the other hand, the functional and physical diversity of IoT IDS systems causes significant issues. These attributes make it troublesome and unrealistic to completely use all IoT elements and properties for IDS self-security. For peculiarity-based IDS, this study proposes and implements a novel component selection and extraction strategy (our strategy). A five-ML algorithm model-based IDS for machine learning-based networks with proper hyperparamater tuning is presented in this paper by examining how the most popular feature selection methods and classifiers are combined, such as K-Nearest Neighbors (KNN) Classifier, Decision Tree (DT) Classifier, Random Forest (RF) Classifier, Gradient Boosting Classifier, and Ada Boost Classifier. The Random Forest (RF) classifier had the highest accuracy of 99.39%. The K-Nearest Neighbor (KNN) classifier exhibited the lowest performance among the evaluated models, achieving an accuracy of 94.84%. This study's models have a significantly higher performance rate than those used in previous studies, indicating that they are more reliable.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02845
A nanolaser with extreme dielectric confinement,['Optics'],"['Meng Xiong', 'Yi Yu', 'Yury Berdnikov', 'Simon Klinck Borregaard', 'Adrian Holm Dubré', 'Elizaveta Semenova', 'Kresten Yvind', 'Jesper Mørk']","The interaction between light and matter can be enhanced by spatially concentrating the light field to boost the photon energy density and increasing the photon dwell time to prolong energy transfer between light and matter. Traditionally, strong spatial light localization has been achieved using plasmonics, which, despite its effectiveness, entails ohmic losses. Recent advances in nanostructured dielectrics offer an avenue for achieving strong light confinement without metallic losses. However, previous studies primarily focused on minimizing the optical mode volume without adequately addressing light-matter interactions. Here, we develop a nanolaser that simultaneously localizes the electromagnetic field and excited carriers within the same region of a dielectric nanobridge. This extreme dielectric confinement of both light and matter achieves a mode volume below the diffraction limit and a subwavelength carrier volume without the introduction of lateral quantum confinement, enabling continuous-wave lasing at room-temperature. Moreover, we observe a strong correlation between the mode field and carrier distribution, and unexpectedly, the enhanced mode field localization automatically leads to more pronounced carrier localization, promoting self-alignment of light and matter, which significantly reduces the laser threshold. We quantify the intensified light-matter interaction with a newly proposed interaction volume, which generalizes the concept of mode volume to a broad class of active media. Our work lays the ground for developing ultra-efficient optoelectronic devices by greatly enhancing light-matter interactions through advanced material nanostructuring.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02844
Batch Normalization Decomposed,['Machine Learning'],"['Ido Nachum', 'Marco Bondaschi', 'Michael Gastpar', 'Anatoly Khina']","\emph{Batch normalization} is a successful building block of neural network architectures. Yet, it is not well understood. A neural network layer with batch normalization comprises three components that affect the representation induced by the network: \emph{recentering} the mean of the representation to zero, \emph{rescaling} the variance of the representation to one, and finally applying a \emph{non-linearity}. Our work follows the work of Hadi Daneshmand, Amir Joudaki, Francis Bach [NeurIPS~'21], which studied deep \emph{linear} neural networks with only the rescaling stage between layers at initialization. In our work, we present an analysis of the other two key components of networks with batch normalization, namely, the recentering and the non-linearity. When these two components are present, we observe a curious behavior at initialization. Through the layers, the representation of the batch converges to a single cluster except for an odd data point that breaks far away from the cluster in an orthogonal direction. We shed light on this behavior from two perspectives: (1) we analyze the geometrical evolution of a simplified indicative model; (2) we prove a stability result for the aforementioned~configuration.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02843
Geographical Information Alignment Boosts Traffic Analysis via Transpose Cross-attention,['Machine Learning'],"['Xiangyu Jiang', 'Xiwen Chen', 'Hao Wang', 'Abolfazl Razi']","Traffic accident prediction is crucial for enhancing road safety and mitigating congestion, and recent Graph Neural Networks (GNNs) have shown promise in modeling the inherent graph-based traffic data. However, existing GNN- based approaches often overlook or do not explicitly exploit geographic position information, which often plays a critical role in understanding spatial dependencies. This is also aligned with our observation, where accident locations are often highly relevant. To address this issue, we propose a plug-in-and-play module for common GNN frameworks, termed Geographic Information Alignment (GIA). This module can efficiently fuse the node feature and geographic position information through a novel Transpose Cross-attention mechanism. Due to the large number of nodes for traffic data, the conventional cross-attention mechanism performing the node-wise alignment may be infeasible in computation-limited resources. Instead, we take the transpose operation for Query, Key, and Value in the Cross-attention mechanism, which substantially reduces the computation cost while maintaining sufficient information. Experimental results for both traffic occurrence prediction and severity prediction (severity levels based on the interval of recorded crash counts) on large-scale city-wise datasets confirm the effectiveness of our proposed method. For example, our method can obtain gains ranging from 1.3% to 10.9% in F1 score and 0.3% to 4.8% in AUC.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02839
Sensing and Mitigation of Multi-Scatterer Self-Interference for Full-Duplex MIMO Communications,['Signal Processing'],"['Anil Kurt', 'Gokhan M. Guvensen']","This paper proposes the joint use of digital self-interference cancellation (DSIC) and spatial suppression to mitigate far-field self-interference (SI) in full-duplex multiple-input multiple-output (MIMO) systems. Far-field SI, caused by echoes from environmental scatterers, is modeled based on the scatterers' angle and delay parameters, stored in a scatterer map. For each scatterer, the most suitable action regarding communication is selected from transmit beamforming, receive beamforming, DSIC, and no-action. This selection is based on simple metrics that show the expected uplink and downlink communication performance. In addition, emerging scatterers that deteriorate the communication are detected, and their delay and angles are acquired, providing an up-to-date scatterer map and presenting a \emph{sensing for communication} case. The proposed selection policy is compared with the individual implementations of DSIC and spatial suppression, highlighting the failure cases for each. It is shown that the proposed policy stays unaffected in these problematic cases and achieves SI-free performance.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02838
"Clusters, Clumps, Dust, & Gas (CCDG) in NGC1614: Bench-marking Cluster Demographics in Extreme Systems",['Astrophysics of Galaxies'],"['Miranda Caputo', 'Rupali Chandar', 'Angus Mok', 'Sean Linden', 'Paul Goudfrooij', 'Bradley C. Whitmore']","Observations of young star clusters in a variety of galaxies have been used to constrain basic properties related to star-formation, such as the fraction of stars found in clusters (Gam) and the shape of the cluster mass function. However, the results can depend heavily on the reliability of the cluster age-dating process and other assumptions. One of the biggest challenges for successful age-dating lies in breaking the age-reddening degeneracy, where older, dust-free clusters and young, reddened clusters can have similar broad-band colors. While this degeneracy affects cluster populations in all galaxies, it is particularly challenging in dusty, extreme star-forming environments systems. We study the cluster demographics in the luminous infrared galaxy NGC1614 using Hubble imaging taken in 8 optical-NIR passbands. For age-dating, we adopt a spectral energy distribution fitting process that limits the maximum allowed reddening by region, and includes Ha photometry directly. We find that without these assumptions, essentially all clusters in the dust-free UV-bright arm which should have ages 50-250Myr are incorrectly assigned ages younger than 10Myr. We find this method greatly reduces the number of clusters in the youngest (tau<10Myrs) age bin and shows a fairly uniform distribution of massive clusters, the most massive being few10^7M. A maximum likelihood fit shows that the cluster mass function is well fitted by a power-law with an index -1.8, with no statistically significant high-mass cutoff. We calculate the fraction of stars born in clusters to be Gam1-10=22.4+_5.7%. The fraction of stars in clusters decreases quickly over time, with Gam10-100= 4.5+_1.1% and Gam100-400=1.7+_0.4%, suggesting that clusters dissolve rapidly over the first ~0.5Gyr. The decreasing fraction of stars in clusters is consistent with the declining shape observed for the cluster age distribution.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02836
Commensal Transient Searches with MeerKAT in Gamma-Ray Burst and Supernova Fields,['High Energy Astrophysical Phenomena'],"['S. I. Chastain', 'A. J. van der Horst', 'A. Horesh', 'A. Rowlinson', 'A. Andersson', 'R. Diretse', 'M. Vaccari', 'R. P. Fender', 'P. A. Woudt']","The sensitivity and field of view of the MeerKAT radio telescope provides excellent opportunities for commensal transient searches. We carry out a commensal transient search in supernova and short gamma-ray burst fields using methodologies established in Chastain et al. (2023). We search for transients in MeerKAT L-band images with 30 minute integration times, finding 13 variable sources. We compare these sources to the VLASS and RACS survey data, and examine possible explanation for the variability. We find that 12 of these sources are consistent with variability due to interstellar scintillation. The remaining source could possibly have some intrinsic variability. We also split the MeerKAT L-band into an upper and lower half, and search for transients in images with an 8 second integration time. We find a source with a duration of 8 to 16 seconds that is highly polarized at the lowest frequencies. We conclude that this source may be consistent with a stellar flare. Finally, we calculate accurate upper and lower limits on the transient rate using transient simulations.△ Less",v1,https://arxiv.org/pdf/2412.02832
Damping Enhancement in YIG at Millikelvin Temperatures due to GGG Substrate,['Mesoscale and Nanoscale Physics'],"['Rostyslav O. Serha', 'Andrey A. Voronov', 'David Schmoll', 'Rebecca Klingbeil', 'Sebastian Knauer', 'Sabri Koraltan', 'Ekaterina Pribytova', 'Morris Lindner', 'Timmy Reimann', 'Carsten Dubs', 'Claas Abert', 'Roman Verba', 'Michal Urbánek', 'Dieter Suess', 'Andrii V. Chumak']","Quantum magnonics aims to exploit the quantum mechanical properties of magnons for nanoscale quantum information technologies. Ferrimagnetic yttrium iron garnet (YIG), which offers the longest magnon lifetimes, is a key material typically grown on gadolinium gallium garnet (GGG) substrates for structural compatibility. However, the increased magnetic damping in YIG/GGG systems below 50$\,$K poses a challenge for quantum applications. Here, we study the damping in a 97$\,$nm-thick YIG film on a 500$\,μ$m-thick GGG substrate at temperatures down to 30$\,$mK using ferromagnetic resonance (FMR) spectroscopy. We show that the dominant physical mechanism for the observed tenfold increase in FMR linewidth at millikelvin temperatures is the non-uniform bias magnetic field generated by the partially magnetized paramagnetic GGG substrate. Numerical simulations and analytical theory show that the GGG-driven linewidth enhancement can reach up to 6.7 times. In addition, at low temperatures and frequencies above 18$\,$GHz, the FMR linewidth deviates from the viscous Gilbert-damping model. These results allow the partial elimination of the damping mechanisms attributed to GGG, which is necessary for the advancement of solid-state quantum technologies.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02827
Many-MobileNet: Multi-Model Augmentation for Robust Retinal Disease Classification,['Computer Vision and Pattern Recognition'],"['Hao Wang', 'Wenhui Zhu', 'Xuanzhao Dong', 'Yanxi Chen', 'Xin Li', 'Peijie Qiu', 'Xiwen Chen', 'Vamsi Krishna Vasa', 'Yujian Xiong', 'Oana M. Dumitrascu', 'Abolfazl Razi', 'Yalin Wang']","In this work, we propose Many-MobileNet, an efficient model fusion strategy for retinal disease classification using lightweight CNN architecture. Our method addresses key challenges such as overfitting and limited dataset variability by training multiple models with distinct data augmentation strategies and different model complexities. Through this fusion technique, we achieved robust generalization in data-scarce domains while balancing computational efficiency with feature extraction capabilities.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02825
Semi-Blind Channel Estimation for Beyond Diagonal RIS,['Signal Processing'],"['Gilderlan T de Araujo', 'Andre L. F. de Almeida']","The channel estimation problem has been widely discussed in traditional reconfigurable intelligent surface assisted multiple-input multiple-output. However, solutions for channel estimation adapted to beyond diagonal RIS need further study, and few recent works have been proposed to tackle this problem. Moreover, methods that avoid or minimize the use of pilot sequences are of interest. This work formulates a data-driven (semi-blind) joint channel and symbol estimation algorithm for beyond diagonal RIS that avoids a prior pilot-assisted stage while providing decoupled estimates of the involved communication channels. The proposed receiver builds upon a PARATUCK tensor model for the received signal, from which a trilinear alternating estimation scheme is derived. Preliminary numerical results demonstrate the proposed method's performance for selected system setups. The symbol error rate performance is also compared with that of a linear receiver operating with perfect knowledge of the cascaded channel.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02824
Tropicalization of $ψ$ classes,['Algebraic Geometry'],"['Renzo Cavalieri', 'Andreas Gross']","Under suitable conditions on a family of logarithmic curves, we endow the tropicalization of the family with an affine structure in a neighborhood of the sections in such a way that the tropical $ψ$ classes from \cite{psi-classes} arise as tropicalizations of algebraic $ψ$ classes.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02817
Unleashing GHOST: An LLM-Powered Framework for Automated Hardware Trojan Design,['Cryptography and Security'],"['Md Omar Faruque', 'Peter Jamieson', 'Ahmad Patooghy', 'Abdel-Hameed A. Badawy']","Traditionally, inserting realistic Hardware Trojans (HTs) into complex hardware systems has been a time-consuming and manual process, requiring comprehensive knowledge of the design and navigating intricate Hardware Description Language (HDL) codebases. Machine Learning (ML)-based approaches have attempted to automate this process but often face challenges such as the need for extensive training data, long learning times, and limited generalizability across diverse hardware design landscapes. This paper addresses these challenges by proposing GHOST (Generator for Hardware-Oriented Stealthy Trojans), an automated attack framework that leverages Large Language Models (LLMs) for rapid HT generation and insertion. Our study evaluates three state-of-the-art LLMs - GPT-4, Gemini-1.5-pro, and Llama-3-70B - across three hardware designs: SRAM, AES, and UART. According to our evaluations, GPT-4 demonstrates superior performance, with 88.88% of HT insertion attempts successfully generating functional and synthesizable HTs. This study also highlights the security risks posed by LLM-generated HTs, showing that 100% of GHOST-generated synthesizable HTs evaded detection by an ML-based HT detection tool. These results underscore the urgent need for advanced detection and prevention mechanisms in hardware security to address the emerging threat of LLM-generated HTs. The GHOST HT benchmarks are available at: https://github.com/HSTRG1/GHOSTbenchmarks.git△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02816
Near-Field Measurement System for the Upper Mid-Band,['Signal Processing'],"['Ali Rasteh', 'Raghavendra Palayam Hari', 'Hao Guo', 'Marco Mezzavilla', 'Sundeep Rangan']","The upper mid-band (or FR3, spanning 6-24 GHz) is a crucial frequency range for next-generation mobile networks, offering a favorable balance between coverage and spectrum efficiency. From another perspective, the systems operating in the near-field in both indoor environment and outdoor environments can support line-of-sight multiple input multiple output (MIMO) communications and be beneficial from the FR3 bands. In this paper, a novel method is proposed to measure the near-field parameters leveraging a recently developed reflection model where the near-field paths can be described by their image points. We show that these image points can be accurately estimated via triangulation from multiple measurements with a small number of antennas in each measurement, thus affording a low-cost procedure for near-field multi-path parameter extraction. A preliminary experimental apparatus is presented comprising 2 transmit and 2 receive antennas mounted on a linear track to measure the 2x2 MIMO channel at various displacements. The system uses a recently-developed wideband radio frequency (RF) transceiver board with fast frequency switching, an FPGA for fast baseband processing, and a new parameter extraction method to recover paths and spherical characteristics from the multiple 2x2 measurements.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02815
Quantum scalar field theory on equal-angular-momenta Myers-Perry-AdS black holes,['High Energy Physics - Theory'],"['Alessandro Monteverdi', 'Elizabeth Winstanley']","We study the canonical quantization of a massive scalar field on a five dimensional, rotating black hole space-time. We focus on the case where the space-time is asymptotically anti-de Sitter and the black hole's two angular momentum parameters are equal. In this situation the geometry possesses additional symmetries which simplify both the mode solutions of the scalar field equation and the stress-energy tensor. When the angular momentum of the black hole is sufficiently small that there is no speed-of-light surface, there exists a Killing vector which is time-like in the region exterior to the event horizon. In this case classical superradiance is absent and we construct analogues of the usual Boulware and Hartle-Hawking quantum states for the quantum scalar field. We compute the differences in expectation values of the square of the quantum scalar field operator and the stress-energy tensor operator between these two quantum states.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02814
Real-Space Imaging of the Band Topology of Transition Metal Dichalcogenides,['Materials Science'],"['Madisen Holbrook', 'Julian Ingham', 'Daniel Kaplan', 'Luke Holtzman', 'Brenna Bierman', 'Nicholas Olson', 'Luca Nashabeh', 'Song Liu', 'Xiaoyang Zhu', 'Daniel Rhodes', 'Katayun Barmak', 'James Hone', 'Raquel Queiroz', 'Abhay Pasupathy']","The topological properties of Bloch bands are intimately tied to the structure of their electronic wavefunctions within the unit cell of a crystal. Here, we show that scanning tunneling microscopy (STM) measurements on the prototypical transition metal dichalcogenide (TMD) semiconductor WSe$_2$ can be used to unambiguously fix the location of the Wannier center of the valence band. Using site-specific substitutional doping, we first determine the position of the atomic sites within STM images, establishing that the maximum electronic density of states at the $K$-point lies between the atoms. In contrast, the maximum density of states at the $Γ$ point is at the atomic sites. This signifies that WSe$_2$ is a topologically obstructed atomic insulator, which cannot be adiabatically transformed to the trivial atomic insulator limit.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02813
"The origin, consequence, and visibility of criticism in science",['Digital Libraries'],"['Bingsheng Chen', 'Dakota Murray', 'Yixuan Liu', 'Albert-László Barabási']","Critique between peers plays a vital role in the production of scientific knowledge. Yet, there is limited empirical evidence on the origins of criticism, its effects on the papers and individuals involved, and its visibility within the scientific literature. Here, we address these gaps through a data-driven analysis of papers that received substantiated and explicit written criticisms. Our analysis draws on data representing over 3,000 ``critical letters'' -- papers explicitly published to critique another -- from four high profile journals, with each letter linked to its target paper. We find that the papers receiving critical letters are disproportionately among the most highly-cited in their respective journal and, to a lesser extent, among the most interdisciplinary and novel. However, despite the theoretical importance of criticism in scientific progress, we observe no evidence that receiving a critical letter affects a paper's citation trajectory or the productivity and citation impact of its authors. One explanation for the limited consequence of critical letters is that they often go unnoticed. Indeed, we find that critical letters attract only a small fraction of the citations received by their targets, even years after publication. An analysis of topical similarity between criticized papers and their citing papers indicates that critical letters are primarily cited by researchers actively engaged in a similar field of study, whereas they are overlooked by more distant communities. Although criticism is celebrated as a cornerstone to science, our findings reveal that it is concentrated on high-impact papers, has minimal measurable consequences, and suffers from limited visibility. These results raise important questions about the role and value of critique in scientific practice.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02809
Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for Action Tracklet Generation,['Computer Vision and Pattern Recognition'],"['Raphael Ruschel', 'Md Awsafur Rahman', 'Hardik Prajapati', 'Suya You', 'B. S. Manjuanth']","Understanding video content is pivotal for advancing real-world applications like activity recognition, autonomous systems, and human-computer interaction. While scene graphs are adept at capturing spatial relationships between objects in individual frames, extending these representations to capture dynamic interactions across video sequences remains a significant challenge. To address this, we present TCDSG, Temporally Consistent Dynamic Scene Graphs, an innovative end-to-end framework that detects, tracks, and links subject-object relationships across time, generating action tracklets, temporally consistent sequences of entities and their interactions. Our approach leverages a novel bipartite matching mechanism, enhanced by adaptive decoder queries and feedback loops, ensuring temporal coherence and robust tracking over extended sequences. This method not only establishes a new benchmark by achieving over 60% improvement in temporal recall@k on the Action Genome, OpenPVSG, and MEVA datasets but also pioneers the augmentation of MEVA with persistent object ID annotations for comprehensive tracklet generation. By seamlessly integrating spatial and temporal dynamics, our work sets a new standard in multi-frame video analysis, opening new avenues for high-impact applications in surveillance, autonomous navigation, and beyond.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02808
STORM: Strategic Orchestration of Modalities for Rare Event Classification,['Computer Vision and Pattern Recognition'],"['Payal Kamboj', 'Ayan Banerjee', 'Sandeep K. S. Gupta']","In domains such as biomedical, expert insights are crucial for selecting the most informative modalities for artificial intelligence (AI) methodologies. However, using all available modalities poses challenges, particularly in determining the impact of each modality on performance and optimizing their combinations for accurate classification. Traditional approaches resort to manual trial and error methods, lacking systematic frameworks for discerning the most relevant modalities. Moreover, although multi-modal learning enables the integration of information from diverse sources, utilizing all available modalities is often impractical and unnecessary. To address this, we introduce an entropy-based algorithm STORM to solve the modality selection problem for rare event. This algorithm systematically evaluates the information content of individual modalities and their combinations, identifying the most discriminative features essential for rare class classification tasks. Through seizure onset zone detection case study, we demonstrate the efficacy of our algorithm in enhancing classification performance. By selecting useful subset of modalities, our approach paves the way for more efficient AI-driven biomedical analyses, thereby advancing disease diagnosis in clinical settings.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02805
Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects,['Computer Vision and Pattern Recognition'],"['Abdurrahman Zeybey', 'Mehmet Ergezer', 'Tommy Nguyen']","3D Gaussian Splatting has advanced radiance field reconstruction, enabling high-quality view synthesis and fast rendering in 3D modeling. While adversarial attacks on object detection models are well-studied for 2D images, their impact on 3D models remains underexplored. This work introduces the Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate adversarial noise targeting the CLIP vision-language model. M-IFGSM specifically alters the object of interest by focusing perturbations on masked regions, degrading the performance of CLIP's zero-shot object detection capability when applied to 3D models. Using eight objects from the Common Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces the accuracy and confidence of the model, with adversarial noise being nearly imperceptible to human observers. The top-1 accuracy in original model renders drops from 95.4\% to 12.5\% for train images and from 91.2\% to 35.4\% for test images, with confidence levels reflecting this shift from true classification to misclassification, underscoring the risks of adversarial attacks on 3D models in applications such as autonomous driving, robotics, and surveillance. The significance of this research lies in its potential to expose vulnerabilities in modern 3D vision models, including radiance fields, prompting the development of more robust defenses and security measures in critical real-world applications.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02803
Closed Neighborhood Balanced Coloring of Graphs,['Combinatorics'],"['K. L. Collins', 'M. Bowie', 'N. B. Fox', 'B. Freyberg', 'J. Hook', 'A. M. Marr', 'C. McBee', 'A. Semanicova-Fenovcikova', 'A. Sinko', 'A. N. Trenk']","A coloring of the vertex set of a graph using the colors red and blue is a closed neighborhood balanced coloring if for each vertex there are an equal number of red and blue vertices in its closed neighborhood. A graph with such a coloring is called a CNBC graph. Freyberg and Marr studied the related class of NBC graphs where closed neighborhood is replaced by open neighborhood. We prove results about CNBC graphs and NBC graphs. We show that the class of CNBC graphs is not hereditary, that the sizes of the color classes can be arbitrarily different, and that if the sizes of the color classes are equal, then a graph is a CNBC graph if and only if its complement is an NBC graph. When the sizes of the color classes are equal, we show that the join of two CNBC graphs is a CNBC graph, and the lexicographic product of a CNBC graph with any graph is a CNBC graph. We prove that the Cartesian product of any CNBC graph and any NBC graph is a CNBC graph, and characterize when a hypercube is an NBC graph or a CNBC graph, but show that the product of two CNBC graphs need not be an NBC graph. We show that the strong product of a CNBC graph with any graph is a CNBC graph. We construct infinite families of circulants that are CNBC graphs, and give characterizations of CNBC trees, generalized Petersen graphs, cubic circulants and quintic circulants when $n\equiv 2 \pmod 4$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02800
A Simulated Galaxy Laboratory: Exploring the Observational Effects on UV Spectral Absorption Line Measurements,['Astrophysics of Galaxies'],"['R. Michael Jennings', 'Alaina Henry', 'Valentin Mauerhofer', 'Timothy Heckman', 'Claudia Scarlata', 'Cody Carr', 'Xinfeng Xu', 'Mason Huberty', 'Simon Gazagnes', 'Anne E. Jaskot', 'Jeremy Blaizot', 'Anne Verhamme', 'Sophia R. Flury', 'Alberto Saldana-Lopez', 'Matthew J. Hayes', 'Maxime Trebitsch']","Ultraviolet absorption line spectroscopy is a sensitive diagnostic for the properties of interstellar and circumgalactic gas. Down-the-barrel observations, where the absorption is measured against the galaxy itself, are commonly used to study feedback from galactic outflows and to make predictions about the leakage of HI ionizing photons into the intergalactic medium. Nonetheless, the interpretation of these observations is challenging and observational compromises are often made in terms of signal-to-noise, spectral resolution, or the use of stacking analyses. In this paper, we present a novel quantitative assessment of UV absorption line measurement techniques by using mock observations of a hydrodynamical simulation. We use a simulated galaxy to create 22,500 spectra in the commonly used SiII lines while also modeling the signal-to-noise and spectral resolution of recent rest-frame UV galaxy surveys at both high and low redshifts. We show that the residual flux of absorption features is easily overestimated for single line measurements and for stacked spectra. Additionally, we explore the robustness of the partial covering model for estimating column densities from spectra and find under-predictions on average of 1.25 dex. We show that the under-prediction is likely caused by high-column-density sight-lines that are optically-thick to dust making them invisible in UV spectra.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02794
"Taurus Database: How to be Fast, Available, and Frugal in the Cloud",['Databases'],"['Alex Depoutovitch', 'Chong Chen', 'Jin Chen', 'Paul Larson', 'Shu Lin', 'Jack Ng', 'Wenlin Cui', 'Qiang Liu', 'Wei Huang', 'Yong Xiao', 'Yongjun He']","Using cloud Database as a Service (DBaaS) offerings instead of on-premise deployments is increasingly common. Key advantages include improved availability and scalability at a lower cost than on-premise alternatives. In this paper, we describe the design of Taurus, a new multi-tenant cloud database system. Taurus separates the compute and storage layers in a similar manner to Amazon Aurora and Microsoft Socrates and provides similar benefits, such as read replica support, low network utilization, hardware sharing and scalability. However, the Taurus architecture has several unique advantages. Taurus offers novel replication and recovery algorithms providing better availability than existing approaches using the same or fewer replicas. Also, Taurus is highly optimized for performance, using no more than one network hop on critical paths and exclusively using append-only storage, delivering faster writes, reduced device wear, and constant-time snapshots. This paper describes Taurus and provides a detailed description and analysis of the storage node architecture, which has not been previously available from the published literature.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02792
An Evolutionary Large Language Model for Hallucination Mitigation,['Computation and Language'],"['Abdennour Boulesnane', 'Abdelhakim Souilah']","The emergence of LLMs, like ChatGPT and Gemini, has marked the modern era of artificial intelligence applications characterized by high-impact applications generating text, images, and videos. However, these models usually ensue with one critical challenge called hallucination: confident presentation of inaccurate or fabricated information. This problem attracts serious concern when these models are applied to specialized domains, including healthcare and law, where the accuracy and preciseness of information are absolute conditions. In this paper, we propose EvoLLMs, an innovative framework inspired by Evolutionary Computation, which automates the generation of high-quality Question-answering (QA) datasets while minimizing hallucinations. EvoLLMs employs genetic algorithms, mimicking evolutionary processes like selection, variation, and mutation, to guide LLMs in generating accurate, contextually relevant question-answer pairs. Comparative analysis shows that EvoLLMs consistently outperforms human-generated datasets in key metrics such as Depth, Relevance, and Coverage, while nearly matching human performance in mitigating hallucinations. These results highlight EvoLLMs as a robust and efficient solution for QA dataset generation, significantly reducing the time and resources required for manual curation.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02790
Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset,['Computation and Language'],"['Tilahun Abedissa Taffa', 'Debayan Banerjee', 'Yaregal Assabie', 'Ricardo Usbeck']","Existing Scholarly Question Answering (QA) methods typically target homogeneous data sources, relying solely on either text or Knowledge Graphs (KGs). However, scholarly information often spans heterogeneous sources, necessitating the development of QA systems that integrate information from multiple heterogeneous data sources. To address this challenge, we introduce Hybrid-SQuAD (Hybrid Scholarly Question Answering Dataset), a novel large-scale QA dataset designed to facilitate answering questions incorporating both text and KG facts. The dataset consists of 10.5K question-answer pairs generated by a large language model, leveraging the KGs DBLP and SemOpenAlex alongside corresponding text from Wikipedia. In addition, we propose a RAG-based baseline hybrid QA model, achieving an exact match score of 69.65 on the Hybrid-SQuAD test set.△ Less",v1,https://arxiv.org/pdf/2412.02788
Accretion of Uranus and Neptune: confronting different giant impact scenarios,['Earth and Planetary Astrophysics'],"['Leandro Esteves', 'André Izidoro', 'Othon C. Winter']","The origins of Uranus and Neptune are not fully understood. Their inclined rotation axes -- obliquities -- suggest that they experienced giant impacts during their formation histories. Simulations modeling their accretion from giant impacts among ~5 Earth masses planetary embryos -- with roughly unity impactors' mass ratios -- have been able to broadly match their current masses, final mass ratio, and obliquity. However, due to angular momentum conservation, planets produced in these impacts tend to rotate too fast, compared to Uranus and Neptune. One potential solution for this problem consists of invoking instead collisions of objects with large mass ratios (e.g. a proto-Uranus with 13 Mearth and an embryo of 1 Mearth). Smooth-particle hydrodynamics simulations show that in this scenario final planets tend to have rotation periods more consistent with those of Uranus and Neptune. Here we performed a large suite of N-body numerical simulations modelling the formation of Uranus and Neptune to compare these different dynamical views. Our simulations start with a population of protoplanets and account for the effects of type-I migration, inclination and eccentricity tidal damping. Our results show that although scenarios allowing for large impactors' mass ratio favour slower rotating planets, the probability of occurring collisions in these specific simulations is significantly low. This is because gas tidal damping is relatively less efficient for low-mass embryos (<~1 Merath) and, consequently, such objects are mostly scattered by more massive objects (~13 Mearth) instead of colliding with them. Altogether, our results show that the probability of broadly matching the masses, mass ratio, and rotation periods of Uranus and Neptune in these two competing formation scenarios is broadly similar, within a factor of ~2, with overall probabilities of the order of ~0.1-1%.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02785
FathomGPT: A Natural Language Interface for Interactively Exploring Ocean Science Data,['Human-Computer Interaction'],"['Nabin Khanal', 'Chun Meng Yu', 'Jui-Cheng Chiu', 'Anav Chaudhary', 'Ziyue Zhang', 'Kakani Katija', 'Angus G. Forbes']","We introduce FathomGPT, an open source system for the interactive investigation of ocean science data via a natural language interface. FathomGPT was developed in close collaboration with marine scientists to enable researchers to explore and analyze the FathomNet image database. FathomGPT provides a custom information retrieval pipeline that leverages OpenAI's large language models to enable: the creation of complex queries to retrieve images, taxonomic information, and scientific measurements; mapping common names and morphological features to scientific names; generating interactive charts on demand; and searching by image or specified patterns within an image. In designing FathomGPT, particular emphasis was placed on enhancing the user's experience by facilitating free-form exploration and optimizing response times. We present an architectural overview and implementation details of FathomGPT, along with a series of ablation studies that demonstrate the effectiveness of our approach to name resolution, fine tuning, and prompt modification. We also present usage scenarios of interactive data exploration sessions and document feedback from ocean scientists and machine learning experts.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02784
WxC-Bench: A Novel Dataset for Weather and Climate Downstream Tasks,['Machine Learning'],"['Rajat Shinde', 'Christopher E. Phillips', 'Kumar Ankur', 'Aman Gupta', 'Simon Pfreundschuh', 'Sujit Roy', 'Sheyenne Kirkland', 'Vishal Gaur', 'Amy Lin', 'Aditi Sheshadri', 'Udaysankar Nair', 'Manil Maskey', 'Rahul Ramachandran']","High-quality machine learning (ML)-ready datasets play a foundational role in developing new artificial intelligence (AI) models or fine-tuning existing models for scientific applications such as weather and climate analysis. Unfortunately, despite the growing development of new deep learning models for weather and climate, there is a scarcity of curated, pre-processed machine learning (ML)-ready datasets. Curating such high-quality datasets for developing new models is challenging particularly because the modality of the input data varies significantly for different downstream tasks addressing different atmospheric scales (spatial and temporal). Here we introduce WxC-Bench (Weather and Climate Bench), a multi-modal dataset designed to support the development of generalizable AI models for downstream use-cases in weather and climate research. WxC-Bench is designed as a dataset of datasets for developing ML-models for a complex weather and climate system, addressing selected downstream tasks as machine learning phenomenon. WxC-Bench encompasses several atmospheric processes from meso-$β$ (20 - 200 km) scale to synoptic scales (2500 km), such as aviation turbulence, hurricane intensity and track monitoring, weather analog search, gravity wave parameterization, and natural language report generation. We provide a comprehensive description of the dataset and also present a technical validation for baseline analysis. The dataset and code to prepare the ML-ready data have been made publicly available on Hugging Face -- https://huggingface.co/datasets/nasa-impact/WxC-Bench△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02780
RIS-Assisted Sensing: A Nested Tensor Decomposition-Based Approach,['Signal Processing'],"['Kenneth Benício', 'Fazal-E-Asim', 'Bruno Sokal', 'André L. F. de Almeida', 'Behrooz Makki', 'Gabor Fodor', 'A. Lee Swindlehurst']","We study a monostatic multiple-input multiple-output sensing scenario assisted by a reconfigurable intelligent surface using tensor signal modeling. We propose a method that exploits the intrinsic multidimensional structure of the received echo signal, allowing us to recast the target sensing problem as a nested tensor-based decomposition problem to jointly estimate the delay, Doppler, and angular information of the target. We derive a two-stage approach based on the alternating least squares algorithm followed by the estimation of the signal parameters via rotational invariance techniques to extract the target parameters. Simulation results show that the proposed tensor-based algorithm yields accurate estimates of the sensing parameters with low complexity.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02778
Hacking CTFs with Plain Agents,['Cryptography and Security'],"['Rustem Turtayev', 'Artem Petrov', 'Dmitrii Volkov', 'Denis Volk']","We saturate a high-school-level hacking benchmark with plain LLM agent design. Concretely, we obtain 95% performance on InterCode-CTF, a popular offensive security benchmark, using prompting, tool use, and multiple attempts. This beats prior work by Phuong et al. 2024 (29%) and Abramovich et al. 2024 (72%).
  Our results suggest that current LLMs have surpassed the high school level in offensive cybersecurity. Their hacking capabilities remain underelicited: our ReAct&Plan prompting strategy solves many challenges in 1-2 turns without complex engineering or advanced harnessing.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02776
Optimizing Large Language Models for Turkish: New Methodologies in Corpus Selection and Training,['Computation and Language'],"['H. Toprak Kesgin', 'M. Kaan Yuce', 'Eren Dogan', 'M. Egemen Uzun', 'Atahan Uz', 'Elif Ince', 'Yusuf Erdem', 'Osama Shbib', 'Ahmed Zeer', 'M. Fatih Amasyali']","In this study, we develop and assess new corpus selection and training methodologies to improve the effectiveness of Turkish language models. Specifically, we adapted Large Language Model generated datasets and translated English datasets into Turkish, integrating these resources into the training process. This approach led to substantial enhancements in model accuracy for both few-shot and zero-shot learning scenarios. Furthermore, the merging of these adapted models was found to markedly improve their performance. Human evaluative metrics, including task-specific performance assessments, further demonstrated that these adapted models possess a greater aptitude for comprehending the Turkish language and addressing logic-based queries. This research underscores the importance of refining corpus selection strategies to optimize the performance of multilingual models, particularly for under-resourced languages like Turkish.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02775
Phase separation and rheology of segregating binary fluid under shear,['Soft Condensed Matter'],"['Daniya Davis', 'Parameshwaran A', 'Bhaskar Sen Gupta']","We employ molecular dynamics simulation to study the phase separation and rheological properties of a three-dimensional binary liquid mixture with hydrodynamics undergoing simple shear deformation. The impact of shear intensity on domain growth is investigated, with a focus on how shear primarily distorts the domains, leading to the formation of anisotropic structures. The structural anisotropy is quantified by evaluating domain sizes along the flow and shear direction. The rheological properties of the system is studied in terms of shear stress and excess viscosity. At low shear rates, the system behaves like a Newtonian fluid. However, the strong-shear case is marked by a transition characterized by non-Newtonian behavior.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02774
Efficient hyperparameter estimation in Bayesian inverse problems using sample average approximation,['Numerical Analysis'],"['Julianne Chung', 'Scot M. Miller', 'Malena Sabate Landman', 'Arvind K. Saibaba']","In Bayesian inverse problems, it is common to consider several hyperparameters that define the prior and the noise model that must be estimated from the data. In particular, we are interested in linear inverse problems with additive Gaussian noise and Gaussian priors defined using Matérn covariance models. In this case, we estimate the hyperparameters using the maximum a posteriori (MAP) estimate of the marginalized posterior distribution. However, this is a computationally intensive task since it involves computing log determinants. To address this challenge, we consider a stochastic average approximation (SAA) of the objective function and use the preconditioned Lanczos method to compute efficient approximations of the function and gradient evaluations. We propose a new preconditioner that can be updated cheaply for new values of the hyperparameters and an approach to compute approximations of the gradient evaluations, by reutilizing information from the function evaluations. We demonstrate the performance of our approach on static and dynamic seismic tomography problems.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02773
Energy-Efficient Cell-Free Massive MIMO with Wireless Fronthaul,['Signal Processing'],"['Ozan Alp Topal', 'Özlem Tuğfe Demir', 'Emil Björnson', 'Cicek Cavdar']","Cell-free massive MIMO improves the fairness among the user equipments (UEs) in the network by distributing many cooperating access points (APs) around the region while connecting them to a centralized cloud-computing unit that coordinates joint transmission/reception. However, the fiber cable deployment for the fronthaul transport network and activating all available antennas at each AP lead to increased deployment cost and power consumption for fronthaul signaling and processing. To overcome these challenges, in this work, we consider wireless fronthaul connections and propose a joint antenna activation and power allocation algorithm to minimize the end-to-end (from radio to cloud) power while satisfying the quality-of-service requirements of the UEs under wireless fronthaul capacity limitations. The results demonstrate that the proposed methodology of deactivating antennas at each AP reduces the power consumption by 50% and 84% compared to the benchmarks based on shutting down APs and minimizing only the transmit power, respectively.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02771
"Discovery and Characterization of an Eccentric, Warm Saturn Transiting the Solar Analog TOI-4994",['Earth and Planetary Astrophysics'],"['Romy Rodriguez Martinez', 'Jason D. Eastman', 'Karen Collins', 'Joseph Rodriguez', 'David Charbonneau', 'Samuel Quinn', 'David W. Latham', 'Carl Ziegler', 'Rafael Brahm', 'Tyler Fairnington', 'Solene Ulmer-Moll', 'Keivan Stassun', 'Olga Suarez', 'Tristan Guillot', 'Melissa Hobson', 'Joshua N. Winn', 'Shubham Kanodia', 'Martin Schlecker', 'R. P. Butler', 'Jeffrey D. Crane', 'Steve Shectman', 'Johanna K. Teske', 'David Osip', 'Yuri Beletsky', 'Matthew P. Battley']","We present the detection and characterization of TOI-4994b (TIC 277128619b), a warm Saturn-sized planet discovered by the NASA Transiting Exoplanet Survey Satellite (TESS). TOI-4994b transits a G-type star (V = 12.6 mag) with a mass, radius, and effective temperature of $M_{\star} =1.005^{+0.064}_{-0.061} M_{\odot}$, $R_{\star} = 1.055^{+0.040}_{-0.037} R_{\odot}$, and $T_{\rm eff} = 5640 \pm 110$ K. We obtained follow-up ground-based photometry from the Las Cumbres Observatory (LCO) and the Antarctic Search for Transiting ExoPlanets (ASTEP) telescopes, and we confirmed the planetary nature of TOI-4994b with multiple radial velocity observations from the PFS, CHIRON, HARPS, FEROS, and CORALIE instruments. From a global fit to the photometry and radial velocities, we determine that TOI-4994b is in a 21.5-day, eccentric orbit ($e = 0.32 \pm 0.04$) and has a mass of $M_{P}= 0.280^{+0.037}_{-0.034} M_{J}$, a radius of $R_{P}= 0.762^{+0.030}_{-0.027}R_{J}$, and a Saturn-like bulk density of $ρ_{p} = 0.78^{+0.16}_{-0.14}$ $\rm g/cm^3$. We find that TOI-4994 is a potentially viable candidate for follow-up stellar obliquity measurements. TOI-4994b joins the small sample of warm Saturn analogs and thus sheds light on our understanding of these rare and unique worlds.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02769
Quaternion-based Unscented Kalman Filter for 6-DoF Vision-based Inertial Navigation in GPS-denied Regions,['Robotics'],"['Khashayar Ghanizadegan', 'Hashim A. Hashim']","This paper investigates the orientation, position, and linear velocity estimation problem of a rigid-body moving in three-dimensional (3D) space with six degrees-of-freedom (6 DoF). The highly nonlinear navigation kinematics are formulated to ensure global representation of the navigation problem. A computationally efficient Quaternion-based Navigation Unscented Kalman Filter (QNUKF) is proposed on $\mathbb{S}^{3}\times\mathbb{R}^{3}\times\mathbb{R}^{3}$ imitating the true nonlinear navigation kinematics and utilize onboard Visual-Inertial Navigation (VIN) units to achieve successful GPS-denied navigation. The proposed QNUKF is designed in discrete form to operate based on the data fusion of photographs garnered by a vision unit (stereo or monocular camera) and information collected by a low-cost inertial measurement unit (IMU). The photographs are processed to extract feature points in 3D space, while the 6-axis IMU supplies angular velocity and accelerometer measurements expressed with respect to the body-frame. Robustness and effectiveness of the proposed QNUKF have been confirmed through experiments on a real-world dataset collected by a drone navigating in 3D and consisting of stereo images and 6-axis IMU measurements. Also, the proposed approach is validated against standard state-of-the-art filtering techniques. IEEE Keywords: Localization, Navigation, Unmanned Aerial Vehicle, Sensor-fusion, Inertial Measurement Unit, Vision Unit.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02768
Endogenous Heteroskedasticity in Linear Models,['Econometrics'],"['Javier Alejo', 'Antonio F. Galvao', 'Julian Martinez-Iriarte', 'Gabriel Montes-Rojas']","Linear regressions with endogeneity are widely used to estimate causal effects. This paper studies a statistical framework that has two common issues, endogeneity of the regressors, and heteroskedasticity that is allowed to depend on endogenous regressors, i.e., endogenous heteroskedasticity. We show that the presence of such conditional heteroskedasticity in the structural regression renders the two-stages least squares estimator inconsistent. To solve this issue, we propose sufficient conditions together with a control function approach to identify and estimate the causal parameters of interest. We establish statistical properties of the estimator, say consistency and asymptotic normality, and propose valid inference procedures. Monte Carlo simulations provide evidence of the finite sample performance of the proposed methods, and evaluate different implementation procedures. We revisit an empirical application about job training to illustrate the methods.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02767
The importance of gas starvation in driving satellite quenching in galaxy groups at $z\sim 0.8$,['Astrophysics of Galaxies'],"['Devontae C. Baxter', 'Sean P. Fillingham', 'Alison L. Coil', 'Michael C. Cooper']","We present results from a Keck/DEIMOS survey to study satellite quenching in group environments at $z \sim 0.8$ within the Extended Groth Strip (EGS). We target $11$ groups in the EGS with extended X-ray emission. We obtain high-quality spectroscopic redshifts for group member candidates, extending to depths over an order of magnitude fainter than existing DEEP2/DEEP3 spectroscopy. This depth enables the first spectroscopic measurement of the satellite quiescent fraction down to stellar masses of $\sim 10^{9.5}~{\rm M}_{\odot}$ at this redshift. By combining an infall-based environmental quenching model, constrained by the observed quiescent fractions, with infall histories of simulated groups from the IllustrisTNG100-1-Dark simulation, we estimate environmental quenching timescales ($τ_{\mathrm{quench}}$) for the observed group population. At high stellar masses (${M}_{\star}=10^{10.5}~{\rm M}_{\odot}$) we find that $τ_{\mathrm{quench}} = 2.4\substack{+0.2 \\ -0.2}$ Gyr, which is consistent with previous estimates at this epoch. At lower stellar masses (${M}_{\star}=10^{9.5}~{\rm M}_{\odot}$), we find that $τ_{\mathrm{quench}}=3.1\substack{+0.5 \\ -0.4}$ Gyr, which is shorter than prior estimates from photometry-based investigations. These timescales are consistent with satellite quenching via starvation, provided the hot gas envelope of infalling satellites is not stripped away. We find that the evolution in the quenching timescale between $0 \lt z \lt 1$ aligns with the evolution in the dynamical time of the host halo and the total cold gas depletion time. This suggests that the doubling of the quenching timescale in groups since $z\sim1$ could be related to the dynamical evolution of groups or a decrease in quenching efficiency via starvation with decreasing redshift.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02766
Massive black holes or stars first: the key is the residual cosmic electron fraction,['Cosmology and Nongalactic Astrophysics'],"['Muhammad A. Latif', 'Sadegh Khochfar']","Recent James Webb Space Telescope observations have unveiled that the first supermassive black holes (SMBHs) were in place at z $\geq$ 10, a few hundred Myrs after the Big Bang. These discoveries are providing strong constraints on the seeding of BHs and the nature of the first objects in the Universe. Here, we study the impact of the freeze-out electron fractions ($f_e$) at the end of the epoch of cosmic recombination on the formation of the first structures in the Universe. At $f_e$ below the current fiducial cosmic values of $\rm \sim 10^{-4}$, the baryonic collapse is delayed due to the lack of molecular hydrogen cooling until the host halo masses are increased by one to two orders of magnitude compared to the standard case and reach the atomic cooling limit. This results in an enhanced enclosed gas mass by more than an order of magnitude and higher inflow rates of up to $0.1~M_{\odot}/{yr}$. Such conditions are conducive to the formation of massive seed BHs with $\sim 10^{4}$ M$_{\odot}$. Our results reveal a new pathway for the formation of massive BH seeds which may naturally arise from free△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02763
Examples of optimal Hölder regularity in semilinear equations involving the fractional Laplacian,['Analysis of PDEs'],"['Gyula Csató', 'Albert Mas']","We discuss the Hölder regularity of solutions to the semilinear equation involving the fractional Laplacian $(-Δ)^s u=f(u)$ in one dimension. We put in evidence a new regularity phenomenon which is a combined effect of the nonlocality and the semilinearity of the equation, since it does not happen neither for local semilinear equations, nor for nonlocal linear equations. Namely, for nonlinearities $f$ in $C^β$ and when $2s+β<1$, the solution is not always $C^{2s+β-ε}$ for all $ε>0$. Instead, in general the solution $u$ is at most $C^{2s/(1-β)}.$△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02762
The imprint of cosmic voids from the DESI Legacy Survey DR9 LRGs in the Planck 2018 lensing map through spectroscopically calibrated mocks,['Cosmology and Nongalactic Astrophysics'],"['S. Sartori', 'P. Vielzeuf', 'S. Escoffier', 'M. C. Cousinou', 'A. Kovács', 'J. DeRose', 'S. Ahlen', 'D. Bianchi', 'D. Brooks', 'E. Burtin', 'T. Claybaugh', 'A. de la Macorra', 'J. E. Forero-Romero', 'J. Garcia-Bellido', 'S. Gontcho A Gontcho', 'G. Gutierrez', 'K. Honscheid', 'R. Kehoe', 'D. Kirkby', 'T. Kisner', 'M. Landriau', 'M. E. Levi', 'A. Meisner', 'R. Miquel', 'J. Moustakas']","The cross-correlation of cosmic voids with the lensing convergence ($κ$) map of the Cosmic Microwave Background (CMB) fluctuations provides a powerful tool to refine our understanding of the cosmological model. However, several studies have reported a moderate tension between the lensing imprint of cosmic voids on the observed CMB and the simulated $\mathrmΛ$CDM signal. To address this ""lensing-is-low"" tension and to obtain new, precise measurements, we exploit the large DESI Legacy Survey Luminous Red Galaxy (LRG) dataset, covering approximately 19,500 $°^2$ of the sky and including about 10 million LRGs at $z < 1.05$. Our $\mathrmΛ$CDM template was created using the Buzzard mocks, which we specifically calibrated to match the clustering properties of the observed galaxy sample by exploiting more than one million DESI spectra. We identified our catalogs of 3D voids in the range $0.35 < z < 0.95$, dividing the sample into bins according to the redshift and $λ_\mathrm{v}$ values of the voids. We report a 14$σ$ detection of the lensing signal, with $A_κ= 1.016 \pm 0.054$, which increases to 17$σ$ when considering the void-in-void ($A_κ= 0.944 \pm 0.064$) and the void-in-cloud ($A_κ= 0.975 \pm 0.060$) populations individually, the highest detection significance for studies of this kind. We observe a full agreement between the observations and $\mathrmΛ$CDM predictions across all redshift bins, sky regions, and void populations considered. In addition to these findings, our analysis highlights the importance of matching sparseness and redshift error distributions between mocks and observations, as well as the role of $λ_\mathrm{v}$ in enhancing the signal-to-noise ratio.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02761
Cosmos-LLaVA: Chatting with the Visual Cosmos-LLaVA: Görselle Sohbet Etmek,['Artificial Intelligence'],"['Ahmed Zeer', 'Eren Dogan', 'Yusuf Erdem', 'Elif Ince', 'Osama Shbib', 'M. Egemen Uzun', 'Atahan Uz', 'M. Kaan Yuce', 'H. Toprak Kesgin', 'M. Fatih Amasyali']","In this study, a Turkish visual instruction model was developed and various model architectures and dataset combinations were analysed to improve the performance of this model. The Cosmos-LLaVA model, which is built by combining different large language models and image coders, is designed to overcome the deficiencies in the Turkish language. In the experiments, the effects of fine-tuning with various datasets on the model performance are analysed in detail. The results show that model architecture and dataset selection have a significant impact on performance.
  Bu çalışmada bir Türkçe görsel talimat modeli geliştirilerek bu modelin performansını artırmaya yönelik çeşitli model mimarileri ve veri kümesi kombinasyonları derinlemesine incelenmiştir. Farklı büyük dil modelleri ve görüntü kodlayıcılarının bir araya getirilmesiyle oluşturulan Cosmos-LLaVA modeli, Türkçe dilindeki eksiklikleri gidermeye yönelik olarak tasarlanmıştır. Yapılan deneylerde, çeşitli veri kümeleri ile yapılan ince ayarların model performansını nasıl etkilediği detaylı olarak ele alınmıştır. Sonuçlar, model mimarisi ve veri kümesi seçiminin performans üzerinde önemli bir etkiye sahip olduğunu göstermektedir.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02760
Dark Galactic subhalos and the Gaia snail,['Astrophysics of Galaxies'],"['Daniel Gilman', 'Jo Bovy', 'Neige Frankel', 'Andrew Benson']","Gaia has revealed a clear signal of disequilibrium in the solar neighborhood in the form of a spiral (or snail) feature in the vertical phase-space distribution. We investigate the possibility that this structure emerges from ongoing perturbations by dark $\left(10^{6} M_{\odot} - 10^8 M_{\odot}\right)$ Galactic subhalos. We develop a probabilistic model for generating subhalo orbits based on a semi-analytic model of structure formation, and combine this framework with an approximate prescription for calculating the response of the disk to external perturbations. We also develop a phenomenological treatment for the diffusion of phase-space spirals caused by gravitational scattering between stars and giant molecular clouds, a process that erases the kinematic signatures of old ($t \gtrsim 0.6$ Gyr) events. Perturbations caused by dark subhalos are, on average, orders of magnitude weaker than those caused by luminous satellite galaxies, but the ubiquity of dark halos predicted by cold dark matter makes them a more probable source of strong perturbation to the dynamics of the solar neighborhood. Dark subhalos alone do not cause enough disturbance to explain the Gaia snail, but they excite fluctuations of $\sim 0.1-0.5 \ \rm{km} \ \rm{s^{-1}}$ in the mean vertical velocity of stars near the Galactic midplane that should persist to the present day. Subhalos also produce correlations between vertical frequency and orbital angle that could be mistaken as originating from a single past disturbance. Our results motivate investigation of the Milky Way's dark satellites by characterizing their kinematic signatures in phase-space spirals across the Galaxy.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02757
From dynamical to steady-state many-body metrology: Precision limits and their attainability with two-body interactions,['Quantum Physics'],"['Ricard Puig', 'Pavel Sekatski', 'Paolo Andrea Erdman', 'Paolo Abiuso', 'John Calsamiglia', 'Martí Perarnau-Llobet']","We consider the estimation of an unknown parameter $θ$ via a many-body probe. The probe is initially prepared in a product state and many-body interactions enhance its $θ$-sensitivity during the dynamics and/or in the steady state. We present bounds on the Quantum Fisher Information, and corresponding optimal interacting Hamiltonians, for two paradigmatic scenarios for encoding $θ$: (i) via unitary Hamiltonian dynamics (dynamical metrology), and (ii) in the Gibbs and diagonal ensembles (time-averaged dephased state), two ubiquitous steady states of many-body open dynamics. We then move to the specific problem of estimating the strength of a magnetic field via interacting spins and derive two-body interacting Hamiltonians that can approach the fundamental precision bounds. In this case, we additionally analyze the transient regime leading to the steady states and characterize tradeoffs between equilibration times and measurement precision. Overall, our results provide a comprehensive picture of the potential of many-body control in quantum sensing.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02754
Interaction effects in quantum geometry,['Strongly Correlated Electrons'],"['Pavlo Sukhachov', 'Niels Henrik Aase', 'Kristian Mæland', 'Asle Sudbø']","Quantum geometry provides important information about the structure and topology of quantum states in various forms of quantum matter. The information contained therein has profound effects on observable quantities such as superconducting weight, Drude weight, and optical responses. Motivated by the recent advances in flat-band interacting systems, we investigate the role of interaction effects on the quantum metric. By using the fermionic Creutz ladder as a representative system, we show that the repulsive Hubbard interaction monotonically suppresses the quantum metric. While the eigenstates and their overlap quantifying the quantum metric can be obtained exactly in the presence of interactions through exact diagonalization, this method is limited to small system sizes. Alternatively, two theoretical proposals, the generalized quantum metric and the dressed quantum metric, suggest using renormalized Green's functions to define the interacting quantum metric. By comparing these analytical approaches with results from exact diagonalization, we show that the dressed quantum metric provides a better fit to the exact diagonalization results. Our conclusion holds for both flat-band and dispersive systems.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02753
On the S-stars' Zone of Avoidance in the Galactic Center,['Astrophysics of Galaxies'],"['Aleksey Generozov', 'Hagai B. Perets', 'Matteo S. Bordoni', 'Guillaume Bourdarot', 'Antonia Drescher', 'Frank Eisenhauer', 'Reinhard Genzel', 'Stefan Gillessen', 'Felix Mang', 'Thomas Ott', 'Diogo C. Ribeiro', 'Rainer Schödel']","This paper investigates the origin and orbital evolution of S-stars in the Galactic Center using models of binary disruption and relaxation processes. We focus on explaining the recently discovered ""zone of avoidance"" in S-star orbital parameters, defined as a region where no S-stars are observed with pericenters $\log(r_p / {\rm AU}) \leq 1.57 + 2.6(1 - e)$ pc. We demonstrate that the observed S-star orbital distributions, including this zone of avoidance and their thermal eccentricity distribution, can be largely explained by continuous disruption of binaries near the central supermassive black hole, followed by orbital relaxation. Our models consider binaries originating from large scales (5--100 pc) and incorporate empirical distributions of binary properties. We simulate close encounters between binaries and the black hole, tracking the remnant stars' orbits. The initially highly eccentric orbits of disrupted binary remnants evolve due to non-resonant and resonant relaxation in the Galactic Center potential. While our results provide insights into the formation mechanism of S-stars, there are limitations, such as uncertainties in the initial binary population and mass-function and simplifications in our relaxation models. Despite these caveats, our study demonstrates the power of using S-star distributions to probe the dynamical history and environment of the central parsec of our Galaxy.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02752
GA-NIFS: The highly overdense system BR1202-0725 at z $\sim$ 4.7. A double AGN with fast outflows plus eight companion galaxies,['Astrophysics of Galaxies'],"['S. Zamora', 'Giacomo Venturi', 'Stefano Carniani', 'Elena Bertola', 'Eleonora Parlanti', 'Michele Perna', 'Santiago Arribas', 'Torsten Böker', 'Andrew J. Bunker', 'Stéphane Charlot', ""Francesco D'Eugenio"", 'Roberto Maiolino', 'Bruno Rodríguez Del Pino', 'Hannah Übler', 'Giovanni Cresci', 'Gareth C. Jones', 'Isabella Lamperti']","Distant quasars (QSOs) in galaxy overdensities are considered key actors in the evolution of the early Universe. In this work, we studied the kinematic and physical properties of the BR1202-0725 system at z=4.7, one of the most overdense fields known in the early Universe, consisting of a QSO, a submillimeter galaxy (SMG), and three Lyman-$α$ emitters. We used data from the JWST/NIRSpec Integral Field Unit (IFU) to analyze the rest-frame optical emission of each source in the system. We estimated a bolometric luminosity of log($L_{\rm bol}/$[erg/s]) = 47.2 $\pm$ 0.4 and a black hole mass of log($M_{\rm BH}/M_\odot$) = 10.1 $\pm$ 0.5 for the QSO, which are consistent with previous measurements obtained with ground-based observations. The NIRSpec spectra of the SMG revealed instead unexpected [OIII] and H$α$+[NII] profiles. The overall [OIII] line profile is blue-shifted by more than 700 km/s relative to the systemic velocity of the galaxy. Additionally, both the [OIII] and H$α$+[NII] lines show prominent broad (1300 km/s), blueshifted wings associated with outflowing ionized gas. The analysis of NIRSpec and X-ray observations indicates that the SMG likely hosts an accreting supermassive black hole as supported by the following results: (i) the excitation diagnostic diagram is consistent with ionization from an active galactic nucleus (AGN); (ii) the X-ray luminosity is higher than $10^{44}$ erg/s; and (iii) it hosts a fast outflow ($v_{\rm out}$ = 5000 km/s), comparable to those observed in luminous QSOs. Therefore, the QSO-SMG pair represents one of the highest-redshift double AGN to date, with a projected separation of 24 kpc. Finally, we investigated the environment of this system and found four new galaxies at the same redshift of the QSO and within a projected distance of 5 kpc from it. This overdense system includes at least ten galaxies in only 980 kpc$^2$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02751
On the detection of stellar wakes in the Milky Way: a deep learning approach,['Astrophysics of Galaxies'],"['Sven Põder', 'Joosep Pata', 'María Benito', 'Isaac Alonso Asensio', 'Claudio Dalla Vecchia']","Due to poor observational constraints on the low-mass end of the subhalo mass function, the detection of dark matter (DM) subhalos on sub-galactic scales would provide valuable information about the nature of DM. Stellar wakes, induced by passing DM subhalos, encode information about the mass of the inducing perturber and thus serve as an indirect probe for the DM substructure within the Milky Way (MW). Our aim is to assess the viability and performance of deep learning searches for stellar wakes in the Galactic stellar halo caused by DM subhalos of varying mass. We simulate massive objects (subhalos) moving through a homogeneous medium of DM and star particles, with phase-space parameters tailored to replicate the conditions of the Galaxy at a specific distance from the Galactic center. The simulation data is used to train deep neural networks with the purpose of inferring both the presence and mass of the moving perturber, and assess subhalo detectability in varying conditions of the Galactic stellar and DM halos. We find that our binary classifier is able to infer the presence of subhalos, showing non-trivial performance down to a subhalo mass of $5 \times 10^7 \rm \, M_\odot$. We also find that our binary classifier is generalisable to datasets describing subhalo orbits at different Galactocentric distances. In a multiple-hypothesis case, we are able to discern between samples containing subhalos of different masses. Out of the phase-space observables available to us, we conclude that overdensity and velocity divergence are the most important features for subhalo detection performance.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02749
History and Habitability of the LP 890-9 Planetary System,['Earth and Planetary Astrophysics'],"['Rory Barnes', 'Laura N. R. do Amaral', 'Jessica Birky', 'Ludmila Carone', 'Peter Driscoll', 'Joseph R. Livesey', 'David Graham', 'Juliette Becker', 'Kaiming Cui', 'Martin Schlecker', 'Rodolfo Garcia', 'Megan Gialluca', 'Arthur Adams', 'MD Redyan Ahmed', 'Paul Bonney', 'Wynter Broussard', 'Chetan Chawla', 'Mario Damasso', 'William C. Danchi', 'Russell Deitrick', 'Elsa Ducrot', 'Emeline F. Fromont', 'Brandt A. L. Gaches', 'Sakshi Gupta', 'Michelle L. Hill']","We present numerous aspects of the evolution of the LP 890-9 (SPECULOOS-2/TOI-4306) planetary system, focusing on the likelihood that planet c can support life. We find that the host star reaches the main sequence in 1 Gyr and that planet c lies close to the inner boundary of the habitable zone. We find the magma ocean stage can last up to 50 Myr, remove 8 Earth-oceans of water, and leave up to 2000 bars of oxygen in the atmosphere. However, if the planet forms with a hydrogen envelope as small as 0.1 Earth-masses, no water will be lost during the star's pre-main sequence phase from thermal escape processes. We find that the planets are unlikely to be in a 3:1 mean motion resonance and that both planets tidally circularize within 0.5 Gyr when tidal dissipation is held constant. However, if tidal dissipation is a function of mantle temperature and rheology, then we find that planet c's orbit may require more than 7 Gyr to circularize, during which time tidal heating may reach hundreds of terawatts. We thus conclude that the habitability of planet c depends most strongly on the initial volatile content and internal properties, but no data yet preclude the viability of an active biosphere on the planet.△ Less",v1,https://arxiv.org/pdf/2412.02743
Dynamical Formation of Regular Black Holes,['General Relativity and Quantum Cosmology'],"['Pablo Bueno', 'Pablo A. Cano', 'Robie A. Hennigar', 'Ángel J. Murcia']","We study dynamical gravitational collapse in a theory with an infinite tower of higher-derivative corrections to the Einstein-Hilbert action and we show that, under very general conditions, it leads to the formation of regular black holes. Our results are facilitated by the use of a class of theories that possess second-order equations on spherically symmetric metrics, but which are general enough to provide a basis for the gravitational effective action. We analytically solve the collapse of a thin shell of dust and show that it inevitably experiences a bounce at small radius and that its motion can be extended to arbitrary proper time. The collapse of the shell always gives rise to a singularity-free, geodesically complete spacetime that contains horizons if the total mass is above a critical value. In that case, the shell bounces into a new universe through a white hole explosion. Our construction provides, to the best of our knowlege, the first fully dynamical description of formation of regular black holes, and it suggests that higher-derivative corrections may be the most natural way to resolve the singularities of Einstein's theory.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02742
Regular black holes from thin-shell collapse,['General Relativity and Quantum Cosmology'],"['Pablo Bueno', 'Pablo A. Cano', 'Robie A. Hennigar', 'Ángel J. Murcia']","We establish that regular black holes can form from gravitational collapse. Our model builds on a recent construction that realized regular black holes as exact solutions to purely gravitational theories that incorporate an infinite tower of higher curvature corrections in any dimension $D \ge 5$ [arXiv:2403.04827]. We identify a two-dimensional Horndeski theory that captures the spherically symmetric dynamics of the theories in question and use this to prove a Birkhoff theorem and obtain the generalized Israel junction conditions. Armed with these tools, we consider the collapse of thin shells of pressureless matter, showing that this leads generically to the formation of regular black holes. The interior dynamics we uncover is intricate, consisting of shell bounces and white hole explosions into a new universe. The result is that regular black holes are the unique spherically symmetric solutions of the corresponding theories and also the endpoint of gravitational collapse of matter. Along the way, we establish evidence for a solution-independent upper bound on the curvature, suggestive of Markov's limiting curvature hypothesis.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02740
Holographic Energy Correlators for Soft Walls,['High Energy Physics - Phenomenology'],"['Csaba Csáki', 'Steven Ferrante', 'Ameen Ismail']","We calculate energy correlators in a general holographic model of confinement, involving an asymptotically anti-de Sitter (AdS) warped extra dimension. Building on a recent computation in a minimal hard-wall model of confinement, we show that the shockwave method for efficiently computing energy correlators in AdS generalizes to an arbitrary warped geometry. This is possible because exact, linear shockwave solutions to the 5D field equations exist in any warped background. We apply our formalism to compute the two-point energy correlator for two simple models of confinement with interesting infrared spectra -- one with a gapped continuum spectrum and one with linear Regge trajectories. The results differ from the simple hard-wall model and from each other, demonstrating that the details of the confining dynamics affect the shape of the energy correlator observables.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02738
Dark matter fraction derived from the M31 rotation curve,['Astrophysics of Galaxies'],"['F. Hammer', 'Y. B. Yang', 'P. Amram', 'L. Chemin', 'G. A. Mamon', 'J. L. Wang', 'I. Akib', 'Y. J. Jiao', 'H. F. Wang']","Mass estimates of a spiral galaxy derived from its rotation curve must account for the galaxy's past accretion history. There are several lines of evidence indicating that M31 experienced a major merger 2 to 3 Gyr ago. Here, we have generated a dynamical model of M31 as a merger remnant that reproduces most of its properties, from the central bar to the outskirts. The model accounts for the past major merger, and reproduces the details of M31's rotation curve, including its 14 kpc bump and the observed increase of velocity beyond 25 kpc. Furthermore, we find non-equilibrium and oscillatory motions in the gas of the merger-remnant outskirts caused by material in a tidal tail returning to the merger remnant. A total dynamical M31 mass of 4.5 $\times 10^{11} M_{\odot}$ within 137 kpc has been obtained after scaling it to the observed HI rotation curve. Within this radial distance, 68% of the total dynamical mass is dark.△ Less",v1,https://arxiv.org/pdf/2412.02737
Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications,['Computer Vision and Pattern Recognition'],"['Daniela Szwarcman', 'Sujit Roy', 'Paolo Fraccaro', 'Þorsteinn Elí Gíslason', 'Benedikt Blumenstiel', 'Rinki Ghosal', 'Pedro Henrique de Oliveira', 'Joao Lucas de Sousa Almeida', 'Rocco Sedona', 'Yanghui Kang', 'Srija Chakraborty', 'Sizhe Wang', 'Ankur Kumar', 'Myscon Truong', 'Denys Godwin', 'Hyunho Lee', 'Chia-Yu Hsu', 'Ata Akbari Asanjan', 'Besart Mujeci', 'Trevor Keenan', 'Paulo Arevalo', 'Wenwen Li', 'Hamed Alemohammad', 'Pontus Olofsson', 'Christopher Hain']","This technical report presents Prithvi-EO-2.0, a new geospatial foundation model that offers significant improvements over its predecessor, Prithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's Harmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M and 600M parameter models incorporate temporal and location embeddings for enhanced performance across various geospatial tasks. Through extensive benchmarking with GEO-Bench, the 600M version outperforms the previous Prithvi-EO model by 8\% across a range of tasks. It also outperforms six other geospatial foundation models when benchmarked on remote sensing tasks from different domains and resolutions (i.e. from 0.1m to 15m). The results demonstrate the versatility of the model in both classical earth observation and high-resolution applications. Early involvement of end-users and subject matter experts (SMEs) are among the key factors that contributed to the project's success. In particular, SME involvement allowed for constant feedback on model and dataset design, as well as successful customization for diverse SME-led applications in disaster response, land use and crop mapping, and ecosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and IBM terratorch, with additional resources on GitHub. The project exemplifies the Trusted Open Science approach embraced by all involved organizations.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02732
Revisiting Lipid Nanoparticle Composition and Structure: A Critical Take on Simulation Approaches,['Soft Condensed Matter'],"['Marius F. W. Trollmann', 'Paolo Rossetti', 'Rainer A. Böckmann']","Comment to article published in Proc. Natl. Acad. Sci. U. S. A.: Garaizar, A. et al. 'Toward understanding lipid reorganization in RNA lipid nanoparticles in acidic environments.' Proc. Natl. Acad. Sci. U. S. A. 121, e2404555121 (2024)△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02731
Shaping AI's Impact on Billions of Lives,['Artificial Intelligence'],"['Mariano-Florentino Cuéllar', 'Jeff Dean', 'Finale Doshi-Velez', 'John Hennessy', 'Andy Konwinski', 'Sanmi Koyejo', 'Pelonomi Moiloa', 'Emma Pierson', 'David Patterson']","Artificial Intelligence (AI), like any transformative technology, has the potential to be a double-edged sword, leading either toward significant advancements or detrimental outcomes for society as a whole. As is often the case when it comes to widely-used technologies in market economies (e.g., cars and semiconductor chips), commercial interest tends to be the predominant guiding factor. The AI community is at risk of becoming polarized to either take a laissez-faire attitude toward AI development, or to call for government overregulation. Between these two poles we argue for the community of AI practitioners to consciously and proactively work for the common good. This paper offers a blueprint for a new type of innovation infrastructure including 18 concrete milestones to guide AI research in that direction. Our view is that we are still in the early days of practical AI, and focused efforts by practitioners, policymakers, and other stakeholders can still maximize the upsides of AI and minimize its downsides.
  We talked to luminaries such as recent Nobelist John Jumper on science, President Barack Obama on governance, former UN Ambassador and former National Security Advisor Susan Rice on security, philanthropist Eric Schmidt on several topics, and science fiction novelist Neal Stephenson on entertainment. This ongoing dialogue and collaborative effort has produced a comprehensive, realistic view of what the actual impact of AI could be, from a diverse assembly of thinkers with deep understanding of this technology and these domains. From these exchanges, five recurring guidelines emerged, which form the cornerstone of a framework for beginning to harness AI in service of the public good. They not only guide our efforts in discovery but also shape our approach to deploying this transformative technology responsibly and ethically.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02730
Resource-Adaptive Successive Doubling for Hyperparameter Optimization with Large Datasets on High-Performance Computing Systems,['Machine Learning'],"['Marcel Aach', 'Rakesh Sarma', 'Helmut Neukirchen', 'Morris Riedel', 'Andreas Lintermann']","On High-Performance Computing (HPC) systems, several hyperparameter configurations can be evaluated in parallel to speed up the Hyperparameter Optimization (HPO) process. State-of-the-art HPO methods follow a bandit-based approach and build on top of successive halving, where the final performance of a combination is estimated based on a lower than fully trained fidelity performance metric and more promising combinations are assigned more resources over time. Frequently, the number of epochs is treated as a resource, letting more promising combinations train longer. Another option is to use the number of workers as a resource and directly allocate more workers to more promising configurations via data-parallel training. This article proposes a novel Resource-Adaptive Successive Doubling Algorithm (RASDA), which combines a resource-adaptive successive doubling scheme with the plain Asynchronous Successive Halving Algorithm (ASHA). Scalability of this approach is shown on up to 1,024 Graphics Processing Units (GPUs) on modern HPC systems. It is applied to different types of Neural Networks (NNs) and trained on large datasets from the Computer Vision (CV), Computational Fluid Dynamics (CFD), and Additive Manufacturing (AM) domains, where performing more than one full training run is usually infeasible. Empirical results show that RASDA outperforms ASHA by a factor of up to 1.9 with respect to the runtime. At the same time, the solution quality of final ASHA models is maintained or even surpassed by the implicit batch size scheduling of RASDA. With RASDA, systematic HPO is applied to a terabyte-scale scientific dataset for the first time in the literature, enabling efficient optimization of complex models on massive scientific data. The implementation of RASDA is available on https://github.com/olympiquemarcel/rasda△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02729
emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation,['Computer Vision and Pattern Recognition'],"['Sasha Salter', 'Richard Warren', 'Collin Schlager', 'Adrian Spurr', 'Shangchen Han', 'Rohin Bhasin', 'Yujun Cai', 'Peter Walkington', 'Anuoluwapo Bolarinwa', 'Robert Wang', 'Nathan Danielson', 'Josh Merel', 'Eftychios Pnevmatikakis', 'Jesse Marshall']","Hands are the primary means through which humans interact with the world. Reliable and always-available hand pose inference could yield new and intuitive control schemes for human-computer interactions, particularly in virtual and augmented reality. Computer vision is effective but requires one or multiple cameras and can struggle with occlusions, limited field of view, and poor lighting. Wearable wrist-based surface electromyography (sEMG) presents a promising alternative as an always-available modality sensing muscle activities that drive hand motion. However, sEMG signals are strongly dependent on user anatomy and sensor placement, and existing sEMG models have required hundreds of users and device placements to effectively generalize. To facilitate progress on sEMG pose inference, we introduce the emg2pose benchmark, the largest publicly available dataset of high-quality hand pose labels and wrist sEMG recordings. emg2pose contains 2kHz, 16 channel sEMG and pose labels from a 26-camera motion capture rig for 193 users, 370 hours, and 29 stages with diverse gestures - a scale comparable to vision-based hand pose datasets. We provide competitive baselines and challenging tasks evaluating real-world generalization scenarios: held-out users, sensor placements, and stages. emg2pose provides the machine learning community a platform for exploring complex generalization problems, holding potential to significantly enhance the development of sEMG-based human-computer interactions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02725
"Precision in the Face of Noise -- Lessons from Kahneman, Siboney, and Sunstein for Radiation Oncology",['Medical Physics'],"['Kareem A. Wahid', 'Clifton D. Fuller', 'David Fuentes']","In this manuscript, we draw on the insights from Kahneman, Sibony, and Sunsteins influential nonfiction book Noise: A Flaw in Human Judgment to explore the concept of unwanted variability in judgment (i.e., noise). We introduce key terms and connect these insights to the field of radiation oncology by illustrating how noise contributes to errors in clinically relevant areas such as contouring. Additionally, we propose practical strategies to reduce noise in radiation oncology, such as through judgment aggregation and the use of artificial intelligence tools, building on the principles outlined in the book.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02724
Advancing Tritium Self-Sufficiency in Fusion Power Plants: Insights from the BABY Experiment,['Plasma Physics'],"['Remi Delaporte-Mathurin', 'Nikola Goles', 'John Ball', 'Collin Dunn', 'Emily Edwards', 'Sara Ferry', 'Edward Lamere', 'Andrew Lanzrath', 'Rick Leccacorvi', 'Samuele Meschini', 'Ethan Peterson', 'Stefano Segantin', 'Rui Vieira', 'Dennis Whyte', 'Weiyue Zhou', 'Kevin Woller']","In the pursuit of fusion power, achieving tritium self-sufficiency stands as a pivotal challenge. Tritium breeding within molten salts is a critical aspect of next-generation fusion reactors, yet experimental measurements of \gls{tbr} have remained elusive. Here we present the results of the \gls{baby} experiment, which represents a pioneering effort in tritium research by utilizing high-energy (\SI{14}{\mega\electronvolt}) neutron irradiation of molten salts, a departure from conventional low-energy neutron approaches. Using a small-scale (\SI{100}{\milli\litre}) molten salt tritium breeding setup, we not only simulated, but also directly measured a \gls{tbr}. This innovative approach provides crucial experimental validation, offering insights unattainable through simulation alone. Moreover, our findings reveal a surprising outcome: tritium was predominantly collected as HT, contrary to the expected TF. This underscores the complexity of tritium behavior in molten salts, highlighting the need for further investigation. This work lays the foundation for a more sophisticated experimental setup, including increasing the volume of the breeder, enhancing neutron detection, and refining tritium collection systems. Such improvements are crucial for advancing our understanding of fusion reactor feasibility and paving the way for future experiments.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02721
Quantum Annealing based Hybrid Strategies for Real Time Route Optimization,['Quantum Physics'],"['Sushil Mario', 'Pavan Teja Pothamsetti', 'Louie Antony Thalakottor', 'Trisha Vishwanath', 'Sanjay H. A', 'Anees Ahmed', 'Salvatore Sinno', 'Shruthi Thuravakkath', 'Sinthuja M']","One of the most well-known problems in transportation and logistics is the Capacitated Vehicle Routing Problem (CVRP). It involves optimizing a set of truck routes to service a set of customers, subject to limits on truck capacity, to reduce travel costs. The biggest challenge faced whilst attempting to solve the issue is that the time complexity of the issue grows exponentially with the number of customers and trucks, rendering it virtually intractable to traditional computers and algorithms. In this paper, we propose a method to circumvent this limitation, employing quantum computers to aid classical computers in solving problems faster while reducing complexity. To obtain our results, we employ two algorithms: Hybrid Two Step (H2S) and Hybrid Three Step (H3S). Both algorithms involve two phases: clustering and routing. It has been observed that both algorithms produce promising results, both in terms of solution time and solution cost.△ Less","21 November, 2024;",https://arxiv.org/pdf/2412.02720
Nuevo modelo para el dimensionamiento de lotes de pedidos en función del volumen de compra y deterioro temporal de los artículos,['Optimization and Control'],"['Margarita Miguelina Mieras', 'Tania Daiana Tobares', 'Ricardo Raúl Palma', 'Antonio José Ramirez-Pastor', 'Fabricio Orlando Sanchez Varretti']","This research presents the development of a new simulation model to determine the optimal order lot sizes in Material Requirements Planning, based on purchase volume and the temporal deterioration of items. The scientific novelty lies in the exhaustive enumeration of all supply strategies, considering when and how much raw material and/or inputs to acquire while simultaneously managing multiple factors. The developed model enables obtaining and visualizing all feasible solutions, though a significant challenge arises when dealing with larger planning horizons. The methodology includes formulating a mathematical equation to calculate the total cost of all supply strategies, taking into account quantity discounts and the maximum allowable shelf life of inventory items. The results demonstrate that the model thoroughly explores the entire search space and identifies the optimal solution. Validation is conducted using the tabu search heuristic, a widely recognized optimization technique. While the heuristic converges toward the global minimum, it requires a significantly higher computational load. In contrast, the developed model identifies the global minimum with fewer calculations, showcasing its efficiency and accuracy.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.02714
Applying IRT to Distinguish Between Human and Generative AI Responses to Multiple-Choice Assessments,['Artificial Intelligence'],"['Alona Strugatski', 'Giora Alexandron']","Generative AI is transforming the educational landscape, raising significant concerns about cheating. Despite the widespread use of multiple-choice questions in assessments, the detection of AI cheating in MCQ-based tests has been almost unexplored, in contrast to the focus on detecting AI-cheating on text-rich student outputs. In this paper, we propose a method based on the application of Item Response Theory to address this gap. Our approach operates on the assumption that artificial and human intelligence exhibit different response patterns, with AI cheating manifesting as deviations from the expected patterns of human responses. These deviations are modeled using Person-Fit Statistics. We demonstrate that this method effectively highlights the differences between human responses and those generated by premium versions of leading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive to the amount of AI cheating in the data. Furthermore, we show that the chatbots differ in their reasoning profiles. Our work provides both a theoretical foundation and empirical evidence for the application of IRT to identify AI cheating in MCQ-based assessments.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.02713
Nutzung von Massespeichern zur Flexibilisierung des Energieverbrauchs: Kosteneffizienter Anlagenbetrieb durch Anpassung an Marktpreise,['Systems and Control'],"['Lukas Peter Wagner', 'Lasse Matthias Reinpold', 'Maximilian Kilthau', 'Felix Gehlhoff', 'Christian Derksen', 'Nils Loose', 'Julian Jepsen', 'Alexander Fay']","The increasing share of renewable energy sources and necessitate new concepts for energy flexible operation of industrial production resources. In this paper, we demonstrate the potential of mass storage to increase energy flexibility in industrial operations through the application of optimized operational planning based on market prices. A wastewater treatment plant equipped with decanters and storages is examined to optimally utilize its energy-flexibility. An MILP model was created for energy-flexible production facilities. The resulting operation plan was transmitted to the plant operators and executed over a period of more than 24 hours. The model shows good agreement with real measurements (average errors between 3% to 7%). The results demonstrate significant potential for cost savings of roughly 56% for the investigated time horizon.
  --
  Der zunehmende Anteil erneuerbarer Energien erfordert neue Konzepte zur energieflexiblen Steuerung industrieller Produktionsanlagen. In diesem Beitrag wird demonstriert, wie das Potenzial von Massespeichern zur Steigerung der Energieflexibilität in industriellen Prozessen durch die Anwendung optimierter Betriebsplanung basierend auf Marktpreisen, genutzt werden kann. Es wird eine Abwasseraufbereitungsanlage mit Dekantern und zugehörigen Massespeichersystemen betrachtet, um deren Energieflexibilität optimiert zu nutzen. Der resultierende Betriebsplan wurde in einem Assistenzsystem an die Anlagenbetreiber übermittelt und über einen Zeitraum von über 24 Stunden ausgeführt. Das Modell zeigt eine gute Übereinstimmung mit den realen Messwerten (Fehlern zwischen 3% und 7%). Alle von den Betreibern gesetzten Betriebsziele wurden erreicht, was die Anwendbarkeit des Modells in industriellen Umgebungen bestätigt. Die Ergebnisse zeigen ein erhebliches Einsparpotenzial von etwa 56% für den untersuchten Zeitraum.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.02708
Gravitational wave memory: further examples,['General Relativity and Quantum Cosmology'],"['P. -M. Zhang', 'Q. -L. Zhao', 'M. Elbistan', 'P. A. Horvathy']",Ehlers and Kundt [1] argued in favor of the velocity effect: particles initally at rest hit by a burst of gravitational waves should fly apart with constant velocity after the wave has passed. Zel'dovich and Polnarev [2] suggested instead that waves generated by flyby would be merely displaced. Their prediction is confirmed provided the wave parameters take some particular values.△ Less,"24 November, 2024;",https://arxiv.org/pdf/2412.02705
Motion Prompting: Controlling Video Generation with Motion Trajectories,['Computer Vision and Pattern Recognition'],"['Daniel Geng', 'Charles Herrmann', 'Junhwa Hur', 'Forrester Cole', 'Serena Zhang', 'Tobias Pfaff', 'Tatiana Lopez-Guevara', 'Carl Doersch', 'Yusuf Aytar', 'Michael Rubinstein', 'Chen Sun', 'Oliver Wang', 'Andrew Owens', 'Deqing Sun']","Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, ""interacting"" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02700
Scaling BERT Models for Turkish Automatic Punctuation and Capitalization Correction,['Computation and Language'],"['Abdulkader Saoud', 'Mahmut Alomeyr', 'Himmet Toprak Kesgin', 'Mehmet Fatih Amasyali']","This paper investigates the effectiveness of BERT based models for automated punctuation and capitalization corrections in Turkish texts across five distinct model sizes. The models are designated as Tiny, Mini, Small, Medium, and Base. The design and capabilities of each model are tailored to address the specific challenges of the Turkish language, with a focus on optimizing performance while minimizing computational overhead. The study presents a systematic comparison of the performance metrics precision, recall, and F1 score of each model, offering insights into their applicability in diverse operational contexts. The results demonstrate a significant improvement in text readability and accuracy as model size increases, with the Base model achieving the highest correction precision. This research provides a comprehensive guide for selecting the appropriate model size based on specific user needs and computational resources, establishing a framework for deploying these models in real-world applications to enhance the quality of written Turkish.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02698
Exploring the Diversity of Faint Satellites in the M81 Group,['Astrophysics of Galaxies'],"['Katya Gozman', 'Eric F. Bell', 'In Sung Jang', 'Jose Marco Arias', 'Jeremy Bailin', 'Roelof S. de Jong', ""Richard D'Souza"", 'Oleg Y. Gnedin', 'Antonela Monachesi', 'Paul A. Price', 'Vaishnav V. Rao', 'Adam Smercina']","In the last decade, we have been able to probe further down the galaxy luminosity function than ever before and expand into the regime of ultra-faint dwarfs (UFDs), which are some of the best probes we have of small-scale cosmology and galaxy formation. Digital sky surveys have enabled the discovery and study of these incredibly low-mass, highly dark-matter dominated systems around the Local Group, but it is critical that we expand the satellite census further out to understand if Milky Way and M31 satellites are representative of dwarf populations in the local Universe. Using data from HST/ACS, we present updated characterization of four satellite systems in the M81 group. These systems - D1005+68, D1006+69, DWJ0954+6821, and D1009+68 - were previously discovered using ground-based Subaru HSC data as overdensities in M81's halo and are now confirmed with HST/ACS by this work. These are all faint (M_V >= -7.9) and consistent with old (~13 Gyr), metal-poor ([M/H] < -1.5) populations. Each system possesses relatively unusual features - including one of the most concentrated satellite galaxies with a Sersic index of n ~ 5, one of the most elliptical galaxies outside the Local Group with an e ~ 0.6, and one of the most compact galaxies for its magnitude. Two of the satellites have very low surface brightness, lower than most known galaxies in this absolute magnitude range. This work previews the scientific promise of the upcoming Rubin Observatory and Roman Telescope for illuminating the diversity of UFDs in the Local Volume and beyond.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02697
Increased Surface Temperatures of Habitable White Dwarf Worlds Relative to Main-Sequence Exoplanets,['Earth and Planetary Astrophysics'],"['Aomawa L. Shields', 'Eric T. Wolf', 'Eric Agol', 'Pier-Emmanuel Tremblay']","Discoveries of giant planet candidates orbiting white dwarf stars and the demonstrated capabilities of the James Webb Space Telescope bring the possibility of detecting rocky planets in the habitable zones of white dwarfs into pertinent focus. We present simulations of an aqua planet with an Earth-like atmospheric composition and incident stellar insolation orbiting in the habitable zone of two different types of stars - a 5000 K white dwarf and main-sequence K-dwarf star Kepler-62 with a similar effective temperature - and identify the mechanisms responsible for the two differing planetary climates. The synchronously-rotating white dwarf planet's global mean surface temperature is 25 K higher than that of the synchronously-rotating planet orbiting Kepler-62, due to its much faster (10-hr) rotation and orbital period. This ultra-fast rotation generates strong zonal winds and meridional flux of zonal momentum, stretching out and homogenizing the scale of atmospheric circulation, and preventing an equivalent build-up of thick, liquid water clouds on the dayside of the planet compared to the synchronous planet orbiting Kepler-62, while also transporting heat equatorward from higher latitudes. White dwarfs may therefore present amenable environments for life on planets formed within or migrated to their habitable zones, generating warmer surface environments than those of planets with main-sequence hosts to compensate for an ever shrinking incident stellar flux.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02694
Molecular gas and dust properties in $z>7$ quasar hosts,['Astrophysics of Galaxies'],"['Francesco Salvestrini', 'Chiara Feruglio', 'Roberta Tripodi', 'Fabio Fontanot', 'Manuela Bischetti', 'Gabriella De Lucia', 'Fabrizio Fiore', 'Michaela Hirschmann', 'Umberto Maio', 'Enrico Piconcelli', 'Ivano Saccheo', 'Alessia Tortosa', 'Rosa Valiante', 'Lizhi Xie', 'Luca Zappacosta']","Observational campaigns hunting the elusive reservoirs of cold gas in the host galaxies of quasars at the Epoch of Reionization (EoR) are crucial to study the formation and evolution of the first massive systems at early epochs.
  We present new Northern Extended Millimetre Array (NOEMA) observations tracing CO(6--5), CO(7--6) emission lines, and the underlying continuum in five of the eight quasars at redshift $z>7$ known to date, thus completing the survey of the cold molecular gas reservoir in the host galaxies of the first quasars.
  Combining NOEMA observations with archival Atacama Large Millimeter Array (ALMA) data available, we model the far-infrared spectral energy distribution with a modified blackbody to measure dust properties and star formation rates.
  We use CO and [CII] lines to derive molecular gas masses, which we compare with results from semi-analytical models and observations of galaxies at different epochs.
  No statistically significant detection of CO emission lines was reported for the five quasars in this sample, resulting in a relatively low amount of cold molecular gas in the host when compared with galaxies at later epochs.
  Nonetheless, gas-to-dust ratios are consistent with the local value, suggesting that the scaling relation between dust and cold gas holds up to $z>7$.
  Quasars at the EoR show star formation efficiencies which are among the highest observed so far, but comparable with that observed in luminous quasar at Cosmic Noon and that predicted for the brightest ($L_{bol}>3\times10^{46}$ erg s$^{-1}$) quasar objects drawn from the semi-analytical model GAEA.
  Quasar host galaxies at the EoR are undergoing an intense phase of star formation, which suggests a strong coupling between the luminous phase of the quasar and the rapid growth of the host.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02688
SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance,['Computer Vision and Pattern Recognition'],"['Viet Nguyen', 'Anh Nguyen', 'Trung Dao', 'Khoi Nguyen', 'Cuong Pham', 'Toan Tran', 'Anh Tran']","Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.△ Less",v1,https://arxiv.org/pdf/2412.02687
Electron Beam Characterization via Quantum Coherent Optical Magnetometry,['Quantum Physics'],"['Nicolas DeStefano', 'Saeed Pegahan', 'Aneesh Ramaswamy', 'Seth Aubin', 'T. Averett', 'Alexandre Camsonne', 'Svetlana Malinovskaya', 'Eugeniy E. Mikhailov', 'Gunn Park', 'Shukui Zhang', 'Irina Novikova']","We present a quantum optics-based detection method for determining the position and current of an electron beam. As electrons pass through a dilute vapor of rubidium atoms, their magnetic field perturb the atomic spin's quantum state and causes polarization rotation of a laser resonant with an optical transition of the atoms. By measuring the polarization rotation angle across the laser beam, we recreate a 2D projection of the magnetic field and use it to determine the e-beam position, size and total current. We tested this method for an e-beam with currents ranging from 30 to 110 μA. Our approach is insensitive to electron kinetic energy, and we confirmed that experimentally between 10 to 20 keV. This technique offers a unique platform for non-invasive characterization of charged particle beams used in accelerators for particle and nuclear physics research.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02686
The Asymptotic Behavior of Attention in Transformers,['Artificial Intelligence'],"['Álvaro Rodríguez Abella', 'João Pedro Silvestre', 'Paulo Tabuada']","A key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token through a transformer. In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transformers. Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature. Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02682
Quasi-normal modes in non-perturbative quantum gravity,['High Energy Physics - Theory'],"['Alexey S. Koshelev', 'Chenxuan Li', 'Anna Tokareva']","Non-pertrubative quantum gravity formulated as a unitary four-dimensional theory suggests that certain amount of non-locality, such as infinite-derivative operators, can be present in the action, in both cases of Analytic Infinite Derivative gravity and Asymptotically Safe gravity. Such operators lead to the emergence of Background Induced States on top of any background deviating from the flat spacetime. Quasi-normal modes (QNMs) corresponding to these excitations are analyzed in the present paper with the use of an example of a static nearly Schwarzschild black hole. We mainly target micro-Black Holes, given that they are strongly affected by the details of UV completion for gravity, while real astrophysical black holes can be well described in EFT framework. We find that frequencies of QNMs are deviating from those in a General Relativity setup and, moreover, that the unstable QNMs are also possible. This leads to the necessity of constraints on gravity modifications or lower bounds on masses of the stable micro-Black Holes or both.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02678
Perspective: Time irreversibility in systems observed at coarse resolution,['Statistical Mechanics'],"['Cai Dieball', 'Aljaž Godec']","A broken time-reversal symmetry, i.e. broken detailed balance, is central to non-equilibrium physics and is a prerequisite for life. However, it turns out to be quite challenging to unambiguously define and quantify time-reversal symmetry (and violations thereof) in practice, that is, from observations. Measurements on complex systems have a finite resolution and generally probe low-dimensional projections of the underlying dynamics, which are well known to introduce memory. In situations where many microscopic states become ""lumped"" onto the same observable ""state"" or when introducing ""reaction coordinates"" to reduce the dimensionality of data, signatures of a broken time-reversal symmetry in the microscopic dynamics become distorted or masked. In this perspective we highlight why in defining and discussing time-reversal symmetry, and quantifying its violations, the precise underlying assumptions on the microscopic dynamics, the coarse graining, and further reductions, are not a technical detail. These assumptions decide whether the conclusions that are drawn are physically sound or inconsistent. We summarize recent findings in the field and reflect upon key challenges.△ Less",v1,https://arxiv.org/pdf/2412.02675
Sample Complexity of Black Box Work Extraction,['Quantum Physics'],"['Shantanav Chakraborty', 'Siddhartha Das', 'Arnab Ghorui', 'Soumyabrata Hazra', 'Uttam Singh']","Extracting work from a physical system is one of the cornerstones of quantum thermodynamics. The extractable work, as quantified by ergotropy, necessitates a complete description of the quantum system. This is significantly more challenging when the state of the underlying system is unknown, as quantum tomography is extremely inefficient. In this article, we analyze the number of samples of the unknown state required to extract work. With only a single copy of an unknown state, we prove that extracting any work is nearly impossible. In contrast, when multiple copies are available, we quantify the sample complexity required to estimate extractable work, establishing a scaling relationship that balances the desired accuracy with success probability. Our work develops a sample-efficient protocol to assess the utility of unknown states as quantum batteries and opens avenues for estimating thermodynamic quantities using near-term quantum computers.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02673
An unbiased statistical study of the metallicity of core-collapse supernovae based on VLT/MUSE integral-field-unit spectroscopy,['Astrophysics of Galaxies'],"['Qiang Xi', 'Ning-Chen Sun', 'Yi-Han Zhao', 'Justyn R. Maund', 'Zexi Niu', 'Adam J. Singleton', 'Jifeng Liu']","Metallicity plays a crucial role in the evolution of massive stars and their final core-collapse supernova (CCSN) explosions. Integral-field-unit (IFU) spectroscopy can provide a spatially resolved view of SN host galaxies and serve as a powerful tool to study SN metallicities. Early transient surveys targeted bright galaxies with high star formation and SN rates; as a result, the discovered SNe are significantly biased toward high metallicities. More recently, the untargeted, wide-field transient surveys, such as ASAS-SN and ZTF, have discovered a large number of SNe without such a bias. In this work, we construct a large and unbiased sample of SNe discovered by (quasi-) untargted searches, consisting of 209 SNe of Types II (with unknown subtypes), IIP, IIn, IIb, Ib and Ic at z $\leq$ 0.02 with VLT/MUSE observations. This is currently the largest CCSN sample with IFU observations. With the strong-line method, we reveal the spatially-resolved metallicity maps of the SN host galaxies and acquire accurate metallicity measurements for the SN sites. Our results show that the SN metallicities range from 12 + log(O/H) = 8.1 to 8.7 dex, and the metallicity distributions for different SN types are very close to each other, with mean and median values of 8.4 - 8.5 dex. We carefully analysed the stochastic sampling effect, showing that our large sample size narrows the 1$σ$ uncertainty down to only 0.05 dex. The apparent metallicity differences among SN types are all within 3$σ$ uncertainties and the metallicity distributions for different SN types are all consistent with being randomly drawn from the same reference distribution. This suggests that metallicity plays a minor role in the origin of different CCSN types and some other metallicity-insensitive processes, such as binary interaction, dominate the distinction of CCSN types.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02667
Late-time HST and JWST Observations of GRB 221009A: Evidence for a Break in the Light Curve at 50 Days,['High Energy Astrophysical Phenomena'],"['Huei Sears', 'Ryan Chornock', 'Peter Blanchard', 'Raffaella Margutti', 'V. Ashley Villar', 'Justin Pierel', 'Patrick J. Vallely', 'Kate D. Alexander', 'Edo Berger', 'Tarraneh Eftekhari', 'Wynn V. Jacobson-Galan', 'Tanmoy Laskar', 'Natalie LeBaron', 'Brian D. Metzger', 'Dan Milisavljevic']","GRB 221009A is one of the brightest transients ever observed with the highest peak gamma-ray flux for a gamma-ray burst (GRB). A type Ic-BL supernova (SN), SN 2022xiw, was definitively detected in late-time JWST spectroscopy (t = 195 days, observer-frame). However, photometric studies have found SN 2022xiw to be less luminous (10-70%) than the canonical GRB-SN, SN 1998bw. We present late-time Hubble Space Telescope (HST)/WFC3 and JWST/NIRCam imaging of the afterglow and host galaxy of GRB 221009A at t ~ 185, 277, and 345 days post-trigger. Our joint archival ground, HST, and JWST light curve fits show strong support for a break in the light curve decay slope at t = 50 +/- 10 days (observer-frame) and a supernova at $1.4^{+0.37}_{-0.40} \times$ the optical/NIR flux of SN 1998bw. This break is consistent with an interpretation as a jet break when requiring slow-cooling electrons in a wind medium with the electron energy spectral index, p > 2, and $ν_m < ν_c$. Our light curve and joint HST/JWST spectral energy distribution (SED) also show evidence for the late-time emergence of a bluer component in addition to the fading afterglow and supernova. We find consistency with the interpretations that this source is either a young, massive, low-metallicity star cluster or a scattered light echo of the afterglow with a SED shape of $f_ν \propto ν^{2.0\pm1.0}$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02663
Unconditional proofs of quantumness between small-space machines,['Computational Complexity'],"['A. C. Cem Say', 'M. Utkan Gezer']","A proof of quantumness is a protocol through which a classical machine can test whether a purportedly quantum device, with comparable time and memory resources, is performing a computation that is impossible for classical computers. Existing approaches to provide proofs of quantumness depend on unproven assumptions about some task being impossible for machines of a particular model under certain resource restrictions. We study a setup where both devices have space bounds $\mathit{o}(\log \log n)$. Under such memory budgets, it has been unconditionally proven that probabilistic Turing machines are unable to solve certain computational problems. We formulate a new class of problems, and show that these problems are polynomial-time solvable for quantum machines, impossible for classical machines, and have the property that their solutions can be ""proved"" by a small-space quantum machine to a classical machine with the same space bound. These problems form the basis of our newly defined protocol, where the polynomial-time verifier's verdict about the tested machine's quantumness is not conditional on an unproven weakness assumption.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02662
Moduli of real (res. quaternionic) L-connections,['Differential Geometry'],['Ayush Jaiswal'],"We have studied irreducible real (respectively, quaternionic) Lie algebroid connections and prove that the Gauge theoretic moduli space has Hausdorff Hilbert manifold structure. This work generalises some known results about simple semi-connections for complex vector bundle on compact complex manifold in real algebraic geometry.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02658
Geometric bound on structure factor,['Mesoscale and Nanoscale Physics'],"['Yugo Onishi', 'Alexander Avdoshkin', 'Liang Fu']","We show that quantum geometry sets a bound on the $q^4$ term in the static structure factor $S(q)$. Bands that saturate this bound satisfy a form of Laplace's equation, leading us to refer to them as \textit{harmonic bands}. We provide some examples of harmonic bands in one- and two-dimensional systems.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02656
LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs,['Robotics'],"['Pranav Doma', 'Aliasghar Arab', 'Xuesu Xiao']","Autonomous navigation guided by natural language instructions is essential for improving human-robot interaction and enabling complex operations in dynamic environments. While large language models (LLMs) are not inherently designed for planning, they can significantly enhance planning efficiency by providing guidance and informing constraints to ensure safety. This paper introduces a planning framework that integrates LLMs with 2D occupancy grid maps and natural language commands to improve spatial reasoning and task execution in resource-limited settings. By decomposing high-level commands and real-time environmental data, the system generates structured navigation plans for pick-and-place tasks, including obstacle avoidance, goal prioritization, and adaptive behaviors. The framework dynamically recalculates paths to address environmental changes and aligns with implicit social norms for seamless human-robot interaction. Our results demonstrates the potential of LLMs to design context-aware system to enhance navigation efficiency and safety in industrial and dynamic environments.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02655
The number of k-potent elements in the quaternion algebra HZp,['Rings and Algebras'],"['Cristina Flaut', 'Andreea Baias']","In this paper we count the number of k-potent elements over HZp , the quaternion algebra over Zp, and we give a descriptive formula for the general case. For k in {3, 4, 5}, we give an explicit formula for these values. Moreover, as an application, we count the number of solutions of the equation xk = 1 over HZp .△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02652
Bridging Hard and Soft: Mechanical Metamaterials Enable Rigid Torque Transmission in Soft Robots,['Robotics'],"['Molly Carton', 'Jakub F. Kowalewski', 'Jiani Guo', 'Jacob F. Alpert', 'Aman Garg', 'Daniel Revier', 'Jeffrey Ian Lipton']","Torque and continuous rotation are fundamental methods of actuation and manipulation in rigid robots. Soft robot arms use soft materials and structures to mimic the passive compliance of biological arms that bend and extend. This use of compliance prevents soft arms from continuously transmitting and exerting torques to interact with their environment. Here, we show how relying on patterning structures instead of inherent material properties allows soft robotic arms to remain compliant while continuously transmitting torque to their environment. We demonstrate a soft robotic arm made from a pair of mechanical metamaterials that act as compliant constant-velocity joints. The joints are up to 52 times stiffer in torsion than bending and can bend up to 45°. This robot arm can continuously transmit torque while deforming in all other directions. The arm's mechanical design achieves high motion repeatability (0.4 mm and 0.1°) when tracking trajectories. We then trained a neural network to learn the inverse kinematics, enabling us to program the arm to complete tasks that are challenging for existing soft robots such as installing light bulbs, fastening bolts, and turning valves. The arm's passive compliance makes it safe around humans and provides a source of mechanical intelligence, enabling it to adapt to misalignment when manipulating objects. This work will bridge the gap between hard and soft robotics with applications in human assistance, warehouse automation, and extreme environments.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02650
Communicate or Sense? AP Mode Selection in mmWave Cell-Free Massive MIMO-ISAC,['Signal Processing'],"['Weixian Yan', 'Ozan Alp Topal', 'Zinat Behdad', 'Ozlem Tugfe Demir', 'Cicek Cavdar']","Integrated sensing and communication (ISAC) is a promising technology for future mobile networks, enabling sensing applications to be performed by existing communication networks, consequently improving the system efficiency. Millimeter wave (mmWave) signals provide high sensing resolution and high data rate but suffer from sensitivity to blockage. Cell-free massive multiple-input multiple-output (MIMO), with a large number of distributed access points (APs), can overcome this challenge by providing macro diversity against changing blockages and can save energy consumption by deactivating unfavorable APs. Thus, in this work, we propose a joint dynamic AP mode selection and power allocation scheme for mmWave cell-free massive MIMO-ISAC, where APs are assigned either as ISAC transmitters, sensing receivers, or shut down. Due to the large size of the original problem, we propose three different sub-optimal algorithms that minimize the number of active APs while guaranteeing the sensing and communication constraints. Numerical results demonstrate that assigning ISAC transmitters only satisfying communication constraints, followed up by sensing receiver assignment only for sensing constraint achieves the best performance-complexity balance.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02649
Gravitational waves from supercooled phase transitions in conformal Majoron models of neutrino mass,['High Energy Physics - Phenomenology'],"['João Gonçalves', 'Danny Marfatia', 'António P. Morais', 'Roman Pasechnik']","We study supercooled first-order phase transitions above the QCD scale in a wide class of conformal Majoron-like U(1)' models that explain the totality of active neutrino oscillation data and produce a detectable stochastic gravitational wave background (SGWB) at LIGO, LISA and ET. We place constraints on the U(1)' breaking scale and gauge coupling using current LIGO-Virgo-Kagra data. We find that strong supercooling can be ruled out in large regions of parameter space if a SGWB is not detected by these experiments. A null signal at LIGO and ET will disfavor a type-I seesaw scale above $10^{14}$ GeV, while a positive signal is a signature of heavy right-handed neutrinos. On the other hand, LISA will be sensitive to seesaw scales as low as a TeV, and could detect a SGWB even if the right-handed neutrinos are decoupled.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02645
Leveraging Tactile Sensing to Render both Haptic Feedback and Virtual Reality 3D Object Reconstruction in Robotic Telemanipulation,['Robotics'],"['Gabriele Giudici', 'Aramis Augusto Bonzini', 'Claudio Coppola', 'Kaspar Althoefer', 'Ildar Farkhatdinov', 'Lorenzo Jamone']","Dexterous robotic manipulator teleoperation is widely used in many applications, either where it is convenient to keep the human inside the control loop, or to train advanced robot agents. So far, this technology has been used in combination with camera systems with remarkable success. On the other hand, only a limited number of studies have focused on leveraging haptic feedback from tactile sensors in contexts where camera-based systems fail, such as due to self-occlusions or poor light conditions like smoke. This study demonstrates the feasibility of precise pick-and-place teleoperation without cameras by leveraging tactile-based 3D object reconstruction in VR and providing haptic feedback to a blindfolded user. Our preliminary results show that integrating these technologies enables the successful completion of telemanipulation tasks previously dependent on cameras, paving the way for more complex future applications.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02644
A Bidirectional Long Short Term Memory Approach for Infrastructure Health Monitoring Using On-board Vibration Response,['Computer Vision and Pattern Recognition'],"['R. R. Samani', 'A. Nunez', 'B. De Schutter']","The growing volume of available infrastructural monitoring data enables the development of powerful datadriven approaches to estimate infrastructure health conditions using direct measurements. This paper proposes a deep learning methodology to estimate infrastructure physical parameters, such as railway track stiffness, using drive-by vibration response signals. The proposed method employs a Long Short-term Memory (LSTM) feature extractor accounting for temporal dependencies in the feature extraction phase, and a bidirectional Long Short-term Memory (BiLSTM) networks to leverage bidirectional temporal dependencies in both the forward and backward paths of the drive-by vibration response in condition estimation phase. Additionally, a framing approach is employed to enhance the resolution of the monitoring task to the beam level by segmenting the vibration signal into frames equal to the distance between individual beams, centering the frames over the beam nodes. The proposed LSTM-BiLSTM model offers a versatile tool for various bridge and railway infrastructure conditions monitoring using direct drive-by vibration response measurements. The results demonstrate the potential of incorporating temporal analysis in the feature extraction phase and emphasize the pivotal role of bidirectional temporal information in infrastructure health condition estimation. The proposed methodology can accurately and automatically estimate railway track stiffness and identify local stiffness reductions in the presence of noise using drive-by measurements. An illustrative case study of vehicle-track interaction simulation is used to demonstrate the performance of the proposed model, achieving a maximum mean absolute percentage error of 1.7% and 0.7% in estimating railpad and ballast stiffness, respectively.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02643
Robust soybean seed yield estimation using high-throughput ground robot videos,['Computer Vision and Pattern Recognition'],"['Jiale Feng', 'Samuel W. Blair', 'Timilehin Ayanlade', 'Aditya Balu', 'Baskar Ganapathysubramanian', 'Arti Singh', 'Soumik Sarkar', 'Asheesh K Singh']","We present a novel method for soybean (Glycine max (L.) Merr.) yield estimation leveraging high throughput seed counting via computer vision and deep learning techniques. Traditional methods for collecting yield data are labor-intensive, costly, prone to equipment failures at critical data collection times, and require transportation of equipment across field sites. Computer vision, the field of teaching computers to interpret visual data, allows us to extract detailed yield information directly from images. By treating it as a computer vision task, we report a more efficient alternative, employing a ground robot equipped with fisheye cameras to capture comprehensive videos of soybean plots from which images are extracted in a variety of development programs. These images are processed through the P2PNet-Yield model, a deep learning framework where we combined a Feature Extraction Module (the backbone of the P2PNet-Soy) and a Yield Regression Module to estimate seed yields of soybean plots. Our results are built on three years of yield testing plot data - 8500 in 2021, 2275 in 2022, and 650 in 2023. With these datasets, our approach incorporates several innovations to further improve the accuracy and generalizability of the seed counting and yield estimation architecture, such as the fisheye image correction and data augmentation with random sensor effects. The P2PNet-Yield model achieved a genotype ranking accuracy score of up to 83%. It demonstrates up to a 32% reduction in time to collect yield data as well as costs associated with traditional yield estimation, offering a scalable solution for breeding programs and agricultural productivity enhancement.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02642
QA-TOOLBOX: Conversational Question-Answering for process task guidance in manufacturing,['Computation and Language'],"['Ramesh Manuvinakurike', 'Elizabeth Watkins', 'Celal Savur', 'Anthony Rhodes', 'Sovan Biswas', 'Gesem Gudino Mejia', 'Richard Beckwith', 'Saurav Sahay', 'Giuseppe Raffa', 'Lama Nachman']","In this work we explore utilizing LLMs for data augmentation for manufacturing task guidance system. The dataset consists of representative samples of interactions with technicians working in an advanced manufacturing setting. The purpose of this work to explore the task, data augmentation for the supported tasks and evaluating the performance of the existing LLMs. We observe that that task is complex requiring understanding from procedure specification documents, actions and objects sequenced temporally. The dataset consists of 200,000+ question/answer pairs that refer to the spec document and are grounded in narrations and/or video demonstrations. We compared the performance of several popular open-sourced LLMs by developing a baseline using each LLM and then compared the responses in a reference-free setting using LLM-as-a-judge and compared the ratings with crowd-workers whilst validating the ratings with experts.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02638
"Harmonic, Holomorphic and Rational Maps from Self-Duality",['High Energy Physics - Theory'],"['L. A. Ferreira', 'L. R. Livramento']","We propose a generalization of the so-called rational map ansatz on the Euclidean space $\mathbb{R}^3$, for any compact simple Lie group $G$ such that $G/{\widehat K}\otimes U(1)$ is an Hermitian symmetric space, for some subgroup ${\widehat K}$ of $G$. It generalizes the rational maps on the two-sphere $SU(2)/U(1)$, and also on $CP^N=SU(N+1)/SU(N)\otimes U(1)$, and opens up the way for applications of such ansätze on non-linear sigma models, Skyrme theory and magnetic monopoles in Yang-Mills-Higgs theories. Our construction is based on a well known mathematical result stating that stable harmonic maps $X$ from the two-sphere $S^2$ to compact Hermitian symmetric spaces $G/{\widehat K}\otimes U(1)$ are holomorphic or anti-holomorphic. We derive such a mathematical result using ideas involving the concept of self-duality, in a way that makes it more accessible to theoretical physicists. Using a topological (homotopic) charge that admits an integral representation, we construct first order partial differential self-duality equations such that their solutions also solve the (second order) Euler-Lagrange associated to the harmonic map energy $E=\int_{S^2} \mid dX\mid^2 dμ$. We show that such solutions saturate a lower bound on the energy $E$, and that the self-duality equations constitute the Cauchy-Riemann equations for the maps $X$. Therefore, they constitute harmonic and (anti)holomorphic maps, and lead to the generalization of the rational map ansätze in $\mathbb{R}^3$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02636
Liquefaction: Privately Liquefying Blockchain Assets,['Cryptography and Security'],"['James Austgen', 'Andrés Fábrega', 'Mahimna Kelkar', 'Dani Vilardell', 'Sarah Allen', 'Kushal Babel', 'Jay Yu', 'Ari Juels']","Inherent in the world of cryptocurrency systems and their security models is the notion that private keys, and thus assets, are controlled by individuals or individual entities.
  We present Liquefaction, a wallet platform that demonstrates the dangerous fragility of this foundational assumption by systemically breaking it. Liquefaction uses trusted execution environments (TEEs) to encumber private keys, i.e., attach rich, multi-user policies to their use. In this way, it enables the cryptocurrency credentials and assets of a single end-user address to be freely rented, shared, or pooled. It accomplishes these things privately, with no direct on-chain traces.
  Liquefaction demonstrates the sweeping consequences of TEE-based key encumbrance for the cryptocurrency landscape. Liquefaction can undermine the security and economic models of many applications and resources, such as locked tokens, DAO voting, airdrops, loyalty points, soulbound tokens, and quadratic voting. It can do so with no on-chain and minimal off-chain visibility. Conversely, we also discuss beneficial applications of Liquefaction, such as privacy-preserving, cost-efficient DAOs and a countermeasure to dusting attacks. Importantly, we describe an existing TEE-based tool that applications can use as a countermeasure to Liquefaction.
  Our work prompts a wholesale rethinking of existing models and enforcement of key and asset ownership in the cryptocurrency ecosystem.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02634
Dirac bracket and Nambu structures,['High Energy Physics - Theory'],"['J. Antonio García', 'Rafael Cruz-Alvarez']","A relation between the Dirac bracket (DB) and Nambu bracket (NB) is presented. The Nambu bracket can be related with Dirac bracket if we can write the DB as a generalized Poisson structure. The NB associated with DB have all the standard properties of the original DB. When the dimension of the phase space is $s+2$ where $s$ is the number of second class constraints, the associated Nambu structure has $s+2$ entries and reduces to the Dirac bracket when $s$ of its entries are fixed to be the second class constraints. In general, when the dimension of phase space is $d=r+s$ a new Nambu structure that describes correctly the constrained dynamics can also be constructed but in thsi case addicional conditidionts are requiered. In that case the associated NB corresponds to a ``Dirac-Nambu'' bracket with $r$ entries.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02633
Mon R2: A Hub-Filament System with an Infrared Bubble at the Hub center,['Astrophysics of Galaxies'],"['L. K. Dewangan', 'N. K. Bhadari', 'A. K. Maity', 'O. R. Jadhav', 'Saurabh Sharma', 'A. Haj Ismail']","A multi-wavelength, multi-scale study of the Mon R2 hub-filament system (HFS) reveals a spiral structure, with the central hub containing more mass than its filaments. ALMA C$^{18}$O(1-0) emission reveals several accreting filaments connected to a molecular ring (size $\sim$0.18 pc $\times$ 0.26 pc). The molecular ring surrounds the infrared (IR) ring (size $\sim$0.12 pc $\times$ 0.16 pc), which is not usually observed. The IR ring encircles IR dark regions and a population of embedded near-IR sources, including the massive stars IRS 1 and IRS 2. ALMA HNC(3-2) line data reveal a mirrored B-shaped feature (extent $\sim$19000 AU $\times$ 39000 AU) toward the eastern part of the molecular ring, suggesting expansion at $\sim$2.25 km s$^{-1}$. Distinct HNC sub-structures in both redshifted and blueshifted velocity components are investigated toward the B-shaped feature. The presence of these braid-like substructures in each velocity component strongly suggests instability in photon-dominated regions. A dusty shell-like feature (extent $\sim$0.04 pc $\times$ 0.07 pc; mass $\sim$7 M$_{\odot}$) hosting IRS 1 is identified in the ALMA 1.14 mm continuum map, centered toward the base of the B-shaped feature. The IR and dense molecular rings are likely shaped by feedback from massive stars, driven by high pressure values between 10$^{-8}$-10$^{-10}$ dynes cm$^{-2}$, observed within a 1 pc range of the B0 ZAMS star powering the ultracompact HII region. Overall, these outcomes support that the Mon R2 HFS transitioned from IR-quiet to IR-bright, driven by the interaction between gas accretion and feedback from massive stars.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02628
Continual Learning of Personalized Generative Face Models with Experience Replay,['Computer Vision and Pattern Recognition'],"['Annie N. Wang', 'Luchao Qi', 'Roni Sengupta']","We introduce a novel continual learning problem: how to sequentially update the weights of a personalized 2D and 3D generative face model as new batches of photos in different appearances, styles, poses, and lighting are captured regularly. We observe that naive sequential fine-tuning of the model leads to catastrophic forgetting of past representations of the individual's face. We then demonstrate that a simple random sampling-based experience replay method is effective at mitigating catastrophic forgetting when a relatively large number of images can be stored and replayed. However, for long-term deployment of these models with relatively smaller storage, this simple random sampling-based replay technique also forgets past representations. Thus, we introduce a novel experience replay algorithm that combines random sampling with StyleGAN's latent space to represent the buffer as an optimal convex hull. We observe that our proposed convex hull-based experience replay is more effective in preventing forgetting than a random sampling baseline and the lower bound.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02627
Time-Reversal Provides Unsupervised Feedback to LLMs,['Computation and Language'],"['Yerram Varun', 'Rahul Madhavan', 'Sravanti Addepalli', 'Arun Suggala', 'Karthikeyan Shanmugam', 'Prateek Jain']","Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.△ Less",v1,https://arxiv.org/pdf/2412.02626
The Dimension of the Disguised Toric Locus of a Reaction Network,['Molecular Networks'],"['Gheorghe Craciun', 'Abhishek Deshpande', 'Jiaxin Jin']","Under mass-action kinetics, complex-balanced systems emerge from biochemical reaction networks and exhibit stable and predictable dynamics. For a reaction network $G$, the associated dynamical system is called $\textit{disguised toric}$ if it can yield a complex-balanced realization on a possibly different network $G_1$. This concept extends the robust properties of toric systems to those that are not inherently toric. In this work, we study the $\textit{disguised toric locus}$ of a reaction network - i.e., the set of positive rate constants that make the corresponding mass-action system disguised toric. Our primary focus is to compute the exact dimension of this locus. We subsequently apply our results to Thomas-type and circadian clock models.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02620
Demonstrating the Advantages of Analog Wafer-Scale Neuromorphic Hardware,['Neural and Evolutionary Computing'],"['Hartmut Schmidt', 'Andreas Grübl', 'José Montes', 'Eric Müller', 'Sebastian Schmitt', 'Johannes Schemmel']","As numerical simulations grow in size and complexity, they become increasingly resource-intensive in terms of time and energy. While specialized hardware accelerators often provide order-of-magnitude gains and are state of the art in other scientific fields, their availability and applicability in computational neuroscience is still limited. In this field, neuromorphic accelerators, particularly mixed-signal architectures like the BrainScaleS systems, offer the most significant performance benefits. These systems maintain a constant, accelerated emulation speed independent of network model and size. This is especially beneficial when traditional simulators reach their limits, such as when modeling complex neuron dynamics, incorporating plasticity mechanisms, or running long or repetitive experiments. However, the analog nature of these systems introduces new challenges. In this paper we demonstrate the capabilities and advantages of the BrainScaleS-1 system and how it can be used in combination with conventional software simulations. We report the emulation time and energy consumption for two biologically inspired networks adapted to the neuromorphic hardware substrate: a balanced random network based on Brunel and the cortical microcircuit from Potjans and Diesmann.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02619
Nondeterministic tree-walking automata are not closed under complementation,['Formal Languages and Automata Theory'],"['Olga Martynova', 'Alexander Okhotin']","It is proved that the family of tree languages recognized by nondeterministic tree-walking automata is not closed under complementation, solving a problem raised by Bojańczyk and Colcombet (""Tree-walking automata do not recognize all regular languages"", SIAM J. Comp. 38 (2008) 658--701). In addition, it is shown that nondeterministic tree-walking automata are stronger than unambiguous tree-walking automata.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02618
Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback,['Machine Learning'],"['Hiroki Furuta', 'Heiga Zen', 'Dale Schuurmans', 'Aleksandra Faust', 'Yutaka Matsuo', 'Percy Liang', 'Sherry Yang']","Large text-to-video models hold immense potential for a wide range of downstream applications. However, these models struggle to accurately depict dynamic object interactions, often resulting in unrealistic movements and frequent violations of real-world physics. One solution inspired by large language models is to align generated outputs with desired outcomes using external feedback. This enables the model to refine its responses autonomously, eliminating extensive manual data collection. In this work, we investigate the use of feedback to enhance the object dynamics in text-to-video models. We aim to answer a critical question: what types of feedback, paired with which specific self-improvement algorithms, can most effectively improve text-video alignment and realistic object interactions? We begin by deriving a unified probabilistic objective for offline RL finetuning of text-to-video models. This perspective highlights how design elements in existing algorithms like KL regularization and policy projection emerge as specific choices within a unified framework. We then use derived methods to optimize a set of text-video alignment metrics (e.g., CLIP scores, optical flow), but notice that they often fail to align with human perceptions of generation quality. To address this limitation, we propose leveraging vision-language models to provide more nuanced feedback specifically tailored to object dynamics in videos. Our experiments demonstrate that our method can effectively optimize a wide variety of rewards, with binary AI feedback driving the most significant improvements in video quality for dynamic interactions, as confirmed by both AI and human evaluations. Notably, we observe substantial gains when using reward signals derived from AI feedback, particularly in scenarios involving complex interactions between multiple objects and realistic depictions of objects falling.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02617
Nonthermal order by nonthermal disorder,['Strongly Correlated Electrons'],"['Francesco Grandi', 'Antonio Picano', 'Ronny Thomale', 'Dante M. Kennes', 'Martin Eckstein']","The quench dynamics of systems exhibiting cooperative or almost competitive orders in equilibrium are explored using Ginzburg-Landau theory plus fluctuations. We show that when the renormalization of the free energy by fluctuations is taken into account, anisotropic stiffnesses and relaxation rates of the order parameters can lead to a stabilization of ordered states at transient free energy minima which are distinct from any (global or local) minima of the equilibrium free energy. This theory demonstrates that nonequilibrium fluctuations play a pivotal role in forming nonthermal orders. As nonthermal order and nonthermal fluctuations mutually stabilize each other over some time, this mechanism could be seen as a nonequilibrium variant of the order-by-disorder phenomenon. We discuss the relevance of these findings for systems with intertwined orders, such as high-temperature superconductors and the kagome metals, as well as for systems that show orbital ordering.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02616
Projection Abstractions in Planning Under the Lenses of Abstractions for MDPs,['Artificial Intelligence'],"['Giuseppe Canonaco', 'Alberto Pozanco', 'Daniel Borrajo']","The concept of abstraction has been independently developed both in the context of AI Planning and discounted Markov Decision Processes (MDPs). However, the way abstractions are built and used in the context of Planning and MDPs is different even though lots of commonalities can be highlighted. To this day there is no work trying to relate and unify the two fields on the matter of abstractions unraveling all the different assumptions and their effect on the way they can be used. Therefore, in this paper we aim to do so by looking at projection abstractions in Planning through the lenses of discounted MDPs. Starting from a projection abstraction built according to Classical or Probabilistic Planning techniques, we will show how the same abstraction can be obtained under the abstraction frameworks available for discounted MDPs. Along the way, we will focus on computational as well as representational advantages and disadvantages of both worlds pointing out new research directions that are of interest for both fields.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02615
GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot,['Computation and Language'],"['Aohan Zeng', 'Zhengxiao Du', 'Mingdao Liu', 'Kedong Wang', 'Shengmin Jiang', 'Lei Zhao', 'Yuxiao Dong', 'Jie Tang']","We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02612
Ground State Energy Estimation on Current Quantum Hardware Through The Variational Quantum Eigensolver: A Comprehensive Study,['Quantum Physics'],"['Nacer Eddine Belaloui', 'Abdellah Tounsi', 'Rabah Abdelmouheymen Khamadja', 'Mohamed Messaoud Louamri', 'Achour Benslama', 'David E. Bernal Neira', 'Mohamed Taha Rouabah']","While numerical simulations are presented in most papers introducing new methods to enhance the VQE performance, comprehensive, comparative, and applied studies remain relatively rare. We present a comprehensive, yet concise guide for the implementation of the VQE for molecular problems on NISQ devices, specifically applied to estimate the ground state energy of the BeH2 molecule using hardware-efficient and chemically informed ansätze. This work clarifies several under-documented aspects in the literature, such as the construction of the electronic Hamiltonian, the transformation of fermionic operators into qubit operators via second quantization, and the mathematical framework's details for the unitary coupled cluster single and double (UCCSD) ansatz. Our methodology, implemented using Qiskit 1.2, the latest release as of the date of this writing, is demonstrated on a noiseless simulator and further tested with noisy quantum circuits. The resilience of the VQE to quantum noise remains an open question. This study compares the computational accuracy of ground state energy estimations for molecules using the VQE across three different current quantum hardware noise models. Furthermore, our experiment on IBM's 156-qubit actual quantum computer revealed valuable insights on the real performance of the VQE on current quantum hardware.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02606
Interpretable Company Similarity with Sparse Autoencoders,['Computation and Language'],"['Marco Molinari', 'Vladimir Tregubiak', 'Victor Shao', 'Abhimanyu Pandey', 'Mateusz Mikolajczak', 'Sebastião Kuznetsov Ryder Torres Pereira']","Determining company similarity is a vital task in finance, underpinning hedging, risk management, portfolio diversification, and more. Practitioners often rely on sector and industry classifications to gauge similarity, such as SIC-codes and GICS-codes, the former being used by the U.S. Securities and Exchange Commission (SEC), and the latter widely used by the investment community. Clustering embeddings of company descriptions has been proposed as a potential technique for determining company similarity, but the lack of interpretability in token embeddings poses a significant barrier to adoption in high-stakes contexts. Sparse Autoencoders have shown promise in enhancing the interpretability of Large Language Models by decomposing LLM activations into interpretable features. In this paper, we explore the use of SAE features in measuring company similarity and benchmark them against (1) SIC codes and (2) Major Group codes. We conclude that SAE features can reproduce and even surpass sector classifications in quantifying fundamental characteristics of companies, evaluated by the correlation of monthly returns, a proxy for similarity, and PnL from cointegration.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02605
Fairness-Regulated Dense Subgraph Discovery,['Social and Information Networks'],"['Emmanouil Kariotakis', 'Nikolaos Sidiropoulos', 'Aritra Konar']","Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations -- the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02604
CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs,['Computation and Language'],"['Abhas Kumar', 'Kapil Pathak', 'Rajesh Kavuru', 'Prabhakar Srinivasan']","This paper analyzes the performance of Small Language Models (SLMs) and Vision Language Models (VLMs) and evaluates the trade-off between model performance and carbon emissions across 4 essential tasks: Image Captioning, Visual Question Answering (VQA), Dialogue Summarization and Text-to-SQL conversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture family are chosen and variants based on model size in terms of the number of parameters, quantization level and fine-tuning parameters are evaluated. The model variant's performance and carbon emissions are calculated. To quantify the trade-off between model performance and carbon emissions, we introduce a novel metric called CEGI (Carbon Efficient Gain Index). This metric represents the carbon emission per unit percentage gain per million trainable parameters . This metric provides a normalized measure to compare model's efficiency in terms of performance improvement relative to their environmental cost. The experiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve performance levels comparable to Large Language Models (LLMs) while producing significantly less carbon emissions. Our findings suggest that the marginal gains in accuracy from larger models do not justify the substantial increase in carbon emissions. Leveraging lower-bit quantization levels, the proposed metric further enhances energy efficiency without compromising performance. This study highlights balancing high performance and environmental sustainability. It offers a valuable metric for selecting models suitable for environmentally-friendly AI development.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02602
MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression Prediction from Whole Slide Histopathology Images,['Computer Vision and Pattern Recognition'],"['Aniruddha Ganguly', 'Debolina Chatterjee', 'Wentao Huang', 'Jie Zhang', 'Alisa Yurovsky', 'Travis Steele Johnson', 'Chao Chen']","Recent advances in Spatial Transcriptomics (ST) pair histology images with spatially resolved gene expression profiles, enabling predictions of gene expression across different tissue locations based on image patches. This opens up new possibilities for enhancing whole slide image (WSI) prediction tasks with localized gene expression. However, existing methods fail to fully leverage the interactions between different tissue locations, which are crucial for accurate joint prediction. To address this, we introduce MERGE (Multi-faceted hiErarchical gRaph for Gene Expressions), which combines a multi-faceted hierarchical graph construction strategy with graph neural networks (GNN) to improve gene expression predictions from WSIs. By clustering tissue image patches based on both spatial and morphological features, and incorporating intra- and inter-cluster edges, our approach fosters interactions between distant tissue locations during GNN learning. As an additional contribution, we evaluate different data smoothing techniques that are necessary to mitigate artifacts in ST data, often caused by technical imperfections. We advocate for adopting gene-aware smoothing methods that are more biologically justified. Experimental results on gene expression prediction show that our GNN method outperforms state-of-the-art techniques across multiple metrics.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02601
"Ion exchange synthesizes layered polymorphs of MgZrN$_2$ and MgHfN$_2$, two metastable semiconductors",['Materials Science'],"['Christopher L. Rom', 'Matthew Jankousky', 'Maxwell Q. Phan', ""Shaun O'Donnell"", 'Corlyn Regier', 'James R. Neilson', 'Vladan Stevanovic', 'Andriy Zakutayev']","The synthesis of ternary nitrides is uniquely difficult, in large part because elemental N$_2$ is relatively inert. However, lithium reacts readily with other metals and N$_2$, making Li-M-N the most numerous sub-set of ternary nitrides. Here, we use Li$_2$ZrN$_2$, a ternary with a simple synthesis recipe, as a precursor for ion exchange reactions towards AZrN$_2$ (A = Mg, Fe, Cu, Zn). In situ synchrotron powder X-ray diffraction studies show that Li$^+$ and Mg$^{2+}$ undergo ion exchange topochemically, preserving the layers of octahedral [ZrN$_6$] to yield a metastable layered polymorph of MgZrN$_2$ (spacegroup $R\overline{3}m$) rather than the calculated ground state structure ($I41/amd$). UV-vis measurements show an optical absorption onset near 2.0 eV, consistent with the calculated bandgap for this polymorph. Our experimental attempts to extend this ion exchange method towards FeZrN$_2$, CuZrN$_2$, and ZnZrN$_2$ resulted in decomposition products (A + ZrN + 1/6 N$_2$), an outcome that our computational results explain via the higher metastability of these phases. We successfully extended this ion exchange method to other Li-M-N precursors by synthesizing MgHfN$_2$ from Li$_2$HfN$_2$. In addition to the discovery of metastable $R\overline{3}m$ MgZrN$_2$ and MgHfN$_2$, this work highlights the potential of the 63 unique Li-M-N phases as precursors to synthesize new ternary nitrides.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02600
Evidence for reduced periodic lattice distortion within the Sb-terminated surface layer of the kagome metal CsV$_3$Sb$_5$,['Strongly Correlated Electrons'],"['Felix Kurtz', 'Gevin von Witte', 'Lukas Jehn', 'Alp Akbiyik', 'Igor Vinograd', 'Matthieu Le Tacon', 'Amir A. Haghighirad', 'Dong Chen', 'Chandra Shekhar', 'Claudia Felser', 'Claus Ropers']","The discovery of the kagome metal CsV$_3$Sb$_5$ sparked broad interest, due to the coexistence of a charge density wave (CDW) phase and possible unconventional superconductivity in the material. In this study, we use low-energy electron diffraction (LEED) with a $μ$m-sized electron beam to explore the periodic lattice distortion at the antimony-terminated surface in the CDW phase. We recorded high-quality backscattering diffraction patterns in ultrahigh vacuum from multiple cleaved samples. Unexpectedly, we did not find superstructure reflexes at intensity levels predicted from dynamical LEED calculations for the reported $2 \times 2 \times 2$ bulk structure. Our results suggest that in CsV$_3$Sb$_5$ the periodic lattice distortion accompanying the CDW is less pronounced at Sb-terminated surfaces than in the bulk.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02599
"Efficient Algorithms for Low Tubal Rank Tensor Approximation with Applications to Image Compression, Super-Resolution and Deep Learning",['Numerical Analysis'],"['Salman Ahmadi-Asl', 'Naeim Rezaeian', 'Cesar F. Caiafa', 'Andre L. F. de Almeidad']","In this paper we propose efficient randomized fixed-precision techniques for low tubal rank approximation of tensors. The proposed methods are faster and more efficient than the existing fixed-precision algorithms for approximating the truncated tensor SVD (T-SVD). Besides, there are a few works on randomized single-pass algorithms for computing low tubal rank approximation of tensors, none of them experimentally reports the robustness of such algorithms for low-rank approximation of real-world data tensors e.g., images and videos. The current single-pass algorithms for tensors are generalizations of those developed for matrices to tensors. However, the single-pass randomized algorithms for matrices have been recently improved and stabilized. Motivated by this progress, in this paper, we also generalize them to the tensor case based on the tubal product (T-product). We conduct extensive simulations to study the robustness of them compared with the existing single-pass randomized algorithms. In particular, we experimentally found that single-pass algorithms with the sketching parameters of equal sizes usually lead to ill-conditioned tensor least-squares problems and inaccurate results. It is experimentally shown that our proposed single-pass algorithms are robust in this sense. Numerical results demonstrate that under the same conditions (setting the same hyper-parameters), our proposed algorithms provide better performance. Three applications to image compression, super-resolution problem and deep learning are also presented.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02598
Randomized algorithms for Kroncecker tensor decomposition and applications,['Numerical Analysis'],"['Salman Ahmadi-Asl', 'Naeim Rezaeian', 'Andre L. F. de Almeida', 'Yipeng Liu']","This paper proposes fast randomized algorithms for computing the Kronecker Tensor Decomposition (KTD). The proposed algorithms can decompose a given tensor into the KTD format much faster than the existing state-of-the-art algorithms. Our principal idea is to use the randomization framework to reduce computational complexity significantly. We provide extensive simulations to verify the effectiveness and performance of the proposed randomized algorithms with several orders of magnitude acceleration compared to the deterministic one. Our simulations use synthetics and real-world datasets with applications to tensor completion, video/image compression, image denoising, and image super-resolution△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02597
Class-wise Autoencoders Measure Classification Difficulty And Detect Label Mistakes,['Machine Learning'],"['Jacob Marks', 'Brent A. Griffin', 'Jason J. Corso']","We introduce a new framework for analyzing classification datasets based on the ratios of reconstruction errors between autoencoders trained on individual classes. This analysis framework enables efficient characterization of datasets on the sample, class, and entire dataset levels. We define reconstruction error ratios (RERs) that probe classification difficulty and allow its decomposition into (1) finite sample size and (2) Bayes error and decision-boundary complexity. Through systematic study across 19 popular visual datasets, we find that our RER-based dataset difficulty probe strongly correlates with error rate for state-of-the-art (SOTA) classification models. By interpreting sample-level classification difficulty as a label mistakenness score, we further find that RERs achieve SOTA performance on mislabel detection tasks on hard datasets under symmetric and asymmetric label noise. Our code is publicly available at https://github.com/voxel51/reconstruction-error-ratios.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02596
Convective environments within Mediterranean cyclones,['Atmospheric and Oceanic Physics'],"['Alice Portal', 'Andrea Angelidou', 'Raphael Rousseau-Rizzi', 'Shira Raveh-Rubin', 'Yonatan Givon', 'Jennifer L Catto', 'Francesco Battaglioli', 'Mateusz Taszarek', 'Emmanouil Flaounas', 'Olivia Martius']","Understanding convective processes leading to severe weather hazards within Mediterranean cyclones is relevant for operational forecasters, insurance industry, and enhancing societal preparedness. In this work we examine the climatological link between Mediterranean cyclones and atmospheric conditions conducive to the formation of severe convection and convective hazards (convective precipitation, lightning and hail potential). Using ATDnet lightning detections we find that, from autumn to spring, 20 to 60% of lightning hours over the Mediterranean basin and adjacent land regions are associated with the presence of a nearby cyclone. Based on reanalysis data, severe convective environments, deep, moist convection (i.e., lightning potential) and related hazards are frequent in the warm sector of Mediterranean cyclones and to the north-east of their centres. In agreement with previous literature, convective processes and hazards peak approximately six hours prior to the time of minimum pressure of the cyclone centre. Moreover, severe convective environments are often detected in cyclone categories typical of transition seasons (especially autumn) and summer, while they are rarer in deep baroclinic cyclones with peak occurrence during winter. Finally, we show that dynamical cyclone features distinguish regions favourable to deep, moist convection. Warm conveyor belts of Mediterranean cyclones, characterised by large-scale ascent and located in regions of high thermodynamic instability, have the largest lightning potential. The potential is only half as intense along the cyclones' cold fronts.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02590
Facet-Hamiltonian cycles in the $B$-permutahedron,['Combinatorics'],"['Nastaran Behrooznia', 'Sofia Brenner', 'Arturo Merino', 'Torsten Mütze', 'Christian Rieck', 'Francesco Verciani']","We construct facet-Hamiltonian cycles in the $B$-permutahedron, resolving a conjecture raised in a recent paper by Akitaya, Cardinal, Felsner, Kleist and Lauff [arxiv.org/abs/2411.02172].△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02584
Turbulent boundary development over an air cavity,['Fluid Dynamics'],"['Abhirath Anand', 'Lina Nikolaidou', 'Christian Poelma', 'Angeliki Laskari']","The turbulent boundary layer (TBL) development over an air cavity is experimentally studied using planar particle image velocimetry. The present flow, representative of those typically encountered in ship air lubrication, resembles the geometrical characteristics of flows over solid bumps studied in literature. However, unlike solid bumps, the cavity has a variable geometry inherent to its dynamic nature. An identification technique based on thresholding of correlation values from particle image correlations is employed to detect the cavity. The TBL does not separate at the leeward side of the cavity owing to a high boundary layer thickness to maximum cavity thickness ratio ($δ/t_{max}=12$). As a consequence of the cavity geometry, the TBL is subjected to alternating streamwise pressure gradients: from an adverse pressure gradient (APG) to a favourable pressure gradient and back to an APG. The mean streamwise velocity and turbulence stresses over the cavity show that the streamwise pressure gradients and air injection are the dominant perturbations to the flow, with streamline curvature concluded to be marginal. Two-point correlations of the wall-normal velocity reveal an increased coherent extent over the cavity and a local anisotropy in regions under an APG, distinct from traditional APG TBLs, suggesting possible history effects.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02583
Segmentation of Coronary Artery Stenosis in X-ray Angiography using Mamba Models,['Image and Video Processing'],"['Ali Rostami', 'Fatemeh Fouladi', 'Hedieh Sajedi']","Coronary artery disease stands as one of the primary contributors to global mortality rates. The automated identification of coronary artery stenosis from X-ray images plays a critical role in the diagnostic process for coronary heart disease. This task is challenging due to the complex structure of coronary arteries, intrinsic noise in X-ray images, and the fact that stenotic coronary arteries appear narrow and blurred in X-ray angiographies. This study employs five different variants of the Mamba-based model and one variant of the Swin Transformer-based model, primarily based on the U-Net architecture, for the localization of stenosis in Coronary artery disease. Our best results showed an F1 score of 68.79% for the U-Mamba BOT model, representing an 11.8% improvement over the semi-supervised approach.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02568
Convergence of a heterogeneous Allen-Cahn equation to weighted mean curvature flow,['Analysis of PDEs'],"['Likhit Ganedi', 'Alice Marveggio', 'Kerrek Stinson']","We consider a variational model for heterogeneous phase separation, based on a diffuse interface energy with moving wells. Our main result identifies the asymptotic behavior of the first variation of the phase field energies as the width of the diffuse interface vanishes. This convergence result allows us to deduce a Gibbs-Thomson relation for heterogeneous surface tensions. Proceeding from this information, we prove that (weak) solutions of the Allen-Cahn equation with space dependent potential converge to a BV solution of weighted mean curvature flow, under an energy convergence hypothesis. Additionally, relying on the relative energy technique, we establish a weak-strong uniqueness principle for solutions of weighted mean curvature flow.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02567
From Kähler Ricci solitons to Calabi-Yau Kähler cones,['Differential Geometry'],"['Vestislav Apostolov', 'Abdellah Lahdili', 'Eveline Legendre']","We show that if $X$ is a smooth Fano manifold which caries a Kähler Ricci soliton, then the canonical cone of the product of $X$ with a complex projective space of sufficiently large dimension is a Calabi--Yau cone. This can be seen as an asymptotic version of a conjecture by Mabuchi and Nikagawa. This result is obtained by the openness of the set of weight functions $v$ over the momentum polytope of a given smooth Fano manifold, for which a $v$-soliton exists. We discuss other ramifications of this approach, including a Licherowicz type obstruction to the existence of a Kähler Ricci soliton and a Fujita type volume bound for the existence of a $v$-soliton.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02564
ComPair-2: A Next Generation Medium Energy Gamma-ray Telescope Prototype,['Instrumentation and Methods for Astrophysics'],"['Regina Caputo', 'Carolyn Kierans', 'Nicholas Cannady', 'Abe Falcone', 'Yasushi Fukazawa', 'Manoj Jadhav', 'Matthew Kerr', 'Nicholas Kirschner', 'Kavic Kumar', 'Adrien Laviron', 'Richard Leys', 'Julie McEnery', 'Jessica Metcalfe', 'Zachary Metzler', 'Nathan Miller', 'John Mitchell', 'Lucas Parker', 'Ivan Peric', 'Jeremy Perkins', 'Bernard Phlips', 'Judith Racusin', 'Makoto Sasaki', 'Kenneth N. Segal', 'Daniel Shy', 'Amanda L. Steinhebel']","Many questions posed in the Astro2020 Decadal survey in both the New Messengers and New Physics and the Cosmic Ecosystems science themes require a gamma-ray mission with capabilities exceeding those of existing (e.g. Fermi, Swift) and planned (e.g. COSI) observatories. ComPair, the Compton Pair telescope, is a prototype of such a next-generation gamma-ray mission. It had its inaugural balloon flight from Ft. Sumner, New Mexico in August 2023. To continue the goals of the ComPair project to develop technologies that will enable a future gamma-ray mission, the next generation of ComPair (ComPair-2) will be upgraded to increase the sensitivity and low-energy transient capabilities of the instrument. These advancements are enabled by AstroPix, a silicon monolithic active pixel sensor, in the tracker and custom dual-gain silicon photomultipliers and front-end electronics in the calorimeter. This effort builds on design work for the All-sky Medium Energy Gamma-ray Observatory eXplorer (AMEGO-X) concept that was submitted the 2021 MIDEX Announcement of Opportunity. Here we describe the ComPair-2 prototype design and integration and testing plans to advance the readiness level of these novel technologies.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02562
User Grouping and Resource Allocation in Multiuser MIMO Systems under SWIPT,['Signal Processing'],"['Javier Rubio', 'Antonio Pascual-Iserte']","This paper considers a broadcast multiple-input multiple-output (MIMO) network with multiple users and simultaneous wireless information and power transfer (SWIPT). In this scenario, it is assumed that some users are able to harvest power from radio frequency (RF) signals to recharge batteries through wireless power transfer from the transmitter, while others are served simultaneously with data transmission. The criterion driving the optimization and design of the system is based on the weighted sum rate for the users being served with data. At the same time, constraints stating minimum per-user harvested powers are included in the optimization problem. This paper derives the structure of the optimal transmit covariance matrices in the case where both types of users are present simultaneously in the network, particularizing the results to the cases where either only harvesting nodes or only information users are to be served. The tradeoff between the achieved weighted sum rate and the powers harvested by the user terminals is analyzed and evaluated using the rate-power (R-P) region. Finally, we propose a two-stage user grouping mechanism that decides which users should be scheduled to receive information and which users should be configured to harvest energy from the RF signals in each particular scheduling period, this being one of the main contributions of this paper.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02561
"Selective Thermalization, Chiral Excitations, and a Case of Quantum Hair in the Presence of Event Horizons",['High Energy Physics - Theory'],"['Akhil U Nair', 'Rakesh K. Jha', 'Prasant Samantray', 'Sashideep Gutti']","The Unruh effect is a well-understood phenomenon, where one considers a vacuum state of a quantum field in Minkowski spacetime, which appears to be thermally populated for a uniformly accelerating Rindler observer. In this article, we derive a variant of the Unruh effect involving two distinct accelerating observers and aim to address the following questions: (i) Is it possible to selectively thermalize a subset of momentum modes for the case of massless scalar fields, and (ii) Is it possible to excite only the left-handed massless fermions while keeping right-handed fermions in a vacuum state or vice versa? To this end, we consider a Rindler wedge $R_1$ constructed from a class of accelerating observers and another Rindler wedge $R_2$ (with $R_2 \subset R_1$) constructed from another class of accelerating observers such that the wedge $R_2$ is displaced along a null direction w.r.t $R_1$ by a parameter $Δ$. By first considering a massless scalar field in the $R_1$ vacuum, we show that if we choose the displacement $Δ$ along one null direction, the positive momentum modes are thermalized, whereas negative momentum modes remain in vacuum (and vice versa if we choose the displacement along the other null direction). We then consider a massless fermionic field in a vacuum state in $R_1$ and show that the reduced state in $R_2$ is such that the left-handed fermions are excited and are thermal for large frequencies. In contrast, the right-handed fermions have negligible particle density and vice versa. We argue that the toy models involving shifted Rindler spacetime may provide insights into the particle excitation aspects of evolving horizons and the possibility of Rindler spacetime having a quantum strand of hair. Additionally, based on our work, we hypothesize that massless fermions underwent selective chiral excitations during the radiation-dominated era of cosmology.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02560
Quadrupolar Density Structures in Driven Magnetic Reconnection Experiments with a Guide Field,['Plasma Physics'],"['T. W. O. Varnish', 'J. Chen', 'S. Chowdhry', 'R. Datta', 'G. V. Dowhan', 'L. S. Horan IV', 'N. M. Jordan', 'E. R. Neill', 'A. P. Shah', 'B. J. Sporer', 'R. Shapovalov', 'R. D. McBride', 'J. D. Hare']","Magnetic reconnection is a ubiquitous process in plasma physics, driving rapid and energetic events such as coronal mass ejections. Reconnection between magnetic fields with arbitrary shear can be decomposed into an anti-parallel, reconnecting component, and a non-reconnecting guide-field component which is parallel to the reconnecting electric field. This guide field modifies the structure of the reconnection layer and the reconnection rate. We present results from experiments on the MAIZE pulsed-power generator (500 kA peak current, 200 ns rise-time) which use two exploding wire arrays, tilted in opposite directions, to embed a guide field in the plasma flows with a relative strength $b\equiv B_g/B_{rec}=\text{0, 0.4, or 1}$. The reconnection layers in these experiments have widths which are less than the ion skin depth, $d_i=c/ω_{pi}$, indicating the importance of the Hall term, which generates a distinctive quadrupolar magnetic field structure along the separatrices of the reconnection layer. Using laser imaging interferometry, we observe quadrupolar structures in the line-integrated electron density, consistent with the interaction of the embedded guide field with the quadrupolar Hall field. Our measurements extend over much larger length scales ($40 d_i$) at higher $β$ ($\sim 1$) than previous experiments, providing an insight into the global structure of the reconnection layer.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02556
Bayesian data analysis for sky-averaged 21-cm experiments with contamination from linearly polarised foreground,['Cosmology and Nongalactic Astrophysics'],"['Emma Shen', 'Dominic Anstey', 'Marta Spinelli', 'Eloy de Lera Acedo', 'Anastasia Fialkov']","The precise measurement of the sky-averaged HI absorption signal between 50 and 200 MHz is the primary goal of global 21-cm cosmology. This measurement has the potential to unravel the underlying physics of cosmic structure formation and evolution during the Cosmic Dawn. It is, however, hindered by various non-smooth, frequency-dependent effects, whose structures resemble those of the signal. One such effect is the leakage of polarised foregrounds into the measured intensity signal: polarised foreground emission undergoes Faraday rotation as it passes through the magnetic fields of the interstellar medium, imprinting a chromatic structure in the relevant frequency range which complicates the extraction of the cosmological HI absorption feature. We investigate the effect of polarised Galactic foregrounds on extracting the global 21-cm signal from simulated data using REACH's data analysis pipeline; the Radio Experiment for the Analysis of Cosmic Hydrogen (REACH) is an experiment designed to detect the sky-averaged 21-cm HI signal from the early Universe using physically informed models. Using the REACH pipeline, we successfully recover an injected global 21-cm signal with an amplitude of approximately 0.16 K, centred between 80 and 120 MHz, achieving a low root-mean-square error (less than 30\% of the injected signal strength) in all the tested cases. This includes scenarios with simulated polarised Galactic diffuse emissions and polarised point source emissions, provided the overall polarisation fraction is below $\sim 3\%$. The linear mixing of contamination, caused by the superposition of multiple patches with varying strengths of Faraday rotation, produces patterns that are more distinct from the global signal. This distinction makes global signal recovery easier compared to contamination resulting from a single, slow oscillation pattern.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02552
Patent-CR: A Dataset for Patent Claim Revision,['Computation and Language'],"['Lekang Jiang', 'Pascal A Scherz', 'Stephan Goetz']","This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models. Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions. In addition, domain-specific models and the method of fine-tuning show promising results. Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard. Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment. This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02549
Plug-and-Play Half-Quadratic Splitting for Ptychography,['Image and Video Processing'],"['Alexander Denker', 'Johannes Hertrich', 'Zeljko Kereta', 'Silvia Cipiccia', 'Ecem Erin', 'Simon Arridge']","Ptychography is a coherent diffraction imaging method that uses phase retrieval techniques to reconstruct complex-valued images. It achieves this by sequentially illuminating overlapping regions of a sample with a coherent beam and recording the diffraction pattern. Although this addresses traditional imaging system challenges, it is computationally intensive and highly sensitive to noise, especially with reduced illumination overlap. Data-driven regularisation techniques have been applied in phase retrieval to improve reconstruction quality. In particular, plug-and-play (PnP) offers flexibility by integrating data-driven denoisers as implicit priors. In this work, we propose a half-quadratic splitting framework for using PnP and other data-driven priors for ptychography. We evaluate our method both on natural images and real test objects to validate its effectiveness for ptychographic image reconstruction.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02548
Fractional Order Distributed Optimization,['Machine Learning'],"['Andrei Lixandru', 'Marcel van Gerven', 'Sergio Pequito']","Distributed optimization is fundamental to modern machine learning applications like federated learning, but existing methods often struggle with ill-conditioned problems and face stability-versus-speed tradeoffs. We introduce fractional order distributed optimization (FrODO); a theoretically-grounded framework that incorporates fractional-order memory terms to enhance convergence properties in challenging optimization landscapes. Our approach achieves provable linear convergence for any strongly connected network. Through empirical validation, our results suggest that FrODO achieves up to 4 times faster convergence versus baselines on ill-conditioned problems and 2-3 times speedup in federated neural network training, while maintaining stability and theoretical guarantees.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02546
Single-atom resolved collective spectroscopy of a one-dimensional atomic array,['Quantum Physics'],"['Britton Hofer', 'Damien Bloch', 'Giulio Biagioni', 'Nathan Bonvalet', 'Antoine Browaeys', 'Igor Ferrier-Barbut']","Ordered atomic arrays feature an enhanced collective optical response compared to random atomic ensembles due to constructive interference in resonant dipole-dipole interactions. One consequence is the existence of a large shift of the transition with respect to the bare atomic frequency. In the linear optics regime (low light intensity), one observes a spectroscopic shift of the Lorentzian atomic line often called the collective Lamb shift. For stronger driving, many excitations are present in the system rendering the calculation of this shift theoretically challenging, but its understanding is important for instance when performing Ramsey spectroscopy in optical clocks. Here we report on the study of the collective optical response of a one-dimensional array of 30 dysprosium atoms. We drive the atoms on the narrow intercombination transition isolating a 2-level system, and measure the atomic state with single-shot state readout using a broad transition. In the linear optics regime, we measure the shift of the resonance in steady state due to dipole interactions, and measure how this shift depends on the interatomic distance. We further resolve at the single atom level how the excitation is distributed over the array. Then, on the same transition we perform Ramsey spectroscopy \emph{i.\,e.}~away from the linear regime. We observe a time-dependent shift, that allows us to draw the connection between the collective Lamb shift observed in the linear optics regime and in the large-excitation case.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02541
Graph-Powered Defense: Controller Area Network Intrusion Detection for Unmanned Aerial Vehicles,['Artificial Intelligence'],"['Reek Majumder', 'Gurcan Comert', 'David Werth', 'Adrian Gale', 'Mashrur Chowdhury', 'M Sabbir Salek']","The network of services, including delivery, farming, and environmental monitoring, has experienced exponential expansion in the past decade with Unmanned Aerial Vehicles (UAVs). Yet, UAVs are not robust enough against cyberattacks, especially on the Controller Area Network (CAN) bus. The CAN bus is a general-purpose vehicle-bus standard to enable microcontrollers and in-vehicle computers to interact, primarily connecting different Electronic Control Units (ECUs). In this study, we focus on solving some of the most critical security weaknesses in UAVs by developing a novel graph-based intrusion detection system (IDS) leveraging the Uncomplicated Application-level Vehicular Communication and Networking (UAVCAN) protocol. First, we decode CAN messages based on UAVCAN protocol specification; second, we present a comprehensive method of transforming tabular UAVCAN messages into graph structures. Lastly, we apply various graph-based machine learning models for detecting cyber-attacks on the CAN bus, including graph convolutional neural networks (GCNNs), graph attention networks (GATs), Graph Sample and Aggregate Networks (GraphSAGE), and graph structure-based transformers. Our findings show that inductive models such as GATs, GraphSAGE, and graph-based transformers can achieve competitive and even better accuracy than transductive models like GCNNs in detecting various types of intrusions, with minimum information on protocol specification, thus providing a generic robust solution for CAN bus security for the UAVs. We also compared our results with baseline single-layer Long Short-Term Memory (LSTM) and found that all our graph-based models perform better without using any decoded features based on the UAVCAN protocol, highlighting higher detection performance with protocol-independent capability.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02539
"Thermodynamics, elastic anomalies and excitations in the field induced phases of CeRh2As2",['Strongly Correlated Electrons'],"['Peter Thalmeier', 'Alireza Akbari', 'Burkhard Schmidt']","The tetragonal heavy fermion compound CeRh2As2 exhibits unconventional superconductivity accompanied by other broken symmetry phases that have been identified as presumably small moment intrinsic antiferromagnetism at low magnetic fields and induced quadrupolar order at higher in-plane fields. The latter may extend to very large pulsed-field range. The phase boundaries can be investigated by following thermodynamic anomalies like specific heat, magnetocaloric coefficient, thermal expansion and magnetostriction. We calculate their discontinuities and identify the influence of the field induced quadrupole on them. Furthermore we investigate the elastic constant anomalies which are determined by the static homogeneous quadrupolar RPA response functions. We present a calculation of these anomalies for the appropriate symmetry mode both in the disordered and ordered regime and investigate their change with applied field. In addition we consider the dynamical momentum dependent magnetic susceptibility and the associated dispersion of low energy magnetic modes and how their characteristics change across the phase boundary.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02537
Exploring Dark Photon Production and Kinetic Mixing Constraints in Heavy-Ion Collisions,['High Energy Physics - Phenomenology'],"['Adrian William Romero Jorge', 'Elena Bratkovskaya', 'Taesoo Song', 'Laura Sagunski']","Vector $U$-bosons, often referred to as 'dark photons', are potential candidates for mediating dark matter interactions. In this study, we outline a procedure to derive theoretical constraints on the upper bound of the kinetic mixing parameter $ε^2(M_U)$ using dilepton data from heavy-ion from SIS to RHIC energies. The analysis is based on the microscopic Parton-Hadron-String Dynamics (PHSD) transport model, which successfully reproduces the measured dilepton spectra in $p+p$, $p+A$, and $A+A$ collisions. Besides the dilepton channels resulting from interactions and decays of Standard Model particles (such as mesons and baryons), we extend the PHSD approach to include the decay of hypothetical $U$-bosons into dileptons, $U \to e^+ e^-$. The production of these $U$-bosons occurs via Dalitz decays of pions, $η$-mesons, $ω$-mesons, Delta resonances, as well as from the decays of vector mesons and $K^+$ mesons. This analysis provides an upper limit on $ε^2(M_U)$ and offers insights into the accuracy required for future experimental searches for dark photons through dilepton experiments.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02536
Defending Against Diverse Attacks in Federated Learning Through Consensus-Based Bi-Level Optimization,['Machine Learning'],"['Nicolás García Trillos', 'Aditya Kumar Akash', 'Sixu Li', 'Konstantin Riedl', 'Yuhua Zhu']","Adversarial attacks pose significant challenges in many machine learning applications, particularly in the setting of distributed training and federated learning, where malicious agents seek to corrupt the training process with the goal of jeopardizing and compromising the performance and reliability of the final models. In this paper, we address the problem of robust federated learning in the presence of such attacks by formulating the training task as a bi-level optimization problem. We conduct a theoretical analysis of the resilience of consensus-based bi-level optimization (CB$^2$O), an interacting multi-particle metaheuristic optimization method, in adversarial settings. Specifically, we provide a global convergence analysis of CB$^2$O in mean-field law in the presence of malicious agents, demonstrating the robustness of CB$^2$O against a diverse range of attacks. Thereby, we offer insights into how specific hyperparameter choices enable to mitigate adversarial effects. On the practical side, we extend CB$^2$O to the clustered federated learning setting by proposing FedCB$^2$O, a novel interacting multi-particle system, and design a practical algorithm that addresses the demands of real-world applications. Extensive experiments demonstrate the robustness of the FedCB$^2$O algorithm against label-flipping attacks in decentralized clustered federated learning scenarios, showcasing its effectiveness in practical contexts.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02535
Enhancing Vector Network Analysis With a Photonic Frequency Extender Setup,['Optics'],"['Alexander Theis', 'Michael Kocybik', 'Maris Bauer', 'Georg von Freymann', 'Fabian Friederich']","We present a photonic ultra-wideband frequency extension for a commercial Vector Network Analyzer (VNA) to perform free-space measurements in a frequency range from 70 GHz up to 520 GHz with a Hz level resolution. The concept is based on the synchronization of continuous-wave (CW) lasers with highly frequency-stable electronic emitter sources as a reference. The use of CW photomixers with bandwidths up to several terahertz allows for the straightforward expansion of the covered frequency range to 1 THz or even beyond. This can be achieved, for instance, by cascading additional lasers within the synchronization scheme. Consequently, the necessity for additional frequency extender modules, as seen in current state-of-the-art VNAs, is eliminated, thereby reducing the complexity and cost of the system significantly. To showcase the capabilities of the Photonic Vector Network Analyzer (PVNA) extender concept, we conducted S21 transmission measurements of various cross-shaped bandpass filters and waveguide-coupled high-frequency (HF) components. Additionally, the analyzed magnitude and phase data were either compared to electro-magnetic (EM) simulations or referenced against data obtained from commercial electronic frequency extender modules.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02532
Active learning of neural population dynamics using two-photon holographic optogenetics,['Neurons and Cognition'],"['Andrew Wagenmaker', 'Lu Mi', 'Marton Rozsa', 'Matthew S. Bull', 'Karel Svoboda', 'Kayvon Daie', 'Matthew D. Golub', 'Kevin Jamieson']","Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02529
The Multimodal Universe: Enabling Large-Scale Machine Learning with 100TB of Astronomical Scientific Data,['Instrumentation and Methods for Astrophysics'],"['The Multimodal Universe Collaboration', 'Jeroen Audenaert', 'Micah Bowles', 'Benjamin M. Boyd', 'David Chemaly', 'Brian Cherinka', 'Ioana Ciucă', 'Miles Cranmer', 'Aaron Do', 'Matthew Grayling', 'Erin E. Hayes', 'Tom Hehir', 'Shirley Ho', 'Marc Huertas-Company', 'Kartheik G. Iyer', 'Maja Jablonska', 'Francois Lanusse', 'Henry W. Leung', 'Kaisey Mandel', 'Juan Rafael Martínez-Galarza', 'Peter Melchior', 'Lucas Meyer', 'Liam H. Parker', 'Helen Qu', 'Jeff Shen']","We present the MULTIMODAL UNIVERSE, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, the MULTIMODAL UNIVERSE contains hundreds of millions of astronomical observations, constituting 100\,TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and ""metadata"". In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the MULTIMODAL UNIVERSE and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02527
LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual Data,['Machine Learning'],"['Hanyu Zhang', 'Chuck Arvin', 'Dmitry Efimov', 'Michael W. Mahoney', 'Dominique Perrault-Joncas', 'Shankar Ramasubramanian', 'Andrew Gordon Wilson', 'Malcolm Wolff']","Modern time-series forecasting models often fail to make full use of rich unstructured information about the time series themselves. This lack of proper conditioning can lead to obvious model failures; for example, models may be unaware of the details of a particular product, and hence fail to anticipate seasonal surges in customer demand in the lead up to major exogenous events like holidays for clearly relevant products. To address this shortcoming, this paper introduces a novel forecast post-processor -- which we call LLMForecaster -- that fine-tunes large language models (LLMs) to incorporate unstructured semantic and contextual information and historical data to improve the forecasts from an existing demand forecasting pipeline. In an industry-scale retail application, we demonstrate that our technique yields statistically significantly forecast improvements across several sets of products subject to holiday-driven demand surges.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02525
Ultrafast Superradiant Scintillation from Weakly Confined CsPbBr3 Nanocrystals,['Optics'],"['Matteo L. Zaffalon', 'Andrea Fratelli', 'Zhanzhao Li', 'Francesco Bruni', 'Ihor Cherniukh', 'Francesco Carulli', 'Francesco Meinardi', 'Maksym V. Kovalenko', 'Liberato Manna', 'Sergio Brovelli']","Efficiency and emission rate are two traditionally conflicting parameters in radiation detection, and achieving their simultaneous maximization could significantly advance ultrafast time-of-flight (ToF) technologies. In this study, we demonstrate that this goal is attainable by harnessing the giant oscillator strength (GOS) inherent to weakly confined perovskite nanocrystals, which enables superradiant scintillation under mildly cryogenic conditions that align seamlessly with ToF technologies. We show that the radiative acceleration due to GOS encompasses both single and multiple exciton dynamics arising from ionizing interactions, further enhanced by suppressed non-radiative losses and Auger recombination at 80 K. The outcome is ultrafast scintillation with 420 ps lifetime and light yield of ~10'000 photons/MeV for diluted NC solutions, all without non-radiative losses. Temperature-dependent light-guiding experiments on test-bed nanocomposite scintillators finally indicate that the light-transport capability remains unaffected by the accumulation of band-edge oscillator strength due to GOS. These findings suggest a promising pathway toward developing ultrafast nanotech scintillators with optimized light output and timing performance.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02516
Multi-timescale synaptic plasticity on analog neuromorphic hardware,['Quantitative Methods'],"['Amani Atoui', 'Jakob Kaiser', 'Sebastian Billaudelle', 'Philipp Spilger', 'Eric Müller', 'Jannik Luboeinski', 'Christian Tetzlaff', 'Johannes Schemmel']","As numerical simulations grow in complexity, their demands on computing time and energy increase. Hardware accelerators offer significant efficiency gains in many computationally intensive scientific fields, but their use in computational neuroscience remains limited. Neuromorphic substrates, such as the BrainScaleS architectures, offer significant advantages, especially for studying complex plasticity rules that require extended simulation runtimes. This work presents the implementation of a calcium-based plasticity rule that integrates calcium dynamics based on the synaptic tagging-and-capture hypothesis on the BrainScaleS-2 system. The implementation of the plasticity rule for a single synapse involves incorporating the calcium dynamics and the plasticity rule equations. The calcium dynamics are mapped to the analog circuits of BrainScaleS-2, while the plasticity rule equations are numerically solved on its embedded digital processors. The main hardware constraints include the speed of the processors and the use of integer arithmetic. By adjusting the timestep of the numerical solver and introducing stochastic rounding, we demonstrate that BrainScaleS-2 accurately emulates a single synapse following a calcium-based plasticity rule across four stimulation protocols and validate our implementation against a software reference model.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02515
A Nonlocal Schwinger Model,['High Energy Physics - Theory'],"['Ludovic Fraser-Taliente', 'Christopher P. Herzog', 'Abhay Shrestha']","We solve a system of massless fermions constrained to two space-time dimensions interacting via a $d$ space-time dimensional Maxwell field. Through dimensional reduction to the defect and bosonization, the system maps to a massless scalar interacting with a nonlocal Maxwell field through a $F φ$-coupling. The $d=2$ dimensional case is the usual Schwinger model where the photon gets a mass. More generally, in $2<d<4$ dimensions, the degrees of freedom map to a scalar which undergoes a renormalization group flow; in the ultraviolet, the scalar is free, while in the infrared it has scaling dimension $(4-d)/2$.
  The infrared is similar to the Wilson-Fisher fixed point, and the physically relevant case $d=4$ becomes infrared trivial in the limit of infinite ultraviolet cut-off, consistent with earlier work on the triviality of conformal surface defects in Maxwell theory.△ Less",v1,https://arxiv.org/pdf/2412.02514
FCL-ViT: Task-Aware Attention Tuning for Continual Learning,['Artificial Intelligence'],"['Anestis Kaimakamidis', 'Ioannis Pitas']","Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN) knowledge to new tasks, without forgetting the old ones. However, modern CL techniques focus on provisioning memory capabilities to existing DNN models rather than designing new ones that are able to adapt according to the task at hand. This paper presents the novel Feedback Continual Learning Vision Transformer (FCL-ViT) that uses a feedback mechanism to generate real-time dynamic attention features tailored to the current task. The FCL-ViT operates in two Phases. In phase 1, the generic image features are produced and determine where the Transformer should attend on the current image. In phase 2, task-specific image features are generated that leverage dynamic attention. To this end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs) are introduced that operate in both phases and are responsible for tuning the TABs attention, respectively. The FCL-ViT surpasses state-of-the-art performance on Continual Learning compared to benchmark methods, while retaining a small number of trainable DNN parameters.△ Less",v1,https://arxiv.org/pdf/2412.02509
"Unveiling exciton formation: exploring the early stages in time, energy and momentum domain",['Materials Science'],"['Valentina Gosetti', 'Jorge Cervantes-Villanueva', 'Selene Mor', 'Davide Sangalli', 'Alberto García-Cristóbal', 'Alejandro Molina-Sánchez', 'Vadim F. Agekyan', 'Manuel Tuniz', 'Denny Puntel', 'Wibke Bronsch', 'Federico Cilento', 'Stefania Pagliara']","Resolving the early-stage dynamics of exciton formation following non-resonant photoexcitation in time, energy, and momentum is quite challenging due to their inherently fast timescales and the proximity of the excitonic state to the bottom of the conduction band. In this study, by combining time- and angle-resolved photoemission spectroscopy with \mathit{ab initio} numerical simulations, we capture the timing of the early-stage exciton dynamics in energy and momentum, starting from the photoexcited population in the conduction band, progressing through the formation of free excitons, and ultimately leading to their trapping in lattice deformations. The chosen material is bismuth tri-iodide ($BiI_3$), a layered semiconductor with a rich landscape of excitons in the electronic structure both in bulk and in monolayer form. The obtained results, providing a full characterization of the exciton formation, elucidate the early stages of the physical phenomena underlying the operation of the ultrafast semiconductor device.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02507
ROVER: A Multi-Season Dataset for Visual SLAM,['Robotics'],"['Fabian Schmidt', 'Constantin Blessing', 'Markus Enzweiler', 'Abhinav Valada']","Robust Simultaneous Localization and Mapping (SLAM) is a crucial enabler for autonomous navigation in natural, unstructured environments such as parks and gardens. However, these environments present unique challenges for SLAM due to frequent seasonal changes, varying light conditions, and dense vegetation. These factors often degrade the performance of visual SLAM algorithms originally developed for structured urban environments. To address this gap, we present ROVER, a comprehensive benchmark dataset tailored for evaluating visual SLAM algorithms under diverse environmental conditions and spatial configurations. We captured the dataset with a robotic platform equipped with monocular, stereo, and RGB-D cameras, as well as inertial sensors. It covers 39 recordings across five outdoor locations, collected through all seasons and various lighting scenarios, i.e., day, dusk, and night with and without external lighting. With this novel dataset, we evaluate several traditional and deep learning-based SLAM methods and study their performance in diverse challenging conditions. The results demonstrate that while stereo-inertial and RGB-D configurations generally perform better under favorable lighting and moderate vegetation, most SLAM systems perform poorly in low-light and high-vegetation scenarios, particularly during summer and autumn. Our analysis highlights the need for improved adaptability in visual SLAM algorithms for outdoor applications, as current systems struggle with dynamic environmental factors affecting scale, feature extraction, and trajectory consistency. This dataset provides a solid foundation for advancing visual SLAM research in real-world, natural environments, fostering the development of more resilient SLAM systems for long-term outdoor localization and mapping. The dataset and the code of the benchmark are available under https://iis-esslingen.github.io/rover.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02506
The ALPINE-ALMA [CII] Survey: Unveiling the baryon evolution in the ISM of $z\sim5$ star-forming galaxies,['Astrophysics of Galaxies'],"['P. Sawant', 'A. Nanni', 'M. Romano', 'D. Donevski', 'G. Bruzual', 'N. Ysard', 'B. C. Lemaux', 'H. Inami', 'F. Calura', 'F. Pozzi', 'K. Małek', 'Junais', 'M. Boquien', 'A. L. Faisst', 'M. Hamed', 'M. Ginolfi', 'G. Zamorani', 'G. Lorenzon', 'J. Molina', 'S. Bardelli', 'E. Ibar', 'D. Vergani', 'C. Di Cesare', 'M. Béthermin', 'D. Burgarella']","Recent observations reveal a rapid dust build-up in high-redshift galaxies (z > 4), challenging current models of galaxy formation. While our understanding of dust production and destruction in the interstellar medium (ISM) is advancing, probing baryonic processes in the early Universe remains a complex task. We characterize the evolution of 98 z~5 star-forming galaxies observed as part of the ALPINE survey by constraining the physical processes underpinning the gas and dust production, consumption, and destruction in their ISM. We make use of chemical evolution models to simultaneously reproduce the observed dust and gas content. For each galaxy, we estimate initial gas mass, inflows and outflows, and efficiencies of dust growth and destruction. We test the models with the canonical Chabrier and top-heavy initial mass functions (IMFs), with the latter enabling rapid dust production on shorter timescales. Our models successfully reproduce gas and dust content in older galaxies (> 600 Myr) regardless of the IMF, with Type II SNe as the primary dust source and no dust growth in ISM with moderate inflow of primordial gas. In case of intermediate-age galaxies (300 - 600 Myr), we reproduce the gas and dust content through Type II SNe and dust growth in ISM, though we observe an over-prediction of dust mass in older galaxies, potentially indicating an unaccounted dust destruction mechanism and/or an overestimation of the observed dust masses. The number of young galaxies (< 300 Myr) reproduced, increases for models assuming top-heavy IMF but with maximal prescriptions of dust production. Galactic outflows are necessary to reproduce observed gas and dust masses. The Chabrier IMF models reproduce 65% of galaxies, while top-heavy IMF models improve this to 93%, easing tensions with observations. Upcoming JWST data will refine these models by resolving degeneracies in intrinsic galaxy properties.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02505
"Analysis, Design, and Fabrication of a High-Gain Low-Profile Metasurface Antenna Using Direct Feeding of Sievenpiper s HIS",['Applied Physics'],"['Alireza Ghaneizadeh', 'Soeren F. Peik', 'Martin Schneider', 'Mojtaba Joodaki']","HISs have recently shown the ability to support leaky waves, and to excite plasmonic and HIS resonance frequency modes for use as an antenna. In this paper, we analyzed, designed, and fabricated a TMA by directly feeding edge-located HIS cells through a microstrip feeding network. In contrast to other metasurface antennas that necessitate an external antenna to excite metasurfaces, our approach is inspired by the TMA design methodology that directly feeds the HIS cells rather than using it as a reflector. We developed a circuit model for the proposed structure and compared the results with those obtained from full-wave simulations. In addition, our further objective was to simplify the structure based on the working principle of the proposed antenna. This objective was achieved by converting square patches into parallel strip lines, leading to an aperture efficiency of 0.77. This simplification also creates additional space to explore various resonant patterns on the top surface and the feeding network on the bottom surface of the TMA. Full-wave simulation results indicate that, despite the compact dimensions of the proposed array with 64 electrically small patch resonators (1.84λ*1.84λ*0.032λ, whereλis the free space wavelength at 6.0 GHz), it achieves a realized gain, HPBW of about 15.1 dBi and 28° respectively at 6 GHz. Finally, we constructed a prototype and conducted measurements to validate the design. Measured results demonstrate good agreement with simulation ones with a gain of about 13.5 (+-0.5) dBi and a HPBW of 27° at 6 GHz. The proposed TMA is scaled to fit within the required dimensions for smart handheld devices at higher frequencies, while maintaining high gain capability. The design s scalability, single-feed, and compact footprint make it optimal for diverse wireless communication systems, such as car to car communications.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02502
The Cost of Consistency: Submodular Maximization with Constant Recourse,['Data Structures and Algorithms'],"['Paul Dütting', 'Federico Fusco', 'Silvio Lattanzi', 'Ashkan Norouzi-Fard', 'Ola Svensson', 'Morteza Zadimoghaddam']","In this work, we study online submodular maximization, and how the requirement of maintaining a stable solution impacts the approximation. In particular, we seek bounds on the best-possible approximation ratio that is attainable when the algorithm is allowed to make at most a constant number of updates per step. We show a tight information-theoretic bound of $\tfrac{2}{3}$ for general monotone submodular functions, and an improved (also tight) bound of $\tfrac{3}{4}$ for coverage functions. Since both these bounds are attained by non poly-time algorithms, we also give a poly-time randomized algorithm that achieves a $0.51$-approximation. Combined with an information-theoretic hardness of $\tfrac{1}{2}$ for deterministic algorithms from prior work, our work thus shows a separation between deterministic and randomized algorithms, both information theoretically and for poly-time algorithms.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02492
Facilitating field-free perpendicular magnetization switching with a Berry curvature dipole in a Weyl semimetal,['Mesoscale and Nanoscale Physics'],"['Dong Li', 'Xing-Yu Liu', 'Xing-Guo Ye', 'Zhen-Cun Pan', 'Wen-Zheng Xu', 'Peng-Fei Zhu', 'An-Qi Wang', 'Kenji Watanabe', 'Takashi Taniguchi', 'Zhi-Min Liao']","We report the synergy between orbital and spin-orbit torques in WTe2/Fe3GeTe2 heterostructures characterized by a Berry curvature dipole. By applying a current along the a axis in WTe2, we detect an out-of-plane magnetization in the system, which we attribute to nonequilibrium orbital magnetization linked to the Berry curvature dipole based on first-principles calculations, manifesting as the orbital Edelstein effect. This effect generates orbital torques that enable field-free perpendicular magnetization switching. Furthermore, by applying a relatively small current along the a axis and a pulsed current along the b axis in WTe2, we demonstrate controllable field-free magnetization switching of the adjacent Fe3GeTe2 layer, independently manipulating the orbital and spin-orbit torques. Our findings not only enhance the understanding of the collaborative dynamics between these torques but also suggest potential applications in magnetoresistive random-access memory.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02491
Room-temperature van der Waals magnetoresistive memories with data writing by orbital current in the Weyl semimetal TaIrTe4,['Mesoscale and Nanoscale Physics'],"['Dong Li', 'Xing-Yu Liu', 'Zhen-Cun Pan', 'An-Qi Wang', 'Jiantian Zhang', 'Peng Yu', 'Zhi-Min Liao']","Current-induced out of plane magnetization has been utilized for field-free switching of ferromagnets with perpendicular magnetic anisotropy. Identifying systems capable of energy-efficiently converting charge currents into out of plane orbit- or spin-polarized currents is crucial for advancing magnetic memory technologies. Here we introduce the Berry curvature dipole as a key evaluation factor, directly measurable through nonlinear Hall effects. In the Weyl semimetal TaIrTe4 used in our experiments, applying a current parallel to the Berry curvature dipole results in out of plane orbital magnetization, which governs the field-free perpendicular magnetization switching in TaIrTe4/Fe3GaTe2 heterostructures. Notably, all-electric control of van der Waals magnetoresistive memory at room temperature has been achieved with a low critical current density 2x10^6A/cm2 for data writing. Our findings reveal the connection between nonlinear Hall effects and field-free magnetization switching, highlighting the potential of the Berry curvature dipole in advancing orbitronics.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02488
Controlling the interaction of tightly focused 10-PW class lasers with multicomponent plasma via target parameters: optimization of electron-positron pair and $γ$-photon sources,['Plasma Physics'],"['A. V. Bashinov', 'E. S. Efimenko', 'A. A. Muraviev', 'E. A. Panova', 'V. D. Volokitin', 'I. B. Meyerov', 'A. V. Kim']","The interaction of multipetawatt lasers with plasma is a complex multiparameter problem, providing a wide field for fundamental research and opening up great opportunities for creating unique sources of high-energy electrons and positrons, dense pair plasma, and $γ$-photons. However, to achieve high efficiency of such a source, it is necessary to use targets with optimized parameters, primarily density and size, for the given laser parameters. With the use of 3D QED-PIC modeling it is shown that, when targets whose size is comparable with the laser wavelength are irradiated by laser beams with a total power of several tens of PW, the total initial number of target electrons may be regarded to be the similarity parameter of laser-plasma interaction. In practice, this can significantly simplify the selection of the targets needed for controlling the interaction and, accordingly, for achieving the specified parameters of the developed electron-positron plasma and $γ$-photon sources. Based on the similarity parameter, various laser-plasma interaction modes are identified, the necessary conditions for their launch are determined, and the properties of the pair particle and $γ$-photon source are revealed. Moreover, qualitative estimates of the quantitative and energy characteristics of such a source are obtained, allowing it to be optimized for various laser beam configurations.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02485
What should a neuron aim for? Designing local objective functions based on information theory,['Information Theory'],"['Andreas C. Schneider', 'Valentin Neuhaus', 'David A. Ehrlich', 'Abdullah Makkeh', 'Alexander S. Ecker', 'Viola Priesemann', 'Michael Wibral']","In modern deep neural networks, the learning dynamics of the individual neurons is often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. We here show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e. feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies.△ Less",v1,https://arxiv.org/pdf/2412.02482
Matrix representation of the results of Picard--Lefschetz--Pham theory near the real plane in $\mathbb{C}^2$,['Mathematical Physics'],"['A. V. Shanin', 'A. I. Korolkov', 'R. C. Assier']","This work is motivated by an attempt to construct a two-dimensional analogue of the Wiener--Hopf method. It is supposed that understanding the branching structure of unknown functions will provide insight into the form in which these functions can be sought. The unknown functions are represented as two-dimensional integrals of analytic functions with parameters, making it natural to study the branching of the integration surface using Picard--Lefschetz theory. A convenient matrix-vector representation of the main result of Picard--Lefschetz theory is introduced, and the following two statements are formulated. First, under certain constraints, the integration surface can be represented as a finite-dimensional vector with coefficients in the group ring over the fundamental group of the space with removed singularities. Second, a bypass in the parameter space around one of the components of the Landau set is described by multiplication by a relatively simple matrix (also composed of elements from the group ring). Successive loops are described by the product of such matrices. As an illustration, it is shown how the matrix formalism yields the Picard-Lefschetz formula and provides nontrivial topological relationships in the parameter space.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02481
A time-discontinuous elasto-plasticity formalism to simulate instantaneous plastic flow bursts,['Computational Physics'],"['Mathias Lamari', 'Pierre Kerfriden', 'Oguz Umut Salman', 'Vladislav Yastrebov', 'Kais Ammar', 'Samuel Forest']","Plastic flow is conventionally treated as continuous in finite element (FE) codes, whether in isotropic, anisotropic plasticity, or crystal plasticity. This approach, derived from continuum mechanics, contradicts the intermittent nature of plasticity at the elementary scale. Understanding crystal plasticity at micro-scale opens the door to new engineering applications, such as microscale machining. In this work, a new approach is proposed to account for the intermittence of plastic deformation while remaining within the framework of continuum mechanics. We introduce a material parameter, the plastic deformation threshold, denoted as $Δp_{min}$, corresponding to the plastic deformation carried by the minimal plastic deformation burst within the material. The incremental model is based on the traditional predictor-corrector algorithm to calculate the elastoplastic behavior of a material subjected to any external loading. The model is presented within the framework of small deformations for von Mises plasticity. To highlight the main features of the approach, the plastic strain increment is calculated using normality rule and consistency conditions, and is accepted only if it exceeds $Δp_{min}$. To achieve this, a time-discontinuous generalization of the Karush-Kuhn-Tucker (KKT) conditions is proposed. The simulations show that the introduction of the plastic threshold allows for the reproduction of the spatiotemporal intermittence of plastic flow, capturing the self-organization of plastic flow in complex loading scenarios within an FE model.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02475
Synthetic Altermagnets,['Materials Science'],"['Ali Asgharpour', 'Bert Koopmans', 'Rembert A. Duine']","Altermagnets, a distinct class of antiferromagnets with electronic structures resembling those of d-wave superconductors, exhibit intriguing properties that have gained significant attention in recent research. In this article, we propose synthetic altermagnets, composed of two anisotropic ferromagnetic layers arranged such that the total net magnetization is zero. We investigate the properties of these synthetic altermagnets, focusing on their electronic band structures, spin current, Berry curvature, and anomalous Hall conductivity. By developing a minimal model for our synthetic altermagnets, we examine the influence of factors such as anisotropic coupling strengths and spin-orbit coupling on the physical phenomena altermagnets manifest. Our findings open a new path for the realization of altermagnetic materials experimentally and highlight their potential applications in magneto-electronics, magneto-optics, and spintronics devices.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02473
Scaling behavior and phases of nonlinear sigma model on real Stiefel manifolds near two dimensions,['Statistical Mechanics'],"['A. M. Gavrilik', 'A. V. Nazarenko']","For a quasi-two-dimensional nonlinear sigma model on the real Stiefel manifolds with a generalized (anisotropic) metric, the equations of a two-charge renormalization group (RG) for the homothety and anisotropy of the metric as effective couplings are obtained in one-loop approximation. Normal coordinates and the curvature tensor are exploited for the renormalization of the metric. The RG trajectories are investigated and the presence of a fixed point common to four critical lines or four phases (tetracritical point) in the general case, or its absence in the case of Abelian structure 8group, is established.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02472
Formulation and Proof of the Gravitational Entropy Bound,['High Energy Physics - Theory'],['Artem Averin'],"We provide a formulation and proof of the gravitational entropy bound. We use a recently given framework which expresses the measurable quantities of a quantum theory as a weighted sum over paths in the theory's phase space. If this framework is applied to a field theory on a spacetime foliated by a hypersurface $Σ,$ the choice of a codimension-2 surface $B$ without boundary contained in $Σ$ specifies a submanifold in the phase space. We show here that this submanifold is naturally restricted to obey an entropy bound if the field theory is diffeomorphism-invariant. We prove this restriction to arise by considering the quantum-mechanical sum of paths in phase space and exploiting the interplay of the commutativity of the sum with diffeomorphism-invariance. The formulation of the entropy bound, which we state and derive in detail, involves a functional $K$ on the submanifold associated to $B.$ We give an explicit construction of $K$ in terms of the Lagrangian. The gravitational entropy bound then states: For any real $\fracλ{\hbar},$ consider the set of states where $K$ takes a value not bigger than $λ$ and let $V$ denote the phase space volume of this set. One has then $\ln (V) \le \fracλ{\hbar}.$ Especially, we show for the Einstein-Hilbert Lagrangian in any dimension with cosmological constant and arbitrary minimally coupled matter, one has $K = \frac{A}{4G}.$ Hereby, $A$ denotes the area of $B$ in a particular state.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02470
Resistive anisotropy in the charge density wave phase of Kagome superconductor CsV3Sb5 thin films,['Superconductivity'],"['Han-Xin Lou', 'Xing-Guo Ye', 'Xin Liao', 'Tong-Yang Zhao', 'An-Qi Wang', 'Da-Peng Yu', 'Zhi-Min Liao']","We investigate the resistive anisotropy in CsV3Sb5 thin films within the charge density wave phase. Using a device structure with twelve electrodes symmetrically distributed in a circular shape, we measure the resistivity anisotropy by varying the current direction. A twofold resistivity anisotropy modulated by temperature is found, which is fully consistent with the electronic nematicity in CsV3Sb5, that is, the spontaneous rotational symmetry breaking by electronic degree of freedom. Additionally, the resistivity anisotropy also shows modest changes by applying magnetic fields, implying the possible chiral charge orders with time-reversal symmetry breaking. These findings provide deep insights into the correlated electronic states in Kagome materials and highlight the unique properties of CsV3Sb5 in the two-dimensional regime.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02469
Separation of left-handed and anomalous right-handed vector operators contributions into the Wtb vertex for single and double resonant top quark production processes using a neural network,['High Energy Physics - Phenomenology'],"['E. Abasov', 'E. Boos', 'V. Bunichev', 'L. Dudko', 'D. Gorin', 'A. Markina', 'M. Perfilov', 'O. Vasilevskii', 'P. Volkov', 'G. Vorotnikov', 'A. Zaborenko']",The paper describes the application of deep neural networks for the searchdeviations from the Standard Model predictions at the Wtb vertex in the processes of single and double resonant top quark production with identical final state tWb. Monte-Carlo events preliminary classified by first level neural network as corresponding to single or double resonant top quark production are analyzed by two second level neural networks if there is a possible contribution of the anomalous right-handed vector operator into Wtb vertex or events are corresponded to the Standard Model. The second level neural networks are different for single and double resonant classes. The classes depend differently on anomalous contribution and such splitting leads to better sensitivity. The developed statistical model is used to set constraints on the anomalous right-handed vector operator at the Wtb vertex in different regions of phase space. It is demonstrated that the proposed method allows to increase the efficiency of a search for the anomalous contributions to the Wtb vertex.△ Less,"3 December, 2024;",https://arxiv.org/pdf/2412.02468
On series expansions of zeros of the deformed exponential function,['Classical Analysis and ODEs'],['Alexey Kuznetsov'],"For $q \in (0, 1)$, the deformed exponential function $f(x) = \sum_{n \geq 1} x^n q^{n(n-1)/2}/n!$ is known to have infinitely many simple and negative zeros $\{x_k(q)\}_{k \geq 1}$. In this paper, we analyze the series expansions of $-x_k(q)/k$ and $k/x_k(q)$ in powers of $q$. We prove that the coefficients of these expansions are rational functions of the form $P_n(k)/Q_n(k)$ and $\widehat{P}_n(k)/Q_n(k)$, where $Q_n(k) \in {\mathbb Z}[k]$ is explicitly defined and the polynomials $P_n(k), \widehat{P}_n(k)\in {\mathbb Z}[k]$ can be computed recursively. We provide explicit formulas for the leading coefficients of $P_n(k)$ and $\widehat{P}_n(k)$ and compute the coefficients of these polynomials for $n \leq 300$. Numerical verification shows that $P_n(k)$ and $\widehat{P}_n(k)$ take non-negative values for all $k \in \mathbb{N}$ and $n\le 300$, offering further evidence in support of conjectures by Alan Sokal.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02462
The Underlying Dynamics of Life and Its Evolution: A Prigogine-Inspired Informational Dissipative System,['Subcellular Processes'],"['Salvatore Chirumbolo', 'Antonio Vella']","Life is fundamentally a scientific enigma. The interplay between chaos, entropy dynamics, and Prigogine's dissipative systems offers profound insights into the emergence, stabilization, and eventual collapse of far-from-equilibrium systems. This study proposes that, alongside thermodynamic dissipative systems as highlighted by Ilya Prigogine, informational dissipative systems actively contribute to granting inanimate matter properties characteristic of living systems, such as autopoiesis. By examining cyclic entropy flows between water topology (Shannon space) and molecular systems (Boltzmann space), the work emphasizes the pivotal role of disquisotropic entropy, an informational entropy reservoir arising from imperfections in molecular structures. The analysis demonstrates that chaos functions as a stabilizing force, enhancing resilience, adaptability, and longevity by delaying thermodynamic equilibrium. This research connects foundational thermodynamic principles with the emergent behavior of chaotic systems, paving the way for a deeper understanding of complexity in natural and technological contexts. By exploring the relationship between chaos, entropy, and dissipative dynamics, the study advances a paradigm where disorder becomes a mechanism to sustain order, a hallmark of life and complex systems.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02459
Orbits of Massive Particles in a Spherically Symmetric Gravitational Field in View of Cosmological Constant,['General Relativity and Quantum Cosmology'],"['Ruslan Nakibov', 'Andrey Ursulov']","In this paper we present the results of a theoretical study of the trajectories of massive particles in the Köttler metric in view of the cosmological constant Λ. For both negative and positive signs of Λ a classification of trajectories is proposed, with entries based on different solutions of the trajectory equation, obtained by the expansion of the corresponding algebraic curve in Puiseux series. We also provide some specific types of trajectories which correspond to different values of the cosmological constant. In the case of negative values of the cosmological constant its upper limit is estimated from the galaxy rotation curves△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02455
"Uncertain Regulations, Definite Impacts: The Impact of the US Securities and Exchange Commission's Regulatory Interventions on Crypto Assets",['General Finance'],"['Aman Saggu', 'Lennart Ante', 'Kaja Kopiec']","This study employs an event study methodology to investigate the market impact of the U.S. Securities and Exchange Commission's (SEC) classification of crypto assets as securities. It explores how SEC interventions influence asset returns and trading volumes, focusing on explicitly named crypto assets. The empirical analysis highlights significant adverse market reactions, notably returns plummeting 12% over one week post-announcement, persisting for a month. We demonstrate that the severity of market reaction depends on sentiment and asset characteristics such as market size, age, volatility, and illiquidity. Further, we identify significant ex-ante trading volume effects indicative of pre-announcement informed trading.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02452
UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors Search,['Data Structures and Algorithms'],"['Anqi Liang', 'Pengcheng Zhang', 'Bin Yao', 'Zhongpu Chen', 'Yitong Song', 'Guangxu Cheng']","This paper presents an efficient and scalable framework for Range Filtered Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors associated with attribute values. Given a query vector $q$ and a range $[l, h]$, RF-ANNS aims to find the approximate $k$ nearest neighbors of $q$ among data whose attribute values fall within $[l, h]$. Existing methods including pre-, post-, and hybrid filtering strategies that perform attribute range filtering before, after, or during the ANNS process, all suffer from significant performance degradation when query ranges shift. Though building dedicated indexes for each strategy and selecting the best one based on the query range can address this problem, it leads to index consistency and maintenance issues.
  Our framework, called UNIFY, constructs a unified Proximity Graph-based (PG-based) index that seamlessly supports all three strategies. In UNIFY, we introduce SIG, a novel Segmented Inclusive Graph, which segments the dataset by attribute values. It ensures the PG of objects from any segment combinations is a sub-graph of SIG, thereby enabling efficient hybrid filtering by reconstructing and searching a PG from relevant segments. Moreover, we present Hierarchical Segmented Inclusive Graph (HSIG), a variant of SIG which incorporates a hierarchical structure inspired by HNSW to achieve logarithmic hybrid filtering complexity. We also implement pre- and post-filtering for HSIG by fusing skip list connections and compressed HNSW edges into the hierarchical graph. Experimental results show that UNIFY delivers state-of-the-art RF-ANNS performance across small, mid, and large query ranges.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02448
Landé g-factors and spin dynamics of charge carriers in CuCl nanocrystals in a glass matrix,['Mesoscale and Nanoscale Physics'],"['Dennis Kudlacik', 'Evgeny A. Zhukov', 'Dmitri R. Yakovlev', 'Gang Qiang', 'Marina A. Semina', 'Aleksandr A. Golovatenko', 'Anna V. Rodina', 'Alexander L. Efros', 'Alexey I. Ekimov', 'Manfred Bayer']","The spin properties of charge carriers confined in CuCl semiconductor nanocrystals (NCs) of different sizes (radius from 1.8 nm up to 28 nm) crystallized in a glass matrix are studied experimentally and theoretically. By means of photoluminescence, spin-flip Raman scattering, time-resolved Faraday ellipticity, and time-resolved differential transmission performed at temperatures in the range of $1.6 - 120$ K at magnetic fields up to 8 T, comprehensive information on the Landé $g$-factors as well as the population and spin dynamics is received. The spin signals are contributed by confined electrons with a $g$-factor close to 2, which shows a weak increase with decreasing NC size, i.e. increasing electron confinement energy. We revisit the theory of exciton confinement as a whole in spherical NCs within the six-band valence band model in order to describe the size dependence of the $Z_3$ and $Z_{1,2}$ exciton energies in CuCl NCs. We demonstrate theoretically that the stronger increase of the $Z_{1,2}$ energy transitions with decreasing radius can be explained by the strong absorption from the excited exciton state caused by strong heavy hole-light hole mixing in the exciton. The parameters of the six-band Hamiltonian describing both the exciton and hole kinetic energies are estimated from the comparison of the calculated and experimental size dependences of the exciton transitions. A theoretical model of the size-dependent Landé $g$-factors for electron and hole confined in spherical NCs of semiconductors with negative spin-orbit splitting of the valence band is developed.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02445
Entanglement and decoherence in cosmology and in analogue gravity experiments,['General Relativity and Quantum Cosmology'],['Amaury Micheli'],"This thesis is dedicated to analysing the generation and destruction of quantum correlations in the context of inflationary cosmology and an experiment of 'analogue' preheating. Inflation is a phase of accelerated expansion of the Universe, preceding the so-called Standard Model of Big Bang cosmology, introduced to solve some shortcomings of this model. It also provides a mechanism for the emergence of primordial inhomogeneities by amplification of initial quantum fluctuations. Inflation is followed by a 'reheating' period, in which most particles are expected to be generated and reach thermal equilibrium, setting the stage for the standard Big Bang of cosmology. During a 'preheating' period, this creation proceeds partly by parametric excitation of resonant modes of the matter fields initially in their vacuum, a genuine quantum process. The physics of both situations, inflation and preheating, is that of a strong classical field acting on a quantum field to produce entangled (quasi-)particles. When the classical source is the space-time metric itself, as in inflation, we are in the framework of Quantum Field Theory in Curved Space-time (QFTCS). The evolution of the generated quantum correlations is the topic of this PhD.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02444
Multi-scale and Multi-path Cascaded Convolutional Network for Semantic Segmentation of Colorectal Polyps,['Image and Video Processing'],"['Malik Abdul Manan', 'Feng Jinchao', 'Muhammad Yaqub', 'Shahzad Ahmed', 'Syed Muhammad Ali Imran', 'Imran Shabir Chuhan', 'Haroon Ahmed Khan']","Colorectal polyps are structural abnormalities of the gastrointestinal tract that can potentially become cancerous in some cases. The study introduces a novel framework for colorectal polyp segmentation named the Multi-Scale and Multi-Path Cascaded Convolution Network (MMCC-Net), aimed at addressing the limitations of existing models, such as inadequate spatial dependence representation and the absence of multi-level feature integration during the decoding stage by integrating multi-scale and multi-path cascaded convolutional techniques and enhances feature aggregation through dual attention modules, skip connections, and a feature enhancer. MMCC-Net achieves superior performance in identifying polyp areas at the pixel level. The Proposed MMCC-Net was tested across six public datasets and compared against eight SOTA models to demonstrate its efficiency in polyp segmentation. The MMCC-Net's performance shows Dice scores with confidence intervals ranging between (77.08, 77.56) and (94.19, 94.71) and Mean Intersection over Union (MIoU) scores with confidence intervals ranging from (72.20, 73.00) to (89.69, 90.53) on the six databases. These results highlight the model's potential as a powerful tool for accurate and efficient polyp segmentation, contributing to early detection and prevention strategies in colorectal cancer.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02443
Artificial Expert Intelligence through PAC-reasoning,['Artificial Intelligence'],"['Shai Shalev-Shwartz', 'Amnon Shashua', 'Gal Beniamini', 'Yoav Levine', 'Or Sharir', 'Noam Wies', 'Ido Ben-Shaul', 'Tomer Nussbaum', 'Shir Granot Peled']","Artificial Expert Intelligence (AEI) seeks to transcend the limitations of both Artificial General Intelligence (AGI) and narrow AI by integrating domain-specific expertise with critical, precise reasoning capabilities akin to those of top human experts. Existing AI systems often excel at predefined tasks but struggle with adaptability and precision in novel problem-solving. To overcome this, AEI introduces a framework for ``Probably Approximately Correct (PAC) Reasoning"". This paradigm provides robust theoretical guarantees for reliably decomposing complex problems, with a practical mechanism for controlling reasoning precision. In reference to the division of human thought into System 1 for intuitive thinking and System 2 for reflective reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning as System 3 for precise reasoning, inspired by the rigor of the scientific method. AEI thus establishes a foundation for error-bounded, inference-time learning.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02441
The influence of the bar on the chaotic dynamics of globular clusters in the central region of the Galaxy,['Astrophysics of Galaxies'],"['Anisa Bajkova', 'Anton Smirnov', 'Vadim Bobylev']","The paper is devoted to the analysis of the influence of the galactic bar on the nature of the orbital motion (chaotic or regular) of globular clusters in the central region of the Galaxy with a radius of 3.5 kpc, which are subject to the greatest influence of the bar. The sample includes 45 globular clusters. To form the 6D phase space required for integrating the orbits, the most accurate astrometric data to date from the Gaia satellite (Vasiliev, Baumgardt, 2021) were used, as well as new refined average distances (Baumgardt, Vasiliev, 2021). The orbits of the globular clusters were obtained both in an axisymmetric potential and in a potential including the bar. The following, most realistic, bar parameters were adopted: mass $10^{10} M_\odot$, semi-major axis length 5 kpc, bar axis rotation angle 25$^o$, angular rotation velocity 40 km s$^{-1}$ kpc$^{-1}$. The analysis of the chaoticity/regularity of the orbital motion in both potentials was carried out using one of the most effective methods, namely, the frequency method, which consists in calculating the drift of fundamental frequencies. As a result, the influence of the bar on the dynamics of each GC of the sample was assessed. It is established that 8 GCs changed regular dynamics to chaotic under the influence of the bar, and 9 GCs changed chaotic dynamics to regular one.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02426
Efficient parallel inversion of ParaOpt preconditioners,['Numerical Analysis'],"['Corentin Bonte', 'Arne Bouillon', 'Giovanni Samaey', 'Karl Meerbergen']","Recently, the ParaOpt algorithm was proposed as an extension of the time-parallel Parareal method to optimal control. ParaOpt uses quasi-Newton steps that each require solving a system of matching conditions iteratively. The state-of-the-art parallel preconditioner for linear problems leads to a set of independent smaller systems that are currently hard to solve. We generalize the preconditioner to the nonlinear case and propose a new, fast inversion method for these smaller systems, avoiding disadvantages of the current options with adjusted boundary conditions in the subproblems.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02425
Hidden magnetic phases in i-MAX compounds,['Strongly Correlated Electrons'],"['Dror Yahav', 'Ariel Maniv', 'Daniel Potashnikov', 'Asaf Pesach', ""El'ad N. Caspi"", 'Arneil P. Reyes', 'Quanzheng Tao', 'Johanna Rosen', 'Eran Maniv']","We uncover a high-field magnetic phase in i-MAX compounds exhibiting a canted antiferromagnetic (AFM) order with unprecedented properties, revealed through NMR and AC susceptibility. Intriguingly, as the atomic number of Rare Earth increases, the transition field of this canted AFM phase grows at the expense of the lower-field AFM state. Our findings point to the complexity of the magnetic structure in i-MAX compounds, demonstrating a non-trivial evolution of their phase diagram while increasing both the atomic number of the Rare Earth element and the external field.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02418
Knowledge-Enhanced Conversational Recommendation via Transformer-based Sequential Modelling,['Information Retrieval'],"['Jie Zou', 'Aixin Sun', 'Cheng Long', 'Evangelos Kanoulas']","In conversational recommender systems (CRSs), conversations usually involve a set of items and item-related entities or attributes, e.g., director is a related entity of a movie. These items and item-related entities are often mentioned along the development of a dialog, leading to potential sequential dependencies among them. However, most of existing CRSs neglect these potential sequential dependencies. In this article, we first propose a Transformer-based sequential conversational recommendation method, named TSCR, to model the sequential dependencies in the conversations to improve CRS. In TSCR, we represent conversations by items and the item-related entities, and construct user sequences to discover user preferences by considering both the mentioned items and item-related entities. Based on the constructed sequences, we deploy a Cloze task to predict the recommended items along a sequence. Meanwhile, in certain domains, knowledge graphs formed by the items and their related entities are readily available, which provide various different kinds of associations among them. Given that TSCR does not benefit from such knowledge graphs, we then propose a knowledge graph enhanced version of TSCR, called TSCRKG. In specific, we leverage the knowledge graph to offline initialize our model TSCRKG, and augment the user sequence of conversations (i.e., sequence of the mentioned items and item-related entities in the conversation) with multi-hop paths in the knowledge graph. Experimental results demonstrate that our TSCR model significantly outperforms state-of-the-art baselines, and the enhanced version TSCRKG further improves recommendation performance on top of TSCR.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02415
A Multi-Agent Framework for Extensible Structured Text Generation in PLCs,['Software Engineering'],"['Donghao Yang', 'Aolang Wu', 'Tianyi Zhang', 'Li Zhang', 'Fang Liu', 'Xiaoli Lian', 'Yuming Ren', 'Jiaji Tian']","Programmable Logic Controllers (PLCs) are microcomputers essential for automating factory operations. Structured Text (ST), a high-level language adhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to express logic succinctly and to seamlessly integrate with other languages within the same standard. However, vendors develop their own customized versions of ST, and the lack of comprehensive and standardized documentation for the full semantics of ST has contributed to inconsistencies in how the language is implemented. Consequently, the steep learning curve associated with ST, combined with ever-evolving industrial requirements, presents significant challenges for developers. In response to these issues, we present AutoPLC, an LLM-based approach designed to automate the generation of vendor-specific ST code. To facilitate effective code generation, we first built a comprehensive knowledge base, including Rq2ST Case Library (requirements and corresponding implementations) and Instruction libraries. Then we developed a retrieval module to incorporate the domain-specific knowledge by identifying pertinent cases and instructions, guiding the LLM to generate code that meets the requirements. In order to verify and improve the quality of the generated code, we designed an adaptable code checker. If errors are detected, we initiate an iterative self-improvement process to instruct the LLM to revise the generated code. We evaluate AutoPLC's performance against seven state-of-the-art baselines using three benchmarks, one for open-source basic ST and two for commercial Structured Control Language (SCL) from Siemens. The results show that our approach consistently achieves superior performance across all benchmarks. Ablation study emphasizes the significance of our modules. Further manual analysis confirm the practical utility of the ST code generated by AutoPLC.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02410
Leveraging Ensemble-Based Semi-Supervised Learning for Illicit Account Detection in Ethereum DeFi Transactions,['Social and Information Networks'],"['Shabnam Fazliani', 'Mohammad Mowlavi Sorond', 'Arsalan Masoudifard']","The advent of smart contracts has enabled the rapid rise of Decentralized Finance (DeFi) on the Ethereum blockchain, offering substantial rewards in financial innovation and inclusivity. However, this growth has also introduced significant security risks, including the proliferation of illicit accounts involved in fraudulent activities. Traditional detection methods are limited by the scarcity of labeled data and the evolving tactics of malicious actors. In this paper, we propose a novel Self-Learning Ensemble-based Illicit account Detection (SLEID) framework to address these challenges. SLEID employs an Isolation Forest for initial outlier detection and a self-training mechanism to iteratively generate pseudo-labels for unlabeled accounts, thereby enhancing detection accuracy. Extensive experiments demonstrate that SLEID significantly outperforms traditional supervised approaches and recent semi-supervised models, achieving superior precision, recall, and F1-scores, particularly in detecting illicit accounts. Compared to state-of-the-art methods, our approach achieves better detection performance while reducing reliance on labeled data. The results affirm SLEID's efficacy as a robust solution for safeguarding the DeFi ecosystem and mitigating risks posed by malicious accounts.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02408
Dry Transfer Based on PMMA and Thermal Release Tape for Heterogeneous Integration of 2D-TMDC Layers,['Applied Physics'],"['Amir Ghiami', 'Hleb Fiadziushkin', 'Tianyishan Sun', 'Songyao Tang', 'Yibing Wang', 'Eva Mayer', 'Jochen M. Schneider', 'Agata Piacentini', 'Max C. Lemme', 'Michael Heuken', 'Holger Kalisch', 'Andrei Vescan']","A reliable and scalable transfer of 2D-TMDCs (two-dimensional transition metal dichalcogenides) from the growth substrate to a target substrate with high reproducibility and yield is a crucial step for device integration. In this work, we have introduced a scalable dry-transfer approach for 2D-TMDCs grown by MOCVD (metal-organic chemical vapor deposition) on sapphire. Transfer to a silicon/silicon dioxide (Si/SiO$_2$) substrate is performed using PMMA (poly(methyl methacrylate)) and TRT (thermal release tape) as sacrificial layer and carrier, respectively. Our proposed method ensures a reproducible peel-off from the growth substrate and better preservation of the 2D-TMDC during PMMA removal in solvent, without compromising its adhesion to the target substrate. A comprehensive comparison between the dry method introduced in this work and a standard wet transfer based on potassium hydroxide (KOH) solution shows improvement in terms of cleanliness and structural integrity for dry-transferred layer, as evidenced by X-ray photoemission and Raman spectroscopy, respectively. Moreover, fabricated field-effect transistors (FETs) demonstrate improvements in subthreshold slope, maximum drain current and device-to-device variability. The dry-transfer method developed in this work enables large-area integration of 2D-TMDC layers into (opto)electronic components with high reproducibility, while better preserving the as-grown properties of the layers.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02407
A Tractable Closed-Form Approximation of the Ergodic Rate in Poisson Cellular Networks,['Signal Processing'],"['Alexis I. Aravanis', 'Thanh Tu Lam', 'Olga Muñoz', 'Antonio Pascual-Iserte', 'Marco Di Renzo']","The employment of stochastic geometry for the analysis and design of ultra dense networks (UDNs) has provided significant insights into network densification. In addition to the characterization of the network performance and behavior, these tools can also be exploited toward solving complex optimization problems that could maximize the capacity benefits arising in UDNs. However, this is preconditioned on the existence of tractable closed form expressions for the considered figures of merit. In this course, the present paper introduces an accurate approximation for the moment generating function (MGF) of the aggregate other-cell interference created by base stations whose positions follow a Poisson point process of given spatial density. Given the pivotal role of the MGF of the aggregate interference in stochastic geometry and the tractability of the derived MGF, the latter can be employed to substantially simplify ensuing stochastic geometry analyses. Subsequently, the present paper employs the introduced MGF to provide closed form expressions for the downlink ergodic capacity for the interference limited case, and validates the accuracy of these expressions by the use of extensive Monte Carlo simulations. The derived expressions depend on the density of users and base stations, setting out a densification road map for network operators and designers of significant value.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02406
OMENN: One Matrix to Explain Neural Networks,['Machine Learning'],"['Adam Wróbel', 'Mikołaj Janusz', 'Bartosz Zieliński', 'Dawid Rymarczyk']","Deep Learning (DL) models are often black boxes, making their decision-making processes difficult to interpret. This lack of transparency has driven advancements in eXplainable Artificial Intelligence (XAI), a field dedicated to clarifying the reasoning behind DL model predictions. Among these, attribution-based methods such as LRP and GradCAM are widely used, though they rely on approximations that can be imprecise.
  To address these limitations, we introduce One Matrix to Explain Neural Networks (OMENN), a novel post-hoc method that represents a neural network as a single, interpretable matrix for each specific input. This matrix is constructed through a series of linear transformations that represent the processing of the input by each successive layer in the neural network. As a result, OMENN provides locally precise, attribution-based explanations of the input across various modern models, including ViTs and CNNs. We present a theoretical analysis of OMENN based on dynamic linearity property and validate its effectiveness with extensive tests on two XAI benchmarks, demonstrating that OMENN is competitive with state-of-the-art methods.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02399
Dynamical large deviations of the fractional Ornstein-Uhlenbeck process,['Statistical Mechanics'],"['Alexander Valov', 'Baruch Meerson']","The fractional Ornstein-Uhleneck (fOU) process is described by the overdamped Langevin equation $\dot{x}(t)+γx=\sqrt{2 D}ξ(t)$, where $ξ(t)$ is the fractional Gaussian noise with the Hurst exponent $0<H<1$. For $H\neq 1/2$ the fOU process is non-Markovian but Gaussian, and it has either vanishing (for $H<1/2$), or divergent (for $H>1/2$) spectral density at zero frequency. For $H>1/2$, the fOU is long-correlated. Here we study dynamical large deviations of the fOU process and focus on the area $A_n=\int_{-T}^{T} x^n(t) dt$, $n=1,2,\ldots$ over a long time window $2T$. Employing the optimal fluctuation method, we determine the optimal path of the conditioned process, which dominates the large-$A_n$ tail of the probability distribution of the area, $\mathcal{P}(A_n,T)\sim \exp[-S(A_n,T)]$. We uncover a nontrivial phase diagram of scaling behaviors of the optimal paths and of the action $S(A_n\equiv 2 a_n T,T)\sim T^{α(H,n)} a^{2/n}_n$ on the $(H,n)$ plane. The phase diagram includes three regions: (i) $H>1-1/n$, where $α(H,n)=2-2H$, and the optimal paths are delocalized, (ii) $n=2$ and $H\leq \frac{1}{2}$, where $α(H,n)=1$, and the optimal paths oscillate with an $H$-dependent frequency, and (iii) $H\leq 1-1/n$ and $n>2$, where $α(H,n)=2/n$, and the optimal paths are strongly localized. We verify our theoretical predictions in large-deviation simulations of the fOU process. By combining the Wang-Landau Monte-Carlo algorithm with the circulant embedding method of generation of stationary Gaussian fields, we were able to measure probability densities as small as $10^{-170}$. We also generalize our findings to other stationary Gaussian processes with either diverging, or vanishing spectral density at zero frequency.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02398
Hanke-Raus heuristic rule for iteratively regularized stochastic gradient descent,['Numerical Analysis'],"['Harshit Bajpai', 'Gaurav Mittal', 'Ankik Kumar Giri']","In this work, we present a novel variant of the stochastic gradient descent method termed as iteratively regularized stochastic gradient descent (IRSGD) method to solve nonlinear ill-posed problems in Hilbert spaces. Under standard assumptions, we demonstrate that the mean square iteration error of the method converges to zero for exact data. In the presence of noisy data, we first propose a heuristic parameter choice rule (HPCR) based on the method suggested by Hanke and Raus, and then apply the IRSGD method in combination with HPCR. Precisely, HPCR selects the regularization parameter without requiring any a-priori knowledge of the noise level. We show that the method terminates in finitely many steps in case of noisy data and has regularizing features. Further, we discuss the convergence rates of the method using well-known source and other related conditions under HPCR as well as discrepancy principle. To the best of our knowledge, this is the first work that establishes both the regularization properties and convergence rates of a stochastic gradient method using a heuristic type rule in the setting of infinite-dimensional Hilbert spaces. Finally, we provide the numerical experiments to showcase the practical efficacy of the proposed method.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02397
Bio-inspired visual relative localization for large swarms of UAVs,['Robotics'],"['Martin Křížek', 'Matouš Vrba', 'Antonella Barišić Kulaš', 'Stjepan Bogdan', 'Martin Saska']","We propose a new approach to visual perception for relative localization of agents within large-scale swarms of UAVs. Inspired by biological perception utilized by schools of sardines, swarms of bees, and other large groups of animals capable of moving in a decentralized yet coherent manner, our method does not rely on detecting individual neighbors by each agent and estimating their relative position, but rather we propose to regress a neighbor density over distance. This allows for a more accurate distance estimation as well as better scalability with respect to the number of neighbors. Additionally, a novel swarm control algorithm is proposed to make it compatible with the new relative localization method. We provide a thorough evaluation of the presented methods and demonstrate that the regressing approach to distance estimation is more robust to varying relative pose of the targets and that it is suitable to be used as the main source of relative localization for swarm stabilization.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02393
Fast ground-to-air transition with avian-inspired multifunctional legs,['Robotics'],"['Won Dong Shin', 'Hoang-Vu Phan', 'Monica A. Daley', 'Auke J. Ijspeert', 'Dario Floreano']","Most birds can navigate seamlessly between aerial and terrestrial environments. Whereas the forelimbs evolved into wings primarily for flight, the hindlimbs serve diverse functions such as walking, hopping, and leaping, and jumping take-off for transitions into flight. These capabilities have inspired engineers to aim for similar multi-modality in aerial robots, expanding their range of applications across diverse environments. However, challenges remain in reproducing multi-modal locomotion, across gaits with distinct kinematics and propulsive characteristics, such as walking and jumping, while preserving lightweight mass for flight. This tradeoff between mechanical complexity and versatility limits most existing aerial robots to only one additional locomotor mode. Here, we overcome the complexity-versatility tradeoff with RAVEN (Robotic Avian-inspired Vehicle for multiple ENvironments), which uses its bird-inspired multi-functional legs to jump rapidly into flight, walk on ground and hop over obstacles and gaps similar to the multi-modal locomotion of birds. We show that jumping for take-off contributes substantially to initial flight take-off speed and, remarkably, that it is more energy-efficient than solely propeller-based take-off. Our analysis suggests an important tradeoff in mass distribution between legs and body among birds adapted for different locomotor strategies, with greater investment in leg mass among terrestrial birds with multi-modal gait demands. Multi-functional robot legs expand opportunities to deploy traditional fixed-wing aircraft in complex terrains through autonomous take-offs and multi-modal gaits.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02389
Topological Valley Photonic Waveguides: Scattering matrix evaluation for linear computing,['Optics'],"['Christian Johnson-Richards', 'Alex Yakovlev', 'Victor Pacheco-Peña']","Topological boundary modes utilizing valley mode waveguides have opened opportunities in, for instance, the design of high transmission waveguides with tolerance to geometrical defects and sharp bends. Applications of these waveguides include linear computational processes and the emulation of logic gates using linear structures, among other scenarios. Here we present the design of a 6-port junction that exhibits equal power splitting to three other ports when excited at single port with no reflections. In studying this structure, a scattering matrix is extracted at telecom wavelengths (around 1550 nm). The linearity of the system along with the scattering matrix are exploited to produce linear operations such as routing of information considering two incident signals or multiple signals applied from different ports. Our work may be exploited to analytically design larger networks without the need of computationally expensive trial and error numerical methods.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02388
Soft-constrained output feedback guaranteed cost equilibria in infinite-horizon uncertain linear-quadratic differential games,['Optimization and Control'],"['Aniruddha Roy', 'Puduru Viswanadha Reddy']","In this paper, we study infinite-horizon linear-quadratic uncertain differential games with an output feedback information structure. We assume linear time-invariant nominal dynamics influenced by deterministic external disturbances, and players' risk preferences are expressed by a soft-constrained quadratic cost criterion over an infinite horizon. We demonstrate that the conditions available in the literature for the existence of a soft-constrained output feedback Nash equilibrium (SCONE) are too stringent to satisfy, even in low-dimensional games. To address this issue, using ideas from suboptimal control, we introduce the concept of a soft-constrained output feedback guaranteed cost equilibrium (SCOGCE). At an SCOGCE, the players' worst-case costs are upper-bounded by a specified cost profile while maintaining an equilibrium property. We show that SCOGCE strategies form a larger class of equilibrium strategies; that is, whenever an SCONE exists, it is also an SCOGCE. We demonstrate that sufficient conditions for the existence of SCOGCE are related to the solvability of a set of coupled bi-linear matrix inequalities. Using semi-definite programming relaxations, we provide linear matrix inequality-based iterative algorithms for the synthesis of SCOGCE strategies. Finally, we illustrate the performance of SCOGCE controllers with numerical examples.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02385
Theory building for empirical software engineering in qualitative research: Operationalization,['Software Engineering'],"['Jorge Pérez', 'Jessica Díaz', 'Ángel González-Prieto', 'Sergio Gil-Borrás']","Context: This work is part of a research project whose ultimate goal is to systematize theory building in qualitative research in the field of software engineering. The proposed methodology involves four phases: conceptualization, operationalization, testing, and application. In previous work, we performed the conceptualization of a theory that investigates the structure of IT departments and teams when software-intensive organizations adopt a culture called DevOps. Objective: This paper presents a set of procedures to systematize the operationalization phase in theory building and their application in the context of DevOps team structures. Method: We operationalize the concepts and propositions that make up our theory to generate constructs and empirically testable hypotheses. Instead of using causal relations to operationalize the propositions, we adopt logical implication, which avoids the problems associated with causal reasoning. Strategies are proposed to ensure that the resulting theory aligns with the criterion of parsimony. Results: The operationalization phase is described from three perspectives: specification, implementation, and practical application. First, the operationalization process is formally defined. Second, a set of procedures for operating both concepts and propositions is described. Finally, the usefulness of the proposed procedures is demonstrated in a case study. Conclusions: This paper is a pioneering contribution in offering comprehensive guidelines for theory operationalization using logical implication. By following established procedures and using concrete examples, researchers can better ensure the success of their theory-building efforts through careful operationalization.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02384
Social patch foraging theory in an egalitarian group,['Biological Physics'],"['Lisa Blum Moyse', 'Ahmed El Hady']","Foraging is a widespread behavior, and being part of a group may bring several benefits compared to solitary foraging, such as collective pooling of information and reducing environmental uncertainty. Often theoretical models of collective behavior use coarse-grained representations, or are too complex for analytical treatment, and generally do not take into account the noisy decision making process implemented by individual agents. This calls for the development of a mechanistic, analytically tractable, and stochastic framework to study the underlying processes of social foraging, tying the microscopic to the macroscopic levels. Based on an evidence accumulation framework, we developed a model of patch-leaving decisions in a large egalitarian group. Across a variety of environmental statistics and information sharing mechanisms, we were able to analytically derive optimal agent strategies. The environmental statistics considered are either two non-depleting or several successive depleting patches. The social information sharing mechanisms are either through observation of others' food rewards or through belief sharing, with continuous sharing, pulsatile observation of others' departures or arrivals, or through counting the number of individuals in a patch. Throughout all these conditions, we quantified how cohesive a group is over time, how much time agents spend on average in a patch and what are their group equilibrium dynamics. We found that social coupling strongly modulates these features across a variety of environmental statistics. This general modeling framework is crucial to both designing social foraging experiments and generating hypotheses that can be tested. Moreover, this framework can be extended to groups exhibiting hierarchical relations.△ Less",v1,https://arxiv.org/pdf/2412.02381
Photoelectron circular dichroism of a chiral molecule induced by resonant interatomic Coulombic decay from an antenna atom,['Atomic and Molecular Clusters'],"['Stefan Yoshi Buhmann', 'Andreas Hans', 'Janine C. Franz', 'Philipp V. Demekhin']","We show that a nonchiral atom can act as an antenna to induce a photoelectron circular dichroism in a nearby chiral molecule in a three-step process: The donor atom (antenna) is initially resonantly excited by circularly polarized radiation. It then transfers its excess energy to the acceptor molecule by means of resonant interatomic Coulombic decay. The latter finally absorbs the energy and emits an electron which exhibits the aforementioned circular dichroism in its angular distribution. We study the process on the basis of the retarded dipole--dipole interaction and report an asymptotic analytic expression for the distance-dependent chiral asymmetry of the photoelectron as induced by resonant interatomic Coulombic decay for random line-of-sight and acceptor orientations. In the nonretarded limit, the predicted chiral asymmetry is reversed as compared to that of a direct photoelectron circular dichroism of the molecule.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02377
Wandering and escaping: recoiling massive black holes in cosmological simulations,['Astrophysics of Galaxies'],"['Chi An Dong-Páez', 'Marta Volonteri', 'Yohan Dubois', 'Ricarda S. Beckmann', 'Maxime Trebitsch']","After a merger of two massive black holes (MBHs), the remnant receives a gravitational wave (GW) recoil kick that can have a strong effect on its future evolution. The magnitude of the kick ($v_\mathrm{recoil}$) depends on the mass ratio and the alignment of the spins and orbital angular momenta, therefore on the previous evolution of the MBHs. We investigate the cosmic effect of GW recoil by running for the first time a high-resolution cosmological simulation including GW recoil that depends on the MBH spins (evolved through accretion and mergers), masses and dynamics computed self-consistently. We also run a twin simulation without GW recoil. The simulations are run down to $z=4.4$. We find that GW recoil reduces the growth of merger remnants, and can have a significant effect on the MBH-galaxy correlations and the merger rate. We find large recoil kicks across all galaxy masses in the simulation, up to a few $10^{11}\,\rm M_\odot$. The effect of recoil can be significant even if the MBHs are embedded in a rotationally supported gaseous structure. We investigate the dynamics of recoiling MBHs and find that MBHs remain in the centre of the host galaxy for low $v_\mathrm{recoil}/v_\mathrm{esc}$ and escape rapidly for high $v_\mathrm{recoil}/v_\mathrm{esc}$. Only if $v_\mathrm{recoil}$ is comparable to $v_\mathrm{esc}$ the MBHs escape the central region of the galaxy but might remain as wandering MBHs until the end of the simulation. Recoiling MBHs are a significant fraction of the wandering MBH population. Although the dynamics of recoiling MBHs may be complex, some retain their initial radial orbits but are difficult to discern from other wandering MBHs on radial orbits. Others scatter with the halo substructure or circularise in the asymmetric potential. Our work highlights the importance of including GW recoil in cosmological simulation models.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02374
The Impact of Featuring Comments in Online Discussions,['Computation and Language'],"['Cedric Waterschoot', 'Ernst van den Hemel', 'Antal van den Bosch']","A widespread moderation strategy by online news platforms is to feature what the platform deems high quality comments, usually called editor picks or featured comments. In this paper, we compare online discussions of news articles in which certain comments are featured, versus discussions in which no comments are featured. We measure the impact of featuring comments on the discussion, by estimating and comparing the quality of discussions from the perspective of the user base and the platform itself. Our analysis shows that the impact on discussion quality is limited. However, we do observe an increase in discussion activity after the first comments are featured by moderators, suggesting that the moderation strategy might be used to increase user engagement and to postpone the natural decline in user activity over time.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02369
GenMix: Effective Data Augmentation with Generative Diffusion Model Image Editing,['Computer Vision and Pattern Recognition'],"['Khawar Islam', 'Muhammad Zaigham Zaheer', 'Arif Mahmood', 'Karthik Nandakumar', 'Naveed Akhtar']","Data augmentation is widely used to enhance generalization in visual classification tasks. However, traditional methods struggle when source and target domains differ, as in domain adaptation, due to their inability to address domain gaps. This paper introduces GenMix, a generalizable prompt-guided generative data augmentation approach that enhances both in-domain and cross-domain image classification. Our technique leverages image editing to generate augmented images based on custom conditional prompts, designed specifically for each problem type. By blending portions of the input image with its edited generative counterpart and incorporating fractal patterns, our approach mitigates unrealistic images and label ambiguity, improving the performance and adversarial robustness of the resulting models. Efficacy of our method is established with extensive experiments on eight public datasets for general and fine-grained classification, in both in-domain and cross-domain settings. Additionally, we demonstrate performance improvements for self-supervised learning, learning with data scarcity, and adversarial robustness. As compared to the existing state-of-the-art methods, our technique achieves stronger performance across the board.△ Less",v1,https://arxiv.org/pdf/2412.02366
Unirationality of instanton moduli space for small charges,['Algebraic Geometry'],"['Dimitri Markushevich', 'Alexander Tikhomirov']",The unirationality of the moduli space of mathematical instantons on the projective 3-space is proved for charges less than or equal to 7.△ Less,"3 December, 2024;",https://arxiv.org/pdf/2412.02363
Characterization of polarising components at cryogenic temperature,['Optics'],"['Thierry Chanelière', 'Alexei D. Chepelianskii']","Controlling polarisation directly at low temperature is crucial for development of optical spectroscopy techniques at sub-Kelvin temperatures, for example, in a hybrid scheme where light is fed into and collected in the cryostat by fibres that are as easy to install as electrical wiring, but where distortions in the fibre need to be compensated for by discrete polarising optical components. The latter are poorly characterised at low temperatures. So we cool-down polarising components from room temperature to 4K and monitor the evolution of the polarisation properties in this range. We test a zero-order half-wave plate, a polarising beamsplitting cube and a dichroic polariser in the optical telecommunication range at 1.5$μ$m. We show that the polarisation is maintained at the $10^{-4}$ level within the whole temperature range. This is consistent with the typical thermal contraction of optical materials. This level of precision is sufficient for many optics experiments at low temperature. We argue that these experiments will allow the design of compact fibre based probes for cryogenic surfaces.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02362
Probing jet dynamics and collimation in radio galaxies. Application to NGC 1052,['High Energy Astrophysical Phenomena'],"['Ainara Saiz-Pérez', 'Christian M. Fromm', 'Manel Perucho', 'Oliver Porth', 'Matthias Kadler', 'Yosuke Mizuno', 'Andrew Chael', 'Karl Mannheim']","Context. Radio galaxies with visible two-sided jet structures, such as NGC 1052, are sources of particular interest to study the collimation and shock structure of active galactic nuclei jets. High-resolution very-long-baseline interferometry observations of such sources can resolve and study the jet collimation profile and probe different physical mechanisms. Aims. In this paper, we study the physics of double-sided radio sources at parsec scales, and in particular investigate whether propagating shocks can give rise to the observed asymmetry between jet and counterjet. Methods. We carry out special relativistic hydrodynamic simulations and perform radiative transfer calculations of an over-pressured perturbed jet. During the radiative transfer calculations we incorporate both thermal and nonthermal emission while taking the finite speed of light into account. To further compare our results to observations, we create more realistic synthetic data including the properties of the observing array as well as the image reconstruction via multifrequency regularized maximum likelihood methods. We finally introduce a semi-automatized method for tracking jet components and extracting jet kinematics. Results. We show that propagating shocks in an inherently symmetric double-sided jet can lead to partially asymmetric jet collimation profiles due to time delay effects and relativistic beaming. These asymmetries may appear on specific epochs, with one jet evolving near conically and the other one parabolically (width profile evolving with a slope of 1 and 0.5, respectively). However, these spurious asymmetries are not significant when observing the source evolve for an extended amount of time. Conclusions. Purely observational effects are not enough to explain a persisting asymmetry in the jet collimation profile of double-sided jet sources and hint at evidence for asymmetrically launched jets.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02358
Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks,['Human-Computer Interaction'],"['Ian Drosos', 'Jack Williams', 'Advait Sarkar', 'Nicholas Wilson']","Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.
  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02357
TITE-CLRM: Towards efficient time-to-event dose-escalation guidance of multi-cycle cancer therapies,['Applications'],"['Lukas Andreas Widmer', 'Sebastian Weber', 'Yunnan Xu', 'Hans-Jochen Weber']","Treatment of cancer has rapidly evolved over time in quite dramatic ways, for example from chemotherapies, targeted therapies to immunotherapies and chimeric antigen receptor T-cells. Nonetheless, the basic design of early phase I trials in oncology still follows pre-dominantly a dose-escalation design. These trials monitor safety over the first treatment cycle in order to escalate the dose of the investigated drug. However, over time studying additional factors such as drug combinations and/or variation in the timing of dosing became important as well. Existing designs were continuously enhanced and expanded to account for increased trial complexity. With toxicities occurring at later stages beyond the first cycle and the need to treat patients over multiple cycles, the focus on the first treatment cycle only is becoming a limitation in nowadays multi-cycle treatment therapies. Here we introduce a multi-cycle time-to-event model (TITE-CLRM: Time-Interval-To-Event Complementary-Loglog Regression Model) allowing guidance of dose-escalation trials studying multi-cycle therapies. The challenge lies in balancing the need to monitor safety of longer treatment periods with the need to continuously enroll patients safely. The proposed multi-cycle time to event model is formulated as an extension to established concepts like the escalation with over dose control principle. The model is motivated from a current drug development project and evaluated in a simulation study.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02355
Reverse Carleson measures for spaces of analytic functions,['Complex Variables'],"['Evgueni Doubtsov', 'Anton Tselishchev', 'Ioann Vasilyev']","Let $X$ be a quasi-Banach space of analytic functions in the unit disc and let $q>0$. A finite positive Borel measure $μ$ in the closed unit disc $\overline{\mathbb{D}}$ is called a $q$-reverse Carleson measure for $X$ if and only if there exists a constant $C>0$ such that $$\|f\|_{X}\leq C \|f\|_{L^q(\overline{\mathbb D},dμ)} $$ for all $f\in X\cap C(\overline{\mathbb D})$. We fully characterize the $q$-reverse Carleson measures with all $q>0$ for Hardy spaces $H^p(\mathbb D)$ with all $0<p\leq \infty$, for the space $\mathrm{BMOA}(\mathbb D)$ and for the Bloch space. In addition, we describe $q$-reverse Carleson measures for the holomorphic Triebel--Lizorkin spaces $HF_0^{q,r}$ and the holomorphic Besov spaces $HB_0^{q,r}$. Related results are obtained for the Hardy spaces and certain holomorphic Triebel--Lizorkin spaces in the unit ball of $\mathbb{C}^d$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02354
LoRA Diffusion: Zero-Shot LoRA Synthesis for Diffusion Model Personalization,['Machine Learning'],"['Ethan Smith', 'Rami Seid', 'Alberto Hojel', 'Paramita Mishra', 'Jianbo Wu']","Low-Rank Adaptation (LoRA) and other parameter-efficient fine-tuning (PEFT) methods provide low-memory, storage-efficient solutions for personalizing text-to-image models. However, these methods offer little to no improvement in wall-clock training time or the number of steps needed for convergence compared to full model fine-tuning. While PEFT methods assume that shifts in generated distributions (from base to fine-tuned models) can be effectively modeled through weight changes in a low-rank subspace, they fail to leverage knowledge of common use cases, which typically focus on capturing specific styles or identities. Observing that desired outputs often comprise only a small subset of the possible domain covered by LoRA training, we propose reducing the search space by incorporating a prior over regions of interest. We demonstrate that training a hypernetwork model to generate LoRA weights can achieve competitive quality for specific domains while enabling near-instantaneous conditioning on user input, in contrast to traditional training methods that require thousands of steps.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02352
Infinitesimal $\mathcal{R}$-matrices for some families of Hopf algebras,['Quantum Algebra'],"['Lucrezia Bottegoni', 'Fabio Renda', 'Andrea Sciandra']","Given a bialgebra $H$ such that the associated trivial topological bialgebra $H[[\hbar]]$ admits a quasitriangular structure $\tilde{\mathcal{R}}=\mathcal{R}(1\otimes 1+\hbarχ+\mathcal{O}(\hbar^2))$, one gets a distinguished element $χ\in H \otimes H$ which is an infinitesimal $\mathcal{R}$-matrix, according to the definition given in [1]. In this paper we classify infinitesimal $\mathcal{R}$-matrices for some families of well-known Hopf algebras, among which are the generalized Kac-Paljutkin Hopf algebras $H_{2n^2}$, the Radford Hopf algebras $H_{(r,n,q)}$, and the Hopf algebras $E(n)$.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02350
The mass of the gluino-glue bound state in large-$N$ $\mathcal{N}=1$ Supersymmetric Yang-Mills theory,['High Energy Physics - Lattice'],"['Claudio Bonanno', 'Margarita García Pérez', 'Antonio González-Arroyo', 'Ken-Ichi Ishikawa', 'Masanori Okawa']","We provide a first-principles non-perturbative determination of the mass of the lightest gluino-gluon bound state (gluino-glue) in large-$N$ $\mathcal{N}=1$ Supersymmetric Yang--Mills theory by means of numerical Monte Carlo simulations of the lattice-discretized theory, and exploiting large-$N$ twisted volume reduction. Our large-$N$ determination is consistent with naive extrapolation of previously-known $\mathrm{SU}(2)$ and $\mathrm{SU}(3)$ results.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02348
The convergence Newton polygon of a $p$-adic differential equation IV : controlling graphs,['Number Theory'],"['Jérôme Poineau', 'Andrea Pulita']","In our previous works we proved a finiteness property of the radii of convergence functions associated with a vector bundle with connection on $p$-adic analytic curves. We showed that the radii are locally constant functions outside a locally finite graph in the curve, called controlling graph. In this paper we refine that finiteness results by giving a bound on the size of the controlling graph in terms of the geometry of the curve and the rank of the module. This is based on super-harmonicity properties of radii of convergence and partial heights of the Newton polygon. Under suitable assumptions, we relate the size of the controlling graph associated with the total height of the convergence Newton polygon to the Euler characteristic in the sense of de Rham cohomology.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02341
"Federated Analytics in Practice: Engineering for Privacy, Scalability and Practicality",['Machine Learning'],"['Harish Srinivas', 'Graham Cormode', 'Mehrdad Honarkhah', 'Samuel Lurye', 'Jonathan Hehir', 'Lunwen He', 'George Hong', 'Ahmed Magdy', 'Dzmitry Huba', 'Kaikai Wang', 'Shen Guo', 'Shoubhik Bhattacharya']","Cross-device Federated Analytics (FA) is a distributed computation paradigm designed to answer analytics queries about and derive insights from data held locally on users' devices. On-device computations combined with other privacy and security measures ensure that only minimal data is transmitted off-device, achieving a high standard of data protection. Despite FA's broad relevance, the applicability of existing FA systems is limited by compromised accuracy; lack of flexibility for data analytics; and an inability to scale effectively. In this paper, we describe our approach to combine privacy, scalability, and practicality to build and deploy a system that overcomes these limitations. Our FA system leverages trusted execution environments (TEEs) and optimizes the use of on-device computing resources to facilitate federated data processing across large fleets of devices, while ensuring robust, defensible, and verifiable privacy safeguards. We focus on federated analytics (statistics and monitoring), in contrast to systems for federated learning (ML workloads), and we flag the key differences.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02340
Sample Efficient Robot Learning in Supervised Effect Prediction Tasks,['Robotics'],"['Mehmet Arda Eren', 'Erhan Oztop']","In self-supervised robot learning, robots actively explore their environments and generate data by acting on entities in the environment. Therefore, an exploration policy is desired that ensures sample efficiency to minimize robot execution costs while still providing accurate learning. For this purpose, the robotic community has adopted Intrinsic Motivation (IM)-based approaches such as Learning Progress (LP). On the machine learning front, Active Learning (AL) has been used successfully, especially for classification tasks. In this work, we develop a novel AL framework geared towards robotics regression tasks, such as action-effect prediction and, more generally, for world model learning, which we call MUSEL - Model Uncertainty for Sample Efficient Learning. MUSEL aims to extract model uncertainty from the total uncertainty estimate given by a suitable learning engine by making use of earning progress and input diversity and use it to improve sample efficiency beyond the state-of-the-art action-effect prediction methods. We demonstrate the feasibility of our model by using a Stochastic Variational Gaussian Process (SVGP) as the learning engine and testing the system on a set of robotic experiments in simulation. The efficacy of MUSEL is demonstrated by comparing its performance to standard methods used in robot action-effect learning. In a robotic tabletop environment in which a robot manipulator is tasked with learning the effect of its actions, the experiments show that MUSEL facilitates higher accuracy in learning action effects while ensuring sample efficiency.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02331
GRAND : Graph Reconstruction from potential partial Adjacency and Neighborhood Data,['Cryptography and Security'],"['Sofiane Azogagh', 'Zelma Aubin Birba', 'Josée Desharnais', 'Sébastien Gambs', 'Marc-Olivier Killijian', 'Nadia Tawbi']","Cryptographic approaches, such as secure multiparty computation, can be used to compute in a secure manner the function of a distributed graph without centralizing the data of each participant. However, the output of the protocol itself can leak sensitive information about the structure of the original graph. In particular, in this work we propose an approach by which an adversary observing the result of a private protocol for the computation of the number of common neighbors between all pairs of vertices, can reconstruct the adjacency matrix of the graph. In fact, this can only be done up to co-squareness, a notion we introduce, as two different graphs can have the same matrix of common neighbors. We consider two models of adversary, one who observes the common neighbors matrix only, and a knowledgeable one, that has a partial knowledge of the original graph. Our results demonstrate that secure multiparty protocols are not enough for privacy protection, especially in the context of highly structured data such as graphs. The reconstruction that we propose is interesting in itself from the point of view of graph theory.△ Less",v1,https://arxiv.org/pdf/2412.02329
Efficient Model Compression Techniques with FishLeg,['Machine Learning'],"['Jamie McGowan', 'Wei Sheng Lai', 'Weibin Chen', 'Henry Aldridge', 'Jools Clarke', 'Jezabel Garcia', 'Rui Xia', 'Yilei Liang', 'Guillaume Hennequin', 'Alberto Bernacchia']","In many domains, the most successful AI models tend to be the largest, indeed often too large to be handled by AI players with limited computational resources. To mitigate this, a number of compression methods have been developed, including methods that prune the network down to high sparsity whilst retaining performance. The best-performing pruning techniques are often those that use second-order curvature information (such as an estimate of the Fisher information matrix) to score the importance of each weight and to predict the optimal compensation for weight deletion. However, these methods are difficult to scale to high-dimensional parameter spaces without making heavy approximations. Here, we propose the FishLeg surgeon (FLS), a new second-order pruning method based on the Fisher-Legendre (FishLeg) optimizer. At the heart of FishLeg is a meta-learning approach to amortising the action of the inverse FIM, which brings a number of advantages. Firstly, the parameterisation enables the use of flexible tensor factorisation techniques to improve computational and memory efficiency without sacrificing much accuracy, alleviating challenges associated with scalability of most second-order pruning methods. Secondly, directly estimating the inverse FIM leads to less sensitivity to the amplification of stochasticity during inversion, thereby resulting in more precise estimates. Thirdly, our approach also allows for progressive assimilation of the curvature into the parameterisation. In the gradual pruning regime, this results in a more efficient estimate refinement as opposed to re-estimation. We find that FishLeg achieves higher or comparable performance against two common baselines in the area, most notably in the high sparsity regime when considering a ResNet18 model on CIFAR-10 (84% accuracy at 95% sparsity vs 60% for OBS) and TinyIM (53% accuracy at 80% sparsity vs 48% for OBS).△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02328
Six-dimensional complex solvmanifolds with non-invariant trivializing sections of their canonical bundle,['Differential Geometry'],['Alejandro Tolcachier'],"It is known that there exist complex solvmanifolds $(Γ\backslash G,J)$ whose canonical bundle is trivialized by a holomorphic section which is not invariant under the action of $G$. The main goal of this article is to classify the six-dimensional Lie algebras corresponding to such complex solvmanifolds, thus extending the previous work of Fino, Otal and Ugarte for the invariant case. To achieve this, we complete the classification of six-dimensional solvable strongly unimodular Lie algebras admitting complex structures and identify among them, the ones admitting complex structures with Chern-Ricci flat metrics. Finally we construct complex solvmanifolds with non-invariant holomorphic sections of their canonical bundle. In particular, we present an example of one such solvmanifold that is not biholomorphic to a complex solvmanifold with an invariant section of its canonical bundle. Additionally, we discover a new $6$-dimensional solvable strongly unimodular Lie algebra equipped with a complex structure that has a non-zero holomorphic $(3,0)$-form.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02325
Anomalous Hall and Nernst effect switching via staggered rotation in a kagome antiferromagnetic semimetal,['Materials Science'],"['Subhadip Pradhan', 'Kartik Samanta', 'Ashis K. Nandy']","The intricate interplay between magnetism and the topology of electronic structures provides a rich avenue for tailoring materials with unique and potent anomalous transport properties. In this paper, we present a strategy for inducing robust Berry curvature and anomalous transverse conductivity in noncollinear antiferromagnets through an unconventional approach termed ``small \textit{staggered rotation} of spin"". Considering noncollinear Mn$_3$Sn, we demonstrate that the positive vector chirality antiferromagnetic configuration, typically associated with a vanishing anomalous Hall effect and Nernst effect, can be manipulated to exhibit finite anomalous Hall conductivity (AHC) and anomalous Nernst conductivity (ANC) through \textit{staggered rotation}. Furthermore, we illustrate that the value and sign of both the AHC and ANC can be tuned through \textit{staggered rotation}. This tuning is intricately influenced by the spin-orbit coupling (SOC) induced gapped nodal line, revealing the critical role of electronic structure modifications in achieving precise control over transport properties.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02324
Quantum state transfer with sufficient fidelity,['Quantum Physics'],"['Luc Vinet', 'Alexei Zhedanov']","We consider an inhomogeneous XX spin chain which interpolates between the Krawtchouk one with perfect state transfer and the homogeneous XX chain. This model can be exploited in order to perform state transfer of a qubit with sufficiently good fidelity. The advantage of this model with respect to the Krawtchouk chain is that while it achieves efficient state transfer, the coupling strengths are capped and do not become excessively large as the number of sites grows.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02321
Design of thermal meta-structures made of functionally graded materials using isogeometric density-based topology optimization,"['Computational Engineering, Finance, and Science']","['Chintan Jansari', 'Stéphane P. A. Bordas', 'Marco Montemurro', 'Elena Atroshchenko']","The thermal conductivity of Functionally Graded Materials (FGMs) can be efficiently designed through topology optimization to obtain thermal meta-structures that actively steer the heat flow. Compared to conventional analytical design methods, topology optimization allows handling arbitrary geometries, boundary conditions and design requirements; and producing alternate designs for non-unique problems. Additionally, as far as the design of meta-structures is concerned, topology optimization does not need intuition-based coordinate transformation or the form invariance of governing equations, as in the case of transformation thermotics. We explore isogeometric density-based topology optimization in the continuous setting, which perfectly aligns with FGMs. In this formulation, the density field, geometry and solution of the governing equations are parameterized using non-uniform rational basis spline entities. Accordingly, the heat conduction problem is solved using Isogeometric Analysis. We design various 2D & 3D thermal meta-structures under different design scenarios to showcase the effectiveness and versatility of our approach. We also design thermal meta-structures based on architected cellular materials, a special class of FGMs, using their empirical material laws calculated via numerical homogenization.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02318
Optimizing Plastic Waste Collection in Water Bodies Using Heterogeneous Autonomous Surface Vehicles with Deep Reinforcement Learning,['Robotics'],"['Alejandro Mendoza Barrionuevo', 'Samuel Yanes Luis', 'Daniel Gutiérrez Reina', 'Sergio L. Toral Marín']","This paper presents a model-free deep reinforcement learning framework for informative path planning with heterogeneous fleets of autonomous surface vehicles to locate and collect plastic waste. The system employs two teams of vehicles: scouts and cleaners. Coordination between these teams is achieved through a deep reinforcement approach, allowing agents to learn strategies to maximize cleaning efficiency. The primary objective is for the scout team to provide an up-to-date contamination model, while the cleaner team collects as much waste as possible following this model. This strategy leads to heterogeneous teams that optimize fleet efficiency through inter-team cooperation supported by a tailored reward function. Different trainings of the proposed algorithm are compared with other state-of-the-art heuristics in two distinct scenarios, one with high convexity and another with narrow corridors and challenging access. According to the obtained results, it is demonstrated that deep reinforcement learning based algorithms outperform other benchmark heuristics, exhibiting superior adaptability. In addition, training with greedy actions further enhances performance, particularly in scenarios with intricate layouts.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02316
Latitude Quenching Nonlinearity in the Solar Dynamo,['Solar and Stellar Astrophysics'],"['Anthony R. Yeates', 'Luca Bertello', 'Alexander A. Pevtsov', 'Alexei A. Pevtsov']","We compare two candidate nonlinearities for regulating the solar cycle within the Babcock-Leighton paradigm: tilt quenching (whereby the tilt of active regions is reduced in stronger cycles) and latitude quenching (whereby flux emerges at higher latitudes in stronger solar cycles). Digitized historical observations are used to build a database of individual magnetic plage regions from 1923 to 1985. The regions are selected by thresholding in Ca II K synoptic maps, with polarities constrained using Mount Wilson Observatory sunspot measurements. The resulting data show weak evidence for tilt quenching, but much stronger evidence for latitude-quenching. Further, we use proxy observations of the polar field from faculae to construct a best-fit surface flux transport model driven by our database of emerging regions. A better fit is obtained when the sunspot measurements are used, compared to a reference model where all polarities are filled using Hale's Law. The optimization suggests clearly that the ""dynamo effectivity range"" of the Sun during this period should be less than 10 degrees; this is also consistent with latitude quenching being dominant over tilt quenching.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02312
An enhanced single Gaussian point continuum finite element formulation using automatic differentiation,"['Computational Engineering, Finance, and Science']","['Njomza Pacolli', 'Ahmad Awad', 'Jannick Kehls', 'Bjorn Sauren', 'Sven Klinkel', 'Stefanie Reese', 'Hagen Holthusen']","This contribution presents an improved low-order 3D finite element formulation with hourglass stabilization using automatic differentiation (AD). Here, the former Q1STc formulation is enhanced by an approximation-free computation of the inverse Jacobian. To this end, AD tools automate the computation and allow a direct evaluation of the inverse Jacobian, bypassing the need for a Taylor series expansion. Thus, the enhanced version, Q1STc+, is introduced. Numerical examples are conducted to compare the performance of both element formulations for finite strain applications, with particular focus on distorted meshes. Moreover, the performance of the new element formulation for an elasto-plastic material is investigated. To validate the obtained results, a volumetric locking-free reference element based on scaled boundary parametrization is used. Both the implementation of the element routine Q1STc+ and the corresponding material subroutine are made accessible to the public at https://doi.org/10.5281/zenodo.14259791△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02309
Large Multimodal Agents for Accurate Phishing Detection with Enhanced Token Optimization and Cost Reduction,['Artificial Intelligence'],"['Fouad Trad', 'Ali Chehab']","With the rise of sophisticated phishing attacks, there is a growing need for effective and economical detection solutions. This paper explores the use of large multimodal agents, specifically Gemini 1.5 Flash and GPT-4o mini, to analyze both URLs and webpage screenshots via APIs, thus avoiding the complexities of training and maintaining AI systems. Our findings indicate that integrating these two data types substantially enhances detection performance over using either type alone. However, API usage incurs costs per query that depend on the number of input and output tokens. To address this, we propose a two-tiered agentic approach: initially, one agent assesses the URL, and if inconclusive, a second agent evaluates both the URL and the screenshot. This method not only maintains robust detection performance but also significantly reduces API costs by minimizing unnecessary multi-input queries. Cost analysis shows that with the agentic approach, GPT-4o mini can process about 4.2 times as many websites per $100 compared to the multimodal approach (107,440 vs. 25,626), and Gemini 1.5 Flash can process about 2.6 times more websites (2,232,142 vs. 862,068). These findings underscore the significant economic benefits of the agentic approach over the multimodal method, providing a viable solution for organizations aiming to leverage advanced AI for phishing detection while controlling expenses.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02301
FL-QDSNNs: Federated Learning with Quantum Dynamic Spiking Neural Networks,['Quantum Physics'],"['Nouhaila Innan', 'Alberto Marchisio', 'Muhammad Shafique']","This paper introduces the Federated Learning-Quantum Dynamic Spiking Neural Networks (FL-QDSNNs) framework, an innovative approach specifically designed to tackle significant challenges in distributed learning systems, such as maintaining high accuracy while ensuring privacy. Central to our framework is a novel dynamic threshold mechanism for activating quantum gates in Quantum Spiking Neural Networks (QSNNs), which mimics classical activation functions while uniquely exploiting quantum operations to enhance computational performance. This mechanism is essential for tackling the typical performance variability across dynamically changing data distributions, a prevalent challenge in conventional QSNNs applications. Validated through extensive testing on datasets including Iris, digits, and breast cancer, our FL-QDSNNs framework has demonstrated superior accuracies-up to 94% on the Iris dataset and markedly outperforms existing Quantum Federated Learning (QFL) approaches. Our results reveal that our FL-QDSNNs framework offers scalability with respect to the number of clients, provides improved learning capabilities, and represents a robust solution to privacy and efficiency limitations posed by emerging quantum hardware and complex QSNNs training protocols. By fundamentally advancing the operational capabilities of QSNNs in real-world distributed environments, this framework can potentially redefine the application landscape of quantum computing in sensitive and critical sectors, ensuring enhanced data security and system performance.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02293
Solvability of a transmission problem in $L^p$-spaces with generalized diffusion equation,['Analysis of PDEs'],['Alexandre Thorel'],"We study a transmission problem, in population dynamics, between two juxtaposed habitats. In each habitat, we consider a generalized diffusion equation composed by the Laplace operator and a biharmonic term. We consider that the coefficients in front of each term could be negative or null. Using semigroups theory and functional calculus, we give some relation between coefficients to obtain the existence and the uniqueness of the classical solution in $L^p$-spaces.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02288
Viscous cosmological fluids and large-scale structure,['General Relativity and Quantum Cosmology'],"['BG Mbewe', 'RR Mekuria', 'S Sahlu', 'A Abebe']","In this paper, we study the viscous fluid cosmological model that when certain conditions are invoked mimics the $Λ$CDM model. The background equations governing the evolution of viscous interacting fluids in a multifluid system are derived. The Markov Chain Monte Carlo (MCMC) simulation is applied to constrain the best-fit cosmological parameters with Supernova Type 1a data. In addition, linear cosmological perturbations are investigated in a dust-matter-dominated frame using a $1+3$ covariant formalism approach. It is evident from the perturbation results obtained that the model predicts the disintegration of bound structures of large-scale structures in the late-time universe.△ Less",v1,https://arxiv.org/pdf/2412.02276
Step-by-Step Guidance to Differential Anemia Diagnosis with Real-World Data and Deep Reinforcement Learning,['Machine Learning'],"['Lillian Muyama', 'Estelle Lu', 'Geoffrey Cheminet', 'Jacques Pouchot', 'Bastien Rance', 'Anne-Isabelle Tropeano', 'Antoine Neuraz', 'Adrien Coulet']","Clinical diagnostic guidelines outline the key questions to answer to reach a diagnosis. Inspired by guidelines, we aim to develop a model that learns from electronic health records to determine the optimal sequence of actions for accurate diagnosis. Focusing on anemia and its sub-types, we employ deep reinforcement learning (DRL) algorithms and evaluate their performance on both a synthetic dataset, which is based on expert-defined diagnostic pathways, and a real-world dataset. We investigate the performance of these algorithms across various scenarios. Our experimental results demonstrate that DRL algorithms perform competitively with state-of-the-art methods while offering the significant advantage of progressively generating pathways to the suggested diagnosis, providing a transparent decision-making process that can guide and explain diagnostic reasoning.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02273
BOTracle: A framework for Discriminating Bots and Humans,['Machine Learning'],"['Jan Kadel', 'August See', 'Ritwik Sinha', 'Mathias Fischer']","Bots constitute a significant portion of Internet traffic and are a source of various issues across multiple domains. Modern bots often become indistinguishable from real users, as they employ similar methods to browse the web, including using real browsers. We address the challenge of bot detection in high-traffic scenarios by analyzing three distinct detection methods. The first method operates on heuristics, allowing for rapid detection. The second method utilizes, well known, technical features, such as IP address, window size, and user agent. It serves primarily for comparison with the third method. In the third method, we rely solely on browsing behavior, omitting all static features and focusing exclusively on how clients behave on a website. In contrast to related work, we evaluate our approaches using real-world e-commerce traffic data, comprising 40 million monthly page visits. We further compare our methods against another bot detection approach, Botcha, on the same dataset. Our performance metrics, including precision, recall, and AUC, reach 98 percent or higher, surpassing Botcha.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02266
Composing Open-domain Vision with RAG for Ocean Monitoring and Conservation,['Computer Vision and Pattern Recognition'],"['Sepand Dyanatkar', 'Angran Li', 'Alexander Dungate']","Climate change's destruction of marine biodiversity is threatening communities and economies around the world which rely on healthy oceans for their livelihoods. The challenge of applying computer vision to niche, real-world domains such as ocean conservation lies in the dynamic and diverse environments where traditional top-down learning struggle with long-tailed distributions, generalization, and domain transfer. Scalable species identification for ocean monitoring is particularly difficult due to the need to adapt models to new environments and identify rare or unseen species. To overcome these limitations, we propose leveraging bottom-up, open-domain learning frameworks as a resilient, scalable solution for image and video analysis in marine applications. Our preliminary demonstration uses pretrained vision-language models (VLMs) combined with retrieval-augmented generation (RAG) as grounding, leaving the door open for numerous architectural, training and engineering optimizations. We validate this approach through a preliminary application in classifying fish from video onboard fishing vessels, demonstrating impressive emergent retrieval and prediction capabilities without domain-specific training or knowledge of the task itself.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02262
Vision Transformers for Weakly-Supervised Microorganism Enumeration,['Computer Vision and Pattern Recognition'],"['Javier Ureña Santiago', 'Thomas Ströhle', 'Antonio Rodríguez-Sánchez', 'Ruth Breu']","Microorganism enumeration is an essential task in many applications, such as assessing contamination levels or ensuring health standards when evaluating surface cleanliness. However, it's traditionally performed by human-supervised methods that often require manual counting, making it tedious and time-consuming. Previous research suggests automating this task using computer vision and machine learning methods, primarily through instance segmentation or density estimation techniques. This study conducts a comparative analysis of vision transformers (ViTs) for weakly-supervised counting in microorganism enumeration, contrasting them with traditional architectures such as ResNet and investigating ViT-based models such as TransCrowd. We trained different versions of ViTs as the architectural backbone for feature extraction using four microbiology datasets to determine potential new approaches for total microorganism enumeration in images. Results indicate that while ResNets perform better overall, ViTs performance demonstrates competent results across all datasets, opening up promising lines of research in microorganism enumeration. This comparative study contributes to the field of microbial image analysis by presenting innovative approaches to the recurring challenge of microorganism enumeration and by highlighting the capabilities of ViTs in the task of regression counting.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02250
Analytical and numerical studies of periodic superradiance,['Atomic Physics'],"['Hideaki Hara', 'Yuki Miyamoto', 'Junseok Han', 'Riku Omoto', 'Yasutaka Imai', 'Akihiro Yoshimi', 'Koji Yoshimura', 'Motohiko Yoshimura', 'Noboru Sasao']","We conduct a theoretical study to understand the periodic superradiance observed in an Er:YSO crystal. First, we construct a model based on the Maxwell-Bloch equations for a reduced level system, a pair of superradiance states and a population reservoir state. We then derive two-variable equations consisting of the coherence and population difference between the two superradiance states, which contain the essential feature of the periodic superradiance. The two-variable equations clarify a mathematical structure of this periodic phenomenon and give analytical forms of the period, pulse duration, and number of emitted photons. Furthermore, analysis of the eigenvalues of the linearized differential equations shows that periodic superradiance can be realized only for certain parameters. Our model successfully reproduces the periodic behavior, but the actual experimental parameters are found to be outside the parameter region for the periodic superradiance. This result implies that some other mechanism(s) is required. As one example, assuming that the field decay rate varies with the electric field, the periodic superradiance can be reproduced even under the actual experimental condition.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02248
Development and Performance of a Static Pluviometer System,['Systems and Control'],"['Parth Saxena', 'Pratham Saxena', 'Adarsh Sowcar', 'Sreeharsha Angara', 'Ashutosh Pandey', 'Thomas Basikolo']","As the frequency and severity of climate-related events such as droughts, floods, and water scarcity continue to escalate, accurate rainfall monitoring becomes increasingly critical. This paper covers various industry methods of measuring rainfall as well as our own ground pluviometer system. Our system consists of an inexpensive static rain gauge that can operate for approximately six to twelve months without maintenance. It utilizes resistive sensing technology accompanied by a microcontroller to measure the water level depth from the device vessel, recording rainfall at an hourly rate. This study also provides a side-by-side comparison of our pluviometer system with an industry rain gauge, the MeteoRain 200 Compact, from Barani Systems, with the differences in data being statistically insignificant. By prioritizing cost, sustainability, simplicity, ease of maintenance, and assembly, this research contributes to essential rainfall monitoring solutions, specifically for developing countries.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02247
A visibility-based angular bispectrum estimator for radio-interferometric data,['Cosmology and Nongalactic Astrophysics'],"['Sukhdeep Singh Gill', 'Somnath Bharadwaj', 'Sk. Saiyad Ali', 'Khandakar Md Asif Elahi']","Considering radio-interferometric observations, we present a fast and efficient estimator to compute the binned angular bispectrum (ABS) from gridded visibility data. The estimator makes use of Fast Fourier Transform (FFT) techniques to compute the bispectrum covering all possible triangle shapes and sizes. Here, we present the formalism of the estimator and validate it using simulated visibility data for the Murchison Widefield Array (MWA) observations at $ν=154.25$ MHz. We find that our estimator is able to faithfully recover the ABS of the simulated sky signal with $\approx 10-15 \%$ accuracy for a wide variety of triangle shapes and sizes across the range of angular multipoles $46 \le \ell \le 1320$. In future work, we plan to apply this to actual data and also generalize it to estimate the three-dimensional redshifted 21-cm bispectrum.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02246
Exponential Stabilization of Linear Systems using Nearest-Action Control with Countable Input Set,['Optimization and Control'],"['Muhammad Zaki Almuzakki', 'Bayu Jayawardhana', 'Aneel Tanwani', 'Antonis I. Vakis']","This paper studies stabilization of linear time-invariant (LTI) systems when control actions can only be realized in finitely many directions where it is possible to actuate uniformly or logarithmically extended positive scaling factors in each direction. Furthermore, a nearest-action selection approach is used to map the continuous measurements to a realizable action where we show that the approach satisfies a weak sector condition for multiple-input multiple-output (MIMO) systems. Using the notion of input-to-state stability, under some assumptions imposed on the transfer function of the system, we show that the closed-loop system converges to the target ball exponentially fast. Moreover, when logarithmic extension for the scaling factors is realizable, the closed-loop system is able to achieve asymptotic stability instead of only practical stability. Finally, we present an example of the application that confirms our analysis.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02238
Testing vs Estimation for Index-Invariant Properties in the Huge Object Model,['Data Structures and Algorithms'],"['Sourav Chakraborty', 'Eldar Fischer', 'Arijit Ghosh', 'Amit Levi', 'Gopinath Mishra', 'Sayantan Sen']","The Huge Object model of property testing [Goldreich and Ron, TheoretiCS 23] concerns properties of distributions supported on $\{0,1\}^n$, where $n$ is so large that even reading a single sampled string is unrealistic. Instead, query access is provided to the samples, and the efficiency of the algorithm is measured by the total number of queries that were made to them.
  Index-invariant properties under this model were defined in [Chakraborty et al., COLT 23], as a compromise between enduring the full intricacies of string testing when considering unconstrained properties, and giving up completely on the string structure when considering label-invariant properties. Index-invariant properties are those that are invariant through a consistent reordering of the bits of the involved strings.
  Here we provide an adaptation of Szemerédi's regularity method for this setting, and in particular show that if an index-invariant property admits an $ε$-test with a number of queries depending only on the proximity parameter $ε$, then it also admits a distance estimation algorithm whose number of queries depends only on the approximation parameter.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02235
Blockchain-Enabled Device-Enhanced Multi-Access Edge Computing in Open Adversarial Environments,['Cryptography and Security'],"['Muhammad Islam', 'Niroshinie Fernando', 'Seng W. Loke', 'Azadeh Ghari Neiat', 'Pubudu N. Pathirana']","We propose Blockchain-enabled Device-enhanced Multi-access Edge Computing (BdMEC). BdMEC extends the Honeybee framework for on-demand resource pooling with blockchain technology to ensure trust, security, and accountability among devices (even when they are owned by different parties). BdMEC mitigates risks from malicious devices by making computations traceable. Our prototype and results demonstrate BdMEC's ability to manage distributed computing tasks efficiently and securely across multiple devices.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02233
Searches for signatures of ultra-light axion dark matter in polarimetry data of the European Pulsar Timing Array,['Cosmology and Nongalactic Astrophysics'],"['N. K. Porayko', 'P. Usynina', 'J. Terol-Calvo', 'J. Martin Camalich', 'G. M. Shaifullah', 'A. Castillo', 'D. Blas', 'L. Guillemot', 'M. Peel', 'C. Tiburzi', 'K. Postnov', 'M. Kramer', 'J. Antoniadis', 'S. Babak', 'A. -S. Bak Nielsen', 'E. Barausse', 'C. G. Bassa', 'C. Blanchard', 'M. Bonetti', 'E. Bortolas', 'P. R. Brook', 'M. Burgay', 'R. N. Caballero', 'A. Chalumeau', 'D. J. Champion']","Ultra-light axion-like particles (ALPs) can be a viable solution to the dark matter problem. The scalar field associated with ALPs, coupled to the electromagnetic field, acts as an active birefringent medium, altering the polarisation properties of light through which it propagates. In particular, oscillations of the axionic field induce monochromatic variations of the plane of linearly polarised radiation of astrophysical signals. The radio emission of millisecond pulsars provides an excellent tool to search for such manifestations, given their high fractional linear polarisation and negligible fluctuations of their polarisation properties. We have searched for the evidence of ALPs in the polarimetry measurements of pulsars collected and preprocessed for the European Pulsar Timing Array (EPTA) campaign. Focusing on the twelve brightest sources in linear polarisation, we searched for an astrophysical signal from axions using both frequentist and Bayesian statistical frameworks. For the frequentist analysis, which uses Lomb-Scargle periodograms at its core, no statistically significant signal has been found. The model used for the Bayesian analysis has been adjusted to accommodate multiple deterministic systematics that may be present in the data. A statistically significant signal has been found in the dataset of multiple pulsars with common frequency between $10^{-8}$ Hz and $2\times10^{-8}$ Hz, which can most likely be explained by the residual Faraday rotation in the terrestrial ionosphere. Strong bounds on the coupling constant $g_{aγ}$, in the same ballpark as other searches, have been obtained in the mass range between $6\times10^{-24}$ eV and $5\times10^{-21}$ eV. We conclude by discussing problems that can limit the sensitivity of our search for ultra-light axions in the polarimetry data of pulsars, and possible ways to resolve them.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02232
First Pulsar Polarization Array Limits on Ultralight Axion-like Dark Matter,['High Energy Astrophysical Phenomena'],"['Xiao Xue', 'Shi Dai', 'Hoang Nhan Luu', 'Tao Liu', 'Jing Ren', 'Jing Shu', 'Yue Zhao', 'Andrew Zic', 'N. D. Ramesh Bhat', 'Zu-Cheng Chen', 'Yi Feng', 'George Hobbs', 'Agastya Kapur', 'Richard N. Manchester', 'Rami Mandow', 'Saurav Mishra', 'Daniel J. Reardon', 'Christopher J. Russell', 'Ryan M. Shannon', 'Shuangqiang Wang', 'Lei Zhang', 'Songbo Zhang', 'Xingjiang Zhu']","We conduct the first-ever Pulsar Polarization Array (PPA) analysis to detect the ultralight Axion-Like Dark Matter (ALDM) using the polarization data of 22 millisecond pulsars from the third data release of Parkes Pulsar Timing Array. As one of the major dark matter candidates, the ultralight ALDM exhibits a pronounced wave nature on astronomical scales and offers a promising solution to small-scale structure issues within local galaxies. While the linearly polarized pulsar light travels through the ALDM galactic halo, its position angle (PA) can be subject to an oscillation induced by the ALDM Chern-Simons coupling with electromagnetic field. The PPA is thus especially suited for detecting the ultralight ALDM by correlating polarization data across the arrayed pulsars. To accomplish this task, we develop an advanced Bayesian analysis framework that allows us to construct pulsar PA residual time series, model noise contributions properly and search for pulsar cross-correlations. We find that for an ALDM density of $ρ_0=0.4\,\textrm{GeV}/\textrm{cm}^3$, the Parkes PPA offers the best global limits on the ALDM Chern-Simons coupling, namely $\lesssim 10^{-13.5}-10^{-12.2}~{\rm GeV}^{-1}$, for the mass range of $10^{-22} - 10^{-21}~{\rm eV}$. The crucial role of pulsar cross-correlation in recognizing the nature of the derived limits is also highlighted.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02229
Construction of exact solutions of nonlinear PDE via dressing chain in 3D,['Exactly Solvable and Integrable Systems'],"['I. T. Habibullin', 'A. R. Khakimova']",The duality between a class of the Davey-Stewartson type coupled systems and a class of two-dimensional Toda type lattices is discussed. A new coupled system related to the recently found lattice is presented. A method for eliminating nonlocalities in coupled systems by virtue of special finite reductions of the lattices is suggested. An original algorithm for constructing explicit solutions of the coupled systems based on the finite reduction of the corresponding lattice is proposed. Some new solutions for coupled systems related to the Volterra lattice are presented as illustrative examples.△ Less,"3 December, 2024;",https://arxiv.org/pdf/2412.02226
Deep learning approach for predicting the replicator equation in evolutionary game theory,['Artificial Intelligence'],['Advait Chandorkar'],"This paper presents a physics-informed deep learning approach for predicting the replicator equation, allowing accurate forecasting of population dynamics. This methodological innovation allows us to derive governing differential or difference equations for systems that lack explicit mathematical models. We used the SINDy model first introduced by Fasel, Kaiser, Kutz, Brunton, and Brunt 2016a to get the replicator equation, which will significantly advance our understanding of evolutionary biology, economic systems, and social dynamics. By refining predictive models across multiple disciplines, including ecology, social structures, and moral behaviours, our work offers new insights into the complex interplay of variables shaping evolutionary outcomes in dynamic systems△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02222
Higher symmetries of the lattices in 3D,['Exactly Solvable and Integrable Systems'],"['I. T. Habibullin', 'A. R. Khakimova']","It is known that there is a duality between the Davey--Stewartson type coupled systems and a class of integrable two--dimensional Toda type lattices. More precisely, the coupled systems are generalized symmetries for the lattices and the lattices can be interpreted as dressing chains for the systems. In our recent study we have found a novel lattice which apparently is not related to the known ones by Miura type transformation. In the article we described higher symmetries to this lattice and derived a new coupled system of the DS type.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02221
Robust Precoding for Multi-User Visible Light Communications with Quantized Channel Information,['Signal Processing'],"['Olga Muñoz', 'Antonio Pascual-Iserte', 'Guillermo San Arranz']","In this paper, we address the design of multi-user multiple-input single-output (MU-MISO) precoders for indoor visible light communication (VLC) systems. The goal is to minimize the transmitted optical power per light emitting diode (LED) under imperfect channel state information (CSI) at the transmitter side. Robust precoders for imperfect CSI available in the literature include noisy and outdated channel estimation cases. However, to the best of our knowledge, no work has considered adding robustness against channel quantization. In this paper, we fill this gap by addressing the case of imperfect CSI due to the quantization of VLC channels. We model the quantization errors in the CSI through polyhedric uncertainty regions. For polyhedric uncertainty regions and positive real channels, as is the case of VLC channels, we show that the robust precoder against channel quantization errors that minimizes the transmitted optical power while guaranteeing a target signal to noise plus interference ratio (SNIR) per user is the solution of a second order cone programming (SOCP) problem. Finally, we evaluate its performance under different quantization levels through numerical simulations.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02219
You (Almost) Can't Beat Brute Force for 3-Matroid Intersection,['Data Structures and Algorithms'],"['Ilan Doron-Arad', 'Ariel Kulik', 'Hadas Shachnai']","The $\ell$-matroid intersection ($\ell$-MI) problem asks if $\ell$ given matroids share a common basis. Already for $\ell = 3$, notable canonical NP-complete special cases are $3$-Dimensional Matching and Hamiltonian Path on directed graphs. However, while these problems admit exponential-time algorithms that improve the simple brute force, the fastest known algorithm for $3$-MI is exactly brute force with runtime $2^{n}/poly(n)$, where $n$ is the number of elements. Our first result shows that in fact, brute force cannot be significantly improved, by ruling out an algorithm for $\ell$-MI with runtime $o\left(2^{n-5 \cdot n^{\frac{1}{\ell-1}} \cdot \log (n)}\right)$, for any fixed $\ell\geq 3$.
  The complexity gap between $3$-MI and the polynomially solvable $2$-matroid intersection calls for a better understanding of the complexity of intermediate problems. One such prominent problem is exact matroid intersection (EMI). Given two matroids whose elements are either red or blue and a number $k$, decide if there is a common basis which contains exactly $k$ red elements. We show that EMI does not admit a randomized polynomial time algorithm. This bound implies that the parameterized algorithm of Eisenbrand et al. (FOCS'24) for exact weight matroid cannot be generalized to matroid intersection.
  We further obtain: (i) an algorithm that solves $\ell$-MI faster than brute force in time $2^{n-Ω\left(\log^2 (n)\right)} $ (ii) a parameterized running time lower bound of $2^{(\ell-2) \cdot k \cdot \log k} \cdot poly(n)$ for $\ell$-MI, where the parameter $k$ is the rank of the matroids. We obtain these two results by generalizing the Monotone Local Search technique of Fomin et al. (J. ACM'19). Broadly speaking, our generalization converts any parameterized algorithm for a subset problem into an exponential-time algorithm which is faster than brute-force.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02217
Recovering implicit physics model under real-world constraints,['Machine Learning'],"['Ayan Banerjee', 'Sandeep K. S. Gupta']","Recovering a physics-driven model, i.e. a governing set of equations of the underlying dynamical systems, from the real-world data has been of recent interest. Most existing methods either operate on simulation data with unrealistically high sampling rates or require explicit measurements of all system variables, which is not amenable in real-world deployments. Moreover, they assume the timestamps of external perturbations to the physical system are known a priori, without uncertainty, implicitly discounting any sensor time-synchronization or human reporting errors. In this paper, we propose a novel liquid time constant neural network (LTC-NN) based architecture to recover underlying model of physical dynamics from real-world data. The automatic differentiation property of LTC-NN nodes overcomes problems associated with low sampling rates, the input dependent time constant in the forward pass of the hidden layer of LTC-NN nodes creates a massive search space of implicit physical dynamics, the physics model solver based data reconstruction loss guides the search for the correct set of implicit dynamics, and the use of the dropout regularization in the dense layer ensures extraction of the sparsest model. Further, to account for the perturbation timing error, we utilize dense layer nodes to search through input shifts that results in the lowest reconstruction loss. Experiments on four benchmark dynamical systems, three with simulation data and one with the real-world data show that the LTC-NN architecture is more accurate in recovering implicit physics model coefficients than the state-of-the-art sparse model recovery approaches. We also introduce four additional case studies (total eight) on real-life medical examples in simulation and with real-world clinical data to show effectiveness of our approach in recovering underlying model in practice.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02215
GIST: Towards Photorealistic Style Transfer via Multiscale Geometric Representations,['Computer Vision and Pattern Recognition'],"['Renan A. Rojas-Gomez', 'Minh N. Do']","State-of-the-art Style Transfer methods often leverage pre-trained encoders optimized for discriminative tasks, which may not be ideal for image synthesis. This can result in significant artifacts and loss of photorealism. Motivated by the ability of multiscale geometric image representations to capture fine-grained details and global structure, we propose GIST: Geometric-based Image Style Transfer, a novel Style Transfer technique that exploits the geometric properties of content and style images. GIST replaces the standard Neural Style Transfer autoencoding framework with a multiscale image expansion, preserving scene details without the need for post-processing or training. Our method matches multiresolution and multidirectional representations such as Wavelets and Contourlets by solving an optimal transport problem, leading to an efficient texture transferring. Experiments show that GIST is on-par or outperforms recent photorealistic Style Transfer approaches while significantly reducing the processing time with no model training.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02214
Transformer-Metric Loss for CNN-Based Face Recognition,['Computer Vision and Pattern Recognition'],"['Pritesh Prakash', 'Ashish Jacob Sam']","In deep learning, the loss function plays a crucial role in optimizing the network. Many recent innovations in loss techniques have been made, and various margin-based angular loss functions (metric loss) have been designed particularly for face recognition. The concept of transformers is already well-researched and applied in many facets of machine vision. This paper presents a technique for loss evaluation that uses a transformer network as an additive loss in the face recognition domain. The standard metric loss function typically takes the final embedding of the main CNN backbone as its input. Here, we employ a transformer-metric loss, a combined approach that integrates both transformer-loss and metric-loss. This research intends to analyze the transformer behavior on the convolution output when the CNN outcome is arranged in a sequential vector. The transformer encoder takes input from the contextual vectors obtained from the final convolution layer of the network. With this technique, we use transformer loss with various base metric-loss functions to evaluate the effect of the combined loss functions. We observe that such a configuration allows the network to achieve SoTA results on various validation datasets with some limitations. This research expands the role of transformers in the machine vision domain and opens new possibilities for exploring transformers as a loss function.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02198
Stimulated Raman Scattering in Nonlinear Silicon Nanophotonic Waveguides: Theory and Applications in Photonic Integrated Circuits,['Optics'],"['Abdurrahman Javid Shaikh', 'Othman Sidek']","Photonics caught world attention since channel capacity limit of metallic interconnects approached due to research and design in high speed digital processors. Use of dielectrics, instead, suitable for light propagation was more attractive due to its extremely wide bandwidth. Many of the devices, both active and passive, have been demonstrated using these insulating materials. Due to its excellent optical characteristics, established fabrication history, and cheaper throughput, silicon found its place in photonics arena. However, due to its indirect band structure, efficient light sources are not possible using silicon as the base material. Nevertheless, techniques such as stimulated Raman scattering and third-harmonic generation have made it possible to avoid this natural hurdle in the path of silicon as a light source. This paper reviews basic theory of stimulated Raman scattering, its applications in the context of silicon based photonic integrated circuits and describes ways to improve this nonlinear effect. This paper also covers few of the most important demonstrations of stimulated Raman scattering published in literature from the last decade.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02194
Comparative Performance of Machine Learning Algorithms for Early Genetic Disorder and Subclass Classification,['Artificial Intelligence'],"['Abu Bakar Siddik', 'Faisal R. Badal', 'Afroza Islam']","A great deal of effort has been devoted to discovering a particular genetic disorder, but its classification across a broad spectrum of disorder classes and types remains elusive. Early diagnosis of genetic disorders enables timely interventions and improves outcomes. This study implements machine learning models using basic clinical indicators measurable at birth or infancy to enable diagnosis in preliminary life stages. Supervised learning algorithms were implemented on a dataset of 22083 instances with 42 features like family history, newborn metrics, and basic lab tests. Extensive hyperparameter tuning, feature engineering, and selection were undertaken. Two multi-class classifiers were developed: one for predicting disorder classes (mitochondrial, multifactorial, and single-gene) and one for subtypes (9 disorders). Performance was evaluated using accuracy, precision, recall, and the F1-score. The CatBoost classifier achieved the highest accuracy of 77% for predicting genetic disorder classes. For subtypes, SVM attained a maximum accuracy of 80%. The study demonstrates the feasibility of using basic clinical data in machine learning models for early categorization and diagnosis across various genetic disorders. Applying ML with basic clinical indicators can enable timely interventions once validated on larger datasets. It is necessary to conduct further studies to improve model performance on this dataset.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02189
Radar-based Measurement of the Body Movements of Multiple Students in Classroom Environments,['Signal Processing'],"['Yu Oshima', 'Tianyi Wang', 'Masaya Kato', 'Haruto Kobayashi', 'Itsuki Iwata', 'Yuji Tanaka', 'Shuqiong Wu', 'Manabu Wakuta', 'Masako Myowa', 'Tomoko Nishimura', 'Atsushi Senju', 'Takuya Sakamoto']","We demonstrate the feasibility of the radar-based measurement of body movements in scenarios involving multiple students using a pair of 79-GHz millimeter-wave radar systems with array antennas. We quantify the body motion using the Doppler frequency calculated from radar echoes. The measurement accuracy is evaluated for two experimental scenarios, namely university students in an office and elementary school students in a classroom. The body movements measured using the two radar systems are compared to evaluate the repeatability and angle dependency of the measurement. Moreover, in the first scenario, we compare the radar-estimated body movement with subjective evaluation scores provided by two evaluators. In the first scenario, the coefficient of correlation between the radar-estimated body movement and the subjective evaluation score is 0.73 on average, with a maximum value of 0.97; in the second scenario, the average correlation coefficient of body movements measured using two radar systems is as high as 0.78. These results indicate that the proposed approach can be used to monitor the body movements of multiple students in realistic scenarios.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02184
Generalizing Weisfeiler-Lehman Kernels to Subgraphs,['Machine Learning'],"['Dongkwan Kim', 'Alice Oh']","Subgraph representation learning has been effective in solving various real-world problems. However, current graph neural networks (GNNs) produce suboptimal results for subgraph-level tasks due to their inability to capture complex interactions within and between subgraphs. To provide a more expressive and efficient alternative, we propose WLKS, a Weisfeiler-Lehman (WL) kernel generalized for subgraphs by applying the WL algorithm on induced $k$-hop neighborhoods. We combine kernels across different $k$-hop levels to capture richer structural information that is not fully encoded in existing models. Our approach can balance expressiveness and efficiency by eliminating the need for neighborhood sampling. In experiments on eight real-world and synthetic benchmarks, WLKS significantly outperforms leading approaches on five datasets while reducing training time, ranging from 0.01x to 0.25x compared to the state-of-the-art.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02181
Improved Complexity for Smooth Nonconvex Optimization: A Two-Level Online Learning Approach with Quasi-Newton Methods,['Optimization and Control'],"['Ruichen Jiang', 'Aryan Mokhtari', 'Francisco Patitucci']","We study the problem of finding an $ε$-first-order stationary point (FOSP) of a smooth function, given access only to gradient information. The best-known gradient query complexity for this task, assuming both the gradient and Hessian of the objective function are Lipschitz continuous, is ${O}(ε^{-7/4})$. In this work, we propose a method with a gradient complexity of ${O}(d^{1/4}ε^{-13/8})$, where $d$ is the problem dimension, leading to an improved complexity when $d = {O}(ε^{-1/2})$. To achieve this result, we design an optimization algorithm that, underneath, involves solving two online learning problems. Specifically, we first reformulate the task of finding a stationary point for a nonconvex problem as minimizing the regret in an online convex optimization problem, where the loss is determined by the gradient of the objective function. Then, we introduce a novel optimistic quasi-Newton method to solve this online learning problem, with the Hessian approximation update itself framed as an online learning problem in the space of matrices. Beyond improving the complexity bound for achieving an $ε$-FOSP using a gradient oracle, our result provides the first guarantee suggesting that quasi-Newton methods can potentially outperform gradient descent-type methods in nonconvex settings.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02175
All Polyhedral Manifolds are Connected by a 2-Step Refolding,['Computational Geometry'],"['Lily Chung', 'Erik D. Demaine', 'Jenny Diomidova', 'Tonan Kamata', 'Jayson Lynch', 'Ryuhei Uehara', 'Hanyu Alice Zhang']","We prove that, for any two polyhedral manifolds P, Q, there is a polyhedral manifold I such that P, I share a common unfolding and I, Q share a common unfolding. In other words, we can unfold P, refold (glue) that unfolding into I, unfold I, and then refold into Q. Furthermore, if P, Q are embedded in 3D, then I can be embedded in 3D (without self-intersection). These results generalize to n given manifolds P_1, P_2, ..., P_n; they all have a common unfolding with an intermediate manifold I. Allowing more than two unfold/refold steps, we obtain stronger results for two special cases: for doubly covered convex planar polygons, we achieve that all intermediate polyhedra are planar; and for tree-shaped polycubes, we achieve that all intermediate polyhedra are tree-shaped polycubes.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02174
Keeping Experts in the Loop: Expert-Guided Optimization for Clinical Data Classification using Large Language Models,['Artificial Intelligence'],"['Nader Karayanni', 'Aya Awwad', 'Chein-Lien Hsiao', 'Surish P Shanmugam']","Since the emergence of Large Language Models (LLMs), the challenge of effectively leveraging their potential in healthcare has taken center stage. A critical barrier to using LLMs for extracting insights from unstructured clinical notes lies in the prompt engineering process. Despite its pivotal role in determining task performance, a clear framework for prompt optimization remains absent. Current methods to address this gap take either a manual prompt refinement approach, where domain experts collaborate with prompt engineers to create an optimal prompt, which is time-intensive and difficult to scale, or through employing automatic prompt optimizing approaches, where the value of the input of domain experts is not fully realized. To address this, we propose StructEase, a novel framework that bridges the gap between automation and the input of human expertise in prompt engineering. A core innovation of the framework is SamplEase, an iterative sampling algorithm that identifies high-value cases where expert feedback drives significant performance improvements. This approach minimizes expert intervention, to effectively enhance classification outcomes. This targeted approach reduces labeling redundancy, mitigates human error, and enhances classification outcomes. We evaluated the performance of StructEase using a dataset of de-identified clinical narratives from the US National Electronic Injury Surveillance System (NEISS), demonstrating significant gains in classification performance compared to current methods. Our findings underscore the value of expert integration in LLM workflows, achieving notable improvements in F1 score while maintaining minimal expert effort. By combining transparency, flexibility, and scalability, StructEase sets the foundation for a framework to integrate expert input into LLM workflows in healthcare and beyond.△ Less","3 December, 2024;",https://arxiv.org/pdf/2412.02173
Electromagnetic interactions in elastic neutrino-nucleon scattering,['High Energy Physics - Phenomenology'],"['Konstantin A. Kouzakov', 'Fedor M. Lazarev', 'Alexander I. Studenikin']","A thorough account of electromagnetic interactions of massive Dirac neutrinos as well as their spin-flavor state in the theoretical formulation of elastic neutrino-nucleon scattering is given. The formalism of neutrino charge, magnetic, electric, and anapole form factors defined as matrices in the mass basis is employed under the assumption of three-neutrino mixing. The flavor and spin change of neutrinos propagating from the source to the detector is taken into account in the form of a spin-flavor density matrix of the neutrino arriving at the detector. The potential effects of the neutrino charge radii, magnetic moments, and spin polarization in the neutrino-nucleon scattering experiments are outlined.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02169
Analyzing the Impact of AI Tools on Student Study Habits and Academic Performance,['Artificial Intelligence'],"['Ben Ward', 'Deepshikha Bhati', 'Fnu Neha', 'Angela Guercio']","This study explores the effectiveness of AI tools in enhancing student learning, specifically in improving study habits, time management, and feedback mechanisms. The research focuses on how AI tools can support personalized learning, adaptive test adjustments, and provide real-time classroom analysis. Student feedback revealed strong support for these features, and the study found a significant reduction in study hours alongside an increase in GPA, suggesting positive academic outcomes. Despite these benefits, challenges such as over-reliance on AI and difficulties in integrating AI with traditional teaching methods were also identified, emphasizing the need for AI tools to complement conventional educational strategies rather than replace them. Data were collected through a survey with a Likert scale and follow-up interviews, providing both quantitative and qualitative insights. The analysis involved descriptive statistics to summarize demographic data, AI usage patterns, and perceived effectiveness, as well as inferential statistics (T-tests, ANOVA) to examine the impact of demographic factors on AI adoption. Regression analysis identified predictors of AI adoption, and qualitative responses were thematically analyzed to understand students' perspectives on the future of AI in education. This mixed-methods approach provided a comprehensive view of AI's role in education and highlighted the importance of privacy, transparency, and continuous refinement of AI features to maximize their educational benefits.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02166
Agri-LLaVA: Knowledge-Infused Large Multimodal Assistant on Agricultural Pests and Diseases,['Computer Vision and Pattern Recognition'],"['Liqiong Wang', 'Teng Jin', 'Jinyu Yang', 'Ales Leonardis', 'Fangyi Wang', 'Feng Zheng']","In the general domain, large multimodal models (LMMs) have achieved significant advancements, yet challenges persist in applying them to specific fields, especially agriculture. As the backbone of the global economy, agriculture confronts numerous challenges, with pests and diseases being particularly concerning due to their complexity, variability, rapid spread, and high resistance. This paper specifically addresses these issues. We construct the first multimodal instruction-following dataset in the agricultural domain, covering over 221 types of pests and diseases with approximately 400,000 data entries. This dataset aims to explore and address the unique challenges in pest and disease control. Based on this dataset, we propose a knowledge-infused training method to develop Agri-LLaVA, an agricultural multimodal conversation system. To accelerate progress in this field and inspire more researchers to engage, we design a diverse and challenging evaluation benchmark for agricultural pests and diseases. Experimental results demonstrate that Agri-LLaVA excels in agricultural multimodal conversation and visual understanding, providing new insights and approaches to address agricultural pests and diseases. By open-sourcing our dataset and model, we aim to promote research and development in LMMs within the agricultural domain and make significant contributions to tackle the challenges of agricultural pests and diseases. All resources can be found at https://github.com/Kki2Eve/Agri-LLaVA.△ Less",v1,https://arxiv.org/pdf/2412.02158
Compromising the Intelligence of Modern DNNs: On the Effectiveness of Targeted RowPress,['Hardware Architecture'],"['Ranyang Zhou', 'Jacqueline T. Liu', 'Sabbir Ahmed', 'Shaahin Angizi', 'Adnan Siraj Rakin']","Recent advancements in side-channel attacks have revealed the vulnerability of modern Deep Neural Networks (DNNs) to malicious adversarial weight attacks. The well-studied RowHammer attack has effectively compromised DNN performance by inducing precise and deterministic bit-flips in the main memory (e.g., DRAM). Similarly, RowPress has emerged as another effective strategy for flipping targeted bits in DRAM. However, the impact of RowPress on deep learning applications has yet to be explored in the existing literature, leaving a fundamental research question unanswered: How does RowPress compare to RowHammer in leveraging bit-flip attacks to compromise DNN performance? This paper is the first to address this question and evaluate the impact of RowPress on DNN applications. We conduct a comparative analysis utilizing a novel DRAM-profile-aware attack designed to capture the distinct bit-flip patterns caused by RowHammer and RowPress. Eleven widely-used DNN architectures trained on different benchmark datasets deployed on a Samsung DRAM chip conclusively demonstrate that they suffer from a drastically more rapid performance degradation under the RowPress attack compared to RowHammer. The difference in the underlying attack mechanism of RowHammer and RowPress also renders existing RowHammer mitigation mechanisms ineffective under RowPress. As a result, RowPress introduces a new vulnerability paradigm for DNN compute platforms and unveils the urgent need for corresponding protective measures.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02156
Revisiting the Initial Steps in Adaptive Gradient Descent Optimization,['Machine Learning'],"['Abulikemu Abuduweili', 'Changliu Liu']","Adaptive gradient optimization methods, such as Adam, are prevalent in training deep neural networks across diverse machine learning tasks due to their ability to achieve faster convergence. However, these methods often suffer from suboptimal generalization compared to stochastic gradient descent (SGD) and exhibit instability, particularly when training Transformer models. In this work, we show the standard initialization of the second-order moment estimation ($v_0 =0$) as a significant factor contributing to these limitations. We introduce simple yet effective solutions: initializing the second-order moment estimation with non-zero values, using either data-driven or random initialization strategies. Empirical evaluations demonstrate that our approach not only stabilizes convergence but also enhances the final performance of adaptive gradient optimizers. Furthermore, by adopting the proposed initialization strategies, Adam achieves performance comparable to many recently proposed variants of adaptive gradient optimization methods, highlighting the practical impact of this straightforward modification.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02153
Mining Tweets to Predict Future Bitcoin Price,['Artificial Intelligence'],"['Ashutosh Hathidara', 'Gaurav Atavale', 'Suyash Chaudhary']","Bitcoin has increased investment interests in people during the last decade. We have seen an increase in the number of posts on social media platforms about cryptocurrency, especially Bitcoin. This project focuses on analyzing user tweet data in combination with Bitcoin price data to see the relevance between price fluctuations and the conversation between millions of people on Twitter. This study also exploits this relationship between user tweets and bitcoin prices to predict the future bitcoin price. We are utilizing novel techniques and methods to analyze the data and make price predictions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02148
Effective Mitigations for Systemic Risks from General-Purpose AI,['Computers and Society'],"['Risto Uuk', 'Annemieke Brouwer', 'Tim Schreier', 'Noemi Dreksler', 'Valeria Pulignano', 'Rishi Bommasani']","The systemic risks posed by general-purpose AI models are a growing concern, yet the effectiveness of mitigations remains underexplored. Previous research has proposed frameworks for risk mitigation, but has left gaps in our understanding of the perceived effectiveness of measures for mitigating systemic risks. Our study addresses this gap by evaluating how experts perceive different mitigations that aim to reduce the systemic risks of general-purpose AI models. We surveyed 76 experts whose expertise spans AI safety; critical infrastructure; democratic processes; chemical, biological, radiological, and nuclear risks (CBRN); and discrimination and bias. Among 27 mitigations identified through a literature review, we find that a broad range of risk mitigation measures are perceived as effective in reducing various systemic risks and technically feasible by domain experts. In particular, three mitigation measures stand out: safety incident reports and security information sharing, third-party pre-deployment model audits, and pre-deployment risk assessments. These measures show both the highest expert agreement ratings (>60\%) across all four risk areas and are most frequently selected in experts' preferred combinations of measures (>40\%). The surveyed experts highlighted that external scrutiny, proactive evaluation and transparency are key principles for effective mitigation of systemic risks. We provide policy recommendations for implementing the most promising measures, incorporating the qualitative contributions from experts. These insights should inform regulatory frameworks and industry practices for mitigating the systemic risks associated with general-purpose AI.△ Less","14 November, 2024;",https://arxiv.org/pdf/2412.02145
Personalized Multimodal Large Language Models: A Survey,['Computer Vision and Pattern Recognition'],"['Junda Wu', 'Hanjia Lyu', 'Yu Xia', 'Zhehao Zhang', 'Joe Barrow', 'Ishita Kumar', 'Mehrnoosh Mirtaheri', 'Hongjie Chen', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Tong Yu', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Nesreen K. Ahmed', 'Yu Wang', 'Xiang Chen', 'Hanieh Deilamsalehy', 'Namyong Park', 'Sungchul Kim', 'Huanrui Yang', 'Subrata Mitra', 'Zhengmian Hu', 'Nedim Lipka', 'Dang Nguyen', 'Yue Zhao']","Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02142
Streamlining Video Analysis for Efficient Violence Detection,['Computer Vision and Pattern Recognition'],"['Gourang Pathak', 'Abhay Kumar', 'Sannidhya Rawat', 'Shikha Gupta']","This paper addresses the challenge of automated violence detection in video frames captured by surveillance cameras, specifically focusing on classifying scenes as ""fight"" or ""non-fight."" This task is critical for enhancing unmanned security systems, online content filtering, and related applications. We propose an approach using a 3D Convolutional Neural Network (3D CNN)-based model named X3D to tackle this problem. Our approach incorporates pre-processing steps such as tube extraction, volume cropping, and frame aggregation, combined with clustering techniques, to accurately localize and classify fight scenes. Extensive experimentation demonstrates the effectiveness of our method in distinguishing violent from non-violent events, providing valuable insights for advancing practical violence detection systems.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.02127
Benchmarking symbolic regression constant optimization schemes,['Machine Learning'],"['L. G. A dos Reis', 'V. L. P. S. Caminha', 'T. J. P. Penna']","Symbolic regression is a machine learning technique, and it has seen many advancements in recent years, especially in genetic programming approaches (GPSR). Furthermore, it has been known for many years that constant optimization of parameters, during the evolutionary search, greatly increases GPSR performance However, different authors approach such tasks differently and no consensus exists regarding which methods perform best. In this work, we evaluate eight different parameter optimization methods, applied during evolutionary search, over ten known benchmark problems, in two different scenarios. We also propose using an under-explored metric called Tree Edit Distance (TED), aiming to identify symbolic accuracy. In conjunction with classical error measures, we develop a combined analysis of model performance in symbolic regression. We then show that different constant optimization methods perform better in certain scenarios and that there is no overall best choice for every problem. Finally, we discuss how common metric decisions may be biased and appear to generate better models in comparison.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02126
Improving Sequential Recommender Systems with Online and In-store User Behavior,['Information Retrieval'],"['Luyi Ma', 'Aashika Padmanabhan', 'Anjana Ganesh', 'Shengwei Tang', 'Jiao Chen', 'Xiaohan Li', 'Lalitesh Morishetti', 'Kaushiki Nag', 'Malay Patel', 'Jason Cho', 'Sushant Kumar', 'Kannan Achan']","Online e-commerce platforms have been extending in-store shopping, which allows users to keep the canonical online browsing and checkout experience while exploring in-store shopping. However, the growing transition between online and in-store becomes a challenge to sequential recommender systems for future online interaction prediction due to the lack of holistic modeling of hybrid user behaviors (online and in-store). The challenges are twofold. First, combining online and in-store user behavior data into a single data schema and supporting multiple stages in the model life cycle (pre-training, training, inference, etc.) organically needs a new data pipeline design. Second, online recommender systems, which solely rely on online user behavior sequences, must be redesigned to support online and in-store user data as input under the sequential modeling setting. To overcome the first challenge, we propose a hybrid, omnichannel data pipeline to compile online and in-store user behavior data by caching information from diverse data sources. Later, we introduce a model-agnostic encoder module to the sequential recommender system to interpret the user in-store transaction and augment the modeling capacity for better online interaction prediction given the hybrid user behavior.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02122
Rethinking Self-Supervised Learning Within the Framework of Partial Information Decomposition,['Computer Vision and Pattern Recognition'],"['Salman Mohamadi', 'Gianfranco Doretto', 'Donald A. Adjeroh']","Self Supervised learning (SSL) has demonstrated its effectiveness in feature learning from unlabeled data. Regarding this success, there have been some arguments on the role that mutual information plays within the SSL framework. Some works argued for increasing mutual information between representation of augmented views. Others suggest decreasing mutual information between them, while increasing task-relevant information. We ponder upon this debate and propose to revisit the core idea of SSL within the framework of partial information decomposition (PID). Thus, with SSL under PID we propose to replace traditional mutual information with the more general concept of joint mutual information to resolve the argument. Our investigation on instantiation of SSL within the PID framework leads to upgrading the existing pipelines by considering the components of the PID in the SSL models for improved representation learning. Accordingly we propose a general pipeline that can be applied to improve existing baselines. Our pipeline focuses on extracting the unique information component under the PID to build upon lower level supervision for generic feature learning and on developing higher-level supervisory signals for task-related feature learning. In essence, this could be interpreted as a joint utilization of local and global clustering. Experiments on four baselines and four datasets show the effectiveness and generality of our approach in improving existing SSL frameworks.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02121
On the convergence of trajectory statistical solutions,['Analysis of PDEs'],"['Anne C. Bronzi', 'Cecilia F. Mondaini', 'Ricardo M. S. Rosa']","In this work, a recently introduced general framework for trajectory statistical solutions is considered, and the question of convergence of families of such solutions is addressed. Conditions for the convergence are given which rely on natural assumptions related to a priori estimates for the individual solutions of typical approximating problems. The first main result is based on the assumption that the superior limit of suitable families of compact subsets of carriers of the family of trajectory statistical solutions be included in the set of solutions of the limit problem. The second main result is a version of the former in the case in which the approximating family is associated with a well-posed system. These two results are then applied to the inviscid limit of incompressible Navier-Stokes system in two and three spatial dimensions, showing, in particular, the existence of trajectory statistical solutions to the two- and three-dimensional Euler equations, in the context of weak and dissipative solutions, respectively. Another application of the second main result is on the Galerkin approximations of statistical solutions of the three-dimensional Navier-Stokes equations.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02117
Quantum theory of neutron in magnetic field and neutron-electron collider,['High Energy Physics - Experiment'],"['A. Bogomyagkov', 'V. Druzhinin', 'E. Levichev', 'A. Milstein', 'I. Okunev', 'S. Taskaev']","The paper presents solution of quantum problem of neutron propagation in the magnetic field with multipole field expansion. Rigorous solution of the Pauli equation for neutron reveals existence of two solutions, finite and infinite, for any miltipole configuration. As an example, we present detailed study of neutron motion in quadrupole and sextupole. Our predictions agree with the results of Stern-Gerlach experiment for neutrons. To verify existence of finite and infinite motion, we discuss an experiment which could be performed in the Budker Insitute of Nuclear Physics using existing equipment. We conclude with the proposal of neutron-electron intersecting ring (neutron-electron ``collider''), which, if realized, would allow to accomplish a number of important experiments devoting to study neutron internal structure.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02115
Machine Learning Methods for Automated Interstellar Object Classification with LSST,['Earth and Planetary Astrophysics'],"['Richard Cloete', 'Peter Vereš', 'Abraham Loeb']","The Legacy Survey of Space and Time, to be conducted with the Vera C. Rubin Observatory, is poised to revolutionize our understanding of the Solar System by providing an unprecedented wealth of data on various objects, including the elusive interstellar objects (ISOs). Detecting and classifying ISOs is crucial for studying the composition and diversity of materials from other planetary systems. However, the rarity and brief observation windows of ISOs, coupled with the vast quantities of data to be generated by LSST, create significant challenges for their identification and classification. This study aims to address these challenges by exploring the application of machine learning algorithms to the automated classification of ISO tracklets in simulated LSST data. We employed various machine learning algorithms, including random forests (RFs), stochastic gradient descent (SGD), gradient boosting machines (GBMs), and neural networks (NNs), to classify ISO tracklets in simulated LSST data. We demonstrate that GBM and RF algorithms outperform SGD and NN algorithms in accurately distinguishing ISOs from other Solar System objects. RF analysis shows that many derived Digest2 values are more important than direct observables in classifying ISOs from the LSST tracklets. The GBM model achieves the highest precision, recall, and F1 score, with values of 0.9987, 0.9986, and 0.9987, respectively. These findings lay the foundation for the development of an efficient and robust automated system for ISO discovery using LSST data, paving the way for a deeper understanding of the materials and processes that shape planetary systems beyond our own. The integration of our proposed machine learning approach into the LSST data processing pipeline will optimize the survey's potential for identifying these rare and valuable objects, enabling timely follow-up observations and further characterization.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02112
Direct Coloring for Self-Supervised Enhanced Feature Decoupling,['Computer Vision and Pattern Recognition'],"['Salman Mohamadi', 'Gianfranco Doretto', 'Donald A. Adjeroh']","The success of self-supervised learning (SSL) has been the focus of multiple recent theoretical and empirical studies, including the role of data augmentation (in feature decoupling) as well as complete and dimensional representation collapse. While complete collapse is well-studied and addressed, dimensional collapse has only gain attention and addressed in recent years mostly using variants of redundancy reduction (aka whitening) techniques. In this paper, we further explore a complementary approach to whitening via feature decoupling for improved representation learning while avoiding representation collapse. In particular, we perform feature decoupling by early promotion of useful features via careful feature coloring. The coloring technique is developed based on a Bayesian prior of the augmented data, which is inherently encoded for feature decoupling. We show that our proposed framework is complementary to the state-of-the-art techniques, while outperforming both contrastive and recent non-contrastive methods. We also study the different effects of coloring approach to formulate it as a general complementary technique along with other baselines.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02109
Evaluating the Impact of Data Augmentation on Predictive Model Performance,['Machine Learning'],"['Valdemar Švábenský', 'Conrad Borchers', 'Elizabeth B. Cloude', 'Atsushi Shimada']","In supervised machine learning (SML) research, large training datasets are essential for valid results. However, obtaining primary data in learning analytics (LA) is challenging. Data augmentation can address this by expanding and diversifying data, though its use in LA remains underexplored. This paper systematically compares data augmentation techniques and their impact on prediction performance in a typical LA task: prediction of academic outcomes. Augmentation is demonstrated on four SML models, which we successfully replicated from a previous LAK study based on AUC values. Among 21 augmentation techniques, SMOTE-ENN sampling performed the best, improving the average AUC by 0.01 and approximately halving the training time compared to the baseline models. In addition, we compared 99 combinations of chaining 21 techniques, and found minor, although statistically significant, improvements across models when adding noise to SMOTE-ENN (+0.014). Notably, some augmentation techniques significantly lowered predictive performance or increased performance fluctuation related to random chance. This paper's contribution is twofold. Primarily, our empirical findings show that sampling techniques provide the most statistically reliable performance improvements for LA applications of SML, and are computationally more efficient than deep generation methods with complex hyperparameter settings. Second, the LA community may benefit from validating a recent study through independent replication.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02108
Molecular optomechanics with atomic antennas,['Quantum Physics'],"['Mikolaj K. Schmidt', 'Alexander A. High', 'Michael J. Steel']","A typical surface-enhanced Raman scattering (SERS) system relies on deeply subwavelength field localization in nanoscale plasmonic cavities to enhance both the excitation and emission of Raman active molecules. Here, we demonstrate that a germanium-vacancy (GeV) defect in diamond can efficiently mediate the excitation process, by acting as a bright atomic antenna. At low temperatures, GeV exhibits low dissipation, allowing it to be efficiently populated by the incident field, resulting in a thousand-fold increase in the efficiency of Raman scattering. Additionally, we show that atomic antenna-enhanced Raman scattering can be distinguished from conventional SERS by tracing the dependence of Stokes intensity on input power, and the pronounced antibunching of the Raman emission.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02106
Tracing magnetic field in super-Alfvenic turbulence with Gradient Technique,['Astrophysics of Galaxies'],"['Ka Wai Ho', 'Alex Lazarian']","Super-Alfvenic turbulence is important for many astrophysical objects, particularly galaxy clusters. In this paper, we explore the accuracy of Synchrotron Intensity Gradients (SIGs) and X-ray intensity gradients to map magnetic fields in super-Alfvenic turbulence for a set of astrophysically relevant parameters of turbulent driving. Analyzing our synthetic observations, we report a good accuracy for both techniques. Our results are suggestive that other types of Gradient Technique (GT) can be successfully employed to trace magnetic fields within super-Alfvenic sub-sonic turbulence.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02102
Crash Severity Risk Modeling Strategies under Data Imbalance,['Machine Learning'],"['Abdullah Al Mamun', 'Abyad Enan', 'Debbie A. Indah', 'Judith Mwakalonge', 'Gurcan Comert', 'Mashrur Chowdhury']","This study investigates crash severity risk modeling strategies for work zones involving large vehicles (i.e., trucks, buses, and vans) when there are crash data imbalance between low-severity (LS) and high-severity (HS) crashes. We utilized crash data, involving large vehicles in South Carolina work zones for the period between 2014 and 2018, which included 4 times more LS crashes compared to HS crashes. The objective of this study is to explore crash severity prediction performance of various models under different feature selection and data balancing techniques. The findings of this study highlight a disparity between LS and HS predictions, with less-accurate prediction of HS crashes compared to LS crashes due to class imbalance and feature overlaps between LS and HS crashes. Combining features from multiple feature selection techniques: statistical correlation, feature importance, recursive elimination, statistical tests, and mutual information, slightly improves HS crash prediction performance. Data balancing techniques such as NearMiss-1 and RandomUnderSampler, maximize HS recall when paired with certain prediction models, such as Bayesian Mixed Logit (BML), NeuralNet, and RandomForest, making them suitable for HS crash prediction. Conversely, RandomOverSampler, HS Class Weighting, and Kernel-based Synthetic Minority Oversampling (K-SMOTE), used with certain prediction models such as BML, CatBoost, and LightGBM, achieve a balanced performance, defined as achieving an equitable trade-off between LS and HS prediction performance metrics. These insights provide safety analysts with guidance to select models, feature selection techniques, and data balancing techniques that align with their specific safety objectives, offering a robust foundation for enhancing work-zone crash severity prediction.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02094
Offline Stochastic Optimization of Black-Box Objective Functions,['Machine Learning'],"['Juncheng Dong', 'Zihao Wu', 'Hamid Jafarkhani', 'Ali Pezeshki', 'Vahid Tarokh']","Many challenges in science and engineering, such as drug discovery and communication network design, involve optimizing complex and expensive black-box functions across vast search spaces. Thus, it is essential to leverage existing data to avoid costly active queries of these black-box functions. To this end, while Offline Black-Box Optimization (BBO) is effective for deterministic problems, it may fall short in capturing the stochasticity of real-world scenarios. To address this, we introduce Stochastic Offline BBO (SOBBO), which tackles both black-box objectives and uncontrolled uncertainties. We propose two solutions: for large-data regimes, a differentiable surrogate allows for gradient-based optimization, while for scarce-data regimes, we directly estimate gradients under conservative field constraints, improving robustness, convergence, and data efficiency. Numerical experiments demonstrate the effectiveness of our approach on both synthetic and real-world tasks.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02089
Comparative Analysis of Black-Box and White-Box Machine Learning Model in Phishing Detection,['Cryptography and Security'],"['Abdullah Fajar', 'Setiadi Yazid', 'Indra Budi']","Background: Explainability in phishing detection model can support a further solution of phishing attack mitigation by increasing trust and understanding how phishing can be detected. Objective: The aims of this study to determine and best recommendation to apply an approach which has several components with abilities to fulfil the critical needs Methods: A methodology starting with analyzing both black-box and white-box models to get the pros and cons specifically in phishing detection. The conclusion of the analysis will be validated by experiment using a set of well-known algorithms and public phishing datasets. Experimental metrics covers 3 measurements such as predictive accuracy and explainability metrics. Conclusion: Both models are comparable in terms of interpretability and consistency, with room for improvement in diverse datasets. EBM as an example of white-box model is generally better suited for applications requiring explainability and actionable insights. Finally, each model, white-box and black-box model has positive and negative aspects both for performance metric and for explainable metric. It is important to consider the objective of model usage.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02084
Implementing An Artificial Quantum Perceptron,['Quantum Physics'],"['Ashutosh Hathidara', 'Lalit Pandey']","A Perceptron is a fundamental building block of a neural network. The flexibility and scalability of perceptron make it ubiquitous in building intelligent systems. Studies have shown the efficacy of a single neuron in making intelligent decisions. Here, we examined and compared two perceptrons with distinct mechanisms, and developed a quantum version of one of those perceptrons. As a part of this modeling, we implemented the quantum circuit for an artificial perception, generated a dataset, and simulated the training. Through these experiments, we show that there is an exponential growth advantage and test different qubit versions. Our findings show that this quantum model of an individual perceptron can be used as a pattern classifier. For the second type of model, we provide an understanding to design and simulate a spike-dependent quantum perceptron. Our code is available at \url{https://github.com/ashutosh1919/quantum-perceptron}△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02083
Integrated Differential Conjugate Homodyne Detection for Quantum Random Number Generation,['Quantum Physics'],"['Christian Carver', 'Jared Marchant', 'Benjamin Fisher', 'Nicholas Townsend', 'Tyler Stowell', 'Austin Barlow', 'Benjamin Arnesen', 'Shiuh-Hua Wood Chiang', 'Ryan M. Camacho']","In this work, we perform on-chip quantum random number generation (QRNG) that uses a novel differential amplifier configuration for conjugate homodyne detection. Leveraging separate integrated photonics and integrated analog circuit platforms, we present an alternative method for QRNG. This approach exploits the observable $\hat{\text{Z}}$, derived from the sum of squared conjugate quadrature distributions which we compare to the traditional single quadrature approach. Utilizing this method, we report a shot noise clearance (SNC) of 25.6 dB and a common mode rejection ratio (CMRR) of 69 dB for our homodyne detection system. We used a variety of design tools to model and predict performance and compare results with our measurements. The realization of our QRNG system consists of a 90° optical hybrid, a dual differential transimpedance amplifier (TIA), and a field-programmable gate array (FPGA) used for the real-time post-processing to produce a uniform random bitstream. The randomness extraction is implemented using a Toeplitz hashing algorithm and is validated by the National Institute of Standards and Technology (NIST) randomness test suites.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02077
Performance Comparison of Deep Learning Techniques in Naira Classification,['Computer Vision and Pattern Recognition'],"['Ismail Ismail Tijjani', 'Ahmad Abubakar Mustapha', ""Isma'il Tijjani Idris""]","The Naira is Nigeria's official currency in daily transactions. This study presents the deployment and evaluation of Deep Learning (DL) models to classify Currency Notes (Naira) by denomination. Using a diverse dataset of 1,808 images of Naira notes captured under different conditions, trained the models employing different architectures and got the highest accuracy with MobileNetV2, the model achieved a high accuracy rate of in training of 90.75% and validation accuracy of 87.04% in classification tasks and demonstrated substantial performance across various scenarios. This model holds significant potential for practical applications, including automated cash handling systems, sorting systems, and assistive technology for the visually impaired. The results demonstrate how the model could boost the Nigerian economy's security and efficiency of financial transactions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02072
"Three Warm Jupiters orbiting TOI-6628, TOI-3837, TOI-5027 and one sub-Saturn orbiting TOI-2328",['Earth and Planetary Astrophysics'],"['Marcelo Tala Pinto', 'Andrés Jordán', 'Lorena Acuña', 'Matías Jones', 'Rafael Brahm', 'Yared Reinarz', 'Jan Eberhardt', 'Néstor Espinoza', 'Thomas Henning', 'Melissa Hobson', 'Felipe Rojas', 'Martin Schlecker', 'Trifon Trifonov', 'Gaspar Bakos', 'Gavin Boyle', 'Zoltan Csubry', 'Joel Hartmann', 'Benjamin Knepper', 'Laura Kreidberg', 'Vincent Suc', 'Johanna Teske', 'R. Paul Butler', 'Jeffrey Crane', 'Steve Schectman', 'Ian Thompson']","We report the discovery and characterization of three new transiting giant planets orbiting TOI-6628, TOI-3837 and TOI-5027, and one new warm sub-Saturn orbiting TOI-2328, whose transits events were detected in the lightcurves of the Transiting Exoplanet Survey Satellite \textbf{(TESS)} space mission. By combining TESS lightcurves with ground-based photometric and spectroscopic follow-up observations we confirm the planetary nature of the observed transits and radial velocity variations. TOI-6628~$b$ has a mass of 0.75$\pm$0.06~$M_\mathrm{J}$, a radius of 0.98$\pm$0.05~$R_J$ and is orbiting a metal-rich star with a period of 18.18424$\pm{0.00001}$ days and an eccentricity of 0.667$\pm0.016$, making it one of the most eccentric orbits of all known warm giants. TOI-3837~$b$ has a mass of 0.59$\pm$0.06~$M_\mathrm{J}$, a radius of 0.96$\pm$0.05~$R_J$ and orbits its host star every 11.88865$\pm$0.00003~days, with a moderate eccentricity of 0.198$^{+0.046}_{-0.058}$. With a mass of 2.01$\pm$0.13~$M_\mathrm{J}$ and a radius of 0.99$^{+0.07}_{-0.12}$ $R_J$, TOI-5027~$b$ orbits its host star in an eccentric orbit with $e$~=~0.395$^{+0.032}_{-0.029}$ every 10.24368$\pm{0.00001}$~days. TOI-2328~$b$ is a Saturn-like planet with a mass of 0.16$\pm$0.02~$M_\mathrm{J}$ and a radius of 0.89$\pm$0.04~$R_J$, orbiting its host star in a nearly circular orbit with $e$~=~0.057$^{+0.046}_{-0.029}$ at an orbital period of 17.10197$\pm{0.00001}$ days.
  All four planets have orbital periods above 10 days, and our planet interior structure models are consistsent a rocky-icy core with a H/He envelope, providing evidence supporting the core accretion model of planet formation for this kind of planets.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02069
Transverse magnetic focusing in two-dimensional hole gases,['Mesoscale and Nanoscale Physics'],"['Yik K. Lee', 'Jackson S. Smith', 'Hong Liu', 'Dimitrie Culcer', 'Oleg P. Sushkov', 'Alexander R. Hamilton', 'Jared H. Cole']","Two-dimensional hole gases (2DHGs) have strong intrinsic spin-orbit coupling and could be used to build spin filters by utilising transverse magnetic focusing (TMF). However, with an increase in the spin degree of freedom, holes demonstrate significantly different behaviour to electrons in TMF experiments, making it difficult to interpret the results of these experiments. In this paper, we numerically model TMF in a 2DHG within a GaAs/Al$_{\mathrm{x}}$Ga$_{\mathrm{1-x}}$As heterostructure. Our band structure calculations show that the heavy $(\langle J_{z} \rangle = \pm\frac{3}{2})$ and light $(\langle J_{z} \rangle = \pm\frac{1}{2})$ hole states in the valence band mix at finite $k$, and the heavy hole subbands which are spin-split due to the Rashba effect are not spin-polarised. This lack of spin polarisation casts doubt on the viability of spin filtering using TMF in 2DHGs within conventional GaAs/Al$_{\mathrm{x}}$Ga$_{\mathrm{1-x}}$As heterostructures. We then calculate transport properties of the 2DHG with spin projection and offer a new perspective on interpreting and designing TMF experiments in 2DHGs.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02067
A Classic-Quantum Hybrid Network Framework: CQH-Net,['Quantum Physics'],"['Ao Liu', 'Cuihong Wen', 'Jieci Wang']","Deep Learning has shown remarkable capabilities in pattern recognition, feature extraction, and classification decision-making. With the rise of quantum computing, the potential of quantum neural networks (QNNs) in Artificial Intelligence is emerging. However, the intrinsic mechanisms and decision transparency of QNNs remain unclear. In this paper, we propose a classic-quantum hybrid network framework (CQH-Net), which uses traditional machine learning methods for feature extraction and quantizes neural networks for classification tasks. We apply CQH-Net to image classification on public datasets. Experimentally, CQH-Net achieves an average convergence rate improvement of 72.8% compared to classical convolutional networks (CNNs) with identical parameters. On the Fashion MNIST dataset, it reaches a final accuracy of 99.02%, representing a significant increase of 5.07% over CNNs. Furthermore, we explore visual explanations for CQH-Net's decision-making process. Results show that the model effectively captures key data features during training and establishes associations between these features and their corresponding categories. This study demonstrates that quantization enhances the models ability to tackle complex classification problems while providing transparency in its decision-making process further supporting quantum advantages in machine learning.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02059
BN-AuthProf: Benchmarking Machine Learning for Bangla Author Profiling on Social Media Texts,['Computation and Language'],"['Raisa Tasnim', 'Mehanaz Chowdhury', 'Md Ataur Rahman']","Author profiling, the analysis of texts to uncover attributes such as gender and age of the author, has become essential with the widespread use of social media platforms. This paper focuses on author profiling in the Bangla language, aiming to extract valuable insights about anonymous authors based on their writing style on social media. The primary objective is to introduce and benchmark the performance of machine learning approaches on a newly created Bangla Author Profiling dataset, BN-AuthProf. The dataset comprises 30,131 social media posts from 300 authors, labeled by their age and gender. Authors' identities and sensitive information were anonymized to ensure privacy. Various classical machine learning and deep learning techniques were employed to evaluate the dataset. For gender classification, the best accuracy achieved was 80% using Support Vector Machine (SVM), while a Multinomial Naive Bayes (MNB) classifier achieved the best F1 score of 0.756. For age classification, MNB attained a maximum accuracy score of 91% with an F1 score of 0.905. This research highlights the effectiveness of machine learning in gender and age classification for Bangla author profiling, with practical implications spanning marketing, security, forensic linguistics, education, and criminal investigations, considering privacy and biases.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02058
Comparative Analysis of Multi-Agent Reinforcement Learning Policies for Crop Planning Decision Support,['Machine Learning'],"['Anubha Mahajan', 'Shreya Hegde', 'Ethan Shay', 'Daniel Wu', 'Aviva Prins']","In India, the majority of farmers are classified as small or marginal, making their livelihoods particularly vulnerable to economic losses due to market saturation and climate risks. Effective crop planning can significantly impact their expected income, yet existing decision support systems (DSS) often provide generic recommendations that fail to account for real-time market dynamics and the interactions among multiple farmers. In this paper, we evaluate the viability of three multi-agent reinforcement learning (MARL) approaches for optimizing total farmer income and promoting fairness in crop planning: Independent Q-Learning (IQL), where each farmer acts independently without coordination, Agent-by-Agent (ABA), which sequentially optimizes each farmer's policy in relation to the others, and the Multi-agent Rollout Policy, which jointly optimizes all farmers' actions for global reward maximization. Our results demonstrate that while IQL offers computational efficiency with linear runtime, it struggles with coordination among agents, leading to lower total rewards and an unequal distribution of income. Conversely, the Multi-agent Rollout policy achieves the highest total rewards and promotes equitable income distribution among farmers but requires significantly more computational resources, making it less practical for large numbers of agents. ABA strikes a balance between runtime efficiency and reward optimization, offering reasonable total rewards with acceptable fairness and scalability. These findings highlight the importance of selecting appropriate MARL approaches in DSS to provide personalized and equitable crop planning recommendations, advancing the development of more adaptive and farmer-centric agricultural decision-making systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02057
"A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil and Sinhala",['Computation and Language'],"['Surangika Ranathunga', 'Asanka Ranasinghea', 'Janaka Shamala', 'Ayodya Dandeniyaa', 'Rashmi Galappaththia', 'Malithi Samaraweeraa']","This paper presents a multi-way parallel English-Tamil-Sinhala corpus annotated with Named Entities (NEs), where Sinhala and Tamil are low-resource languages. Using pre-trained multilingual Language Models (mLMs), we establish new benchmark Named Entity Recognition (NER) results on this dataset for Sinhala and Tamil. We also carry out a detailed investigation on the NER capabilities of different types of mLMs. Finally, we demonstrate the utility of our NER system on a low-resource Neural Machine Translation (NMT) task. Our dataset is publicly released: https://github.com/suralk/multiNER.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02056
FoveaSPAD: Exploiting Depth Priors for Adaptive and Efficient Single-Photon 3D Imaging,['Image and Video Processing'],"['Justin Folden', 'Atul Ingle', 'Sanjeev J. Koppal']","Fast, efficient, and accurate depth-sensing is important for safety-critical applications such as autonomous vehicles. Direct time-of-flight LiDAR has the potential to fulfill these demands, thanks to its ability to provide high-precision depth measurements at long standoff distances. While conventional LiDAR relies on avalanche photodiodes (APDs), single-photon avalanche diodes (SPADs) are an emerging image-sensing technology that offer many advantages such as extreme sensitivity and time resolution. In this paper, we remove the key challenges to widespread adoption of SPAD-based LiDARs: their susceptibility to ambient light and the large amount of raw photon data that must be processed to obtain in-pixel depth estimates. We propose new algorithms and sensing policies that improve signal-to-noise ratio (SNR) and increase computing and memory efficiency for SPAD-based LiDARs. During capture, we use external signals to \emph{foveate}, i.e., guide how the SPAD system estimates scene depths. This foveated approach allows our method to ``zoom into'' the signal of interest, reducing the amount of raw photon data that needs to be stored and transferred from the SPAD sensor, while also improving resilience to ambient light. We show results both in simulation and also with real hardware emulation, with specific implementations achieving a 1548-fold reduction in memory usage, and our algorithms can be applied to newly available and future SPAD arrays.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02052
Impact of Data Snooping on Deep Learning Models for Locating Vulnerabilities in Lifted Code,['Cryptography and Security'],"['Gary A. McCully', 'John D. Hastings', 'Shengjie Xu']","This study examines the impact of data snooping on neural networks for vulnerability detection in lifted code, building on previous research which used word2vec, and unidirectional and bidirectional transformer-based embeddings. The research specifically focuses on how model performance is affected when embedding models are trained on datasets, including samples also used for neural network training and validation. The results show that introducing data snooping did not significantly alter model performance, suggesting that data snooping had a minimal impact or that samples randomly dropped as part of the methodology contained hidden features critical to achieving optimal performance. In addition, the findings reinforce the conclusions of previous research, which found that models trained with GPT-2 embeddings consistently outperformed neural networks trained with other embeddings. The fact that this holds even when data snooping is introduced into the embedding model indicates GPT-2's robustness in representing complex code features, even under less-than-ideal conditions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02048
Simplifying HPC resource selection: A tool for optimizing execution time and cost on Azure,"['Distributed, Parallel, and Cluster Computing']","['Marco A. S. Netto', 'Wolfgang De Savador', 'Davide Vanzo']","Azure Cloud offers a wide range of resources for running HPC workloads, requiring users to configure their deployment by selecting VM types, number of VMs, and processes per VM. Suboptimal decisions may lead to longer execution times or additional costs for the user. We are developing an open-source tool to assist users in making these decisions by considering application input parameters, as they influence resource consumption. The tool automates the time-consuming process of setting up the cloud environment, executing the benchmarking runs, handling output, and providing users with resource selection recommendations as high level insights on run times and costs across different VM types and number of VMs. In this work, we present initial results and insights on reducing the number of cloud executions needed to provide such guidance, leveraging data analytics and optimization techniques with two well-known HPC applications: OpenFOAM and LAMMPS.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02047
Delta invariants of plumbed 3-manifolds,['Geometric Topology'],"['Shimal Harichurn', 'András Némethi', 'Josef Svoboda']","We study the minimal $q$-exponent $Δ$ in the BPS $q$-series $\widehat{Z}$ of negative definite plumbed 3-manifolds equipped with a spin$^c$-structure. We express $Δ$ of Seifert manifolds in terms of an invariant commonly used in singularity theory. We provide several examples illustrating the interesting behaviour of $Δ$ for non-Seifert manifolds. Finally, we compare $Δ$ invariants with correction terms in Heegaard--Floer homology.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02042
"High-resolution, Wide-frequency-range Magnetic Spectroscopy with Solid-state Spin Ensembles",['Quantum Physics'],"['Zechuan Yin', 'Justin J. Welter', 'Connor A. Hart', 'Paul V. Petruzzi', 'Ronald L. Walsworth']","Quantum systems composed of solid-state electronic spins can be sensitive detectors of narrowband magnetic fields. A prominent example is the nitrogen-vacancy (NV) center in diamond, which has been employed for magnetic spectroscopy with high spatial and spectral resolution. However, NV-diamond spectroscopy protocols are typically based on dynamical decoupling sequences, which are limited to low-frequency signals ($\lesssim{20}\,$MHz) due to the technical requirements on microwave (MW) pulses used to manipulate NV electronic spins. In this work, we experimentally demonstrate a high-resolution magnetic spectroscopy protocol that integrates a quantum frequency mixing (QFM) effect in a dense NV ensemble with coherently averaged synchronized readout (CASR) to provide both a wide range of signal frequency detection and sub-Hz spectral resolution. We assess the sensitivity of this QFM-CASR protocol across a frequency range of 10$\,$MHz to 4$\,$GHz. By measuring the spectra of multi-frequency signals near 0.6, 2.4 and 4$\,$GHz, we demonstrate sub-Hz spectral resolution with a nT-scale noise floor for the target signal, and precise phase measurement with error $<1^\circ$. Compared to state-of-the-art NV-diamond techniques for narrowband magnetic spectroscopy, the QFM-CASR protocol greatly extends the detectable frequency range, enabling applications in high-frequency radio frequency (RF) and MW signal microscopy and analysis, as well as tesla-scale nuclear magnetic resonance (NMR) spectroscopy of small samples.△ Less",v1,https://arxiv.org/pdf/2412.02040
Mutli-View 3D Reconstruction using Knowledge Distillation,['Computer Vision and Pattern Recognition'],"['Aditya Dutt', 'Ishikaa Lunawat', 'Manpreet Kaur']","Large Foundation Models like Dust3r can produce high quality outputs such as pointmaps, camera intrinsics, and depth estimation, given stereo-image pairs as input. However, the application of these outputs on tasks like Visual Localization requires a large amount of inference time and compute resources. To address these limitations, in this paper, we propose the use of a knowledge distillation pipeline, where we aim to build a student-teacher model with Dust3r as the teacher and explore multiple architectures of student models that are trained using the 3D reconstructed points output by Dust3r. Our goal is to build student models that can learn scene-specific representations and output 3D points with replicable performance such as Dust3r. The data set we used to train our models is 12Scenes. We test two main architectures of models: a CNN-based architecture and a Vision Transformer based architecture. For each architecture, we also compare the use of pre-trained models against models built from scratch. We qualitatively compare the reconstructed 3D points output by the student model against Dust3r's and discuss the various features learned by the student model. We also perform ablation studies on the models through hyperparameter tuning. Overall, we observe that the Vision Transformer presents the best performance visually and quantitatively.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02039
CaliScalpel: In-Situ and Fine-Grained Qubit Calibration Integrated with Surface Code Quantum Error Correction,['Quantum Physics'],"['Xiang Fang', 'Keyi Yin', 'Yuchen Zhu', 'Jixuan Ruan', 'Dean Tullsen', 'Zhiding Liang', 'Andrew Sornborger', 'Ang Li', 'Travis Humble', 'Yufei Ding', 'Yunong Shi']","Quantum Error Correction (QEC) is a cornerstone of fault-tolerant, large-scale quantum computing. However, qubit error drift significantly degrades QEC performance over time, necessitating periodic calibration. Traditional calibration methods disrupt quantum states, requiring system downtime and making in situ calibration infeasible. We present CaliScalpel, an innovative framework for in situ calibration in surface codes. The core idea behind CaliScalpel is leveraging code deformation to isolate qubits undergoing calibration from logical patches. This allows calibration to proceed concurrently with computation, while code enlargement maintains error correction capabilities with minimal qubit overhead. Additionally, CaliScalpel incorporates optimized calibration schedules derived from detailed device characterization, effectively minimizing physical error rates. Our results show that CaliScalpel achieves concurrent calibration and computation with modest qubit overhead and negligible execution time impact, marking a significant step toward practical in situ calibration in surface-code-based quantum computing systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02036
LLMs4Life: Large Language Models for Ontology Learning in Life Sciences,['Artificial Intelligence'],"['Nadeen Fathallah', 'Steffen Staab', 'Alsayed Algergawy']","Ontology learning in complex domains, such as life sciences, poses significant challenges for current Large Language Models (LLMs). Existing LLMs struggle to generate ontologies with multiple hierarchical levels, rich interconnections, and comprehensive class coverage due to constraints on the number of tokens they can generate and inadequate domain adaptation. To address these issues, we extend the NeOn-GPT pipeline for ontology learning using LLMs with advanced prompt engineering techniques and ontology reuse to enhance the generated ontologies' domain-specific reasoning and structural depth. Our work evaluates the capabilities of LLMs in ontology learning in the context of highly specialized and complex domains such as life science domains. To assess the logical consistency, completeness, and scalability of the generated ontologies, we use the AquaDiva ontology developed and used in the collaborative research center AquaDiva as a case study. Our evaluation shows the viability of LLMs for ontology learning in specialized domains, providing solutions to longstanding limitations in model performance and scalability.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02035
Cargo Delivery to Cells Using Laser-Irradiated Carbon-Black-Loaded PDMS,['Materials Science'],"['Weilu Shen', 'Anqi Chen', 'Gurminder K. Paink', 'Nicole Black', 'David Weitz', 'Eric Mazur']","Effective intracellular delivery is essential for successful gene editing of cells. Spatially selective delivery to cells that is simultaneously precise, consistent, and non-destructive remains challenging using conventional state-of-the-art techniques. Here, we introduce a carrier-free method for spatiotemporal delivery of fluorescently labeled cargo into both adherent and suspension cells using carbon-black-embedded polydimethylsiloxane (PDMS) substrates irradiated by nanosecond laser pulses. This low-cost, biocompatible material, coupled with an optical approach, enables scalable, spatially selective, and sequential delivery of multiple cargo molecules, including FITC-dextran and siRNA, to a broad range of cells. Notably, we achieved siRNA delivery into the cytoplasm of hard-to-transfect K562 cells with 45% efficiency, while maintaining nearly 100% cell viability.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02032
A family of polylogarithmic integrals,['Classical Analysis and ODEs'],"['Anthony Sofo', 'Jean-Christophe Pain', 'Victor Scharaschkin']","In this paper we investigate a class of integrals that were encountered in the course of a work on statistical plasma physics, in the so-called Sommerfeld temperature-expansion of the electronic entropy. We show that such integrals, involving some parameters, can be fully described in closed form represented by special functions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02031
Multi-component secluded WIMP dark matter and Dirac neutrino masses with an extra Abelian gauge symmetry,['High Energy Physics - Phenomenology'],"['Kimy Agudelo', 'Diego Restrepo', 'Andrés Rivera', 'David Suarez']","Scenarios for secluded WIMP dark matter models have been extensively studied in simplified versions. This paper shows a complete UV realization of a secluded WIMP dark matter model with an extra Abelian gauge symmetry that includes two-component dark matter candidates, where the dark matter conversion process plays a significant role in determining the relic density in the Universe. The model contains two new unstable mediators: a dark Higgs and a dark photon. It generates Dirac neutrino masses and can be tested in future direct detection experiments of dark matter. The model is also compatible with cosmological and theoretical constraints, including the branching ratio of Standard model particles into invisible, Big Bang nucleosynthesis restrictions, and the number of relativistic degrees of freedom in the early Universe, even without kinetic mixing.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02027
"Cutting Soft Matter: Scaling relations controlled by toughness, friction, and wear",['Soft Condensed Matter'],"['Bharath Antarvedi Goda', 'Zhenwei Ma', 'Stefano Fregonese', 'Mattia Bacca']","Cutting mechanics of soft solids is gaining rapid attention thanks to its promising benefits in material characterization and other applications. However, a full understanding of the physical phenomena is still missing, and several questions remain outstanding. E.g.: How can we directly and reliably measure toughness from cutting experiments? What is the role of blade sharpness? In this paper, we explore the simple problem of wire cutting, where blade sharpness is only defined by the wire radius. Through finite element analysis, we obtain a simple scaling relation between the wire radius and the steady-state cutting force per unit sample thickness. The cutting force is independent of the wire radius if the latter is below a transition length, while larger radii produce a linear force-radius correlation. The minimum cutting force, for small radii, is given by cleavage toughness, i.e., the surface energy required to break covalent bonds in the crack plane. The force-radius slope is instead given by the wear shear strength in the material. Via cutting experiments on polyacrylamide gels, we find that the magnitude of shear strength is close to the work of fracture of the material, i.e., the critical strain energy density required to break a pristine sample in uniaxial tension. The work of fracture characterizes the toughening contribution from the fracture process zone (FPZ), which adds to cleavage toughness. Our study provides two important messages, that answer the above questions: Toughness can be estimated from wire-cutting experiments from the intercept of the force-radius linear correlation, as previously explored. However, as we discovered, this only estimates cleavage toughness. Additionally, the force-radius slope is correlated with the work of fracture, giving an estimation of the dissipative contributions from the FPZ.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02021
Sensing Few Electrons Floating on Helium with High-Electron-Mobility Transistors,['Mesoscale and Nanoscale Physics'],"['Mayer M. Feldman', 'Gordian Fuchs', 'Tiffany Liu', ""Luke A. D'Imperio"", 'M. David Henry', 'Eric A. Shaner', 'Stephen A. Lyon']","We report on low-frequency measurements of few electrons floating on superfluid helium using a bespoke cryogenic cascode amplifier circuit built with off-the-shelf GaAs High-Electron-Mobility Transistors (HEMTs). We integrate this circuit with a Charge-Coupled Device (CCD) to transport the electrons on helium and characterize its performance. We show that this circuit has a Signal-to-Noise ratio (SNR) of $\thicksim$ 2$\frac{e}{\sqrt{Hz}}$ at 102 kHz, an order of magnitude improvement from previous implementations and provides a compelling alternative to few electron sensing with high frequency resonators.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02018
Certifying nontriviality of Ceresa classes of curves,['Algebraic Geometry'],"['Jordan Ellenberg', 'Adam Logan', 'Padmavathi Srinivasan']","The Ceresa cycle is a canonical algebraic $1$-cycle on the Jacobian of an algebraic curve. We construct an algorithm which, given a curve over a number field, often provides a certificate that the Ceresa cycle is non-torsion, without relying on the presence of any additional symmetries of the curve. Under the hypothesis that the Sato--Tate group is the whole of $\operatorname*{GSp}$, we prove that if the Ceresa class (the image of the Ceresa cycle in étale cohomology) is non-torsion, then the algorithm will eventually terminate with a certificate attesting to this fact.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02015
Dynamic Prediction of High-density Generalized Functional Data with Fast Generalized Functional Principal Component Analysis,['Methodology'],"['Ying Jin', 'Andrew Leroux']","Dynamic prediction, which typically refers to the prediction of future outcomes using historical records, is often of interest in biomedical research. For datasets with large sample sizes, high measurement density, and complex correlation structures, traditional methods are often infeasible because of the computational burden associated with both data scale and model complexity. Moreover, many models do not directly facilitate out-of-sample predictions for generalized outcomes. To address these issues, we develop a novel approach for dynamic predictions based on a recently developed method estimating complex patterns of variation for exponential family data: fast Generalized Functional Principal Components Analysis (fGFPCA). Our method is able to handle large-scale, high-density repeated measures much more efficiently with its implementation feasible even on personal computational resources (e.g., a standard desktop or laptop computer). The proposed method makes highly flexible and accurate predictions of future trajectories for data that exhibit high degrees of nonlinearity, and allows for out-of-sample predictions to be obtained without reestimating any parameters. A simulation study is designed and implemented to illustrate the advantages of this method. To demonstrate its practical utility, we also conducted a case study to predict diurnal active/inactive patterns using accelerometry data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014. Both the simulation study and the data application demonstrate the better predictive performance and high computational efficiency of the proposed method compared to existing methods. The proposed method also obtains more personalized prediction that improves as more information becomes available, which is an essential goal of dynamic prediction that other methods fail to achieve.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02014
ALMA-IMF XVI: Mass-averaged temperature of cores and protostellar luminosities in the ALMA-IMF protoclusters,['Astrophysics of Galaxies'],"['F. Motte', 'Y. Pouteau', 'T. Nony', ""P. Dell'Ova"", 'A. Gusdorf', 'N. Brouillet', 'A. M. Stutz', 'S. Bontemps', 'A. Ginsburg', 'T. Csengeri', ""A. Men'shchikov"", 'M. Valeille-Manet', 'F. Louvet', 'M. Bonfand', 'R. Galván-Madrid', 'R. H. Álvarez-Gutiérrez', 'M. Armante', 'L. Bronfman', 'H. -R. V. Chen', 'N. Cunningham', 'D. Díaz-González', 'P. Didelon', 'M. Fernández-López', 'F. Herpin', 'N. Kessler']","ALMA-IMF imaged 15 massive protoclusters down to a resolution of of 2 kau scales, identifying about 1000 star-forming cores. The mass and luminosity of these cores, which are fundamental physical characteristics, are difficult to determine, a problem greatly exacerbated at the distances >2 kpc of ALMA-IMF protoclusters. We combined new datasets and radiative transfer modeling to characterize these cores. We estimated their mass-averaged temperature and the masses these estimates imply. For 1/6 of the sample, we measured the bolometric luminosities, implementing deblending corrections when necessary. We used spectral energy distribution (SED) analysis obtained with the PPMAP Bayesian procedure, which aims to preserve the best angular resolution of the input data. We extrapolated the luminosity and dust temperature images provided by PPMAP at 2.5"" resolution to estimate those of individual cores, which were identified at higher angular resolution. To do this, we applied approximate radiative transfer relationships between the luminosity of a protostar and the temperature of its surrounding envelope and between the external heating of prestellar cores and their temperatures. For the first time, we provide data-informed estimates of dust temperatures for 883 cores identified with ALMA-IMF: 17-31 K and 28-79 K (5th and 95th percentiles, up to 127 K) for the 617 prestellar and 266 protostellar cores, respectively. We also measured protostellar luminosities spanning 20-80 000 Lsun. For hot cores, we estimated systematically lower temperatures than studies based on complex organic molecules. We established a mass-luminosity evolutionary diagram, for the first time at the core spatial resolution and for a large sample of high-mass protostellar cores. The ALMA-IMF data favor a scenario in which protostars accrete their mass from a larger mass reservoir than their host cores.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02011
Optimizing Genetic Algorithms Using the Binomial Distribution,['Neural and Evolutionary Computing'],['Vincent A. Cicirello'],"Evolutionary algorithms rely very heavily on randomized behavior. Execution speed, therefore, depends strongly on how we implement randomness, such as our choice of pseudorandom number generator, or the algorithms used to map pseudorandom values to specific intervals or distributions. In this paper, we observe that the standard bit-flip mutation of a genetic algorithm (GA), uniform crossover, and the GA control loop that determines which pairs of parents to cross are all in essence binomial experiments. We then show how to optimize each of these by utilizing a binomial distribution and sampling algorithms to dramatically speed the runtime of a GA relative to the common implementation. We implement our approach in the open-source Java library Chips-n-Salsa. Our experiments validate that the approach is orders of magnitude faster than the common GA implementation, yet produces solutions that are statistically equivalent in solution quality.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02009
Unveiling Interpretability in Self-Supervised Speech Representations for Parkinson's Diagnosis,['Computer Vision and Pattern Recognition'],"['David Gimeno-Gómez', 'Catarina Botelho', 'Anna Pompili', 'Alberto Abad', 'Carlos-D. Martínez-Hinarejos']","Recent works in pathological speech analysis have increasingly relied on powerful self-supervised speech representations, leading to promising results. However, the complex, black-box nature of these embeddings and the limited research on their interpretability significantly restrict their adoption for clinical diagnosis. To address this gap, we propose a novel, interpretable framework specifically designed to support Parkinson's Disease (PD) diagnosis. Through the design of simple yet effective cross-attention mechanisms for both embedding- and temporal-level analysis, the proposed framework offers interpretability from two distinct but complementary perspectives. Experimental findings across five well-established speech benchmarks for PD detection demonstrate the framework's capability to identify meaningful speech patterns within self-supervised representations for a wide range of assessment tasks. Fine-grained temporal analyses further underscore its potential to enhance the interpretability of deep-learning pathological speech models, paving the way for the development of more transparent, trustworthy, and clinically applicable computer-assisted diagnosis systems in this domain. Moreover, in terms of classification accuracy, our method achieves results competitive with state-of-the-art approaches, while also demonstrating robustness in cross-lingual scenarios when applied to spontaneous speech production.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02006
Open Source Evolutionary Computation with Chips-n-Salsa,['Neural and Evolutionary Computing'],['Vincent A. Cicirello'],"When it was first introduced, the Chips-n-Salsa Java library provided stochastic local search and related algorithms, with a focus on self-adaptation and parallel execution. For the past four years, we expanded its scope to include evolutionary computation. This paper concerns the evolutionary algorithms that Chips-n-Salsa now provides, which includes multiple evolutionary models, common problem representations, a wide range of mutation and crossover operators, and a variety of benchmark problems. Well-defined Java interfaces enable easily integrating custom representations and evolutionary operators, as well as defining optimization problems. Chips-n-Salsa's evolutionary algorithms include implementations with adaptive mutation and crossover rates, as well as both sequential and parallel execution. Source code is maintained on GitHub, and immutable artifacts are regularly published to the Maven Central Repository to enable easily importing into projects for reproducible builds. Effective development processes such as test-driven development, as well as a variety of static analysis tools help ensure code quality.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02004
On complexity of alternating link equivalence,['Geometric Topology'],"['Touseef Haider', 'Anastasiia Tsvietkova']","Link equivalence up to isotopy in a 3-space is the problem that lies at the root of knot theory, and is important in 3-dimensional topology and geometry. We consider its restriction to alternating links, given by two alternating diagrams with $n_1$ and $n_2$ crossings, and show that this problem has polynomial algorithm in terms of $max\{n_1, n_2\}$. For the proof, we use Tait flyping conjectures, observations from the work of Lackenby, Menasco, Sundberg and Thistlethwaite on alternating links, and algorithmic complexity of some problems from graph theory and topological graph theory.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02003
"Comparing Dynamics, Pinning and Ratchet Effects for Skyrmionium, Skyrmions, and Antiskyrmions",['Mesoscale and Nanoscale Physics'],"['J. C. Bellizotti Souza', 'N. P. Vizarim', 'C. J. O. Reichhardt', 'C. Reichhardt', 'P. A. Venegas']","We compare the driven dynamics of skyrmions, antiskyrmions, and skyrmionium interacting with random disorder, circular defects, and asymmetric potentials. When interacting with a line defect at a constant drive, skyrmions and antiskyrmions show an acceleration effect for motion along the wall and a drop in velocity when they can cross the barrier. In contrast, skyrmionium travels at a reduced velocity when moving along a wall, and exhibits an increase in velocity once it can cross the barrier. For point defects, skyrmionium can be pinned for a finite fixed period of time, while for skyrmions and antiskyrmions, the Magnus force creates a deflection from the defect and an acceleration effect. For a given drive, skyrmionium moves twice as fast as skyrmions; however, skyrmionium is more susceptible to pinning effects than skyrmions and antiskyrmions. Additionally, there is a critical threshold where the skyrmionium transforms to a skyrmion that is associated with a drop in the velocity of the texture. We show that all three textures exhibit diode and ratchet effects when interacting with an asymmetric substrate, but skyrmions and antiskyrmions show a stronger ratcheting effect than skyrmionium due to the Magnus force.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.02001
Hamiltonian characterisation of multi-time processes with classical memory,['Quantum Physics'],"['Kaumudibikash Goswami', 'Abhinash Kumar Roy', 'Varun Srivastava', 'Barr Perez', 'Christina Giarmatzi', 'Alexei Gilchrist', 'Fabio Costa']","A central problem in the study of open quantum systems is the characterisation of non-Markovian processes, where an environment retains memory of its interaction with the system. A key distinction is whether or not this memory can be simulated classically, as this can lead to efficient modelling and noise mitigation. Powerful tools have been developed recently within the process matrix formalism, a framework that conveniently characterises all multi-time correlations through a sequence of measurements. This leads to a detailed classification of classical and quantum-memory processes and provides operational procedures to distinguish between them. However, these results leave open the question of what type of system-environment interactions lead to classical memory. More generally, process-matrix methods lack a direct connection to joint system-environment evolution, a cornerstone of open-system modelling. In this work, we characterise Hamiltonian and circuit-based models of system-environment interactions leading to classical memory. We show that general time-dependent Hamiltonians with product eigenstates, and where the environment's eigenstates form a time-independent, orthonormal basis, always produce a particular type of classical memory: probabilistic mixtures of unitary processes. Additionally, we show that the most general type of classical-memory processes can be generated by a quantum circuit in which system and environment interact through a specific class of controlled unitaries. Our results establish the first strong link between process-matrix methods and traditional Hamiltonian-based approaches to open quantum systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01998
Local and nonlocal critical growth anisotropic quasilinear elliptic systems,['Analysis of PDEs'],"['Artur Jorge Marinho', 'Kanishka Perera']","In this paper we prove new multiplicity results for a critical growth anisotropic quasilinear elliptic system that is coupled through a subcritical perturbation term. We identify a certain scaling for the system and a parameter γ related to this scaling that determines the geometry of the associated variational functional. This leads to a natural classification of different nonlinear regimes for the system in terms of scaling properties of the perturbation term. We give three different types of multiplicity results in the three regimes γ = 1, γ > 1, and γ < 1. Proofs of our multiplicity results are based on a new abstract critical point theorem for symmetric functionals on product spaces, which we prove using the piercing property of the Z2-cohomological index of Fadell and Rabinowitz. This abstract result only requires a local (PS) condition and is therefore applicable to systems with critical growth. It is of independent interest as it has wide applicability to many different types of critical elliptic systems. We also indicate how it can be applied to obtain similar multiplicity results for nonlocal systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01997
Real-Time Multilingual Sign Language Processing,['Computation and Language'],['Amit Moryossef'],"Sign Language Processing (SLP) is an interdisciplinary field comprised of Natural Language Processing (NLP) and Computer Vision. It is focused on the computational understanding, translation, and production of signed languages. Traditional approaches have often been constrained by the use of gloss-based systems that are both language-specific and inadequate for capturing the multidimensional nature of sign language. These limitations have hindered the development of technology capable of processing signed languages effectively.
  This thesis aims to revolutionize the field of SLP by proposing a simple paradigm that can bridge this existing technological gap. We propose the use of SignWiring, a universal sign language transcription notation system, to serve as an intermediary link between the visual-gestural modality of signed languages and text-based linguistic representations.
  We contribute foundational libraries and resources to the SLP community, thereby setting the stage for a more in-depth exploration of the tasks of sign language translation and production. These tasks encompass the translation of sign language from video to spoken language text and vice versa. Through empirical evaluations, we establish the efficacy of our transcription method as a pivot for enabling faster, more targeted research, that can lead to more natural and accurate translations across a range of languages.
  The universal nature of our transcription-based paradigm also paves the way for real-time, multilingual applications in SLP, thereby offering a more inclusive and accessible approach to language technology. This is a significant step toward universal accessibility, enabling a wider reach of AI-driven language technologies to include the deaf and hard-of-hearing community.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01991
Terahertz stimulated parametric downconversion of a magnon mode in an antiferromagnet,['Materials Science'],"['Zhuquan Zhang', 'Yu-Che Chien', 'Man Tou Wong', 'Frank Y. Gao', 'Zi-Jie Liu', 'Xiaoxuan Ma', 'Shixun Cao', 'Edoardo Baldini', 'Keith A. Nelson']","In condensed matter systems, interactions between collective modes offer avenues for nonlinear coherent manipulation of coupled excitations and quantum phases. Antiferromagnets, with their inherently coupled magnon modes, provide a promising platform for nonlinear control of microscopic spin waves and macroscopic magnetization. However, nonlinear magnon-magnon interactions have been only partially elaborated, leaving key gaps in the prospects for potential ultrahigh-bandwidth magnonic signal processing. Here, we use a pair of intense terahertz pulses to sequentially excite two distinct coherent magnon modes in an antiferromagnet and find that the magnon mode with a lower frequency undergoes amplification when the higher-frequency mode is driven. We unveil the nonlinear excitation pathways of this stimulated parametric downconversion process by using polarization-selective two-dimensional terahertz spectroscopy. Our work provides fundamental insights into nonlinear magnonics in antiferromagnets, laying the groundwork for forthcoming spintronic and magnonic devices based on nonlinear magnon-magnon interactions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01989
HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment,['Computer Vision and Pattern Recognition'],"['Armin Shafiee Sarvestani', 'Sheyang Tang', 'Zhou Wang']","Mesh quality assessment (MQA) models play a critical role in the design, optimization, and evaluation of mesh operation systems in a wide variety of applications. Current MQA models, whether model-based methods using topology-aware features or projection-based approaches working on rendered 2D projections, often fail to capture the intricate interactions between texture and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid full-reference colored MQA framework that integrates model-based and projection-based approaches, capturing complex interactions between textural information and 3D structures for enriched quality representations. Our method employs graph learning to extract detailed 3D representations, which are then projected to 2D using a novel feature rendering process that precisely aligns them with colored projections. This enables the exploration of geometry-texture interactions via cross-attention, producing comprehensive mesh quality representations. Extensive experiments demonstrate HybridMQA's superior performance across diverse datasets, highlighting its ability to effectively leverage geometry-texture interactions for a thorough understanding of mesh quality. Our implementation will be made publicly available.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01986
Improving feature interactions at Pinterest under industry constraints,['Information Retrieval'],"['Siddarth Malreddy', 'Matthew Lawhon', 'Usha Amrutha Nookala', 'Aditya Mantha', 'Dhruvil Deven Badani']","Adopting advances in recommendation systems is often challenging in industrial settings due to unique constraints. This paper aims to highlight these constraints through the lens of feature interactions. Feature interactions are critical for accurately predicting user behavior in recommendation systems and online advertising. Despite numerous novel techniques showing superior performance on benchmark datasets like Criteo, their direct application in industrial settings is hindered by constraints such as model latency, GPU memory limitations and model reproducibility. In this paper, we share our learnings from improving feature interactions in Pinterest's Homefeed ranking model under such constraints. We provide details about the specific challenges encountered, the strategies employed to address them, and the trade-offs made to balance performance with practical limitations. Additionally, we present a set of learning experiments that help guide the feature interaction architecture selection. We believe these insights will be useful for engineers who are interested in improving their model through better feature interaction learning.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01985
Pooling Solvent Mixtures for Solvation Free Energy Predictions,['Chemical Physics'],"['Roel J. Leenhouts', 'Nathan Morgan', 'Emad Al Ibrahim', 'William H. Green', 'Florence H. Vermeire']","Solvation free energy is an important design parameter in reaction kinetics and separation processes, making it a critical property to predict during process development. In previous research, directed message passing neural networks (D-MPNN) have successfully been used to predict solvation free energies and enthalpies in organic solvents. However, solvent mixtures provide greater flexibility for optimizing solvent interactions than monosolvents. This work aims to extend our previous models to mixtures. To handle mixtures in a permutation invariant manner we propose a pooling function; MolPool. With this pooling function, the machine learning models can learn and predict properties for an arbitrary number of molecules. The novel SolProp-mix software that applies MolPool to D-MPNN was compared to state-of-the-art architectures for predicting mixture properties and validated with our new database of COSMOtherm calculations; BinarySolv-QM. To improve predictions towards experimental accuracy, the network was then fine-tuned on experimental data in monosolvents. To demonstrate the benefit of this transfer learning methodology, experimental datasets of solvation free energies in binary (BinarySolv-Exp) and ternary (TernarySolv-Exp) solvent mixtures were compiled from data on vapor-liquid equilibria and activity coefficients. The neural network performed better than COSMOtherm calculations with an MAE of 0.25 kcal/mol and an RMSE of 0.37 kcal/mol for non-aqueous mixed solvents. Additionally, the ability to capture trends for a varying mixture composition was validated successfully. Our model's ability to accurately predict mixture properties from the combination of in silico data and pure component experimental data is promising given the scarcity of experimental data for mixtures in many fields.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01982
Human-centred test and evaluation of military AI,['Human-Computer Interaction'],"['David Helmer', 'Michael Boardman', 'S. Kate Conroy', 'Adam J. Hepworth', 'Manoj Harjani']","The REAIM 2024 Blueprint for Action states that AI applications in the military domain should be ethical and human-centric and that humans must remain responsible and accountable for their use and effects. Developing rigorous test and evaluation, verification and validation (TEVV) frameworks will contribute to robust oversight mechanisms. TEVV in the development and deployment of AI systems needs to involve human users throughout the lifecycle. Traditional human-centred test and evaluation methods from human factors need to be adapted for deployed AI systems that require ongoing monitoring and evaluation. The language around AI-enabled systems should be shifted to inclusion of the human(s) as a component of the system. Standards and requirements supporting this adjusted definition are needed, as are metrics and means to evaluate them. The need for dialogue between technologists and policymakers on human-centred TEVV will be evergreen, but dialogue needs to be initiated with an objective in mind for it to be productive. Development of TEVV throughout system lifecycle is critical to support this evolution including the issue of human scalability and impact on scale of achievable testing. Communication between technical and non technical communities must be improved to ensure operators and policy-makers understand risk assumed by system use and to better inform research and development. Test and evaluation in support of responsible AI deployment must include the effect of the human to reflect operationally realised system performance. Means of communicating the results of TEVV to those using and making decisions regarding the use of AI based systems will be key in informing risk based decisions regarding use.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01978
A Table Theorem for Surfaces with Odd Euler Characteristic,['Geometric Topology'],['Ali Naseri Sadr'],We use the square peg problem for smooth curves to prove a generalized table Theorem for real valued functions on Riemannian surfaces with odd Euler characteristic. We then use this result to prove the table conjecture for even functions on the two sphere.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01977
Designing Optically Addressable Nitrogen-Vacancy Centers in Ultra-Small Nanodiamonds: Insights from First-Principles Calculations,['Computational Physics'],"['Arpan Kundu', 'Francesco Martinelli', 'Giulia Galli']","Ultrasmall nanodiamonds (USNDs) are promising platforms for fluorescent and quantum sensing applications. Here we present first-principles electronic structure calculations of color centers in USNDs, specifically the nitrogen-vacancy (NV-) and we investigate their optical addressability as a function of the surface termination. We consider both isolated nanoparticles and arrays of USNDs with different degrees of packing, and we include quantum vibronic effects in our analysis, using stochastic methods. We find that the NV in USNDs can be stabilized in a negative charge state if the nanoparticles are terminated by fluorine, hydroxyl, and ether. While fluorine terminations can be used for fluorescent bio-tags, we suggest that hydroxyl and ether terminations are beneficial for quantum sensing applications. We also find that the NV- can be stabilized in arrays of USNDs when inter-particle separations are larger than the diameter of the nanoparticle. Interestingly, the phonon renormalizations of single-particle energy levels found in arrays contribute to the charge stability of negatively charged NV centers.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01976
Reactive Synthesis of Sensor Revealing Strategies in Hypergames on Graphs,['Computer Science and Game Theory'],"['Sumukha Udupa', 'Ahmed Hemida', 'Charles A. Kamhoua', 'Jie Fu']","In many security applications of cyber-physical systems, a system designer must guarantee that critical missions are satisfied against attacks in the sensors and actuators of the CPS. Traditional security design of CPSs often assume that attackers have complete knowledge of the system. In this article, we introduce a class of deception techniques and study how to leverage asymmetric information created by deception to strengthen CPS security. Consider an adversarial interaction between a CPS defender and an attacker, who can perform sensor jamming attacks. To mitigate such attacks, the defender introduces asymmetrical information by deploying a ""hidden sensor,"" whose presence is initially undisclosed but can be revealed if queried. We introduce hypergames on graphs to model this game with asymmetric information. Building on the solution concept called subjective rationalizable strategies in hypergames, we identify two stages in the game: An initial game stage where the defender commits to a strategy perceived rationalizable by the attacker until he deviates from the equilibrium in the attacker's perceptual game; Upon the deviation, a delay-attack game stage starts where the defender plays against the attacker, who has a bounded delay in attacking the sensor being revealed. Based on backward induction, we develop an algorithm that determines, for any given state, if the defender can benefit from hiding a sensor and revealing it later. If the answer is affirmative, the algorithm outputs a sensor revealing strategy to determine when to reveal the sensor during dynamic interactions. We demonstrate the effectiveness of our deceptive strategies through two case studies related to CPS security applications.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01975
Learning a Filtered Backprojection Reconstruction Method for Photoacoustic Computed Tomography with Hemispherical Measurement Geometries,['Medical Physics'],"['Panpan Chen', 'Seonyeong Park', 'Refik Mert Cam', 'Hsuan-Kai Huang', 'Alexander A. Oraevsky', 'Umberto Villa', 'Mark A. Anastasio']","In certain three-dimensional (3D) applications of photoacoustic computed tomography (PACT), including \textit{in vivo} breast imaging, hemispherical measurement apertures that enclose the object within their convex hull are employed for data acquisition. Data acquired with such measurement geometries are referred to as \textit{half-scan} data, as only half of a complete spherical measurement aperture is employed. Although previous studies have demonstrated that half-scan data can uniquely and stably reconstruct the sought-after object, no closed-form reconstruction formula for use with half-scan data has been reported. To address this, a semi-analytic reconstruction method in the form of filtered backprojection (FBP), referred to as the half-scan FBP method, is developed in this work. Because the explicit form of the filtering operation in the half-scan FBP method is not currently known, a learning-based method is proposed to approximate it. The proposed method is systematically investigated by use of virtual imaging studies of 3D breast PACT that employ ensembles of numerical breast phantoms and a physics-based model of the data acquisition process. The method is subsequently applied to experimental data acquired in an \textit{in vivo} breast PACT study. The results confirm that the half-scan FBP method can accurately reconstruct 3D images from half-scan data. Importantly, because the sought-after inverse mapping is well-posed, the reconstruction method remains accurate even when applied to data that differ considerably from those employed to learn the filtering operation.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01971
On the Theoretical Foundations of Data Exchange Economies,['Computer Science and Game Theory'],"['Hannaneh Akrami', 'Bhaskar Ray Chaudhury', 'Jugal Garg', 'Aniket Murhekar']","The immense success of ML systems relies heavily on large-scale, high-quality data. The high demand for data has led to many paradigms that involve selling, exchanging, and sharing data, motivating the study of economic processes with data as an asset. However, data differs from classical economic assets in terms of free duplication: there is no concept of limited supply since it can be replicated at zero marginal cost. This distinction introduces fundamental differences between economic processes involving data and those concerning other assets.
  We study a parallel to exchange (Arrow-Debreu) markets where data is the asset. Here, agents with datasets exchange data fairly and voluntarily, aiming for mutual benefit without monetary compensation. This framework is particularly relevant for non-profit organizations that seek to improve their ML models through data exchange, yet are restricted from selling their data for profit.
  We propose a general framework for data exchange, built on two core principles: (i) fairness, ensuring that each agent receives utility proportional to their contribution to others; contributions are quantifiable using standard credit-sharing functions like the Shapley value, and (ii) stability, ensuring that no coalition of agents can identify an exchange among themselves which they unanimously prefer to the current exchange. We show that fair and stable exchanges exist for all monotone continuous utility functions. Next, we investigate the computational complexity of finding approximate fair and stable exchanges. We present a local search algorithm for instances with monotone submodular utility functions, where each agent contributions are measured using the Shapley value. We prove that this problem lies in CLS under mild assumptions. Our framework opens up several intriguing theoretical directions for research in data economics.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01968
Understanding process-structure-property relation for elastoplastic behavior of polymer nanocomposites with agglomeration anomalies and gradient interphase percolation,['Soft Condensed Matter'],"['Prajakta Prabhune', 'Anlan Chen', 'Yigitcan Comlek', 'Wei Chen', 'L. Catherine Brinson']","For polymer nanocomposites disordered microstructural nature makes processing control and hence tailoring properties to desired values a challenge. Understanding process-structure-property relation can provide guidelines for process and constituents design. Our work explores nuances of PSP relation for polymer nanocomposites with attractive pairing between particles and polymer bulk. Past works have shown that particle functionalization can help tweak these interactions in attractive or repulsive directions and can produce slow or fast decay of stiffness properties in polymer nanocomposites. In absence of any nano or micro-scale local property measurement, we develop a material model that can represent decay for small strain elastoplastic properties in interfacial regions and simulate representative or statistical volume element behavior. The interfacial elastoplastic material model is devised by combining local stiffness and glass transition measurements that were obtained by previous researchers by atomic force microscopy and fluorescence microscopy. This is further combined with a microstructural design of experiments for agglomerated nanocomposite systems. Agglomerations are particle aggregations that are processing artefacts that result from lack of processing control. Twin screw extrusion process can reduce extent of aggregation in hot pressed samples via erosion or rupture depending on screw rpms and toque. We connect this process-structure relation to structure-property relation that emerges from our study. We discover that balancing between local stress concentration zone and interfacial property decay governs how fast yield stress can improve if we break down agglomeration via erosion. Rupture is relatively less effective in helping improve nanocomposite yield strength.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01967
Implementing Semiclassical Szegedy Walks in Classical-Quantum Circuits for Homomorphic Encryption,['Quantum Physics'],"['Sergio A. Ortega', 'Pablo Fernández', 'Miguel A. Martin-Delgado']","As cloud services continue to expand, the security of private data stored and processed in these environments has become paramount. This work delves into quantum homomorphic encryption (QHE), an emerging technology that facilitates secure computation on encrypted quantum data without revealing the underlying information. We reinterpret QHE schemes through classical-quantum circuits, enhancing efficiency and addressing previous limitations related to key computations. Our approach eliminates the need for exponential key preparation by calculating keys in real-time during simulation, leading to a linear complexity in classically controlled gates. We also investigate the $T/T^{\dagger}$-gate complexity associated with various quantum walks, particularly Szegedy quantum and semiclassical algorithms, demonstrating efficient homomorphic implementations across different graph structures. Our simulations, conducted in Qiskit, validate the effectiveness of QHE for both standard and semiclassical walks. The rules for the homomorphic evaluation of the reset and intermediate measurement operations have also been included to perform the QHE of semiclassical walks. Additionally, we introduce the CQC-QHE library, a comprehensive tool that simplifies the construction and simulation of classical-quantum circuits tailored for quantum homomorphic encryption. Future work will focus on optimizing classical functions within this framework and exploring broader graph types to enhance QHE applications in practical scenarios.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01966
On global Schubert varieties for the general linear group,['Algebraic Geometry'],"['Pramod N. Achar', 'Andrea Bourque']","We give a ""lattice-theoretic"" description of the global Schubert variety for $\mathrm{GL}_n$ associated to any dominant coweight.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01962
The use of large language models to enhance cancer clinical trial educational materials,['Computation and Language'],"['Mingye Gao', 'Aman Varshney', 'Shan Chen', 'Vikram Goddla', 'Jack Gallifant', 'Patrick Doyle', 'Claire Novack', 'Maeve Dillon-Martin', 'Teresia Perkins', 'Xinrong Correia', 'Erik Duhaime', 'Howard Isenstein', 'Elad Sharon', 'Lisa Soleymani Lehmann', 'David Kozono', 'Brian Anthony', 'Dmitriy Dligach', 'Danielle S. Bitterman']","Cancer clinical trials often face challenges in recruitment and engagement due to a lack of participant-facing informational and educational resources. This study investigated the potential of Large Language Models (LLMs), specifically GPT4, in generating patient-friendly educational content from clinical trial informed consent forms. Using data from ClinicalTrials.gov, we employed zero-shot learning for creating trial summaries and one-shot learning for developing multiple-choice questions, evaluating their effectiveness through patient surveys and crowdsourced annotation. Results showed that GPT4-generated summaries were both readable and comprehensive, and may improve patients' understanding and interest in clinical trials. The multiple-choice questions demonstrated high accuracy and agreement with crowdsourced annotators. For both resource types, hallucinations were identified that require ongoing human oversight. The findings demonstrate the potential of LLMs ""out-of-the-box"" to support the generation of clinical trial education materials with minimal trial-specific engineering, but implementation with a human-in-the-loop is still needed to avoid misinformation risks.△ Less",v1,https://arxiv.org/pdf/2412.01955
The Landscape of Causal Discovery Data: Grounding Causal Discovery in Real-World Applications,['Machine Learning'],"['Philippe Brouillard', 'Chandler Squires', 'Jonas Wahl', 'Konrad P. Kording', 'Karen Sachs', 'Alexandre Drouin', 'Dhanya Sridhar']","Causal discovery aims to automatically uncover causal relationships from data, a capability with significant potential across many scientific disciplines. However, its real-world applications remain limited. Current methods often rely on unrealistic assumptions and are evaluated only on simple synthetic toy datasets, often with inadequate evaluation metrics. In this paper, we substantiate these claims by performing a systematic review of the recent causal discovery literature. We present applications in biology, neuroscience, and Earth sciences - fields where causal discovery holds promise for addressing key challenges. We highlight available simulated and real-world datasets from these domains and discuss common assumption violations that have spurred the development of new methods. Our goal is to encourage the community to adopt better evaluation practices by utilizing realistic datasets and more adequate metrics.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01953
No Free Lunch for Stochastic Gradient Langevin Dynamics,['Computation'],"['Natesh S. Pillai', 'Aaron Smith', 'Azeem Zaman']","As sample sizes grow, scalability has become a central concern in the development of Markov chain Monte Carlo (MCMC) methods. One general approach to this problem, exemplified by the popular stochastic gradient Langevin dynamics (SGLD) algorithm, is to use a small random subsample of the data at every time step. This paper, building on recent work such as \cite{nagapetyan2017true,JohndrowJamesE2020NFLf}, shows that this approach often fails: while decreasing the sample size increases the speed of each MCMC step, for typical datasets this is balanced by a matching decrease in accuracy. This result complements recent work such as \cite{nagapetyan2017true} (which came to the same conclusion, but analyzed only specific upper bounds on errors rather than actual errors) and \cite{JohndrowJamesE2020NFLf} (which did not analyze nonreversible algorithms and allowed for logarithmic improvements).△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01952
Self-Improvement in Language Models: The Sharpening Mechanism,['Artificial Intelligence'],"['Audrey Huang', 'Adam Block', 'Dylan J. Foster', 'Dhruv Rohatgi', 'Cyril Zhang', 'Max Simchowitz', 'Jordan T. Ash', 'Akshay Krishnamurthy']","Recent work in language modeling has raised the possibility of self-improvement, where a language models evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new perspective on the capabilities of self-improvement through a lens we refer to as sharpening. Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to ``sharpen'' the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner aims to sharpen a pre-trained base policy via sample access, and establish fundamental limits. Then we analyze two natural families of self-improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self-improvement by leveraging online exploration, bypassing the need for coverage. Finally, we empirically validate the sharpening mechanism via inference-time and amortization experiments. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.△ Less",v1,https://arxiv.org/pdf/2412.01951
Identifying Key Nodes for the Influence Spread using a Machine Learning Approach,['Social and Information Networks'],"['Mateusz Stolarski', 'Adam Piróg', 'Piotr Bródka']","The identification of key nodes in complex networks is an important topic in many network science areas. It is vital to a variety of real-world applications, including viral marketing, epidemic spreading and influence maximization. In recent years, machine learning algorithms have proven to outperform the conventional, centrality-based methods in accuracy and consistency, but this approach still requires further refinement. What information about the influencers can be extracted from the network? How can we precisely obtain the labels required for training? Can these models generalize well? In this paper, we answer these questions by presenting an enhanced machine learning-based framework for the influence spread problem. We focus on identifying key nodes for the Independent Cascade model, which is a popular reference method. Our main contribution is an improved process of obtaining the labels required for training by introducing 'Smart Bins' and proving their advantage over known methods. Next, we show that our methodology allows ML models to not only predict the influence of a given node, but to also determine other characteristics of the spreading process-which is another novelty to the relevant literature. Finally, we extensively test our framework and its ability to generalize beyond complex networks of different types and sizes, gaining important insight into the properties of these methods.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01949
Iterative variational learning of committor-consistent transition pathways using artificial neural networks,['Computational Physics'],"['Alberto Megías', 'Sergio Contreras Arredondo', 'Cheng Giuseppe Chen', 'Chenyu Tang', 'Benoît Roux', 'Christophe Chipot']","This contribution introduces a neural-network-based approach to discover meaningful transition pathways underlying complex biomolecular transformations in coherence with the committor function. The proposed path-committor-consistent artificial neural network (PCCANN) iteratively refines the transition pathway by aligning it to the gradient of the committor. This method addresses the challenges of sampling in molecular dynamics simulations rare events in high-dimensional spaces, which is often limited computationally. Applied to various benchmark potentials and biological processes such as peptide isomerization and protein-model folding, PCCANN successfully reproduces established dynamics and rate constants, while revealing bifurcations and alternate pathways. By enabling precise estimation of transition states and free-energy barriers, this approach provides a robust framework for enhanced-sampling simulations of rare events in complex biomolecular systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01947
The Reality of AI and Biorisk,['Artificial Intelligence'],"['Aidan Peppin', 'Anka Reuel', 'Stephen Casper', 'Elliot Jones', 'Andrew Strait', 'Usman Anwar', 'Anurag Agrawal', 'Sayash Kapoor', 'Sanmi Koyejo', 'Marie Pellat', 'Rishi Bommasani', 'Nick Frosst', 'Sara Hooker']","To accurately and confidently answer the question 'could an AI model or system increase biorisk', it is necessary to have both a sound theoretical threat model for how AI models or systems could increase biorisk and a robust method for testing that threat model. This paper provides an analysis of existing available research surrounding two AI and biorisk threat models: 1) access to information and planning via large language models (LLMs), and 2) the use of AI-enabled biological tools (BTs) in synthesizing novel biological artifacts. We find that existing studies around AI-related biorisk are nascent, often speculative in nature, or limited in terms of their methodological maturity and transparency. The available literature suggests that current LLMs and BTs do not pose an immediate risk, and more work is needed to develop rigorous approaches to understanding how future models could increase biorisks. We end with recommendations about how empirical work can be expanded to more precisely target biorisk and ensure rigor and validity of findings.△ Less",v1,https://arxiv.org/pdf/2412.01946
Ultrafast dynamic Coulomb screening of X-ray core excitons in photoexcited semiconductors,['Materials Science'],"['Thomas C. Rossi', 'Lu Qiao', 'Conner P. Dykstra', 'Ronaldo Rodrigues Pela', 'Richard Gnewkow', 'Rachel F. Wallick', 'John H. Burke', 'Erin Nicholas', 'Anne-Marie March', 'Gilles Doumy', 'D. Bruce Buchholz', 'Christiane Deparis', 'Jesus Zuñiga-Pérez', 'Michael Weise', 'Klaus Ellmer', 'Mattis Fondell', 'Claudia Draxl', 'Renske M. van der Veen']","Ultrafast X-ray spectroscopy has been revolutionized in recent years due to the advent of fourth-generation X-ray facilities. In solid-state materials, core excitons determine the energy and line shape of absorption features in core-level spectroscopies such as X-ray absorption spectroscopy. The screening of core excitons is an inherent many-body process that can reveal insight into charge-transfer excitations and electronic correlations. Under non-equilibrium conditions such as after photoexcitation, however, core-exciton screening is still not fully understood. Here we demonstrate the dynamic Coulomb screening of core excitons induced by photoexcited carriers by employing X-ray transient absorption (XTA) spectroscopy with picosecond time resolution. Our interpretation is supported by state-of-the-art ab initio theory, combining constrained and real-time time-dependent density functional theory with many-body perturbation theory. Using ZnO as an archetypal wide band-gap semiconductor, we show that the Coulomb screening by photoexcited carriers at the Zn K-edge leads to a decrease in the core-exciton binding energy, which depends nonlinearly on both the excitation density and the distribution of photoexcited carriers in reciprocal space. The effect of Coulomb screening dominates over Pauli blocking in the XTA spectra. We show that dynamic core-exciton screening is also observed at other X-ray absorption edges and theoretically predict the effect of core-exciton screening on the femtosecond time scale for the case of ZnO, a major step towards hard X-ray excitonics. The results have implications for the interpretation of ultrafast X-ray spectra in general and their use in tracking charge carrier dynamics in complex materials on atomic length scales.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01945
Global Average Feature Augmentation for Robust Semantic Segmentation with Transformers,['Computer Vision and Pattern Recognition'],"['Alberto Gonzalo Rodriguez Salgado', 'Maying Schen', 'Philipp Harzig', 'Peter Mayer', 'Jose M. Alvarez']","Robustness to out-of-distribution data is crucial for deploying modern neural networks. Recently, Vision Transformers, such as SegFormer for semantic segmentation, have shown impressive robustness to visual corruptions like blur or noise affecting the acquisition device. In this paper, we propose Channel Wise Feature Augmentation (CWFA), a simple yet efficient feature augmentation technique to improve the robustness of Vision Transformers for semantic segmentation. CWFA applies a globally estimated perturbation per encoder with minimal compute overhead during training. Extensive evaluations on Cityscapes and ADE20K, with three state-of-the-art Vision Transformer architectures : SegFormer, Swin Transformer, and Twins demonstrate that CWFA-enhanced models significantly improve robustness without affecting clean data performance. For instance, on Cityscapes, a CWFA-augmented SegFormer-B1 model yields up to 27.7% mIoU robustness gain on impulse noise compared to the non-augmented SegFormer-B1. Furthermore, CWFA-augmented SegFormer-B5 achieves a new state-of-the-art 84.3% retention rate, a 0.7% improvement over the recently published FAN+STL.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01941
Kernel-Free Universum Quadratic Surface Twin Support Vector Machines for Imbalanced Data,['Machine Learning'],"['Hossein Moosaei', 'Milan Hladík', 'Ahmad Mousavi', 'Zheming Gao', 'Haojie Fu']","Binary classification tasks with imbalanced classes pose significant challenges in machine learning. Traditional classifiers often struggle to accurately capture the characteristics of the minority class, resulting in biased models with subpar predictive performance. In this paper, we introduce a novel approach to tackle this issue by leveraging Universum points to support the minority class within quadratic twin support vector machine models. Unlike traditional classifiers, our models utilize quadratic surfaces instead of hyperplanes for binary classification, providing greater flexibility in modeling complex decision boundaries. By incorporating Universum points, our approach enhances classification accuracy and generalization performance on imbalanced datasets. We generated four artificial datasets to demonstrate the flexibility of the proposed methods. Additionally, we validated the effectiveness of our approach through empirical evaluations on benchmark datasets, showing superior performance compared to conventional classifiers and existing methods for imbalanced classification.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01936
Cross Domain Adaptation using Adversarial networks with Cyclic loss,['Machine Learning'],"['Manpreet Kaur', 'Ankur Tomar', 'Srijan Mishra', 'Shashwat Verma']","Deep Learning methods are highly local and sensitive to the domain of data they are trained with. Even a slight deviation from the domain distribution affects prediction accuracy of deep networks significantly. In this work, we have investigated a set of techniques aimed at increasing accuracy of generator networks which perform translation from one domain to the other in an adversarial setting. In particular, we experimented with activations, the encoder-decoder network architectures, and introduced a Loss called cyclic loss to constrain the Generator network so that it learns effective source-target translation. This machine learning problem is motivated by myriad applications that can be derived from domain adaptation networks like generating labeled data from synthetic inputs in an unsupervised fashion, and using these translation network in conjunction with the original domain network to generalize deep learning networks across domains.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01935
"A Shared Standard for Valid Measurement of Generative AI Systems' Capabilities, Risks, and Impacts",['Computers and Society'],"['Alexandra Chouldechova', 'Chad Atalla', 'Solon Barocas', 'A. Feder Cooper', 'Emily Corvi', 'P. Alex Dow', 'Jean Garcia-Gathright', 'Nicholas Pangakis', 'Stefanie Reed', 'Emily Sheng', 'Dan Vann', 'Matthew Vogel', 'Hannah Washington', 'Hanna Wallach']","The valid measurement of generative AI (GenAI) systems' capabilities, risks, and impacts forms the bedrock of our ability to evaluate these systems. We introduce a shared standard for valid measurement that helps place many of the disparate-seeming evaluation practices in use today on a common footing. Our framework, grounded in measurement theory from the social sciences, extends the work of Adcock & Collier (2001) in which the authors formalized valid measurement of concepts in political science via three processes: systematizing background concepts, operationalizing systematized concepts via annotation procedures, and applying those procedures to instances. We argue that valid measurement of GenAI systems' capabilities, risks, and impacts, further requires systematizing, operationalizing, and applying not only the entailed concepts, but also the contexts of interest and the metrics used. This involves both descriptive reasoning about particular instances and inferential reasoning about underlying populations, which is the purview of statistics. By placing many disparate-seeming GenAI evaluation practices on a common footing, our framework enables individual evaluations to be better understood, interrogated for reliability and validity, and meaningfully compared. This is an important step in advancing GenAI evaluation practices toward more formalized and theoretically grounded processes -- i.e., toward a science of GenAI evaluations.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01934
PROFIT: A PROximal FIne Tuning Optimizer for Multi-Task Learning,['Computer Vision and Pattern Recognition'],"['Anirudh S Chakravarthy', 'Shuai Kyle Zheng', 'Xin Huang', 'Sachithra Hemachandra', 'Xiao Zhang', 'Yuning Chai', 'Zhao Chen']","Fine-tuning pre-trained models has become invaluable in computer vision and robotics. Recent fine-tuning approaches focus on improving efficiency rather than accuracy by using a mixture of smaller learning rates or frozen backbones. To return the spotlight to model accuracy, we present PROFIT, one of the first optimizers specifically designed for incrementally fine-tuning converged models on new tasks or datasets. Unlike traditional optimizers such as SGD or Adam, which make minimal assumptions due to random initialization, PROFIT leverages the structure of a converged model to regularize the optimization process, leading to improved results. By employing a simple temporal gradient orthogonalization process, PROFIT outperforms traditional fine-tuning methods across various tasks: image classification, representation learning, and large-scale motion prediction. Moreover, PROFIT is encapsulated within the optimizer logic, making it easily integrated into any training pipeline with minimal engineering effort. A new class of fine-tuning optimizers like PROFIT can drive advancements as fine-tuning and incremental training become increasingly prevalent, reducing reliance on costly model training from scratch.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01930
Overscreened spin-$\frac{1}{2}$ Kondo impurity and Shiba state at the edge of a one-dimensional spin-1 superconducting wire,['Strongly Correlated Electrons'],"['Pradip Kattel', 'Abay Zhakenov', 'Natan Andrei']","We consider a model describing a system where the superconductivity competes with the overscreened Kondo effect. The model consists of a single spin$-\frac{1}{2}$ quantum impurity at the edge of a quantum wire where spin$-1$ bulk fermions interact attractively, generating a (superconducting) mass gap. The competition between the Kondo screening and the superconductivity leads to a rich phase structure. We find that for strong Kondo coupling, there is a regime of phase space where the Kondo phase is stable with the impurity \textit{overscreened} by a multiparticle Kondo effect, and a Kondo scale is dynamically generated. When the bulk and boundary interaction strength are comparable, we find that a midgap state appears in the spectrum and screens the impurity, while in the ground state, the impurity is unscreened. This midgap state is akin to the Yu-Shiba-Rushinov (YSR) states that exist in the entire phase space in the BCS superconductor. Moreover, when the bulk superconducting interaction strength is stronger than the boundary Kondo interaction strength, the impurity can no longer be screened. Further, between the Kondo and YSR phases, we find a novel phase where, while the Kondo cloud overscreens the impurity, a boundary excitation exists that has vanishing energy in the thermodynamic limit. Similar phase diagrams that result from competition between different mechanisms were found for other models, too: the dissipative Kondo system, where dissipation competes with screening; the Kondo impurity coupled to spin-1/2 attractively interacting fermions where condensation competes with screening; and the XXX-Kondo model, where the lattice cutoff and the bulk spin interaction compete with screening.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01924
Late-time Evolution and Instabilities of Tidal Disruption Disks,['High Energy Astrophysical Phenomena'],"['Anthony L. Piro', 'Brenna Mockler']","Observations of tidal disruption events (TDEs) on a timescale of years after the main flare show evidence of continued activity in the form of optical/UV emission, quasi-periodic eruptions, and delayed radio flares. Motivated by this, we explore the time evolution of these disks using semi-analytic models to follow the changing disk properties and feeding rate to the central black hole (BH). We find that thermal instabilities typically begin $\sim150-250\,{\rm days}$ after the TDE, causing the disk to cycle between high and low accretion states for up to $\sim10-20\,{\rm yrs}$. The high state is super-Eddington, which may be associated with outflows that eject $\sim10^{-3}-10^{-1}\,M_\odot$ with a range of velocities of $\sim0.03-0.3c$ over a span of a couple of days and produce radio flares. In the low state, the accretion rate slowly grows over many months to years as continued fallback accretion builds the mass of the disk. In this phase, the disk may reach luminosities of $\sim10^{41}-10^{42}\,{\rm erg\,s^{-1}}$ in the UV as seen in some late-time observations. We highlight the importance of the iron-opacity ""bump"" at $\approx2\times10^5\,{\rm K}$ in generating sufficiently high luminosities. This work suggests that joint optical/UV observations with radio monitoring could be key for following the disk state as the radio flares are produced.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01922
Spin Qubit Performance at the Error Correction Threshold: Advancing Quantum Information Processing Above 700 mK,['Quantum Physics'],"['S. Amitonov', 'A. Aprà', 'M. Asker', 'R. Bals', 'B. Barry', 'I. Bashir', 'E. Blokhina', 'P. Giounanlis', 'M. Harkin', 'P. Hanos-Puskai', 'I. Kriekouki', 'D. Leipold', 'M. Moras', 'N. Murphy', 'N. Petropoulos', 'C. Power', 'A. Sammak', 'N. Samkharadze', 'A. Semenov', 'A. Sokolov', 'D. Redmond', 'C. Rohrbacher', 'X. Wu']","This paper presents a characterization of a two-qubit processor in a 6-quantum dot array in SiGe, from the perspective of its quantum information processing capabilities. The analysis includes randomized benchmarking of single- and two-qubit gates, SPAM characterization, and Bell's state tomography; all basic functionality required for universal quantum computation. In light of our efforts to combine spin qubits with integrated cryogenic electronics, we evaluate the qubits' performance metrics at 300mK and 740mK. The latter temperature lies within the realistic thermal budget for integrated cryogenic electronics, making it particularly relevant for assessing qubit performance in practical scenarios.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01920
Turbulence-Induced Safety Factor Profile Flattening at Rational Surfaces in Tokamaks with Low Magnetic Shear,['Plasma Physics'],"['Arnas Volčokas', 'Justin Ball', 'Giovanni Di Giannatale', 'Stephan Brunner']","In this paper, we investigate the effects of ion-scale turbulence-generated currents on the local safety factor profile under conditions of low magnetic shear and proximity to rational surfaces, relevant to Internal Transport Barrier (ITB) formation. Our results show that turbulent currents can generate stationary zonal magnetic potential corrugations, producing a stepped safety factor profile with extended regions of zero magnetic shear. This change significantly affects turbulence self-interaction, resulting in a substantial decrease in turbulent transport, indicating a potential triggering mechanism for transport barrier formation.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01913
"CFT Correlators from (0,2) Heterotic String",['High Energy Physics - Theory'],"['Amit Giveon', 'Akikazu Hashimoto', 'David Kutasov']","In \cite{Giveon:2024fhz}, we argued that the (0,2) heterotic string gives rise in spacetime to left and right-moving symmetric product CFT's. In this paper we confirm this claim by showing that it computes correlation functions in these CFT's.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01912
Topological analysis of brain dynamical signals reveals signatures of seizure susceptibility,['Physics and Society'],"['Maxime Lucas', 'Damien Francois', 'Cristina Donato', 'Alexander Skupin', 'Daniele Proverbio']","Epilepsy is known to drastically alter brain dynamics during seizures (ictal periods). However, whether epilepsy may alter brain dynamics during background (non-ictal) periods is less understood. To investigate this, we analyzed the brain activity of epileptic zebrafish as animal models, for two genetic conditions and two fishlines. The recordings were automatically segmented and labeled with machine learning, and then analyzed using Persistent Homology, a method from Topological Data Analysis, which reveals patterns in the topology of brain dynamics in a noise-robust and networkbased manner. We find that ictal and non-ictal periods can be distinguished from the topology of their dynamics, regardless of fishline or genetic condition, which validates our method. Additionally, within a single fishline wild type, we can distinguish the non-ictal periods of seizure-prone and seizure-free individuals. This suggests the presence of topological signatures of the epileptic brain, even during non-ictal periods. In general, our results suggest that Topological Data Analysis can be used as a general quantitative method to screen for dynamical markers of seizure susceptibility also in other species.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01911
Event-Based Framework for Agile Resilience in Criticality-Aware Wireless Networks,['Information Theory'],"['Yasemin Karacora', 'Christina Chaccour', 'Aydin Sezgin', 'Walid Saad']","As mission- and safety-critical wireless applications grow in complexity and diversity, next-generation wireless systems must meet increasingly stringent and multifaceted requirements. These systems demand resilience along with enhanced intelligence and adaptability to ensure reliable communication under diverse conditions. This paper proposes an event-based multi-stage resilience framework, offering a guideline for efficiently integrating a combination of error mitigation techniques. The framework is applied to a case study focusing on uplink transmission of mixed-criticality data in the presence of random link blockages. The proposed scheme combines multiple blockage mitigation strategies - rate-splitting multiple access (RSMA), one-sided access point cooperation, and central decoding - within an event-driven algorithm. Each method, increasing in effectiveness and complexity, is activated sequentially to systematically overcome blockages. We model a mixed-criticality queuing system and formulate two transmit power allocation problems, one for separate decoding and one for central decoding, to ensure queue stability and fairness. Simulations evaluate the delay performance under varying blockage durations and examine the cost tradeoffs among resilience mechanisms within the proposed framework. The results suggest that passive robustness strategies effectively handle frequent short-term fluctuations, while more complex adaptation becomes germane for rare and prolonged blockages. Additionally, the results emphasize the importance of criticality-awareness for resilient communication design.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01909
D-Brane Systems in Twisted Holography and SO/Sp Chiral Algebras,['High Energy Physics - Theory'],['Adrián López-Raven'],"We study correlation functions of baryon and determinant operators for the chiral algebras obtained from the twist of N = 4 SYM with U(N) gauge group. In the context of Twisted Holography, we conjecture that a dual description should involve a D1-D5 brane system, and we construct from the correlators a candidate dual brane in the form of a derived coherent sheaf in SL(2,C). Extending this analysis, we compute similar baryon/determinant correlators of chiral algebras in symmetric and antisymmetric representations of SO and Sp gauge groups and construct the candidate dual branes. These branes exhibit Z_2 identifications consistent with conjectures relating SO/Sp chiral algebras to Kodaira-Spencer theory on SL(2,C)/Z_2 and the Type I topological string on SL(2,C).△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01905
Precision study of the massive Schwinger model near quantum criticality,['High Energy Physics - Theory'],"['Erick Arguello Cruz', 'Grigory Tarnopolsky', 'Yuan Xin']","We perform a numerical analysis of the massive Schwinger model in the presence of a background electric field. Using the Density Matrix Renormalization Group (DMRG) approach, we efficiently compute the spectrum of the Schwinger model on a staggered lattice with up to 3000 qubits. As a result, we achieve a precise computation of the critical mass of the massive Schwinger model to five digits using four different 'criticality criteria', observing perfect agreement among them. Additionally, we discuss the effect of a four-fermion operator deformation of the Schwinger model and compute the critical mass for various values of the deformation parameter.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01902
The multiple classes of ultra-diffuse galaxies: Can we tell them apart?,['Astrophysics of Galaxies'],"['Maria Luisa Buzzo', 'Duncan A. Forbes', 'Thomas H. Jarrett', 'Francine R. Marleau', 'Pierre-Alain Duc', 'Jean P. Brodie', 'Aaron J. Romanowsky', 'Anna Ferré-Mateu', 'Michael Hilker', 'Jonah S. Gannon', 'Joel Pfeffer', 'Lydia Haacke']","This study compiles stellar populations and internal properties of ultra-diffuse galaxies (UDGs) to highlight correlations with their local environment, globular cluster (GC) richness, and star formation histories. Complementing our sample of 88 UDGs, we include 36 low-surface brightness dwarf galaxies with UDG-like properties, referred to as NUDGes (nearly-UDGs). All galaxies were studied using the same spectral energy distribution fitting methodology to explore what sets UDGs apart from other galaxies. We show that NUDGes are similar to UDGs in all properties except for being, by definition, smaller and having higher surface brightness. We find that UDGs and NUDGes show similar behaviours in their GC populations, with the most metal-poor galaxies hosting consistently more GCs on average. This suggests that GC content may provide an effective way to distinguish extreme galaxies within the low surface brightness regime alongside traditional parameters like size and surface brightness. We confirm previous results using clustering algorithms that UDGs split into two main classes, which might be associated with the formation pathways of a puffy dwarf and a failed galaxy. The clustering applied to the UDGs+NUDGes dataset yields an equivalent result. The difference in mass contained in the GC system suggests that galaxies in different environments have not simply evolved from one another but may have formed through distinct processes.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01901
Estimating energy levels from lattice QCD correlation functions using a transfer matrix formalism,['High Energy Physics - Lattice'],"['Debsubhra Chakraborty', 'Dhruv Sood', 'Archana Radhakrishnan', 'Nilmani Mathur']","We present an efficient method for estimating energy levels from lattice QCD correlation functions by computing the eigenvalues of the transfer matrix of the corresponding lattice QCD Hamiltonian. This method is equivalent to the Lanczos procedure introduced recently (Wagman-2024), but more general and simpler to implement. We demonstrate the efficacy of the method by calculating the two lowest energy levels of a large number of hadrons including a few nuclei. While the signal-to-noise problem does not improve significantly, the extracted energy levels are found to be more reliable, compared to those extracted by conventional methods. Within a given statistical dataset the proposed method can effectively account for both statistical uncertainties and systematic errors, including those arising from the fitting window, and hence can reliably be employed in lattice QCD calculations.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01900
From eccentric binaries to nonstationary gravitational wave backgrounds,['General Relativity and Quantum Cosmology'],"['Mikel Falxa', 'Hippolyte Quelquejay Leclere', 'Alberto Sesana']","A large population of binary systems in the Universe emitting gravitational waves (GW) would produce a stochastic noise, known as the gravitational wave background (GWB). The properties of the GWB directly depend on the attributes of its constituents. If the binary systems are in eccentric orbits, it is well established that the GW power they radiate strongly depends on their instantaneous orbital phase. Consequently, their power spectrum varies over time, and the resulting GWB can appear nonstationary. In this work, we estimate the amplitude of time-dependent fluctuations in the GWB power spectrum as a function of the eccentricity of the binaries. Specifically, we focus on the GWB produced by a population of supermassive black hole binaries (SMBHB) that should be observable by pulsar timing arrays (PTA). We show that a large population of homogeneously distributed equal SMBHBs produces nonstationary features that are undetectable by current PTA datasets. However, using more realistic and astrophysically motivated populations of SMBHBs, we show that the nonstationarity might become very large and detectable, especially in the case of more massive and eccentric populations. In particular, when one binary is slightly brighter than the GWB, we demonstrate that time fluctuations can become significant. This is also true for individual binary systems with a low signal-to-noise ratio (SNR) relative to the GWB (SNR $\approx$ 1), which standard data analysis methods would struggle to detect. The detection of nonstationary features in the GWB could indicate the presence of some relatively bright GW sources in eccentric orbits, offering new insights into the origins of the signal.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01899
CO2-rich protoplanetary discs as a probe of dust radial drift & trapping,['Earth and Planetary Astrophysics'],"['Andrew D. Sellek', 'Marissa Vlasblom', 'Ewine F. van Dishoeck']","MIR spectra imply considerable chemical diversity in the inner regions of protoplanetary discs: some are H2O-dominated, others by CO2. Sublimating ices from radially drifting dust grains are often invoked to explain some of this diversity, particularly the H2O-rich discs. We use a 1D protoplanetary disc evolution code to model how radially drifting dust grains that transport ices inwards to snowlines impact the chemistry of the inner regions of protoplanetary discs. We explore differences between smooth discs and those where radial drift is impeded by dust trapping outside gas gaps and quantify the effects of gap location and formation time. Discs evolve through an initial H2O-rich phase due to sublimating ices, followed by a CO2-rich phase as H2O vapour advects onto the star and CO2 advects into the inner disc from its snowline. The inclusion of traps hastens the transition between the phases, raising the CO2/H2O ratio; gaps opened early or close-in produce lower increases by blocking more CO2 ice from reaching the inner disc. This leads to a potential correlation between CO2/H2O and gap location that occurs on Myr timescales for fiducial parameters. We produce synthetic spectra from the models which we analyse with 0D LTE slab models to understand how this evolution may be expressed observationally. Whether the evolution can be retrieved depends on the contribution of dust grains to the optical depth: dust that couples to the gas after crossing the H2O snowline can add to the continuum optical depth and obscure the delivered H2O, largely hiding the evolution in its visible column density. However, the CO2/H2O visible column density ratio is only weakly sensitive to dust continuum obscuration. This suggests it may be a clearer tracer of the impact of transport on chemistry than individual column densities for spectra that show weak features probing deep enough in the disc. (Abridged)△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01895
Performance of Photometric Template Fitting for Ultra-High Redshift Galaxies,['Instrumentation and Methods for Astrophysics'],"['Thorbjørn Clausen', 'Charles L. Steinhardt', 'Arden Shao', 'Gaurav Senthil Kumar']","JWST has allowed the discovery of a significant population of galaxies at z > 10. Our understanding of the astrophysical properties of these ultra-high redshift galaxies relies on fitting templates, developed using astrophysical models representing our current understanding of high-redshift galaxies. In this work, the highest confidence recent JWST spectroscopic observations are used to evaluate the performance of several high-redshift templates based on two tests: (1) comparing photometric redshifts against spectroscopic redshifts; and (2) comparing the reconstructed spectral energy distributions against observed SEDs. Strict sample selection and error-propagation by bootstrapping is employed to make results robust towards future JWST systematics mitigation. It is shown that some templates perform adequately at high redshift prediction, given a sample selection by observational filters and depth. Other templates work better for SED fitting, but a few objects remain unrepresented in their spectra. We conclude that although templates are usable, models are not yet able to reliably extract astrophysical properties.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01893
Prospects for quantum process tomography at high energies,['High Energy Physics - Phenomenology'],"['Clelia Altomonte', 'Alan J. Barr', 'Michał Eckstein', 'Paweł Horodecki', 'Kazuki Sakurai']","In quantum information theory, the evolution of an open quantum system -- a unitary evolution followed by a measurement -- is described by a quantum channel or, more generally, a quantum instrument. In this work, we formulate spin and flavour measurements in collider experiments as a quantum instrument. We demonstrate that the Choi matrix, which completely determines input-output transitions, can be both theoretically computed from a given model and experimentally reconstructed from a set of final state measurements (quantum state tomography) using varied input states. The reconstruction of the Choi matrix, known as quantum process tomography, offers a powerful new approach for probing potential extensions of the Standard Model within the quantum field theory framework and also provides a fundamental test of quantum mechanics itself. As an example, we outline a quantum process tomography approach applied to the $e^+ e^- \to t \bar{t}$ process at a polarized lepton collider.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01892
A note on loop resummation in de Sitter spacetime with the wavefunction of the universe approach,['High Energy Physics - Theory'],"['Javier Huenupi', 'Ellie Hughes', 'Gonzalo A. Palma', 'Spyros Sypsas']","We analyze the computation of $n$-point correlation functions in de Sitter spacetime, including loop corrections, using the wavefunction of the universe approach. This method consists of two stages employing distinct Feynman rules. First, one must compute the wavefunction coefficients using interactions as vertices. Then, in the second stage, one computes correlation functions using wavefunction coefficients as vertices. For massless fields, loop corrections in the first stage are free of infrared (IR) divergences, which leads to the question of how this matches the well-known IR behavior of correlators obtained via other methods. By considering a scalar field with an arbitrary potential, we compute $n$-point correlation functions to first order in the potential but to all orders in loops. We find that, although loop integrals in the first stage are indeed IR convergent, the second procedure reintroduces the IR divergence. We discuss how this induces renormalization of the interaction potential such that the final result combining both steps exactly matches the form of $n$-point functions previously calculated with other methods.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01891
The Millennium and Astrid galaxies in effective field theory: comparison with galaxy-halo connection models at the field level,['Cosmology and Nongalactic Astrophysics'],"['Mikhail M. Ivanov', 'Carolina Cuesta-Lazaro', 'Andrej Obuljen', 'Michael W. Toomey', 'Yueying Ni', 'Sownak Bose', 'Boryana Hadzhiyska', 'César Hernández-Aguayo', 'Lars Hernquist', 'Rahul Kannan', 'Volker Springel']","Cosmological analyses of redshift space clustering data are primarily based on using luminous ``red'' galaxies (LRGs) and ``blue'' emission line galaxies (ELGs) to trace underlying dark matter. Using the large high-fidelity high-resolution MillenniumTNG (MTNG) and Astrid simulations, we study these galaxies with the effective field theory (EFT)-based field level forward model. We confirm that both red and blue galaxies can be accurately modeled with EFT at the field level and their parameters match those of the phenomenological halo-based models. Specifically, we consider the state of the art Halo Occupation Distribution (HOD) and High Mass Quenched (HMQ) models for the red and blue galaxies, respectively. Our results explicitly confirm the validity of the halo-based models on large scales beyond the two-point statistics. In addition, we validate the field-level HOD/HMQ-based priors for EFT full-shape analysis. We find that the local bias parameters of the ELGs are in tension with the predictions of the LRG-like HOD models and present a simple analytic argument explaining this phenomenology. We also confirm that ELGs exhibit weaker non-linear redshift-space distortions (``fingers-of-God''), suggesting that a significant fraction of their data should be perturbative. We find that the response of EFT parameters to galaxy selection is sensitive to assumptions about baryonic feedback, suggesting that a detailed understanding of feedback processes is necessary for robust predictions of EFT parameters. Finally, using neural density estimation based on paired HOD-EFT parameter samples, we obtain optimal HOD models that reproduce the clustering of Astrid and MTNG galaxies.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01888
A Comprehensive Photometric Selection of `Little Red Dots' in MIRI Fields: An IR-Bright LRD at $z=3.1386$ with Warm Dust Emission,['Astrophysics of Galaxies'],"['Guillermo Barro', 'Pablo G. Perez-Gonzalez', 'Dale D. Kocevski', 'Elizabeth J. McGrath', 'Gene C. K. Leung', 'Fergus Cullen', 'James S. Dunlop', 'Richard S. Ellis', 'Steven L. Finkelstein', 'Norman A. Grogin', 'Garth Illingworth', 'Jeyhan S. Kartaltepe', 'Anton M. Koekemoer', 'Ray A. Lucas', 'Ross J. McLure', 'Guang Yang']","JWST has revealed a population of compact `Little Red Dots' (LRDs) at $z\gtrsim4$, with red rest-frame optical and blue UV colors. These objects are likely compact dusty starbursts or heavily reddened AGNs, playing a pivotal role in early black hole growth, dust production, and stellar assembly. We introduce a new photometric selection to identify LRDs over a broad range in redshifts and rest-frame UV-to-NIR colors enabling a more complete census of the population. This method identifies 248 LRDs with F444W$<27$ mag over 263 arcmin$^2$ in the JADES, PRIMER-COSMOS, and UDS fields with MIRI coverage, increasing the number density by $\times$1.7 compared to previous samples, suggesting that previous census were underestimated. Most LRDs are detected in MIRI/F770W but only 7% (17) are detected in F1800W. We use MIRI-based rest-frame [1$-$3 $μ$m] colors to trace dust emission. F1800W-detected LRDs have a median [1$-$3 $μ$m]$=1.5$ mag, with a broad scatter indicative of diverse dust emission properties. About 20% exhibit [1$-$3 $μ$m]$<1$ mag colors consistent with negligible dust emission, but the majority show significant dust emission at 3 $μ$m (f$^{\rm dust}_{3μm}\lesssim0.8$) from the galaxy ISM or a hot-dust-deficient AGN torus. A correlation between bluer UV-to-NIR colors and stronger IR emission suggests that the bluest LRDs may resemble unobscured QSOs. We report a LRD at $z_{\rm spec}=3.1386$, detected in MIRI, Spitzer/MIPS, and Herschel/PACS. Its IR SED rises steeply at $λ_{\rm rest}>6~μ$m and peaks near $\sim40~μ$m, providing the first direct evidence of warm dust emission (T$=50-100$ K) in a LRD.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01887
Disordered topological crystalline phases,['Mesoscale and Nanoscale Physics'],"['Adam Yanis Chaou', 'Mateo Moreno-Gonzalez', 'Alexander Altland', 'Piet W. Brouwer']","The imposition of crystalline symmetries is known to lead to a rich variety of insulating and superconducting topological phases. These include higher-order topological phases and obstructed atomic limits with and without filling anomalies. We here comprehensively classify such topological crystalline phases (TCPs) with mirror, twofold rotation, and inversion symmetries in the presence of disorder that preserves the crystalline symmetry on average. We find that the inclusion of disorder leads to a simplification of the classification in comparison to the clean case. We also find that, while clean TCPs evade a general bulk-boundary principle, disordered TCPs admit a complete bulk-boundary correspondence, according to which (bulk) topological phases are topologically equivalent if and only if they have the same anomalous boundary states and filling anomaly. We corroborate the stability of disordered TCPs by way of field-theoretic, numerical and symmetry-based analyses in various case studies. While the boundary signatures of most disordered TCPs are similar to their clean counterparts, the addition of disorder to certain mirror-symmetric TCPs results in novel higher-order statistical topological phases, in which zero-energy hinge states have critical wavefunction statistics, while remaining protected from Anderson localization.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01883
High-energy interactions of charged black holes in full general relativity II: Near-extremal merger remnants and universality with the irreducible mass,['General Relativity and Quantum Cosmology'],"['M A. M. Smith', 'Vasileios Paschalidis', 'Gabriele Bozzola']","In a previous paper, arXiv:2411.11960 [gr-qc], we initiated a study of high-energy interactions of charged binary black holes near the scattering threshold, focusing on zoom-whirl orbits. In this second paper in our series, we focus on merger remnant properties and energetics with new simulations of equal-mass, equal-charge, nonspinning binary black holes with variable impact parameter. We find near-extremal merger remnants with Kerr-Newman parameter reaching $Υ_f = 0.97$, and observe that the maximum $Υ_f$ increases monotonically with $λ$ for a fixed initial Lorentz factor. We find that binaries with larger $λ$ radiate less total energy despite having stronger electromagnetic emission. The maximum energy radiated by a binary in our study is $31\%$ of its gravitational mass. Increasing $λ$ has little effect on the maximum angular momentum radiated, which was $\approx 72\%$ of the spacetime total angular momentum for each $λ$ explored here. Lastly, we provide additional evidence for the universality with the irreducible mass that we discovered in arXiv:2411.11960 [gr-qc]. The black hole horizon areal radius determines a fundamental, gauge-invariant length scale governing BH interactions near the scattering threshold.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01881
A sensitivity analysis of the modeling of Polycyclic Aromatic Hydrocarbon emission in galaxies,['Astrophysics of Galaxies'],"['Alexandros Maragkoudakis', 'Christiaan Boersma', 'Pasquale Temi', 'Jesse D. Bregman', 'Louis J. Allamandola', 'Vincent Esposito', 'Alessandra Ricca', 'Els Peeters']","We have conducted a sensitivity analysis on the mid-infrared spectral decomposition of galaxies and the modeling of the PAH emission spectrum with the NASA Ames PAH Infrared Spectroscopic Database (PAHdb) to assess the variance on the average galaxy PAH population properties under a grid of different modeling parameters. We find that the SL and SL+LL Spitzer-IRS decomposition with PAHFIT provides consistent modeling and recovery of the 5-15 $μ$m PAH emission spectrum. For PAHdb modeling, application of a redshift to the calculated spectra to account for anharmonic effects introduces a $15\%$-$20\%$ variance on the derived parameters, while its absence improves the fits by $\sim13\%$. The 4.00-$α$ release of PAHdb achieves the complete modeling of the 6-15 $μ$m PAH spectrum, including the full 6.2 $μ$m band, improving the average fitting uncertainty by a factor of 2. The optimal PAHdb modeling configuration requires selection of pure PAHs without applying a redshift to the bands. Although quantitatively the PAHdb-derived parameters change under different modeling configurations or database versions, their variation follows a linear scaling, with previously reported trends remaining qualitatively valid. PAHdb modeling of JWST observations, and JWST observations smoothed and resampled to the Spitzer-IRS resolution and dispersion have consistent PAHdb derived parameters. Decomposition with different codes, such as PAHFIT and CAFE, produce PAH emission spectra with noticeable variation in the 11-15~$μ$m region, driving a $\sim7\%$ difference in the neutral PAH fraction under PAHdb modeling. A new library of galaxy PAH emission templates is delivered to be utilized in galaxy SED modeling.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01875
The Design and Implementation of a Quantum Information Science Undergraduate Program,['Physics Education'],"['Sarah Blanchette', 'Danièle Normandin', 'Michel Pioro-Ladrière', 'Lyne St-Hilaire', 'Armand Soldera', 'Karl Thibault', 'Dave Touchette']","Quantum information science is a burgeoning research field attracting vast public and private investment in the last decade. This quick rise has led to a talent gap, where there are more open positions than new graduates who can fill these roles. To meet this critical need, higher education has been challenged to react accordingly by assuring a flow of highly skilled individuals who must be trained quickly. We thus present how Université de Sherbrooke, in Quebec, Canada, responded by creating and launching an innovative undergraduate degree in quantum information science, aiming to address this gap by training quantum software developers in three and a half years. At the end of this program, they will be ready to join the quantum workforce. We detail the creative process leading to a coherent curriculum, as well as why the local ecosystem led to these choices. The guiding principles and lessons learned during the development of this interdepartmental and interfaculty degree are shared to inspire other quantum education institutions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01874
Uranus Study Report: KISS,['Instrumentation and Methods for Astrophysics'],"['Mark Hofstadter', 'Ravit Helled', 'David J. Stevenson', 'Bethany Ehlmann', 'Mandy Bethkenhagen', 'Hao Cao', 'Junjie Dong', 'Maryame El Moutamid', 'Anton Ermakov', 'Jim Fuller', 'Tristan Guillot', 'Benjamin Idini', 'Andre Izidoro', 'Yohai Kaspi', 'Tanja Kovacevic', 'Valéry Lainey', 'Steve Levin', 'Jonathan Lunine', 'Christopher Mankovich', 'Stephen Markham', 'Marius Millot', 'Olivier Mousis', 'Simon Müller', 'Nadine Nettelmann', 'Francis Nimmo']","Determining the internal structure of Uranus is a key objective for planetary science. Knowledge of Uranus's bulk composition and the distribution of elements is crucial to understanding its origin and evolutionary path. In addition, Uranus represents a poorly understood class of intermediate-mass planets (intermediate in size between the relatively well studied terrestrial and gas giant planets), which appear to be very common in the Galaxy. As a result, a better characterization of Uranus will also help us to better understand exoplanets in this mass and size regime. Recognizing the importance of Uranus, a Keck Institute for Space Studies (KISS) workshop was held in September 2023 to investigate how we can improve our knowledge of Uranus's internal structure in the context of a future Uranus mission that includes an orbiter and a probe. The scientific goals and objectives of the recently released Planetary Science and Astrobiology Decadal Survey were taken as our starting point. We reviewed our current knowledge of Uranus's interior and identified measurement and other mission requirements for a future Uranus spacecraft, providing more detail than was possible in the Decadal Survey's mission study and including new insights into the measurements to be made. We also identified important knowledge gaps to be closed with Earth-based efforts in the near term that will help guide the design of the mission and interpret the data returned.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01872
Quantifying Imaginarity in Neutrino Systems,['High Energy Physics - Phenomenology'],"['Ashutosh Kumar Alok', 'Trambak Jyoti Chall', 'Neetu Raj Singh Chundawat', 'Yu-Feng Li']","It is a fundamental question why quantum mechanics employs complex numbers rather than solely real numbers. In this letter, we conduct the first analysis of imaginarity quantification in neutrino flavor and spin-flavor oscillations. As quantum systems in coherent superposition, neutrinos are ideal candidates for quantifying imaginarity within the resource theoretic framework, using measures such as the $\ell_1$-norm and the relative entropy of imaginarity. Our findings reveal that even in the case of two-flavor mixing, these measures of imaginarity are nonzero. The measures of imaginarity reach their extreme values when the probabilistic features of quantum theory are fully maximized, i.e., both the transitional and survival probabilities are approximately equal, averaging around $1/2$. We further extend our analysis to explore the dynamics of three-flavor neutrino mixing, incorporating the effects of a nonzero CP phase. Our study reveals that the imaginarity in neutrino systems is not solely attributed to the CP-violating phase. More importantly, it can also arise from the intrinsic quantum dynamics of the neutrino mixing system itself.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01871
Composition of Experts: A Modular Compound AI System Leveraging Large Language Models,['Machine Learning'],"['Swayambhoo Jain', 'Ravi Raju', 'Bo Li', 'Zoltan Csaki', 'Jonathan Li', 'Kaizhao Liang', 'Guoyao Feng', 'Urmish Thakkar', 'Anand Sampat', 'Raghu Prabhakar', 'Sumati Jairath']","Large Language Models (LLMs) have achieved remarkable advancements, but their monolithic nature presents challenges in terms of scalability, cost, and customization. This paper introduces the Composition of Experts (CoE), a modular compound AI system leveraging multiple expert LLMs. CoE leverages a router to dynamically select the most appropriate expert for a given input, enabling efficient utilization of resources and improved performance. We formulate the general problem of training a CoE and discuss inherent complexities associated with it. We propose a two-step routing approach to address these complexities that first uses a router to classify the input into distinct categories followed by a category-to-expert mapping to obtain desired experts. CoE offers a flexible and cost-effective solution to build compound AI systems. Our empirical evaluation demonstrates the effectiveness of CoE in achieving superior performance with reduced computational overhead. Given that CoE comprises of many expert LLMs it has unique system requirements for cost-effective serving. We present an efficient implementation of CoE leveraging SambaNova SN40L RDUs unique three-tiered memory architecture. CoEs obtained using open weight LLMs Qwen/Qwen2-7B-Instruct, google/gemma-2-9b-it, google/gemma-2-27b-it, meta-llama/Llama-3.1-70B-Instruct and Qwen/Qwen2-72B-Instruct achieve a score of $59.4$ with merely $31$ billion average active parameters on Arena-Hard and a score of $9.06$ with $54$ billion average active parameters on MT-Bench.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01868
Mathematical Frameworks of All-Quantum Mode Adaptable Communication Processors,['Quantum Physics'],"['Donya Sadat Rezaeishad', 'Mohammad Rezai', 'Foroogh Sadat Tabataba', 'Jawad A. Salehi']","The diversity of quantum communication protocols and their rapid growth entail the development of the Quantum Internet, the interconnection of various quantum communication nodes and systems. One of the challenges posed by this development is all-quantum mode adaptation, which is essential for adapting different quantum nodes to the common network, enhancing the efficiency and scalability of quantum information transmission. This paper investigates the mathematical frameworks of all-quantum mode adaptation, focusing on three essential elements-mode expansion, mode reduction, and mode mapping-which are crucial for managing compatibility among different quantum signals. By leveraging various degrees of freedom of photons, we demonstrate the possibility of adaptable all-quantum processors that can dynamically adjust to different communication environments, addressing real-world challenges in the Quantum Internet.△ Less",v1,
A Second Soul: Celebrating the Many Languages of Programming -- Festschrift in Honor of Peter Thiemann's Sixtieth Birthday,['Programming Languages'],"['Annette Bieniusa', 'Markus Degen', 'Stefan Wehr']","This Festschrift is dedicated to Peter Thiemann on the occasion of his sixtieth birthday, celebrating his significant contributions to the field of programming languages. Over the span of more than three decades, Peter has worked on a wide array of topics. This collection of five articles reflects the diversity of his work. The articles cover areas such as partial evaluation and reversible programming, proof assistants and dependent types, discrete mathematics and dynamic programming, functional and object-oriented programming.△ Less","30 November, 2024;",
Ladder equation for the three-particle vertex and its approximate solution,['Other Condensed Matter'],"['Patrick Kappl', 'Tin Ribic', 'Anna Kauch', 'Karsten Held']","We generalize the three two-particle Bethe-Salpeter equations to ten three-particle ladders. These equations are exact and yield the exact three-particle vertex, if we knew the three-particle vertex irreducible in one of the ten channels. However, as we do not have this three-particle irreducible vertex at hand, we approximate this building block for the ladder by the sum of two-particle irreducible vertices each connecting two fermionic lines. The comparison to the exact solution shows that this approximation is only good for rather weak interactions and even than only qualitatively - at least for the non-linear response function analyzed.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.01848
Feature importance of socio-economic parameters in Tuberculosis modeling,['Physics and Society'],"['Andrei Neverov', 'Olga Krivorotko']","This paper considers the problem of modeling epidemic outbreaks in different regions with a common model, that uses additional information about these regions to adjust its parameters and relieve us of mundanity of data collecting, and inverse problem solving for each region separately. To that end, we study tuberculosis and HIV dynamics in regions of Russian Federation from 2009 to 2023 in connection with number of socio-economic parameters. SIR-like model was taken and modified as a dynamic model for tuberculosis-HIV co-infection and inverse problem of transfer rates between compartments was solved, based on statistical data of diseases incidence. To shorten the list of socio-economic parameters we make use of Shapley vector that allows us to estimate importance of these parameters in reconstruction of differential model parameters using regression algorithms.△ Less","22 November, 2024;",https://arxiv.org/pdf/2412.01844
The Role of Social Interactions in Mitigating Psychological Distress During the COVID-19 Pandemic: A Study in Sri Lanka,['Physics and Society'],"['Isuru Thilakasiri', 'Tharaka Fonseka', 'Isuri Mapa', 'Roshan Godaliyadda', 'Vijitha Herath', 'Ramila Thowfeek', 'Anuruddhika Rathnayake', 'Parakrama Ekanayake', 'Janaka Ekanayake']","Massive changes in many aspects related to social groups of different socioeconomic backgrounds were caused by the COVID-19 pandemic and as a result, the overall state of mental health was severely affected globally. This study examined how the pandemic affected Sri Lankan citizens representing a range of socioeconomic backgrounds in terms of their mental health. The data used in this research was gathered from 3020 households encompassing 12,000 people using a nationwide face-to-face survey. Four distinct factors were identified by factor analysis (FA) that was conducted and subsequently, the population was clustered using unsupervised clustering to determine which socioeconomic categories were affected similarly. Two population subgroups were thus identified, their respective relationships to the retrieved components and their demographics were thoroughly examined and interpreted. This resulted in the identification of contrasting perspectives between the two groups towards the maintenance and the state of social relationships during the pandemic, which revealed that one group were more 'outgoing' in nature resulting in their mental state being comparatively better in coping with the pandemic. The other group was seen to be more 'reserved' showing an opposite reaction towards social connections while their mental well-being declined showing symptoms such as loneliness, and emptiness.△ Less","21 November, 2024;",https://arxiv.org/pdf/2412.01843
Dynamics of Resource Allocation in O-RANs: An In-depth Exploration of On-Policy and Off-Policy Deep Reinforcement Learning for Real-Time Applications,['Networking and Internet Architecture'],"['Manal Mehdaoui', 'Amine Abouaomar']","Deep Reinforcement Learning (DRL) is a powerful tool used for addressing complex challenges in mobile networks. This paper investigates the application of two DRL models, on-policy and off-policy, in the field of resource allocation for Open Radio Access Networks (O-RAN). The on-policy model is the Proximal Policy Optimization (PPO), and the off-policy model is the Sample Efficient Actor-Critic with Experience Replay (ACER), which focuses on resolving the challenges of resource allocation associated with a Quality of Service (QoS) application that has strict requirements. Motivated by the original work of Nessrine Hammami and Kim Khoa Nguyen, this study is a replication to validate and prove the findings. Both PPO and ACER are used within the same experimental setup to assess their performance in a scenario of latency-sensitive and latency-tolerant users and compare them. The aim is to verify the efficacy of on-policy and off-policy DRL models in the context of O-RAN resource allocation. Results from this replication contribute to the ongoing scientific research and offer insights into the reproducibility and generalizability of the original research. This analysis reaffirms that both on-policy and off-policy DRL models have better performance than greedy algorithms in O-RAN settings. In addition, it confirms the original observations that the on-policy model (PPO) gives a favorable balance between energy consumption and user latency, while the off-policy model (ACER) shows a faster convergence. These findings give good insights to optimize resource allocation strategies in O-RANs. Index Terms: 5G, O-RAN, resource allocation, ML, DRL, PPO, ACER.△ Less","17 November, 2024;",https://arxiv.org/pdf/2412.01839
Eye dominance and testing order effects in the circularly-oriented macular pigment optical density measurements that rely on the perception of structured light-based stimuli,['Neurons and Cognition'],"['Mukhit Kulmaganbetov', 'Taranjit Singh', 'Dmitry Pushin', 'Pinki Chahal', 'David Cory', 'Davis Garrad', 'Connor Kapahi', 'Melanie Mungalsingh', 'Iman Salehi', 'Andrew Silva', 'Ben Thompson', 'Zhangting Wang', 'Dusan Sarenac']","Psychophysical discrimination of structured light (SL) stimuli may be useful in screening for various macular disorders, including degenerative macular diseases. The circularly-oriented macular pigment optical density (coMPOD), calculated from the discrimination performance of SL-induced entoptic phenomena, may reveal a novel functional biomarker of macular health. In this study, we investigated the potential influence of eye dominance and testing order effects on SL-based stimulus perception, factors that potentially influence the sensitivity of screening tests based on SL technology. A total of 28 participants (aged 18-38 years) were selected for the study after undergoing a comprehensive eye examination. A psychophysical task was performed where various SL-based entoptic images with multiple azimuthal fringes rotating with a specific temporal frequency were projected onto the participants' retinas. By occluding the central areas of entoptic images, we measured the retinal eccentricity ($R$) of the perceivable area of the stimuli. The slope of the coMPOD profile ($a$-value) was calculated for each participant using a spatiotemporal sensitivity model that takes into account the perceptual threshold measurements of structured light stimuli with varying spatial densities and temporal frequencies. The Pearson correlation coefficient between eye dominance and testing order effects was $r=0.8$ ($p<0.01$). The Bland-Altman plots for both factors indicated zero bias. The results indicate repeatable measurements for both eyes, implying minimal impact from eye dominance and testing order on SL-based stimulus perception. The results provide a foundation for future studies exploring the clinical utility of SL tools in eye health.△ Less","16 November, 2024;",https://arxiv.org/pdf/2412.01836
Explainable Artificial Intelligence for Medical Applications: A Review,['Machine Learning'],"['Qiyang Sun', 'Alican Akman', 'Björn W. Schuller']","The continuous development of artificial intelligence (AI) theory has propelled this field to unprecedented heights, owing to the relentless efforts of scholars and researchers. In the medical realm, AI takes a pivotal role, leveraging robust machine learning (ML) algorithms. AI technology in medical imaging aids physicians in X-ray, computed tomography (CT) scans, and magnetic resonance imaging (MRI) diagnoses, conducts pattern recognition and disease prediction based on acoustic data, delivers prognoses on disease types and developmental trends for patients, and employs intelligent health management wearable devices with human-computer interaction technology to name but a few. While these well-established applications have significantly assisted in medical field diagnoses, clinical decision-making, and management, collaboration between the medical and AI sectors faces an urgent challenge: How to substantiate the reliability of decision-making? The underlying issue stems from the conflict between the demand for accountability and result transparency in medical scenarios and the black-box model traits of AI. This article reviews recent research grounded in explainable artificial intelligence (XAI), with an emphasis on medical practices within the visual, audio, and multimodal perspectives. We endeavour to categorise and synthesise these practices, aiming to provide support and guidance for future researchers and healthcare professionals.△ Less","15 November, 2024;",https://arxiv.org/pdf/2412.01829
RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations,['Computer Vision and Pattern Recognition'],"['Savya Khosla', 'Sethuraman T V', 'Alexander Schwing', 'Derek Hoiem']","We present RELOCATE, a simple training-free baseline designed to perform the challenging task of visual query localization in long videos. To eliminate the need for task-specific training and efficiently handle long videos, RELOCATE leverages a region-based representation derived from pretrained vision models. At a high level, it follows the classic object localization approach: (1) identify all objects in each video frame, (2) compare the objects with the given query and select the most similar ones, and (3) perform bidirectional tracking to get a spatio-temporal response. However, we propose some key enhancements to handle small objects, cluttered scenes, partial visibility, and varying appearances. Notably, we refine the selected objects for accurate localization and generate additional visual queries to capture visual variations. We evaluate RELOCATE on the challenging Ego4D Visual Query 2D Localization dataset, establishing a new baseline that outperforms prior task-specific methods by 49% (relative improvement) in spatio-temporal average precision.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01826
GETAE: Graph information Enhanced deep neural NeTwork ensemble ArchitecturE for fake news detection,['Artificial Intelligence'],"['Ciprian-Octavian Truică', 'Elena-Simona Apostol', 'Marius Marogel', 'Adrian Paschke']","In today's digital age, fake news has become a major problem that has serious consequences, ranging from social unrest to political upheaval. To address this issue, new methods for detecting and mitigating fake news are required. In this work, we propose to incorporate contextual and network-aware features into the detection process. This involves analyzing not only the content of a news article but also the context in which it was shared and the network of users who shared it, i.e., the information diffusion. Thus, we propose GETAE, \underline{G}raph Information \underline{E}nhanced Deep Neural Ne\underline{t}work Ensemble \underline{A}rchitectur\underline{E} for Fake News Detection, a novel ensemble architecture that uses textual content together with the social interactions to improve fake news detection. GETAE contains two Branches: the Text Branch and the Propagation Branch. The Text Branch uses Word and Transformer Embeddings and a Deep Neural Network based on feed-forward and bidirectional Recurrent Neural Networks (\textsc{[Bi]RNN}) for learning novel contextual features and creating a novel Text Content Embedding. The Propagation Branch considers the information propagation within the graph network and proposes a Deep Learning architecture that employs Node Embeddings to create novel Propagation Embedding. GETAE Ensemble combines the two novel embeddings, i.e., Text Content Embedding and Propagation Embedding, to create a novel \textit{Propagation-Enhanced Content Embedding} which is afterward used for classification. The experimental results obtained on two real-world publicly available datasets, i.e., Twitter15 and Twitter16, prove that using this approach improves fake news detection and outperforms state-of-the-art models.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01825
World-consistent Video Diffusion with Explicit 3D Modeling,['Computer Vision and Pattern Recognition'],"['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu']","Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01821
Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis,['Computer Vision and Pattern Recognition'],"['Anton Voronov', 'Denis Kuznedelev', 'Mikhail Khoroshikh', 'Valentin Khrulkov', 'Dmitry Baranchuk']","This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then argue that scale-wise transformers do not require causality and propose a non-causal counterpart facilitating ~11% faster sampling and lower memory usage while also achieving slightly better generation quality. Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. By disabling guidance at these scales, we achieve an additional sampling acceleration of ~20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7 times faster.△ Less",v1,https://arxiv.org/pdf/2412.01819
[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster,['Computer Vision and Pattern Recognition'],"['Qizhe Zhang', 'Aosong Cheng', 'Ming Lu', 'Zhiyong Zhuo', 'Minqi Wang', 'Jiajun Cao', 'Shaobo Guo', 'Qi She', 'Shanghang Zhang']","Large vision-language models (VLMs) often rely on a substantial number of visual tokens when interacting with large language models (LLMs), which has proven to be inefficient. Recent efforts have aimed to accelerate VLM inference by pruning visual tokens. Most existing methods assess the importance of visual tokens based on the text-visual cross-attentions in LLMs. In this study, we find that the cross-attentions between text and visual tokens in LLMs are inaccurate. Pruning tokens based on these inaccurate attentions leads to significant performance degradation, especially at high reduction ratios. To this end, we introduce FasterVLM, a simple yet effective training-free visual token pruning method that evaluates the importance of visual tokens more accurately by utilizing attentions between the [CLS] token and image tokens from the visual encoder. Since FasterVLM eliminates redundant visual tokens immediately after the visual encoder, ensuring they do not interact with LLMs and resulting in faster VLM inference. It is worth noting that, benefiting from the accuracy of [CLS] cross-attentions, FasterVLM can prune 95\% of visual tokens while maintaining 90\% of the performance of LLaVA-1.5-7B. We apply FasterVLM to various VLMs, including LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA, to demonstrate its effectiveness. Experimental results show that our FasterVLM maintains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing text-visual attention-based methods. Our code is available at https://github.com/Theia-4869/FasterVLM.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01818
Efficient Semantic Communication Through Transformer-Aided Compression,['Machine Learning'],"['Matin Mortaheb', 'Mohammad A. Amir Khojastepour', 'Sennur Ulukus']","Transformers, known for their attention mechanisms, have proven highly effective in focusing on critical elements within complex data. This feature can effectively be used to address the time-varying channels in wireless communication systems. In this work, we introduce a channel-aware adaptive framework for semantic communication, where different regions of the image are encoded and compressed based on their semantic content. By employing vision transformers, we interpret the attention mask as a measure of the semantic contents of the patches and dynamically categorize the patches to be compressed at various rates as a function of the instantaneous channel bandwidth. Our method enhances communication efficiency by adapting the encoding resolution to the content's relevance, ensuring that even in highly constrained environments, critical information is preserved. We evaluate the proposed adaptive transmission framework using the TinyImageNet dataset, measuring both reconstruction quality and accuracy. The results demonstrate that our approach maintains high semantic fidelity while optimizing bandwidth, providing an effective solution for transmitting multi-resolution data in limited bandwidth conditions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01817
Free boundary regularity for semilinear variational problems with a topological constraint,['Analysis of PDEs'],"['Michael Novack', 'Daniel Restrepo', 'Anna Skorobogatova']","We study a class of semilinear free boundary problems in which admissible functions $u$ have a topological constraint, or spanning condition, on their 1-level set. This constraint forces $\{u=1\}$, which is the free boundary, to behave like a surface with some special types of singularities attached to a fixed boundary frame, in the spirit of the Plateau problem of Harrison-Pugh. Two such free boundary problems are the minimization of capacity among surfaces sharing a common boundary and an Allen-Cahn formulation of the Plateau problem. We establish the existence of minimizers and study the regularity of solutions to the Euler-Lagrange equations, obtaining the optimal Lipschitz regularity for solutions and analytic regularity for the free boundaries away from a codimension two singular set. The singularity models for these problems are given by conical critical points of the minimal capacity problem, which are closely related to spectral optimal partition and segregation problems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01813
Anomalous geometric transport signatures of topological Euler class,['Mesoscale and Nanoscale Physics'],"['Ashwat Jain', 'Wojciech J. Jankowski', 'Robert-Jan Slager']","We investigate Riemannian quantum-geometric structures in semiclassical transport features of two-dimensional multigap topological phases. In particular, we study nonlinear Hall-like bulk electric current responses and, accordingly, semiclassical equations of motion induced by the presence of a topological Euler invariant. We provide analytic understanding of these quantities by phrasing them in terms of momentum-space geodesics and geodesic deviation equations and further corroborate these insights with numerical solutions. Within this framework, we moreover uncover anomalous bulk dynamics associated with the second- and third-order nonlinear Hall conductivities induced by a patch Euler invariant. As a main finding, our results show how one can reconstruct the Euler invariant on coupling to electric fields at nonlinear order and from the gradients of the electric fields. Furthermore, we comment on the possibility of deducing the non-trivial non-Abelian Euler class invariant specifically in second-order nonlinear ballistic conductance measurements within a triple-contact setup, which was recently proposed to probe the Euler characteristics of more general Fermi surfaces. Generally, our results provide a route for deducing the topology in real materials that exhibit the Euler invariant by analyzing bulk electrical currents.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01810
Holography for BCFTs with Multiple Boundaries: Multi-Splitting Quenches,['High Energy Physics - Theory'],"['Joseph Lap', 'Berndt Mueller', 'Andreas Schaefer', 'Clemens Seidl']",We elaborate on the method introduced in arXiv:2403.02165 for holographic duals of Boundary Conformal Field Theories (BCFTs) with multiple boundaries. Using these advances we calculate the entanglement entropy as a function of time for 1+1-dimensional CFTs that are split into $N$ subsystems. We give explicit results for $N = 4$ and $N = 17$. We find that all qualitative differences that arise for larger $N$ are present for $N = 4$.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01808
Random Tree Model of Meaningful Memory,['Statistical Mechanics'],"['Weishun Zhong', 'Tankut Can', 'Antonis Georgiou', 'Ilya Shnayderman', 'Mikhail Katkov', 'Misha Tsodyks']","Traditional studies of memory for meaningful narratives focus on specific stories and their semantic structures but do not address common quantitative features of recall across different narratives. We introduce a statistical ensemble of random trees to represent narratives as hierarchies of key points, where each node is a compressed representation of its descendant leaves, which are the original narrative segments. Recall is modeled as constrained by working memory capacity from this hierarchical structure. Our analytical solution aligns with observations from large-scale narrative recall experiments. Specifically, our model explains that (1) average recall length increases sublinearly with narrative length, and (2) individuals summarize increasingly longer narrative segments in each recall sentence. Additionally, the theory predicts that for sufficiently long narratives, a universal, scale-invariant limit emerges, where the fraction of a narrative summarized by a single recall sentence follows a distribution independent of narrative length.△ Less",v1,https://arxiv.org/pdf/2412.01806
Euler-Kronecker constants of modular forms: beyond Dirichlet $L$-series,['Number Theory'],"['Steven Charlton', 'Anna Medvedovsky', 'Pieter Moree']","The Euler-Kronecker constants related to congruences of Fourier coefficients of modular forms that have been computed so far, involve logarithmic derivatives of Dirichlet $L$-series as most complicated functions (to the best of our knowledge). However, generically the more complicated Artin $L$-series will make their appearance. Here we work out some simple examples involving an Artin $L$-series related to an ${\mathfrak S}_3$, respectively~${\mathfrak S}_4$ extension. These examples are related to a mod-2 congruence for $X_0(11)$, respectively a mod-59 congruence for $ΔE_4$ conjectured by Serre and Swinnerton-Dyer and proved by Haberland. The latter example solves a problem posed by Ciolan, Languasco and the third author in 2023.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01803
SceneFactor: Factored Latent 3D Diffusion for Controllable 3D Scene Generation,['Computer Vision and Pattern Recognition'],"['Alexey Bokhovkin', 'Quan Meng', 'Shubham Tulsiani', 'Angela Dai']","We present SceneFactor, a diffusion-based approach for large-scale 3D scene generation that enables controllable generation and effortless editing. SceneFactor enables text-guided 3D scene synthesis through our factored diffusion formulation, leveraging latent semantic and geometric manifolds for generation of arbitrary-sized 3D scenes. While text input enables easy, controllable generation, text guidance remains imprecise for intuitive, localized editing and manipulation of the generated 3D scenes. Our factored semantic diffusion generates a proxy semantic space composed of semantic 3D boxes that enables controllable editing of generated scenes by adding, removing, changing the size of the semantic 3D proxy boxes that guides high-fidelity, consistent 3D geometric editing. Extensive experiments demonstrate that our approach enables high-fidelity 3D scene synthesis with effective controllable editing through our factored diffusion approach.△ Less",v1,https://arxiv.org/pdf/2412.01801
HPRM: High-Performance Robotic Middleware for Intelligent Autonomous Systems,['Robotics'],"['Jacky Kwok', 'Shulu Li', 'Marten Lohstroh', 'Edward A. Lee']","The rise of intelligent autonomous systems, especially in robotics and autonomous agents, has created a critical need for robust communication middleware that can ensure real-time processing of extensive sensor data. Current robotics middleware like Robot Operating System (ROS) 2 faces challenges with nondeterminism and high communication latency when dealing with large data across multiple subscribers on a multi-core compute platform. To address these issues, we present High-Performance Robotic Middleware (HPRM), built on top of the deterministic coordination language Lingua Franca (LF). HPRM employs optimizations including an in-memory object store for efficient zero-copy transfer of large payloads, adaptive serialization to minimize serialization overhead, and an eager protocol with real-time sockets to reduce handshake latency. Benchmarks show HPRM achieves up to 173x lower latency than ROS2 when broadcasting large messages to multiple nodes. We then demonstrate the benefits of HPRM by integrating it with the CARLA simulator and running reinforcement learning agents along with object detection workloads. In the CARLA autonomous driving application, HPRM attains 91.1% lower latency than ROS2. The deterministic coordination semantics of HPRM, combined with its optimized IPC mechanisms, enable efficient and predictable real-time communication for intelligent autonomous systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01799
The Newly Discovered Nova Super-Remnant Surrounding Recurrent Nova T Coronae Borealis: Will it Light Up During the Coming Eruption?,['Solar and Stellar Astrophysics'],"['Michael M. Shara', 'Kenneth M. Lanzetta', 'Alexandra Masegian', 'James T. Garland', 'Stefan Gromoll', 'Joanna Mikolajewska', 'Mikita Misiura', 'David Valls-Gabaud', 'Frederick M. Walter', 'John K. Webb']","A century or less separates the thermonuclear-powered eruptions of recurrent novae in the hydrogen-rich envelopes of massive white dwarfs. The colliding ejecta of successive recurrent nova events are predicted to always generate very large (tens of parsecs) super-remnants; only two examples are currently known. T CrB offers an excellent opportunity to test this prediction. As it will almost certainly undergo its next, once-in ~80-year recurrent nova event between 2024 and 2026, we carried out very deep narrowband and continuum imaging to search for the predicted, piled-up ejecta of the past millenia. While nothing is detected in continuum or narrowband [OIII] images, a ~30-parsec-diameter, faint nebulosity surrounding T CrB is clearly present in deep Halpha, [NII] and [SII] narrowband Condor Array Telescope imagery. We predict that these newly detected nebulosities, as well as the recent ejecta that have not yet reached the super-remnant, are far too optically-thin to capture all but a tiny fraction of the photons emitted by RN flashes. We thus predict that fluorescent light echoes will NOT be detectable following the imminent nova flash of T CrB. Dust may be released by the T CrB red giant wind in pre-eruption outbursts, but we have no reliable estimates of its quantity or geometrical distribution. While we cannot predict the morphology or intensity of dust-induced continuum light echoes following the coming flash, we encourage multi-epoch Hubble Space Telescope optical imaging as well as James Webb Space Telescope infrared imaging of T CrB during the year after it erupts.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01797
Suppression of the collisionless tearing mode by flow shear: implications for reconnection onset in the Alfvénic solar wind,['Space Physics'],"['A. Mallet', 'S. Eriksson', 'M. Swisdak', 'J. Juno']","We analyse the collisionless tearing mode instability of a current sheet with a strong shear flow across the layer. The growth rate decreases with increasing shear flow, and is completely stabilized as the shear flow becomes Alfvénic. We also show that in the presence of strong flow shear, the tearing mode growth rate decreases with increasing background ion-to-electron temperature ratio, the opposite behaviour to the tearing mode without flow shear. We find that even a relatively small flow shear is enough to dramatically alter the scaling behaviour of the mode, because the growth rate is small compared to the shear flow across the ion scales (but large compared to shear flow across the electron scales). Our results may explain the relative absence of reconnection events in the near-Sun Alfvénic solar wind observed recently by NASA's Parker Solar Probe.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01796
The Jet Paths of Radio AGN and their Cluster Weather,['Astrophysics of Galaxies'],"['E. Vardoulaki', 'V. Backöfer', 'A. Finoguenov', 'F. Vazza', 'J. Comparat', 'G. Gozaliasl', 'I. H. Whittam', 'C. L. Hale', 'J. R. Weaver', 'A. M. Koekemoer', 'J. D. Collier', 'B. Frank', 'I. Heywood', 'S. Sekhar', 'A. R. Taylor', 'S. Pinjarkar', 'M. J. Hardcastle', 'T. Shimwell', 'M. Hoeft', 'S. V. White', 'F. An', 'F. Tabatabaei', 'Z. Randriamanakoto', 'M. D. Filipovic']","We studied bent radio sources within X-ray galaxy groups in the COSMOS and XMM-LSS fields, using radio data from the MeerKAT International GHz Tiered Extragalactic Explorations data release 1 (MIGHTEE-DR1) at 1.2-1.3 GHz (angular resolutions of 8.9"" and 5""; <rms> ~ 3.5 and 5.5 uJy/beam). Bent radio active galactic nuclei (AGN) were identified via visual inspection. Our analysis included 19 bent radio AGN in the COSMOS field and 17 in the XMM-LSS field which lie within X-ray galaxy groups (2x10^13 >= M200c/Msun = 3x10^14). We investigated the relationship between their bending angle (BA) - the angle formed by the jets or lobes of two-sided radio sources associated with AGN - and properties of their host galaxies and large-scale environment probed by the X-ray galaxy groups. Our key findings are: a) In the XMM-LSS field, we observed a strong correlation between the linear projected size of the bent AGN, the group halo mass, and the projected distance from the group centre. This trend, consistent with previous studies, was not detected in the COSMOS sample. b) The BA is a function of environmental density, with the type of medium playing a significant role. Additionally, at z <= 0.5 we found a higher number of bent sources (BA <= 160deg) compared to higher redshifts (z ~ 1), by a factor of >1.5. This trend aligns with magnetohydrodynamic simulations, which suggest that denser environments and longer interaction times at lower redshifts contribute to this effect. Comparison with the literature suggests that jet bending in galaxy groups within the redshift range 0.1 < z < 1.2 is primarily driven by ram pressure exerted on the jets, which occurs during quiescent phases of AGN activity. This study underscores the role of environmental interactions in shaping the morphology of radio AGN within galaxy groups, providing insights into the interplay between large-scale structure and AGN physics.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01795
IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models,['Computer Vision and Pattern Recognition'],"['Khaled Abud', 'Sergey Lavrushkin', 'Alexey Kirillov', 'Dmitriy Vatolin']","Diffusion-based models have recently transformed conditional image generation, achieving unprecedented fidelity in generating photorealistic and semantically accurate images. However, consistently generating high-quality images remains challenging, partly due to the lack of mechanisms for conditioning outputs on perceptual quality. In this work, we propose methods to integrate image quality assessment (IQA) models into diffusion-based generators, enabling quality-aware image generation. First, we experiment with gradient-based guidance to optimize image quality directly and show this approach has limited generalizability. To address this, we introduce IQA-Adapter, a novel architecture that conditions generation on target quality levels by learning the relationship between images and quality scores. When conditioned on high target quality, IQA-Adapter shifts the distribution of generated images towards a higher-quality subdomain. This approach achieves up to a 10% improvement across multiple objective metrics, as confirmed by a subjective study, while preserving generative diversity and content. Additionally, IQA-Adapter can be used inversely as a degradation model, generating progressively more distorted images when conditioned on lower quality scores. Our quality-aware methods also provide insights into the adversarial robustness of IQA models, underscoring the potential of quality conditioning in generative modeling and the importance of robust IQA methods.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01794
DextrAH-RGB: Visuomotor Policies to Grasp Anything with Dexterous Hands,['Robotics'],"['Ritvik Singh', 'Arthur Allshire', 'Ankur Handa', 'Nathan Ratliff', 'Karl Van Wyk']","One of the most important yet challenging skills for a robot is the task of dexterous grasping of a diverse range of objects. Much of the prior work is limited by the speed, dexterity, or reliance on depth maps. In this paper, we introduce DextrAH-RGB, a system that can perform dexterous arm-hand grasping end2end from stereo RGB input. We train a teacher fabric-guided policy (FGP) in simulation through reinforcement learning that acts on a geometric fabric action space to ensure reactivity and safety. We then distill this teacher FGP into a stereo RGB-based student FGP in simulation. To our knowledge, this is the first work that is able to demonstrate robust sim2real transfer of an end2end RGB-based policy for complex, dynamic, contact-rich tasks such as dexterous grasping. Our policies are able to generalize grasping to novel objects with unseen geometry, texture, or lighting conditions during training. Videos of our system grasping a diverse range of unseen objects are available at \url{https://dextrah-rgb.github.io/}△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.01791
UNIONS: a direct measurement of intrinsic alignment with BOSS/eBOSS spectroscopy,['Cosmology and Nongalactic Astrophysics'],"['Fabian Hervas Peters', 'Martin Kilbinger', 'Romain Paviot', 'Lucie Baumont', 'Elisa Russier', 'Ziwen Zhang', 'Calum Murray', 'Valeria Pettorino', 'Thomas de Boer', 'Sébastien Fabbro', 'Sacha Guerrini', 'Hendrik Hildebrandt', 'Mike Hudson', 'Ludovic Van Waerbeke', 'Anna Wittje']","During their formation, galaxies are subject to tidal forces, which create correlations between their shapes and the large-scale structure of the Universe, known as intrinsic alignment. This alignment is a contamination for cosmic-shear measurements as one needs to disentangle correlations induced by external lensing effects from those intrinsically present in galaxies. We constrain the amplitude of intrinsic alignment and test models by making use of the overlap between the Ultraviolet Near-Infrared Optical Northern Survey (UNIONS) covering $3500 \, \mathrm{deg}^2$, and spectroscopic data from the Baryon Oscillation Spectroscopic Survey (BOSS/eBOSS). By comparing our results to measurements from other lensing surveys on the same spectroscopic tracers, we can test the reliability of these estimates and verify they are not survey dependent. We measure projected correlation functions between positions and ellipticities, which we model with perturbation theory to constrain the commonly used non-linear alignment model and its higher-order expansion. Using the non-linear alignment model, we obtain a $13σ$ detection with CMASS galaxies, a $3σ$ detection with LRGs, and a detection compatible with the null hypothesis for ELGs. We test the tidal alignment and tidal torque model, a higher-order alignment model, which we find to be in good agreement with the non-linear alignment prediction and for which we can constrain the second-order parameters. We show a strong scaling of our intrinsic alignment amplitude with luminosity. We demonstrate that the UNIONS sample is robust against systematic contributions, particularly concerning PSF biases. We reached a reasonable agreement when comparing our measurements to other lensing samples for the same spectroscopic samples. We take this agreement as an indication that direct measurements of intrinsic alignment are mature for stage IV priors.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01790
The Dark Side of Double-Tensor Multiplets,['High Energy Physics - Theory'],"['Laura Andrianopoli', 'Giuseppe Casale', 'Lucrezia Ravera', 'Alberto Santambrogio']","We explore the properties of a set of free double-tensor multiplets in $N=2$ supersymmetry, focusing on their behavior within rigid superspace. These multiplets can be obtained from hypermultiplets by Hodge-dualizing half of their scalars, and feature an off-shell matching of bosonic and fermionic degrees of freedom. Despite this fact, the supersymmetry algebra results to close only on-shell. Our analysis is conducted both in superspace, using the geometric (rheonomic) approach, and in spacetime, comparing how our results are obtained in the two approaches. Notably, the cohomology of superspace requires that the scalars Hodge-dual to the antisymmetric tensors crucially contribute to the superspace description of the tensors super-field strengths. This shows an inherent non-locality of the theory, already in the free case, which however does not forbid a Lagrangian description.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01788
Hard Constraint Guided Flow Matching for Gradient-Free Generation of PDE Solutions,['Machine Learning'],"['Chaoran Cheng', 'Boran Han', 'Danielle C. Maddix', 'Abdul Fatir Ansari', 'Andrew Stuart', 'Michael W. Mahoney', 'Yuyang Wang']","Generative models that satisfy hard constraints are crucial in many scientific and engineering applications where physical laws or system requirements must be strictly respected. However, many existing constrained generative models, especially those developed for computer vision, rely heavily on gradient information, often sparse or computationally expensive in fields like partial differential equations (PDEs). In this work, we introduce a novel framework for adapting pre-trained, unconstrained flow-matching models to satisfy constraints exactly in a zero-shot manner without requiring expensive gradient computations or fine-tuning. Our framework, ECI sampling, alternates between extrapolation (E), correction (C), and interpolation (I) stages during each iterative sampling step of flow matching sampling to ensure accurate integration of constraint information while preserving the validity of the generation. We demonstrate the effectiveness of our approach across various PDE systems, showing that ECI-guided generation strictly adheres to physical constraints and accurately captures complex distribution shifts induced by these constraints. Empirical results demonstrate that our framework consistently outperforms baseline approaches in various zero-shot constrained generation tasks and also achieves competitive results in the regression tasks without additional fine-tuning.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01786
Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models,['Artificial Intelligence'],"['Cameron Tice', 'Philipp Alexander Kreer', 'Nathan Helm-Burger', 'Prithviraj Singh Shahani', 'Fedor Ryzhenkov', 'Jacob Haimes', 'Felix Hofstätter', 'Teun van der Weij']","Capability evaluations play a critical role in ensuring the safe deployment of frontier AI systems, but this role may be undermined by intentional underperformance or ``sandbagging.'' We present a novel model-agnostic method for detecting sandbagging behavior using noise injection. Our approach is founded on the observation that introducing Gaussian noise into the weights of models either prompted or fine-tuned to sandbag can considerably improve their performance. We test this technique across a range of model sizes and multiple-choice question benchmarks (MMLU, AI2, WMDP). Our results demonstrate that noise injected sandbagging models show performance improvements compared to standard models. Leveraging this effect, we develop a classifier that consistently identifies sandbagging behavior. Our unsupervised technique can be immediately implemented by frontier labs or regulatory bodies with access to weights to improve the trustworthiness of capability evaluations.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01784
Transfer Learning for Control Systems via Neural Simulation Relations,['Systems and Control'],"['Alireza Nadali', 'Bingzhuo Zhong', 'Ashutosh Trivedi', 'Majid Zamani']","Transfer learning is an umbrella term for machine learning approaches that leverage knowledge gained from solving one problem (the source domain) to improve speed, efficiency, and data requirements in solving a different but related problem (the target domain). The performance of the transferred model in the target domain is typically measured via some notion of loss function in the target domain. This paper focuses on effectively transferring control logic from a source control system to a target control system while providing approximately similar behavioral guarantees in both domains. However, in the absence of a complete characterization of behavioral specifications, this problem cannot be captured in terms of loss functions. To overcome this challenge, we use (approximate) simulation relations to characterize observational equivalence between the behaviors of two systems.
  Simulation relations ensure that the outputs of both systems, equipped with their corresponding controllers, remain close to each other over time, and their closeness can be quantified {\it a priori}. By parameterizing simulation relations with neural networks, we introduce the notion of \emph{neural simulation relations}, which provides a data-driven approach to transfer any synthesized controller, regardless of the specification of interest, along with its proof of correctness. Compared with prior approaches, our method eliminates the need for a closed-loop mathematical model and specific requirements for both the source and target systems. We also introduce validity conditions that, when satisfied, guarantee the closeness of the outputs of two systems equipped with their corresponding controllers, thus eliminating the need for post-facto verification. We demonstrate the effectiveness of our approach through case studies involving a vehicle and a double inverted pendulum.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01783
HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing,['Cryptography and Security'],"['Lajos Muzsai', 'David Imolai', 'András Lukács']","We introduce HackSynth, a novel Large Language Model (LLM)-based agent capable of autonomous penetration testing. HackSynth's dual-module architecture includes a Planner and a Summarizer, which enable it to generate commands and process feedback iteratively. To benchmark HackSynth, we propose two new Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms PicoCTF and OverTheWire. These benchmarks include two hundred challenges across diverse domains and difficulties, providing a standardized framework for evaluating LLM-based penetration testing agents. Based on these benchmarks, extensive experiments are presented, analyzing the core parameters of HackSynth, including creativity (temperature and top-p) and token utilization. Multiple open source and proprietary LLMs were used to measure the agent's capabilities. The experiments show that the agent performed best with the GPT-4o model, better than what the GPT-4o's system card suggests. We also discuss the safety and predictability of HackSynth's actions. Our findings indicate the potential of LLM-based agents in advancing autonomous penetration testing and the importance of robust safeguards. HackSynth and the benchmarks are publicly available to foster research on autonomous cybersecurity solutions.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01778
On closed characteristics of minimal action on a convex three-sphere,['Symplectic Geometry'],"['Alberto Abbondandolo', 'Oliver Edtmair', 'Jungsoo Kang']",We prove that every closed characteristic of minimal action on the boundary of a uniformly convex domain in $\R^4$ bounds a disk-like global surface of section. A corollary is that the cylindrical symplectic capacity of a convex body in $\R^4$ coincides with the minimal action of a closed generalized characteristic on its boundary.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01777
Importance of precipitation on the slowdown of creep behaviour induced by pressure-solution,['Materials Science'],"['Alexandre Sac-Morane', 'Hadrien Rattez', 'Manolis Veveakis']","Pressure-solution is a chemo-mechanical process, involving dissolution at grain/asperity contacts and precipitation away from them. It induces a compaction in time of rocks and sediments. The present study investigates numerically the impact of precipitation on the slowdown of creep behavior induced by pressure-solution. A recently published framework, called the Phase-Field Discrete Element Model, is carefully calibrated against existing indentation experiments and validated for other rate-limiting scenarios. It is shown that when precipitation is relatively slow, the slowdown of pressure-solution is due to a chemical mechanism (accumulation of solute concentration within the pore space), whereas, at fast precipitation, the slowdown is due to a mechanical mechanism (stress reduction at the contact).△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01775
FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning,['Machine Learning'],"['Lisha Chen', 'AFM Saif', 'Yanning Shen', 'Tianyi Chen']","Finding specific preference-guided Pareto solutions that represent different trade-offs among multiple objectives is critical yet challenging in multi-objective problems. Existing methods are restrictive in preference definitions and/or their theoretical guarantees. In this work, we introduce a Flexible framEwork for pREfeRence-guided multi-Objective learning (FERERO) by casting it as a constrained vector optimization problem. Specifically, two types of preferences are incorporated into this formulation -- the relative preference defined by the partial ordering induced by a polyhedral cone, and the absolute preference defined by constraints that are linear functions of the objectives. To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants. Notably, this is the first single-loop primal algorithm for constrained vector optimization to our knowledge. The proposed algorithms adaptively adjust to both constraint and objective values, eliminating the need to solve different subproblems at different stages of constraint satisfaction. Experiments on multiple benchmarks demonstrate the proposed method is very competitive in finding preference-guided optimal solutions. Code is available at https://github.com/lisha-chen/FERERO/.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01773
Robot Learning with Super-Linear Scaling,['Robotics'],"['Marcel Torne', 'Arhan Jain', 'Jiayi Yuan', 'Vidaaranya Macha', 'Lars Ankile', 'Anthony Simeonov', 'Pulkit Agrawal', 'Abhishek Gupta']","Scaling robot learning requires data collection pipelines that scale favorably with human effort. In this work, we propose Crowdsourcing and Amortizing Human Effort for Real-to-Sim-to-Real(CASHER), a pipeline for scaling up data collection and learning in simulation where the performance scales superlinearly with human effort. The key idea is to crowdsource digital twins of real-world scenes using 3D reconstruction and collect large-scale data in simulation, rather than the real-world. Data collection in simulation is initially driven by RL, bootstrapped with human demonstrations. As the training of a generalist policy progresses across environments, its generalization capabilities can be used to replace human effort with model generated demonstrations. This results in a pipeline where behavioral data is collected in simulation with continually reducing human effort. We show that CASHER demonstrates zero-shot and few-shot scaling laws on three real-world tasks across diverse scenarios. We show that CASHER enables fine-tuning of pre-trained policies to a target scenario using a video scan without any additional human effort. See our project website: https://casher-robot-learning.github.io/CASHER/△ Less",v1,https://arxiv.org/pdf/2412.01770
Commit0: Library Generation from Scratch,['Software Engineering'],"['Wenting Zhao', 'Nan Jiang', 'Celine Lee', 'Justin T Chiu', 'Claire Cardie', 'Matthias Gallé', 'Alexander M Rush']","With the goal of benchmarking generative systems beyond expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library's API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, Commit0 is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. Commit0 also offers an interactive environment where models receive static analysis and execution feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01769
Bluetooth Low Energy Dataset Using In-Phase and Quadrature Samples for Indoor Localization,['Machine Learning'],"['Samuel G. Leitch', 'Qasim Zeeshan Ahmed', 'Ben Van Herbruggen', 'Mathias Baert', 'Jaron Fontaine', 'Eli De Poorter', 'Adnan Shahid', 'Pavlos I. Lazaridis']","One significant challenge in research is to collect a large amount of data and learn the underlying relationship between the input and the output variables. This paper outlines the process of collecting and validating a dataset designed to determine the angle of arrival (AoA) using Bluetooth low energy (BLE) technology. The data, collected in a laboratory setting, is intended to approximate real-world industrial scenarios. This paper discusses the data collection process, the structure of the dataset, and the methodology adopted for automating sample labeling for supervised learning. The collected samples and the process of generating ground truth (GT) labels were validated using the Texas Instruments (TI) phase difference of arrival (PDoA) implementation on the data, yielding a mean absolute error (MAE) at one of the heights without obstacles of $25.71^\circ$. The distance estimation on BLE was implemented using a Gaussian Process Regression algorithm, yielding an MAE of $0.174$m.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01767
Type Ia supernova progenitors: a contemporary view of a long-standing puzzle,['Solar and Stellar Astrophysics'],"['Ashley J. Ruiter', 'Ivo R. Seitenzahl']","Type Ia supernovae (SNe Ia) are runaway thermonuclear explosions in white dwarfs that result in the disruption of the white dwarf star, and possibly its nearby stellar companion. SNe Ia occur over an immense range of stellar population age and host galaxy environments, and play a critical role in the nucleosynthesis of intermediate-mass and iron-group elements, primarily the production of nickel, iron, cobalt, chromium, and manganese. Though the nature of their progenitors is still not well-understood, SNe Ia are unique among stellar explosions in that the majority of them exhibit a systematic lightcurve relation: more luminous supernovae dim more slowly over time than less luminous supernovae in optical light (intrinsically brighter SNe Ia have broader lightcurves). This feature, unique to SNe Ia, is rather remarkable and allows their peak luminosities to be determined with fairly high accuracy out to cosmological distances via measurement of their lightcurve decline. Further, studying SNe Ia gives us important insights into binary star evolution physics, since it is widely agreed that the progenitors of SNe Ia are binary (possibly multiple) star systems. In this review, we give a current update on the different proposed Type Ia supernova progenitors, including descriptions of possible binary star configurations, and their explosion mechanisms, from a theoretical perspective. We additionally give a brief overview of the historical (focusing on the more recent) observational work that has helped the astronomical community to understand the nature of the most important distance indicators in cosmology.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01766
Planning and Reasoning with 3D Deformable Objects for Hierarchical Text-to-3D Robotic Shaping,['Robotics'],"['Alison Bartsch', 'Amir Barati Farimani']","Deformable object manipulation remains a key challenge in developing autonomous robotic systems that can be successfully deployed in real-world scenarios. In this work, we explore the challenges of deformable object manipulation through the task of sculpting clay into 3D shapes. We propose the first coarse-to-fine autonomous sculpting system in which the sculpting agent first selects how many and where to place discrete chunks of clay into the workspace to create a coarse shape, and then iteratively refines the shape with sequences of deformation actions. We leverage large language models for sub-goal generation, and train a point cloud region-based action model to predict robot actions from the desired point cloud sub-goals. Additionally, our method is the first autonomous sculpting system that is a real-world text-to-3D shaping pipeline without any explicit 3D goals or sub-goals provided to the system. We demonstrate our method is able to successfully create a set of simple shapes solely from text-based prompting. Furthermore, we explore rigorously how to best quantify success for the text-to-3D sculpting task, and compare existing text-image and text-point cloud similarity metrics to human evaluations for this task. For experimental videos, human evaluation details, and full prompts, please see our project website: https://sites.google.com/andrew.cmu.edu/hierarchicalsculpting△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01765
Capacity Constraints in Principal-Agent Problems,['Theoretical Economics'],['Aubrey Clark'],Adding a capacity constraint to a hidden-action principal-agent problem results in the same set of Pareto optimal contracts as the unconstrained problem where output is scaled down by a constant factor. This scaling factor is increasing in the agent's capacity to exert effort.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01760
Structure-Guided Input Graph for GNNs facing Heterophily,['Machine Learning'],"['Victor M. Tenorio', 'Madeline Navarro', 'Samuel Rey', 'Santiago Segarra', 'Antonio G. Marques']","Graph Neural Networks (GNNs) have emerged as a promising tool to handle data exhibiting an irregular structure. However, most GNN architectures perform well on homophilic datasets, where the labels of neighboring nodes are likely to be the same. In recent years, an increasing body of work has been devoted to the development of GNN architectures for heterophilic datasets, where labels do not exhibit this low-pass behavior. In this work, we create a new graph in which nodes are connected if they share structural characteristics, meaning a higher chance of sharing their labels, and then use this new graph in the GNN architecture. To do this, we compute the k-nearest neighbors graph according to distances between structural features, which are either (i) role-based, such as degree, or (ii) global, such as centrality measures. Experiments show that the labels are smoother in this newly defined graph and that the performance of GNN architectures improves when using this alternative structure.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01757
Adversarial Sample-Based Approach for Tighter Privacy Auditing in Final Model-Only Scenarios,['Cryptography and Security'],"['Sangyeon Yoon', 'Wonje Jeung', 'Albert No']","Auditing Differentially Private Stochastic Gradient Descent (DP-SGD) in the final model setting is challenging and often results in empirical lower bounds that are significantly looser than theoretical privacy guarantees. We introduce a novel auditing method that achieves tighter empirical lower bounds without additional assumptions by crafting worst-case adversarial samples through loss-based input-space auditing. Our approach surpasses traditional canary-based heuristics and is effective in both white-box and black-box scenarios. Specifically, with a theoretical privacy budget of $\varepsilon = 10.0$, our method achieves empirical lower bounds of $6.68$ in white-box settings and $4.51$ in black-box settings, compared to the baseline of $4.11$ for MNIST. Moreover, we demonstrate that significant privacy auditing results can be achieved using in-distribution (ID) samples as canaries, obtaining an empirical lower bound of $4.33$ where traditional methods produce near-zero leakage detection. Our work offers a practical framework for reliable and accurate privacy auditing in differentially private machine learning.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01756
Human-Machine Interfaces for Subsea Telerobotics: From Soda-straw to Natural Language Interactions,['Robotics'],"['Adnan Abdullah', 'Ruo Chen', 'David Blow', 'Thanakon Uthai', 'Eric Jing Du', 'Md Jahidul Islam']","This review explores the evolution of human-machine interfaces (HMIs) for subsea telerobotics, tracing back the transition from traditional first-person ""soda-straw"" consoles (narrow field-of-view camera feed) to advanced interfaces powered by gesture recognition, virtual reality, and natural language models. First, we discuss various forms of subsea telerobotics applications, current state-of-the-art (SOTA) interface systems, and the challenges they face in robust underwater sensing, real-time estimation, and low-latency communication. Through this analysis, we highlight how advanced HMIs facilitate intuitive interactions between human operators and robots to overcome these challenges. A detailed review then categorizes and evaluates the cutting-edge HMI systems based on their offered features from both human perspectives (e.g., enhancing operator control and situational awareness) and machine perspectives (e.g., improving safety, mission accuracy, and task efficiency). Moreover, we examine the literature on bidirectional interaction and intelligent collaboration in terms of sensory feedback and intuitive control mechanisms for both physical and virtual interfaces. The paper concludes by identifying critical challenges, open research questions, and future directions, emphasizing the need for multidisciplinary collaboration in subsea telerobotics.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01753
Estimation of the Plant Controller Communication Time-Delay Considering PMSG-Based Wind Turbines,['Systems and Control'],"['Pablo Marchi', 'Pablo Gill Estevez', 'Alejandro Otero', 'Cecilia Galarza']","The communication control delay between the inverters and the power plant controller can be caused by several factors related to the communication link between them. Under undesirable conditions, high delay values can produce oscillations in the wind power plant that can affect the rest of the power system. In this work, we present a new robust methodology for wind turbines to estimate the value of the communication control delay using PMU data. Several scenarios are considered where external faults are simulated and the performance of the algorithm is evaluated based on dynamic state estimation of the mathematical model of the wind turbine. In this paper, we have shown that the characterization of the delay can be performed offering the transmission system operator an online tool to identify the most suited communication delay for the plant controller models used in dynamic studies.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01751
Probing higher moments of pion parton distribution functions,['High Energy Physics - Lattice'],"['Anthony Francis', 'Patrick Fritzsch', 'Rohith Karur', 'Jangho Kim', 'Giovanni Pederiva', 'Dimitra A. Pefkou', 'Antonio Rago', 'Andrea Shindler', 'André Walker-Loud', 'Savvas Zafeiropoulos']","We present the first numerical investigation of the method proposed in Ref. [1] to utilize gradient flow to obtain precise determinations of higher moments of PDFs from lattice QCD, circumventing power divergent mixing with lower dimensional operators. We apply this method to obtain moments of the isovector PDF of the pion using four Stabilized Wilson Fermion ensembles with $m_π\simeq 411$ MeV and lattice spacings $a \simeq 0.064, 0.077, 0.094$, and $0.12$ fm. We present preliminary results of ratios of three-point functions as a function of flow time, which can be used to extract the ratios $\left\langle x^2 \right\rangle/\left\langle x \right\rangle$ and $\left\langle x^3 \right\rangle/\left\langle x \right\rangle$. We find that a significantly higher precision can be achieved with this method compared to the canonical approach, which requires boosting and cannot reach higher than the $\left\langle x^3 \right\rangle$ moment.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01750
Ab initio study of Proximity-Induced Superconductivity in PbTe/Pb heterostructures,['Superconductivity'],"['R. Reho', 'A. R. Botello-Méndez', 'Zeila Zanolli']","Semiconductor-superconductor hybrid devices have been proposed as promising platforms for detecting and analyzing Majorana zero modes, which find applications in topological quantum computing. In this work, we solve the Kohn-Sham Density Functional Theory and Bogoliubov-de Gennes equations to describe the normal and superconducting properties of a PbTe/Pb heterostructure. We resolve a proximity-induced superconducting gap on the PbTe side. The hybridization between PbTe and Pb causes the emergence of a soft Bardeen-Cooper-Schrieffer-like superconducting gap. We compute the anomalous charge density in real space, estimating its decay length and showing that the pairing potential is anisotropic, which is a necessary condition for unconventional superconductivity. Contrary to the models that predict Majorana zero modes in these interfaces, we find a significantly large Schottky barrier in the normal state preventing the emergence of zero modes. Our findings strengthen the understanding of the physics governing PbTe/Pb hybrid devices and their viability for Majorana zero modes applications.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01749
CBOL-Tuner: Classifier-pruned Bayesian optimization to explore temporally structured latent spaces for particle accelerator tuning,['Machine Learning'],"['Mahindra Rautela', 'Alan Williams', 'Alexander Scheinker']","Complex dynamical systems, such as particle accelerators, often require complicated and time-consuming tuning procedures for optimal performance. It may also be required that these procedures estimate the optimal system parameters, which govern the dynamics of a spatiotemporal beam -- this can be a high-dimensional optimization problem. To address this, we propose a Classifier-pruned Bayesian Optimization-based Latent space Tuner (CBOL-Tuner), a framework for efficient exploration within a temporally-structured latent space. The CBOL-Tuner integrates a convolutional variational autoencoder (CVAE) for latent space representation, a long short-term memory (LSTM) network for temporal dynamics, a dense neural network (DNN) for parameter estimation, and a classifier-pruned Bayesian optimizer (C-BO) to adaptively search and filter the latent space for optimal solutions. CBOL-Tuner demonstrates superior performance in identifying multiple optimal settings and outperforms alternative global optimization methods.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01748
Subconvex bound for Rankin-Selberg $L$-functions in prime power level,['Number Theory'],['Aritra Ghosh'],"Let $f$ be a $p$-primitive cusp form of level $p^{4r}$, where local representation of $f$ be supercuspidal at $p$, $p$ being an odd prime, $r\geq 1$ and $g$ be a Hecke-Maass or holomorphic primitive cusp form for $\mathrm{SL}(2,\mathbb{Z})$. A subconvex bound for the central values of the Rankin-Selberg $L$-functions $L(s, f \otimes g )$ is give by $$L(\frac{1}{2}, f \otimes g ) \ll_{f,g,ε}p^{\frac{23r}{12} +ε}.$$△ Less",v1,https://arxiv.org/pdf/2412.01739
Efficiency of parallel computations of gravitational forces by TreeCode method in N-body models,['Computational Physics'],"['Nikolay M. Kuzmin', 'Danila S. Sirotin', 'Alexander V. Khoperskov']","Modeling of collisionless galactic systems is based on the N-body model, which requires large computational resources due to the long-range nature of gravitational forces. The most common method for calculating gravity is the TreeCode algorithm, which provides a faster calculation of the force compared to the direct summation of contributions from all particles for N-body simulation. An analysis of the computational efficiency is performed for models with the number of particles up to $10^{8}$. We considered several processors with different architectures in order to determine the performance of parallel simulations based on the OpenMP standard. An analysis of the use of extra threads in addition to physical cores shows an increase in simulation performance only when all logical threads are loaded, which doubles the total number of threads. This gives an increase in the efficiency of parallel computing by 20 percent on average.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01733
Quasi-optimal sampling from Gibbs states via non-commutative optimal transport metrics,['Quantum Physics'],"['Ángela Capel', 'Paul Gondolf', 'Jan Kochanowski', 'Cambyse Rouzé']","We study the problem of sampling from and preparing quantum Gibbs states of local commuting Hamiltonians on hypercubic lattices of arbitrary dimension. We prove that any such Gibbs state which satisfies a clustering condition that we coin decay of matrix-valued quantum conditional mutual information (MCMI) can be quasi-optimally prepared on a quantum computer. We do this by controlling the mixing time of the corresponding Davies evolution in a normalized quantum Wasserstein distance of order one. To the best of our knowledge, this is the first time that such a non-commutative transport metric has been used in the study of quantum dynamics, and the first time quasi-rapid mixing is implied by solely an explicit clustering condition. Our result is based on a weak approximate tensorization and a weak modified logarithmic Sobolev inequality for such systems, as well as a new general weak transport cost inequality. If we furthermore assume a constraint on the local gap of the thermalizing dynamics, we obtain rapid mixing in trace distance for interactions beyond the range of two, thereby extending the state-of-the-art results that only cover the nearest neighbor case. We conclude by showing that systems that admit effective local Hamiltonians, like quantum CSS codes at high temperature, satisfy this MCMI decay and can thus be efficiently prepared and sampled from.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01732
A slot-based energy storage decision-making approach for optimal Off-Grid telecommunication operator,['Optimization and Control'],"['Youssef Ait El Mahjoub', 'Jean-Michel Fourneau']","This paper proposes a slot-based energy storage approach for decision-making in the context of an Off-Grid telecommunication operator. We consider network systems powered by solar panels, where harvest energy is stored in a battery that can also be sold when fully charged. To reflect real-world conditions, we account for non-stationary energy arrivals and service demands that depend on the time of day, as well as the failure states of PV panel. The network operator we model faces two conflicting objectives: maintaining the operation of its infrastructure and selling (or supplying to other networks) surplus energy from fully charged batteries. To address these challenges, we developed a slot-based Markov Decision Process (MDP) model that incorporates positive rewards for energy sales, as well as penalties for energy loss and battery depletion. This slot-based MDP follows a specific structure we have previously proven to be efficient in terms of computational performance and accuracy. From this model, we derive the optimal policy that balances these conflicting objectives and maximizes the average reward function. Additionally, we present results comparing different cities and months, which the operator can consider when deploying its infrastructure to maximize rewards based on location-specific energy availability and seasonal variations.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01731
Unraveling the Dusty Environment Around RT Vir,['Solar and Stellar Astrophysics'],"['Michael D. Preston', 'Angela K. Speck', 'Beth Sargent', 'Sean Dillon']","Infrared studies of asymptotic giant branch (AGB) stars are critical to our understanding of the formation of cosmic dust. In this investigation, we explore the mid-to-far-infrared emission of oxygen rich AGB star RT Virginis. This optically thin dusty environment has unusual spectral features when compared to other stars in its class. To explore this enigmatic object we use the 1-D radiative transfer modeling code DUSTY. Modeled spectra are compared with observations from the Infrared Space Observatory (ISO), InfraRed Astronomical Satellite (IRAS), the Herschel Space Observatory and a host of other sources to determine the properties of RT Vir's circumstellar material. Our models suggest a set of two distant and cool dust shells at low optical depths (tauV,inner = 0.16, tauV,outer = 0.06), with inner dust temperatures: T1 = 330K, T3 = 94K. Overall, these dust shells exhibit a chemical composition consistent with dust typically found around O-rich AGB stars. However, the distribution of materials differs significantly. The inner shell consists of a mixture of silicates, Al2O3, FeO, and Fe, while the outer shell primarily contains crystalline Al2O3 polymorphs. This chemical change is indicative of two distinct epochs of dust formation around RT Vir. These changes in dust composition are driven by either changes in the pressure-temperature conditions around the star, or by a decrease in the C/O ratio due to hot-bottom burning.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01726
Attacks on multimodal models,['Computer Vision and Pattern Recognition'],"['Viacheslav Iablochnikov', 'Alexander Rogachev']","Today, models capable of working with various modalities simultaneously in a chat format are gaining increasing popularity. Despite this, there is an issue of potential attacks on these models, especially considering that many of them include open-source components. It is important to study whether the vulnerabilities of these components are inherited and how dangerous this can be when using such models in the industry. This work is dedicated to researching various types of attacks on such models and evaluating their generalization capabilities. Modern VLM models (LLaVA, BLIP, etc.) often use pre-trained parts from other models, so the main part of this research focuses on them, specifically on the CLIP architecture and its image encoder (CLIP-ViT) and various patch attack variations for it.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01725
Enhanced production of 60Fe in massive stars,['Nuclear Experiment'],"['A. Spyrou', 'D. Richman', 'A. Couture', 'C. E. Fields', 'S. N. Liddick', 'K. Childers', 'B. P. Crider', 'P. A. DeYoung', 'A. C. Dombos', 'P. Gastis', 'M. Guttormsen', 'K. Hermansen', 'A. C. Larsen', 'R. Lewis', 'S. Lyons', 'J. E. Midtbø', 'S. Mosby', 'D. Muecher', 'F. Naqvi', 'A. Palmisano-Kyle', 'G. Perdikakis', 'C. Prokop', 'H. Schatz', 'M. K. Smith', 'C. Sumithrarachchi']","Massive stars are a major source of chemical elements in the cosmos, ejecting freshly produced nuclei through winds and core-collapse supernova explosions into the interstellar medium. Among the material ejected, long lived radioisotopes, such as 60Fe (iron) and 26Al (aluminum), offer unique signs of active nucleosynthesis in our galaxy. There is a long-standing discrepancy between the observed 60Fe/26Al ratio by γ-ray telescopes and predictions from supernova models. This discrepancy has been attributed to uncertainties in the nuclear reaction networks producing 60Fe, and one reaction in particular, the neutron-capture on 59Fe. Here we present experimental results that provide a strong constraint on this reaction. We use these results to show that the production of 60Fe in massive stars is higher than previously thought, further increasing the discrepancy between observed and predicted 60Fe/26Al ratios. The persisting discrepancy can therefore not be attributed to nuclear uncertainties, and points to issues in massive-star models.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01723
Correspondence and Inverse Correspondence for Input/Output Logic and Region-Based Theories of Space,['Logic in Computer Science'],"['Andrea De Domenico', 'Ali Farjami', 'Krishna Manoorkar', 'Alessandra Palmigiano', 'Mattia Panettiere', 'Xiaolong Wang']","We further develop the algebraic approach to input/output logic initiated in \cite{wollic22}, where subordination algebras and a family of their generalizations were proposed as a semantic environment of various input/output logics. In particular: we extend the modal characterizations of a finite number of well known conditions on normative and permission systems, as well as on subordination, precontact, and dual precontact algebras developed in \cite{de2024obligations}, to those corresponding to the infinite class of {\em clopen-analytic inequalities} in a modal language consisting both of positive and of negative unary modal operators; we characterize the syntactic shape of first-order conditions on algebras endowed with subordination, precontact, and dual precontact relations which guarantees these conditions to be the first-order correspondents of axioms in the modal language above; we introduce algorithms for computing the first-order correspondents of modal axioms on algebras endowed with subordination, precontact, and dual precontact relations, and conversely, for computing the modal axioms of which the conditions satisfying the suitable syntactic shape are the first-order correspondents; finally, we extend Celani's dual characterization results between subordination lattices and subordination spaces to a wider environment which also encompasses precontact and dual precontact relations, and relative to an infinite class of first order conditions relating subordination, precontact and dual precontact relations on distributive lattices. The modal characterizations established in the present paper pave the way to establishing faithful embeddings for infinite classes of input/output logics, and hence to their implementation in LogiKEy, Isabelle/HOL, Lean, or other interactive systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01722
"HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving",['Computer Vision and Pattern Recognition'],"['Hongyu Zhou', 'Longzhong Lin', 'Jiabao Wang', 'Yichong Lu', 'Dongfeng Bai', 'Bingbing Liu', 'Yue Wang', 'Andreas Geiger', 'Yiyi Liao']","In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, We tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01718
Extreme-ultraviolet spatiotemporal vortices via high harmonic generation,['Optics'],"['Rodrigo Martin-Hernandez', 'Guan Gui', 'Luis Plaja', 'Henry K. Kapteyn', 'Margaret M. Murnane', 'Chen-Ting Liao', 'Miguel A. Porras', 'Carlos Hernandez-Garcia']","Spatiotemporal optical vortices (STOV) are space-time structured light pulses with a unique topology that couples spatial and temporal domains and carry transverse orbital angular momentum (OAM). Up to now, their generation has been limited to the visible and infrared regions of the spectrum. During the last decade, it was shown that through the process of high-order harmonic generation (HHG) it is possible to up-convert spatial optical vortices that carry longitudinal OAM from the near-infrared into the extreme-ultraviolet (EUV), thereby producing vortices with distinct femtosecond and attosecond structure. In this work we demonstrate theoretically and experimentally the generation of EUV spatiotemporal and spatiospectral vortices using near infrared STOV driving laser pulses. We use analytical expressions for focused STOVs to perform macroscopic calculations of HHG that are directly compared to the experimental results. As STOV beams are not eigenmodes of propagation, we characterize the highly-charged EUV STOVs both in the near and far fields, to show that they represent conjugated spatiotemporal and spatiospectral vortex pairs. Our work provides high-frequency light beams topologically coupled at the nanometer/attosecond scales domains with transverse OAM, that could be suitable to explore electronic dynamics in magnetic materials, chiral media, and nanostructures.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01716
"Joint Phase Time Array: Opportunities, Challenges and System Design Considerations",['Signal Processing'],"['Young-Han Nam', 'Ahmad AlAmmouri', 'Jianhua Mo', 'Jianzhong Chalrie Zhang']","This paper presents a novel approach to designing millimeter-wave (mmWave) cellular communication systems, based on joint phase time array (JPTA) radio frequency (RF) frontend architecture. JPTA architecture comprises time-delay components appended to conventional phase shifters, which offer extra degrees of freedom to be exploited for designing frequency-selective analog beams. Hence, a mmWave device equipped with JPTA can receive and transmit signals in multiple directions in a single time slot per RF chain, one direction per frequency subband, which alleviates the traditional constraint of one analog beam per transceiver chain per time slot. The utilization of subband-specific analog beams offers a new opportunity in designing mmWave systems, allowing for enhanced cell capacity and reduced pilot overhead. To understand the practical feasibility of JPTA, a few challenges and system design considerations are discussed in relation to the performance and complexity of the JPTA systems. For example, frequency-selective beam gain losses are present for the subband analog beams, e.g., up to 1 dB losses for 2 subband cases, even with the state-of-the-art JPTA delay and phase optimization methods. Despite these side effects, system-level analysis reveals that the JPTA system is capable of improving cell capacity: the 5%-tile throughput by up to 65%.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01714
Bisequent Calculi for Neutral Free Logic with Definite Descriptions,['Logic in Computer Science'],"['Andrzej Indrzejczak', 'Yaroslav Petrukhin']","We present a bisequent calculus (BSC) for the minimal theory of definite descriptions (DD) in the setting of neutral free logic, where formulae with non-denoting terms have no truth value. The treatment of quantifiers, atomic formulae and simple terms is based on the approach developed by Pavlović and Gratzl. We extend their results to the version with identity and definite descriptions. In particular, the admissibility of cut is proven for this extended system.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01712
Associated production of $J/ψ$ and direct photon in the NRQCD and the ICEM using the high-energy factorization,['High Energy Physics - Phenomenology'],"['Lev Alimov', 'Anton Karpishkov', 'Vladimir Saleev']","We study the associated $J/ψ$ and direct photon production in the high-energy factorization, as it is formulated in the parton Reggeization approach, using two different models for the hadronization of a heavy quark-antiquark pair into a heavy quarkonium, namely the non-relativistic quantum chromodynamics (NRQCD) and the improved color evaporation model (ICEM). We find essential differences in the predictions for cross-section and transverse momenta spectra obtained using the NRQCD and the ICEM, which can be used to discriminate between these models. Our prediction for cross-sections of the associated $J/ψ$ and direct photon production at the LHC energies slightly overestimates the results obtained early in the next-to-leading order (NLO) calculation in the collinear parton model (CPM). We predict different two-particle correlation spectra in the associated $J/ψ$ and direct photon production which may be of interest for experimental study.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01710
Uncertainty-Aware Regularization for Image-to-Image Translation,['Computer Vision and Pattern Recognition'],"['Anuja Vats', 'Ivar Farup', 'Marius Pedersen', 'Kiran Raja']","The importance of quantifying uncertainty in deep networks has become paramount for reliable real-world applications. In this paper, we propose a method to improve uncertainty estimation in medical Image-to-Image (I2I) translation. Our model integrates aleatoric uncertainty and employs Uncertainty-Aware Regularization (UAR) inspired by simple priors to refine uncertainty estimates and enhance reconstruction quality. We show that by leveraging simple priors on parameters, our approach captures more robust uncertainty maps, effectively refining them to indicate precisely where the network encounters difficulties, while being less affected by noise. Our experiments demonstrate that UAR not only improves translation performance, but also provides better uncertainty estimations, particularly in the presence of noise and artifacts. We validate our approach using two medical imaging datasets, showcasing its effectiveness in maintaining high confidence in familiar regions while accurately identifying areas of uncertainty in novel/ambiguous scenarios.△ Less","24 November, 2024;",https://arxiv.org/pdf/2412.01705
Uniform Cut-free Bisequent Calculi for Three-valued Logics,['Logic in Computer Science'],"['Andrzej Indrzejczak', 'Yaroslav Petrukhin']","We present a uniform characterisation of three-valued logics by means of the bisequent calculus (BSC). It is a generalised form of a sequent calculus (SC) where rules operate on the ordered pairs of ordinary sequents. BSC may be treated as the weakest kind of system in the rich family of generalised SC operating on items being some collections of ordinary sequents, like hypersequent and nested sequent calculi. It seems that for many non-classical logics, including some many-valued, paraconsistent and modal logics, the reasonably modest generalisation of standard SC offered by BSC is sufficient. In this paper we examine a variety of three-valued logics and show how they can be formalised in the framework of BSC. We present a constructive syntactic proof provided that these systems are cut-free, satisfy the subformula property, and allow one to prove the interpolation theorem in many cases.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01700
Uncovering the Effects of Array Mutual Coupling in 21-cm Experiments with the SKA-Low Radio Telescope,['Cosmology and Nongalactic Astrophysics'],"[""Oscar S. D. O'Hara"", 'Quentin Gueuning', 'Eloy de Lera Acedo', 'Fred Dulwich', 'John Cumner', 'Dominic Anstey', 'Anthony Brown', 'Anastasia Fialkov', 'Jiten Dhandha', 'Andrew Faulkner', 'Yuchen Liu']","We investigate the impact of Mutual Coupling (MC) between antennas on the time-delay power spectrum response of the core of the SKA-Low radio telescope. Using two in-house tools - Fast Array Simulation Tool (FAST) (a fast full-wave electromagnetic solver) and OSKAR (a GPU-accelerated radio telescope simulator) - we simulate station beams and compute visibilities for various array layouts (regular, sunflower, and random). Simulations are conducted in an Epoch of Reionisation subband between 120-150~MHz, with a fine frequency resolution of 100~kHz, enabling the investigation of late delays. Our results show that MC effects significantly increase foreground leakage into longer delays, especially for regular station layouts. For 21-cm science, foreground spill-over into the 21-cm window extends beyond $k_{\parallel} \sim 2$~h$^{-1}$Mpc for all station layouts and across all $k_{\perp}$ modes, completely obscuring the detection window. We find that attempting to remove the foreground contribution from the visibilities using an approximated beam model, based on the average embedded element pattern or interpolating the embedded element patterns from a coarse channel rate of 781~kHz, results in residuals around 1% ($\sim 10^{11}~\mathrm{mK}^2$h$^{-3}\mathrm{Mpc}^3$) which is still around 7 orders of magnitude brighter than the expected level of the EoR signal ($\sim 10^{4}~\mathrm{mK}^2$h$^{-3}\mathrm{Mpc}^3$). We also find that station beam models with at least 4-5 significant digits in the far-field pattern and high spectral resolution are needed for effective foreground removal. Our research provides critical insights into the role of MC in SKA-Low experiments and highlights the computational challenges of fully integrating array patterns that account for MC effects into processing pipelines.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01699
UV-processing of icy pebbles in the outer parts of VSI-turbulent disks,['Earth and Planetary Astrophysics'],"['Lizxandra Flores-Rivera', 'Michiel Lambrechts', 'Sacha Gavino', 'Sebastian Lorek', 'Mario Flock', 'Anders Johansen', 'Andrea Mignone']","Icy dust particles emerge in star-forming clouds and are subsequently incorporated in protoplanetary disks, where they coagulate into larger pebbles up to mm in size. In the disk midplane, ices are shielded from UV radiation, but moderate levels of disk turbulence can lift small particles to the disk surface, where they can be altered, or destroyed. Nevertheless, studies of comets and meteorites generally find that ices at least partly retained their interstellar medium (ISM) composition before being accreted onto these minor bodies. Here we model this process through hydrodynamical simulations with VSI-driven turbulence in the outer protoplanetary disk. We use the PLUTO code in a 2.5 D global accretion setup and include Lagrangian dust particles of 0.1 and 1 mm sizes. In a post-processing step, we use the RADMC3D code to generate the local UV radiation field to assess the level of ice processing of pebbles. We find that a small fraction ($\sim$17$\%$) of 100 $μ$m size particles are frequently lifted up to $Z/R=0.2$ which can result in the loss of their pristine composition as their residence time in this layer allows for effective CO and water photodissociation. The larger 1 mm size particles remain UV-shielded in the disk midplane throughout the dynamical evolution of the disk. Our results indicate that the assembly of icy bodies via the accretion of drifting mm-size icy pebbles can explain the presence of pristine ice from the ISM, even in VSI-turbulent disks. Nevertheless, particles $\leq$ 100 $μ$m experience efficient UV processing and may mix with unaltered icy pebbles, resulting in a less ISM-like composition in the midplane.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01698
An alternative GPU acceleration for a pseudopotential plane-waves density functional theory code with applications to metallic systems,['Materials Science'],"['Xuejun Gong', 'Andrea Dal Corso']","We present an alternative GPU acceleration for plane waves pseudopotentials electronic structure codes designed for systems that have small unit cells but require a large number of k points to sample the Brillouin zone as happens, for instance, in metals. We discuss the diagonalization of the Kohn and Sham equations and the solution of the linear system derived in density functional perturbation theory. Both problems take advantage from a rewriting of the routine that applies the Hamiltonian to the Bloch wave-functions to work simultaneously (in parallel on the GPU threads) on the wave-functions with different wave-vectors k, as many as allowed by the GPU memory. Our implementation is written in CUDA Fortran and makes extensive use of kernel routines that run on the GPU (GLOBAL routines) or can be called from inside the GPU threads (DEVICE routines). We compare our method with the CPUs only calculation and with the approach currently implemented in Quantum ESPRESSO that uses GPU accelerated libraries for the FFT and for the linear algebra tasks such as the matrix-matrix multiplications as well as OpenACC directives for loop parallelization. We show in a realistic example that our method can give a significant improvement in the cases for which it has been designed.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01695
Beyond the Long Wavelength Approximation: Next-generation Gravitational-Wave Detectors and Frequency-dependent Antenna Patterns,['General Relativity and Quantum Cosmology'],"['Andrea Virtuoso', 'Edoardo Milotti']","The response of a gravitational-wave (GW) interferometer is spatially modulated and is described by two antenna patterns, one for each polarization state of the waves. The antenna patterns are derived from the shape and size of the interferometer, usually under the assumption that the interferometer size is much smaller than the wavelength of the gravitational waves (long wavelength approximation, LWA). This assumption is well justified as long as the frequency of the gravitational waves is well below the free spectral range (FSR) of the Fabry-Perot cavities in the interferometer arms as it happens for current interferometers ($\mathrm{FSR}=37.5$~kHz for the LIGO interferometers and $\mathrm{FSR}=50$~kHz for Virgo and KAGRA). However, the LWA can no longer be taken for granted with third--generation instruments (Einstein Telescope, Cosmic Explorer and LISA) because of their longer arms. This has been known for some time, and previous analyses have mostly been carried out in the frequency domain. In this paper, we explore the behavior of the frequency--dependent antenna patterns in the time domain and in the time--frequency domain, with specific reference to the searches of short GW transients. We analyze the profound changes in the concept of Dominant Polarization Frame, which must be generalized in a nontrivial way, we show that the conventional likelihood-based analysis of coherence in different interferometers can no longer be applied as in current analysis pipelines, and that methods based on the null stream in triangular (60°) interferometers no longer work. Overall, this paper establishes methods and tools that can be used to overcome these difficulties in the unmodeled analysis of short GW transients.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01693
Can We Afford The Perfect Prompt? Balancing Cost and Accuracy with the Economical Prompting Index,['Computation and Language'],"['Tyler McDonald', 'Anthony Colosimo', 'Yifeng Li', 'Ali Emami']","As prompt engineering research rapidly evolves, evaluations beyond accuracy are crucial for developing cost-effective techniques. We present the Economical Prompting Index (EPI), a novel metric that combines accuracy scores with token consumption, adjusted by a user-specified cost concern level to reflect different resource constraints. Our study examines 6 advanced prompting techniques, including Chain-of-Thought, Self-Consistency, and Tree of Thoughts, across 10 widely-used language models and 4 diverse datasets. We demonstrate that approaches such as Self-Consistency often provide statistically insignificant gains while becoming cost-prohibitive. For example, on high-performing models like Claude 3.5 Sonnet, the EPI of simpler techniques like Chain-of-Thought (0.72) surpasses more complex methods like Self-Consistency (0.64) at slight cost concern levels. Our findings suggest a reevaluation of complex prompting strategies in resource-constrained scenarios, potentially reshaping future research priorities and improving cost-effectiveness for end-users.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01690
"Influence of light, temperature and iron oxidation state on the dissolution rate of combusted iron particles in oxalic acid",['Chemical Physics'],"['M. Lausch', 'Y. Ruan', 'P. Brockmann', 'A. Zimina', 'B. J. M. Etzold', 'J. Hussong']","In this study, the influence of temperature $(40-80 ^\circ\mathrm{C})$ and light exposure on the dissolution of combusted iron particles in aqueous oxalic acid $(0.45~\mathrm{mol/L})$ is experimentally investigated. Unlike previous studies, real combusted iron particles with varying fuel-to-air equivalence ratios were used instead of model oxides. In-situ video recordings reveal the evolution of particle size and morphology. Increasing temperature and short-wavelength light exposure enhance the reaction rate, with light-induced effects only becoming significant above $40 ^\circ$C for the duration of the experiments. This behavior differs significantly from hematite/maghemite oxides, attributed to the internal Fe phase structure of the combusted iron particles. At $80^\circ$C with additional light irradiation, a sudden decrease in reaction rate is observed due to solid ferrous oxide formation. While the fuel-to-air ratio induces differences in iron oxide phase composition, it does not affect the dissolution combusted iron particles significantly.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01689
A Well-Characterized Survey for Centaurs in Pan-STARRS1,['Earth and Planetary Astrophysics'],"['Jacob A. Kurlander', 'Matthew J. Holman', 'Pedro H. Bernardinelli', 'Mario Juric', 'Aren N. Heinze', 'Matthew J. Payne']","To prepare for the upcoming Legacy Survey of Space and Time, we develop methods for quantifying the selection function of a wide-field survey as a function of all six orbital parameters and absolute magnitude. We perform a HelioLinC3D search for Centaurs in the Pan-STARRS1 detection catalog and use a synthetic debiasing population to characterize our survey's selection function. We find nine new objects, including Centaur 2010 RJ$_{226}$, among 320 real objects, along with $\sim$70,000 debiasing objects. We use the debiasing population to fit a selection function and apply the selection function to a model Centaur population with literature orbital and size distributions. We confirm the model's marginal distributions but reject its joint distribution, and estimate an intrinsic population of 21,400$^{+3,400}_{-2,800}$ Centaurs with $H_r < 13.7$. The discovery of only nine new objects in archival data verifies that the Pan-STARRS discovery pipeline had high completeness, but also shows that new linking algorithms can contribute even to traditional single-tracklet surveys. As the first systematic application of HelioLinC3D to a survey with extensive sky coverage, this project proves the viability of HelioLinC3D as a discovery algorithm for big-data wide-field surveys.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01687
Direct linearisation of the non-commutative Kadomtsev-Petviashvili equations,['Exactly Solvable and Integrable Systems'],"['Gordon Blower', 'Simon J. A. Malham']","We prove that the non-commutative Kadomtsev-Petviashvili (KP) equation and a `lifted' modified Kadomtsev-Petviashvili (mKP) equation are directly linearisable, and thus integrable in this sense. There are several versions of the non-commutative mKP equations, including the two-dimensional generalisations of the non-commutative modified Korteweg-de Vries (mKdV) equation and its alternative form (amKdV). Herein we derive the `lifted' mKP equation, whose solutions are the natural two-dimensional extension of those for the non-commutative mKdV equation derived in Blower and Malham. We also present the log-potential form of the mKP equation, from which all of these non-commutative mKP equations can be derived. To achieve the integrability results, we construct the pre-Poppe algebra that underlies the KP and mKP equations. This is a non-commutative polynomial algebra over the real line generated by the solution (and its partial derivatives) to the linearised form of the KP and mKP equations. The algebra is endowed with a pre-Poppe product, based on the product rule for semi-additive operators pioneered by Poppe for the commutative KP equation. Integrability corresponds to establishing a particular polynomial expansion in the respective pre-Poppe algebra. We also present numerical simulations of soliton-like interactions for the non-commutative KP equation.△ Less",v1,https://arxiv.org/pdf/2412.01686
Fano resonance in XUV generated by helium with few-cycle intense laser pulses and its classical analogy,['Optics'],"['S. A. Bondarenko', 'V. V. Strelkov']","We integrate numerically the Schrödinger equation for the model helium atom irradiated by intense few-cycle laser pulse and find the emitted XUV spectra. They demonstrate resonant peaks at the frequencies of transitions from the doubly-excited autoionizing states (AISs) to the ground state. We study the properties of these peaks depending on the laser pulse duration and find that the decay of the AISs due to photoionization by the laser field affects them. Moreover, we consider the classical system of two coupled oscillators and find that both the quantum (the atom with AIS in the field) and the classical (the coupled oscillators with friction) systems demonstrate Fano-like resonant peak described by an essentially complex asymmetry parameter. We find a remarkable similarity in the behavior of these systems and conclude that the classical system of coupled oscillators with friction is an analogy of the AIS having an extra decay channel in addition to the autoionization one.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01685
Condensation dynamics in a two-dimensional photonic crystal waveguide,['Optics'],"['Maria Efthymiou-Tsironi', 'Antonio Gianfrate', 'Dimitrios Trypogeorgos', 'Charly Leblanc', 'Fabrizio Riminucci', 'Grazia Salerno', 'Milena De Giorgi', 'Dario Ballarini', 'Daniele Sanvitto']","Exciton-polariton condensation occurs at the extrema of the underlying dispersion where the density of states diverges and carriers can naturally accumulate. The existence of multiple such points leads to coupling and competition between the associated modes and dynamical redistribution of the carriers in the dispersion. Here, we directly engineer the above situation via subwavelength periodic patterning of a two-dimensional nanostructure. This leads to multimode condensation into a pair of symmetric condensates that form at high-momenta, accidental-coupling points, and a high-symmetry $Γ$-point with a bound-in-the-continuum (BiC) state. The dynamical behaviour of the system reveals the non-simultaneous appearance of these condensates and the interplay of non-trivial gain and relaxation mechanisms. We fully characterise the quasi-static and dynamical regime of this artificial crystal and the properties of the different condensates. This understanding is necessary when band-structure engineering techniques are used to achieve precise control of condensate formation with given energy and momentum.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01684
Thermoelectric transport of strained CsK$_2$Sb: Role of momentum-dependent scattering in low-dimensional Fermi surfaces,['Materials Science'],"['Øven A. Grimenes', 'G. Jeffrey Snyder', 'Ole M. Løvvik', 'Kristian Berland']","In this first-principles study, we investigated the thermoelectric properties of the full-Heusler compound CsK$_2$Sb at different compressive strains. We predict a p-type figure of merit ($zT$) of 2.6 at 800 K, in line with previous high $zT$ predictions. This high $zT$ arises from a low lattice thermal conductivity of 0.35 Wm$^{-1}$K$^{-1}$ and a valence band structure exhibiting significant effective mass anisotropy, forming tube-like energy isosurfaces below the band edge, a characteristic of two dimensional (2D) systems that leads to a large number of high mobility charge carriers. The extended energy isosurfaces also correspond to large electron-scattering space, but with realistic wave function- and momentum-dependent electron scattering, we found this effect to be largely compensated by reduced scattering rates for larger momentum ${\mathbf{q}}$. The electronic transport properties were further enhanced with compressive strain, increasing the power factor by up to 66 %, in part due to enhanced 2D-features of the valence band. Unfortunately, compressive strain also increase the lattice thermal conductivity, resulting in a maximum p-type $zT=2.7$ at 1 % compressive strain. In the conduction band, strain can align the $Γ$- and X-centered valleys, resulting in the optimal n-type $zT$ increasing from 0.9 to 2.3 when adding 2 % compressive strain. Thus, the material class can have potential for both good p- and n- type thermoelectricity.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01681
Coherent emission from single impurities in ZnSe through resonant excitation,['Quantum Physics'],"['Yuxi Jiang', 'Christine Falter', 'Robert M. Pettit', 'Nils von den Driesch', 'Yurii Kutovyi', 'Amirehsan Alizadeh Herfati', 'Alexander Pawlis', 'Edo Waks']","Impurity-bound excitons in II-VI semiconductors are promising optically active solid-state spin qubit systems. Previous work relied on incoherent optical excitation to generate photons from these impurities. However, many quantum applications require resonant driving to directly excite optical transitions and maintain coherence. Here, we demonstrate coherent optical emission from a resonantly driven single impurity-bound exciton in ZnSe. We observe resonance fluorescence and verify the emission coherence through polarization interferometry. Resonant excitation also enables the direct measurement of the Debye-Waller factor, determined to be 0.94, indicating high efficiency emission to the zero-phonon line. Time-resolved resonance fluorescence measurements reveal a fast optically driven ionization process attributed to Auger recombination, along with a slower spontaneous ionization process having a lifetime of 21 μs due to charge tunneling from the impurity. We demonstrate that a low-power incoherent pump laser efficiently stabilizes the charge of the impurity-bound exciton on the timescale of 9.3 ns. Our results pave the way for direct coherent optical and spin control through resonant excitation of impurity-bound excitons in II-VI semiconductors.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01677
Counting polarizations on abelian varieties with group action,['Algebraic Geometry'],"['Robert Auffarth', 'Angel Carocca', 'Rubí E. Rodríguez']","Let $\mathcal{A}_g$ be the moduli space of principally polarized abelian varieties. We study the problem of counting the number of principal polarizations modulo the natural action of the automorphism group of the abelian variety on a very general element of a positive dimensional component of $\mathrm{Sing}(\mathcal{A}_g)$, and show that this number is not always 1.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01676
Multimodal azimuthal oscillations in electron beam generated $\textbf{E} \times \textbf{B}$ plasma,['Plasma Physics'],"['Nirbhav Singh Chopra', 'Mina Papahn Zadeh', 'Mikhail Tyushev', 'Andrei Smolyakov', 'Alexandre Likhanskii', 'Yevgeny Raitses']","Electron beam (e-beam) generated plasmas with applied cross electric and magnetic $\left( \textbf{E} \times \textbf{B} \right)$ fields are promising for low-damage material processing. However, these plasmas can be subject to the formation of azimuthally propagating structures that enhance the radial transport of energetic charged species, which can harm the gentle processing capability of the plasma. In this work we investigate the azimuthal structure formation in an e-beam generated $\textbf{E} \times \textbf{B}$ plasma using experimental diagnostics and 2D3V particle-in-cell simulations. Our findings demonstrate the formation of multiple simultaneously occurring azimuthally propagating modes that exhibit a nontrivial radial dependence. It is suggested that the multimodal azimuthal spectrum is caused by the complex nature of the ion dynamics in the plasma.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01675
Causal Discovery by Interventions via Integer Programming,['Machine Learning'],"['Abdelmonem Elrefaey', 'Rong Pan']","Causal discovery is essential across various scientific fields to uncover causal structures within data. Traditional methods relying on observational data have limitations due to confounding variables. This paper presents an optimization-based approach using integer programming (IP) to design minimal intervention sets that ensure causal structure identifiability. Our method provides exact and modular solutions that can be adjusted to different experimental settings and constraints. We demonstrate its effectiveness through comparative analysis across different settings, demonstrating its applicability and robustness.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01674
Spatial SIR epidemic model with varying infectivity without movement of individuals: Law of Large Numbers,['Probability'],"['Armand Kanga', 'Etienne Pardoux']","In this work, we use a new approach to study the spread of an infectious disease. Indeed, we study a SIR epidemic model with variable infectivity, where the individuals are distributed over a compact subset $D$ of $\R^d$. We define empirical measures which describe the evolution of the state (susceptible, infectious, recovered) of the individuals in the various locations, and the total force of infection in the population. In our model, the individuals do not move. We establish a law of large numbers for these measures, as the population size tends to infinity.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01673
Gen-SIS: Generative Self-augmentation Improves Self-supervised Learning,['Computer Vision and Pattern Recognition'],"['Varun Belagali', 'Srikar Yellapragada', 'Alexandros Graikos', 'Saarthak Kapse', 'Zilinghan Li', 'Tarak Nath Nandi', 'Ravi K Madduri', 'Prateek Prasanna', 'Joel Saltz', 'Dimitris Samaras']","Self-supervised learning (SSL) methods have emerged as strong visual representation learners by training an image encoder to maximize similarity between features of different views of the same image. To perform this view-invariance task, current SSL algorithms rely on hand-crafted augmentations such as random cropping and color jittering to create multiple views of an image. Recently, generative diffusion models have been shown to improve SSL by providing a wider range of data augmentations. However, these diffusion models require pre-training on large-scale image-text datasets, which might not be available for many specialized domains like histopathology. In this work, we introduce Gen-SIS, a diffusion-based augmentation technique trained exclusively on unlabeled image data, eliminating any reliance on external sources of supervision such as text captions. We first train an initial SSL encoder on a dataset using only hand-crafted augmentations. We then train a diffusion model conditioned on embeddings from that SSL encoder. Following training, given an embedding of the source image, this diffusion model can synthesize its diverse views. We show that these `self-augmentations', i.e. generative augmentations based on the vanilla SSL encoder embeddings, facilitate the training of a stronger SSL encoder. Furthermore, based on the ability to interpolate between images in the encoder latent space, we introduce the novel pretext task of disentangling the two source images of an interpolated synthetic image. We validate Gen-SIS's effectiveness by demonstrating performance improvements across various downstream tasks in both natural images, which are generally object-centric, as well as digital histopathology images, which are typically context-based.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01672
Verified Foundations for Differential Privacy,['Cryptography and Security'],"['Markus de Medeiros', 'Muhammad Naveed', 'Tancrede Lepoint', 'Temesghen Kahsai', 'Tristan Ravitch', 'Stefan Zetzsche', 'Anjali Joshi', 'Joseph Tassarotti', 'Aws Albarghouthi', 'Jean-Baptiste Tristan']","Differential privacy (DP) has become the gold standard for privacy-preserving data analysis, but implementing it correctly has proven challenging. Prior work has focused on verifying DP at a high level, assuming the foundations are correct and a perfect source of randomness is available. However, the underlying theory of differential privacy can be very complex and subtle. Flaws in basic mechanisms and random number generation have been a critical source of vulnerabilities in real-world DP systems.
  In this paper, we present SampCert, the first comprehensive, mechanized foundation for differential privacy. SampCert is written in Lean with over 12,000 lines of proof. It offers a generic and extensible notion of DP, a framework for constructing and composing DP mechanisms, and formally verified implementations of Laplace and Gaussian sampling algorithms. SampCert provides (1) a mechanized foundation for developing the next generation of differentially private algorithms, and (2) mechanically verified primitives that can be deployed in production systems. Indeed, SampCert's verified algorithms power the DP offerings of Amazon Web Services (AWS), demonstrating its real-world impact.
  SampCert's key innovations include: (1) A generic DP foundation that can be instantiated for various DP definitions (e.g., pure, concentrated, Rényi DP); (2) formally verified discrete Laplace and Gaussian sampling algorithms that avoid the pitfalls of floating-point implementations; and (3) a simple probability monad and novel proof techniques that streamline the formalization. To enable proving complex correctness properties of DP and random number generation, SampCert makes heavy use of Lean's extensive Mathlib library, leveraging theorems in Fourier analysis, measure and probability theory, number theory, and topology.△ Less",v1,https://arxiv.org/pdf/2412.01671
Enhanced feature encoding and classification on distributed quantum hardware,['Quantum Physics'],"['Roberto Moretti', 'Andrea Giachero', 'Voica Radescu', 'Michele Grossi']","The steady progress of quantum hardware is motivating the search for novel quantum algorithm optimization strategies for near-term, real-world applications. In this study, we propose a novel feature map optimization strategy for Quantum Support Vector Machines (QSVMs), designed to enhance binary classification while taking into account backend-specific parameters, including qubit connectivity, native gate sets, and circuit depth, which are critical factors in noisy intermediate scale quantum (NISQ) devices. The dataset we utilised belongs to the neutrino physics domain, with applications in the search for neutrinoless double beta decay. A key contribution of this work is the parallelization of the classification task to commercially available superconducting quantum hardware to speed up the genetic search processes. The study was carried out by partitioning each quantum processing unit (QPU) into several sub-units with the same topology to implement individual QSVM instances. We conducted parallelization experiments with three IBM backends with more than 100 qubits, ranking the sub-units based on their susceptibility to noise. Data-driven simulations show how, under certain restrictions, parallelized genetic optimization can occur with the tested devices when retaining the top 20% ranked sub-units in the QPU.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01664
Cycle-Free Polytopal Mesh Sweeping for Boltzmann Transport,['Numerical Analysis'],"['Ansar Calloo', 'Matthew Evans', 'Henry Lockyer', 'François Madiot', 'Tristan Pryer', 'Luca Zanetti']","We introduce a novel property of bounded Voronoi tessellations that
  enables cycle-free mesh sweeping algorithms. We prove that a
  topological sort of the dual graph of any Voronoi tessellation is
  feasible in any flow direction and dimension, allowing
  straightforward application to discontinuous Galerkin (DG)
  discretisations of first-order hyperbolic partial differential
  equations and the Boltzmann Transport Equation (BTE) without
  requiring flux-cycle corrections.
  We also present an efficient algorithm to perform the topological
  sort on the dual mesh nodes, ensuring a valid sweep ordering. This
  result expands the applicability of DG methods for transport
  problems on polytopal meshes by providing a robust framework for
  scalable, parallelised solutions. To illustrate its effectiveness,
  we conduct a series of computational experiments showcasing a DG
  scheme for BTE, demonstrating both computational efficiency and
  adaptability to complex geometries.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01660
Moire magnetism in CrBr3 multilayers emerging from differential strain,['Materials Science'],"['Fengrui Yao', 'Dario Rossi', 'Ivo A. Gabrovski', 'Volodymyr Multian', 'Nelson Hua', 'Kenji Watanabe', 'Takashi Taniguchi', 'Marco Gibertini', 'Ignacio Gutierrez-Lezama', 'Louk Rademaker', 'Alberto F. Morpurgo']","Interfaces between twisted 2D materials host a wealth of physical phenomena originating from the long-scale periodicity associated with the resulting moire structure. Besides twisting, an alternative route to create structures with comparably long or even longer periodicities is inducing a differential strain between adjacent layers in a van der Waals (vdW) material. Despite recent theoretical efforts analyzing its benefits, this route has not yet been implemented experimentally. Here we report evidence for the simultaneous presence of ferromagnetic and antiferromagnetic regions in CrBr3 _a hallmark of moire magnetism_ from the observation of an unexpected magnetoconductance in CrBr3 tunnel barriers with ferromagnetic Fe3GeTe2 and graphene electrodes. The observed magnetoconductance evolves with temperature and magnetic field as the magnetoconductance measured in small angle CrBr3 twisted junctions, in which moire magnetism occurs. Consistent with Raman measurements and theoretical modeling, we attribute the phenomenon to the presence of a differential strain in the CrBr3 multilayer, which locally modifies the stacking and the interlayer exchange between adjacent CrBr3 layers, resulting in spatially modulated spin textures. Our conclusions indicate that inducing differential strain in vdW multilayers is a viable strategy to create moire-like superlattices, which in the future may offer in-situ continuous tunability even at low temperatures.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01658
PassionNet: An Innovative Framework for Duplicate and Conflicting Requirements Identification,['Software Engineering'],"['Summra Saleem', 'Muhammad Nabeel Asim', 'Andreas Dengel']","Early detection and resolution of duplicate and conflicting requirements can significantly enhance project efficiency and overall software quality. Researchers have developed various computational predictors by leveraging Artificial Intelligence (AI) potential to detect duplicate and conflicting requirements. However, these predictors lack in performance and requires more effective approaches to empower software development processes. Following the need of a unique predictor that can accurately identify duplicate and conflicting requirements, this research offers a comprehensive framework that facilitate development of 3 different types of predictive pipelines: language models based, multi-model similarity knowledge-driven and large language models (LLMs) context + multi-model similarity knowledge-driven. Within first type predictive pipelines landscape, framework facilitates conflicting/duplicate requirements identification by leveraging 8 distinct types of LLMs. In second type, framework supports development of predictive pipelines that leverage multi-scale and multi-model similarity knowledge, ranging from traditional similarity computation methods to advanced similarity vectors generated by LLMs. In the third type, the framework synthesizes predictive pipelines by integrating contextual insights from LLMs with multi-model similarity knowledge. Across 6 public benchmark datasets, extensive testing of 760 distinct predictive pipelines demonstrates that hybrid predictive pipelines consistently outperforms other two types predictive pipelines in accurately identifying duplicate and conflicting requirements. This predictive pipeline outperformed existing state-of-the-art predictors performance with an overall performance margin of 13% in terms of F1-score△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01657
On the strength of underscreening,['Statistical Mechanics'],"['Andreas Härtel', 'Roland Kjellander']","This paper constitutes a presentation of work in progress at a discussion session of the conference Dense ionic fluids Faraday Discussion, 8-10 July 2024, London, and is published on pages 293-295 in Ref. [Faraday Discuss. 2024, volume 253, page 289].△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01653
Robust and Transferable Backdoor Attacks Against Deep Image Compression With Selective Frequency Prior,['Computer Vision and Pattern Recognition'],"['Yi Yu', 'Yufei Wang', 'Wenhan Yang', 'Lanqing Guo', 'Shijian Lu', 'Ling-Yu Duan', 'Yap-Peng Tan', 'Alex C. Kot']","Recent advancements in deep learning-based compression techniques have surpassed traditional methods. However, deep neural networks remain vulnerable to backdoor attacks, where pre-defined triggers induce malicious behaviors. This paper introduces a novel frequency-based trigger injection model for launching backdoor attacks with multiple triggers on learned image compression models. Inspired by the widely used DCT in compression codecs, triggers are embedded in the DCT domain. We design attack objectives tailored to diverse scenarios, including: 1) degrading compression quality in terms of bit-rate and reconstruction accuracy; 2) targeting task-driven measures like face recognition and semantic segmentation. To improve training efficiency, we propose a dynamic loss function that balances loss terms with fewer hyper-parameters, optimizing attack objectives effectively. For advanced scenarios, we evaluate the attack's resistance to defensive preprocessing and propose a two-stage training schedule with robust frequency selection to enhance resilience. To improve cross-model and cross-domain transferability for downstream tasks, we adjust the classification boundary in the attack loss during training. Experiments show that our trigger injection models, combined with minor modifications to encoder parameters, successfully inject multiple backdoors and their triggers into a single compression model, demonstrating strong performance and versatility. (*Due to the notification of arXiv ""The Abstract field cannot be longer than 1,920 characters"", the appeared Abstract is shortened. For the full Abstract, please download the Article.)△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01646
Regularisation by Gaussian rough paths,['Probability'],"['Konstantinos Dareiotis', 'Máté Gerencsér', 'Khoa Lê', 'Chengcheng Ling']","The aim of the paper is to show the probabilistically strong well-posedness of rough differential equations with distributional drifts driven by the Gaussian rough path lift of fractional Brownian motion with Hurst parameter $H\in(1/3,1/2)$. We assume that the noise is nondegenerate and the drift lies in the Besov-Hölder space $\mathcal{C}^α$ for some $α>1-1/(2H)$. The latter condition matches the one of the additive noise case, thereby providing a multiplicative analogue of Catellier-Gubinelli in the regime $H\in(1/3,1/2)$.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01645
Test properties of some Cohen-Macaulay modules and criteria for local rings via finite vanishing of (co)homologies,['Commutative Algebra'],"['Souvik Dey', 'Dipankar Ghosh', 'Aniruddha Saha']","In this article, we deduce test properties, in the sense of finitely many vanishing of Ext or Tor, of CM (Cohen-Macaulay) modules whose multiplicity and number of generators (resp., type) are related by certain inequalities. We apply these test behaviour, along with other results, to characterize various kinds of local rings, including hypersurface rings of multiplicity at most two, via finite vanishing of Ext or Tor involving such CM modules. As further applications, we verify the long-standing (Generalized) Auslander-Reiten Conjecture for every CM module of minimal multiplicity.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01636
On the Weak Convergence of the Function-Indexed Sequential Empirical Process and its Smoothed Analogue under Nonstationarity,['Probability'],"['Florian Alexander Scholze', 'Ansgar Steland']","We study the sequential empirical process indexed by general function classes and its smoothed set-indexed analogue. Sufficient conditions for asymptotic equicontinuity and weak convergence are provided for nonstationary arrays of time series, in terms of uniform moment bounds for partial sums and, for the set-indexed smoothed process, $L_p$-Lipschitz regularity. This yields comprehensive general results on the weak convergence of sequential empirical processes, which are applicable to various notions of dependence. Especially, we show that our moment conditions imply the weak convergence of the sequential process under essentially the same mild assumptions (on the degree of dependence and the complexity of the indexing function class) as known for the classical empirical process. This is exemplified in detail for nonstationary $α$-mixing time series. Core ingredients of the proofs are a novel maximal inequality for nonmeasurable stochastic processes, uniform chaining arguments and suitable pathwise uniform Lipschitz properties.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01635
The origin of the ferroelectric-like orthorhombic phase in oxygen-deficient HfO2-y nanoparticles,['Materials Science'],"['Eugene A. Eliseev', 'Iryna V. Kondakova', 'Yuri O. Zagorodniy', 'Hanna V. Shevilakova', 'Oksana V. Leshchenko', 'Victor N. Pavlikov', 'Lesya P. Yurchenko', 'Myroslav V. Karpets', 'Anna N. Morozovska']","In this work we established the relationship between the crystalline structure symmetry, point defects and possible appearance of the ferroelectric-like polarization in HfO2-y nanoparticles. Notably, that XRD and EPR analysis revealed the formation of the ferroelectric-like orthorhombic phase in the oxygen-deficient HfO2-y nanoparticles (pure and doped with rare-earth element yttrium). DFT calculations showed that small HfO2 nanoparticles may become polar, especially in the presence of impurity atoms and/or oxygen vacancies. To explain the experimental results, we have modified the effective LGD model through the parameterization approach, focusing on the Landau expansion coefficients associated with the polar (FE) and antipolar (AFE) orderings, which agrees with the performed DFT calculations. The effective LGD model can be useful for the development of the novel generation of silicon-compatible ferroelectric nanomaterials based on the HfxZr1-xO2-y.△ Less",v1,https://arxiv.org/pdf/2412.01632
Discontinuous structural transitions in fluids with competing interactions,['Soft Condensed Matter'],"['Ana M. Montero', 'Santos B. Yuste', 'Andrés Santos', 'Mariano López de Haro']","This paper explores how competing interactions in the intermolecular potential of fluids affect their structural transitions. The study employs a versatile potential model with a hard core followed by two constant steps, representing wells or shoulders, analyzed in both one-dimensional (1D) and three-dimensional (3D) systems. Comparing these dimensionalities highlights the effect of confinement on structural transitions. Exact results are derived for 1D systems, while the Rational-Function Approximation is used for unconfined 3D fluids. Both scenarios confirm that when the steps are repulsive, the wavelength of the oscillatory decay of the total correlation function evolves with temperature either continuously or discontinuously. In the latter case, a discontinuous oscillation crossover line emerges in the temperature--density plane. For an attractive first step and a repulsive second step, a Fisher--Widom line appears. Although the 1D and 3D results share common features, dimensionality introduces differences: these behaviors occur in distinct temperature ranges, require deeper wells, or become attenuated in 3D. Certain features observed in 1D may vanish in 3D. We conclude that fluids with competing interactions exhibit a rich and intricate pattern of structural transitions, demonstrating the significant influence of dimensionality and interaction features.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01629
Using Large Language Models in Automatic Hint Ranking and Generation Tasks,['Computation and Language'],"['Jamshid Mozafari', 'Florian Gerhold', 'Adam Jatowt']","The use of Large Language Models (LLMs) has increased significantly recently, with individuals frequently interacting with chatbots to receive answers to a wide range of questions. In an era where information is readily accessible, it is crucial to stimulate and preserve human cognitive abilities and maintain strong reasoning skills. This paper addresses such challenges by promoting the use of hints as an alternative or a supplement to direct answers. We first introduce a manually constructed hint dataset, WIKIHINT, which includes 5,000 hints created for 1,000 questions. We then finetune open-source LLMs such as LLaMA-3.1 for hint generation in answer-aware and answer-agnostic contexts. We assess the effectiveness of the hints with human participants who try to answer questions with and without the aid of hints. Additionally, we introduce a lightweight evaluation method, HINTRANK, to evaluate and rank hints in both answer-aware and answer-agnostic settings. Our findings show that (a) the dataset helps generate more effective hints, (b) including answer information along with questions generally improves hint quality, and (c) encoder-based models perform better than decoder-based models in hint ranking.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01626
Evolution of the UV slope of galaxies at cosmic morning (z > 4): the properties of extremely blue galaxies,['Astrophysics of Galaxies'],"['D. Dottorini', 'A. Calabrò', 'L. Pentericci', 'S. Mascia', 'M. Llerena', 'L. Napolitano', 'P. Santini', 'G. Roberts-Borsani', 'M. Castellano', 'R. Amorín', 'M. Dickinson', 'A. Fontana', 'N. Hathi', 'M. Hirschmann', 'A. Koekemoer', 'R. A. Lucas', 'E. Merlin', 'A. Morales', 'F. Pacucci', 'S. Wilkins', 'P. Arrabal Haro', 'M. Bagley', 'S. Finkelstein', 'J. Kartaltepe', 'C. Papovich']","We present an analysis of the UV continuum slope, beta, using a sample of 733 galaxies selected from a mixture of JWST ERS/GTO/GO observational programs and with z > 4. We consider spectroscopic data obtained with the low resolution PRISM/CLEAR NIRSpec configuration. Studying the correlation of beta with M_UV we find a decreasing trend of beta = (-0.056 +- 0.017) M_UV - (3.01 +- 0.34), consistent with brighter galaxies having redder beta as found in previous works. However, analysing the trend in separate redshift bins, we find that at high redshift the relation becomes much flatter, consistent with a flat slope. Furthermore, we find that beta decreases with redshift with an evolution as beta = (-0.075 +- 0.010) z - (1.496 +- 0.056), consistent with most previous results that show a steepening of the spectra going at higher z. We then select a sample of galaxies with extremely blue slopes (beta < -2.6): such slopes are steeper than what is predicted by stellar evolution models, even for dust free, young, metal poor populations, when the contribution of nebular emission is included. We select 51 extremely blue galaxies (XBGs) and we investigate the possible physical origin of their steep slopes, comparing them to a sub-sample of redder galaxies (matched in redshift and M_UV). We find that XBGs have younger stellar populations, stronger ionization fields, lower dust attenuation, and lower but not pristine metallicity (~ 10% solar) compared to red galaxies. However, these properties alone cannot explain the extreme beta values. By using indirect inference of Lyman continuum escape, using the most recent models, we estimate escape fractions f_esc > 10% in at least 25% of XBGs, while all the red sources have smaller f_esc. A reduced nebular continuum contribution as due to either a high escape fraction or to a bursty star-formation history is likely the origin of the extremely blue slopes.△ Less",v1,https://arxiv.org/pdf/2412.01623
NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers,['Computation and Language'],"['Angel Yahir Loredo Lopez', 'Tyler McDonald', 'Ali Emami']","Large Language Models (LLMs) have shown impressive performance on various benchmarks, yet their ability to engage in deliberate reasoning remains questionable. We present NYT-Connections, a collection of 358 simple word classification puzzles derived from the New York Times Connections game. This benchmark is designed to penalize quick, intuitive ""System 1"" thinking, isolating fundamental reasoning skills. We evaluated six recent LLMs, a simple machine learning heuristic, and humans across three configurations: single-attempt, multiple attempts without hints, and multiple attempts with contextual hints. Our findings reveal a significant performance gap: even top-performing LLMs like GPT-4 fall short of human performance by nearly 30%. Notably, advanced prompting techniques such as Chain-of-Thought and Self-Consistency show diminishing returns as task difficulty increases. NYT-Connections uniquely combines linguistic isolation, resistance to intuitive shortcuts, and regular updates to mitigate data leakage, offering a novel tool for assessing LLM reasoning capabilities.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01621
Scattering in $e^- -(pe^-)$ and $μ^- -(pμ^-)$ systems: mass dependent and mass independent features of cross sections above the degenerated thresholds,['Atomic Physics'],"['V. A. Gradusov', 'S. L. Yakovlev']","Ab initio calculation of low energy scattering of electrons (muons) off hydrogen (muonic hydrogen) are performed on the basis of Faddeev-Merkuriev (FM) equations. The explicit contribution of induced dipole interaction in the asymptotic behavior of the wave function components has been incorporated into FM formalism. Elastic and inelastic cross sections have been calculated with high energy resolution in the vicinity of $n=2,3$ exited states thresholds of respective atoms. The Gailitis-Dumburg oscillations are discovered in some of calculated cross sections.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01620
If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World,['Computation and Language'],['Adrian de Wynter'],"Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT, particularly those outside of its marketed use as task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22 times more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations for research and industry to address loneliness.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01617
Edge-Minimum Walk of Modular Length in Polynomial Time,['Data Structures and Algorithms'],"['Antoine Amarilli', 'Benoît Groz', 'Nicole Wein']","We study the problem of finding, in a directed graph, an st-walk of length r mod q which is edge-minimum, i.e., uses the smallest number of distinct edges. Despite the vast literature on paths and cycles with modularity constraints, to the best of our knowledge we are the first to study this problem. Our main result is a polynomial-time algorithm that solves this task when r and q are constants.
  We also show how our proof technique gives an algorithm to solve a generalization of the well-known Directed Steiner Network problem, in which connections between endpoint pairs are required to satisfy modularity constraints on their length. Our algorithm is polynomial when the number of endpoint pairs and the modularity constraints on the pairs are constants.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01614
Topological Representations of Free Numerical Semigroups via Iterated Torus Knots,['Geometric Topology'],"['Patricio Almirón', 'Adrián Olivares-Fernández']","In this paper we will associate a family $\{K_1,\dots,K_l\}\subset \mathbb{S}^3$ of iterated torus knots to a given free numerical semigroup. We will describe the fundamental group of the knot complement of each knot of the family. Finally, we will show that all knots in the family have same Alexander polynomial and it coincides (up to a factor) with the Poincaré series of the free numerical semigroup. As a consequence, we will provide families of iterated torus knots with the same Alexander polynomial of an irreducible plane curve singularity but which are non-isotopic to its associated knot.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01613
Optimizing LoRa for Edge Computing with TinyML Pipeline for Channel Hopping,['Networking and Internet Architecture'],"['Marla Grunewald', 'Mounir Bensalem', 'Admela Jukan']","We propose to integrate long-distance LongRange (LoRa) communication solution for sending the data from IoT to the edge computing system, by taking advantage of its unlicensed nature and the potential for open source implementations that are common in edge computing. We propose a channel hoping optimization model and apply TinyML-based channel hoping model based for LoRa transmissions, as well as experimentally study a fast predictive algorithm to find free channels between edge and IoT devices. In the open source experimental setup that includes LoRa, TinyML and IoT-edge-cloud continuum, we integrate a novel application workflow and cloud-friendly protocol solutions in a case study of plant recommender application that combines concepts of microfarming and urban computing. In a LoRa-optimized edge computing setup, we engineer the application workflow, and apply collaborative filtering and various machine learning algorithms on application data collected to identify and recommend the planting schedule for a specific microfarm in an urban area. In the LoRa experiments, we measure the occurrence of packet loss, RSSI, and SNR, using a random channel hoping scheme to compare with our proposed TinyML method. The results show that it is feasible to use TinyML in microcontrollers for channel hopping, while proving the effectiveness of TinyML in learning to predict the best channel to select for LoRa transmission, and by improving the RSSI by up to 63 %, SNR by up to 44 % in comparison with a random hopping mechanism.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01609
"Cloudy and the High-Resolution Microcalorimeter Revolution: Optical, UV, and X-ray Spectra of One-electron Systems",['High Energy Astrophysical Phenomena'],"['Chamani M. Gunasekera', 'Peter A. M. van Hoof', 'Marios Chatzikos', 'Gary J. Ferland']","The majority of the baryonic matter in the universe is in the form of astrophysical plasmas. The mass of the hot X-ray emitting gas in a cluster of galaxies has more mass than the galaxies in the cluster. With the launch of the XRISM microcalorimeter mission, space-based X-ray observations will achieve a record spectral resolving power of $R\equiv E/ΔE \sim 1200$. With this resolving power, emission features associated with fine-structure energy levels of some species will be resolved, sometimes for the first time. The plasma code, CLOUDY, was not originally designed for high-resolution X-ray spectroscopy and throughout its history did not resolve fine-structure components of Lyman lines. Here we expand CLOUDY to resolve these fine-structure energy levels and obtain predicted X-ray spectra that match the resolution of new microcalorimeter observations. We show how the Lyman lines can be used as column density indicators and examine their sensitivity to external radiation fields and turbulence.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01606
Agentic-HLS: An agentic reasoning based high-level synthesis system using large language models (AI for EDA workshop 2024),['Artificial Intelligence'],"['Ali Emre Oztas', 'Mahdi Jelodari']","Our aim for the ML Contest for Chip Design with HLS 2024 was to predict the validity, running latency in the form of cycle counts, utilization rate of BRAM (util-BRAM), utilization rate of lookup tables (uti-LUT), utilization rate of flip flops (util-FF), and the utilization rate of digital signal processors (util-DSP). We used Chain-of-thought techniques with large language models to perform classification and regression tasks. Our prediction is that with larger models reasoning was much improved. We release our prompts and propose a HLS benchmarking task for LLMs.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01604
Arabic Handwritten Document OCR Solution with Binarization and Adaptive Scale Fusion Detection,['Computer Vision and Pattern Recognition'],"['Alhossien Waly', 'Bassant Tarek', 'Ali Feteha', 'Rewan Yehia', 'Gasser Amr', 'Ahmed Fares']","The problem of converting images of text into plain text is a widely researched topic in both academia and industry. Arabic handwritten Text Recognation (AHTR) poses additional challenges due to diverse handwriting styles and limited labeled data. In this paper we present a complete OCR pipeline that starts with line segmentation using Differentiable Binarization and Adaptive Scale Fusion techniques to ensure accurate detection of text lines. Following segmentation, a CNN-BiLSTM-CTC architecture is applied to recognize characters. Our system, trained on the Arabic Multi-Fonts Dataset (AMFDS), achieves a Character Recognition Rate (CRR) of 99.20% and a Word Recognition Rate (WRR) of 93.75% on single-word samples containing 7 to 10 characters, along with a CRR of 83.76% for sentences. These results demonstrate the system's strong performance in handling Arabic scripts, establishing a new benchmark for AHTR systems.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01601
An implementation of neural simulation-based inference for parameter estimation in ATLAS,['High Energy Physics - Experiment'],['ATLAS Collaboration'],"Neural simulation-based inference is a powerful class of machine-learning-based methods for statistical inference that naturally handles high-dimensional parameter estimation without the need to bin data into low-dimensional summary histograms. Such methods are promising for a range of measurements, including at the Large Hadron Collider, where no single observable may be optimal to scan over the entire theoretical phase space under consideration, or where binning data into histograms could result in a loss of sensitivity. This work develops a neural simulation-based inference framework for statistical inference, using neural networks to estimate probability density ratios, which enables the application to a full-scale analysis. It incorporates a large number of systematic uncertainties, quantifies the uncertainty due to the finite number of events in training samples, develops a method to construct confidence intervals, and demonstrates a series of intermediate diagnostic checks that can be performed to validate the robustness of the method. As an example, the power and feasibility of the method are assessed on simulated data for a simplified version of an off-shell Higgs boson couplings measurement in the four-lepton final states. This approach represents an extension to the standard statistical methodology used by the experiments at the Large Hadron Collider, and can benefit many physics analyses.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01600
An efficient slope stability algorithm with physically consistent parametrisation of slip surfaces,"['Computational Engineering, Finance, and Science']","['Leonardo Maria Lalicata', 'Andrea Bressan', 'Simone Pittaluga', 'Lorenzo Tamellini', 'Domenico Gallipoli']","This paper presents an optimised algorithm implementing the method of slices for analysing the stability of slopes. The algorithm adopts an improved physically based parameterisation of slip lines according to their geometrical characteristics at the endpoints, which facilitates the identification of all viable failure mechanisms while excluding unrealistic ones. The minimisation routine combines a preliminary discrete calculation of the factor of safety over a coarse grid covering the above parameter space with a subsequent continuous exploration of the most promising region via the simplex optimisation. This reduces computational time up to about 92% compared to conventional approaches that rely on the discrete calculation of the factor of safety over a fine grid covering the entire search space. Significant savings of computational time are observed with respect to recently published heuristic algorithms, which enable a continuous exploration of the entire parametric space. These efficiency gains are particularly advantageous for numerically demanding applications like, for example, the statistical assessment of slopes with uncertain mechanical, hydraulic and geometrical properties. The novel physically based parametrisation of the slip geometry and the adoption of a continuous local search allow exploration of parameter combinations that are necessarily neglected by standard grid-based approaches, leading to an average improvement in accuracy of about 5%.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01598
FEVER-OOD: Free Energy Vulnerability Elimination for Robust Out-of-Distribution Detection,['Computer Vision and Pattern Recognition'],"['Brian K. S. Isaac-Medina', 'Mauricio Che', 'Yona F. A. Gaus', 'Samet Akcay', 'Toby P. Breckon']","Modern machine learning models, that excel on computer vision tasks such as classification and object detection, are often overconfident in their predictions for Out-of-Distribution (OOD) examples, resulting in unpredictable behaviour for open-set environments. Recent works have demonstrated that the free energy score is an effective measure of uncertainty for OOD detection given its close relationship to the data distribution. However, despite free energy-based methods representing a significant empirical advance in OOD detection, our theoretical analysis reveals previously unexplored and inherent vulnerabilities within the free energy score formulation such that in-distribution and OOD instances can have distinct feature representations yet identical free energy scores. This phenomenon occurs when the vector direction representing the feature space difference between the in-distribution and OOD sample lies within the null space of the last layer of a neural-based classifier. To mitigate these issues, we explore lower-dimensional feature spaces to reduce the null space footprint and introduce novel regularisation to maximize the least singular value of the final linear layer, hence enhancing inter-sample free energy separation. We refer to these techniques as Free Energy Vulnerability Elimination for Robust Out-of-Distribution Detection (FEVER-OOD). Our experiments show that FEVER-OOD techniques achieve state of the art OOD detection in Imagenet-100, with average OOD false positive rate (at 95% true positive rate) of 35.83% when used with the baseline Dream-OOD model.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01596
Average-Cost MDPs with Infinite State and Action Sets: New Sufficient Conditions for Optimality Inequalities and Equations,['Optimization and Control'],"['Eugene A. Feinberg', 'Pavlo O. Kasyanov', 'Liliia S. Paliichuk']",This paper studies discrete-time average-cost infinite-horizon Markov decision processes (MDPs) with Borel state and action sets. It introduces new sufficient conditions for validity of optimality inequalities and optimality equations for MDPs with weakly and setwise continuous transition probabilities.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01594
ALMA Observations of Massive Clouds in the Central Molecular Zone: External-Pressure-Confined Dense Cores and Salpeter-like Core Mass Functions,['Astrophysics of Galaxies'],"['Zhenying Zhang', 'Xing Lu', 'Tie Liu', 'Sheng-Li Qin', 'Adam Ginsburg', 'Yu Cheng', 'Hauyu Baobab Liu', 'Daniel L. Walker', 'Xindi Tang', 'Shanghuo Li', 'Qizhou Zhang', 'Thushara Pillai', 'Jens Kauffmann', 'Cara Battersby', 'Siyi Feng', 'Suinan Zhang', 'Qi-Lao Gu', 'Fengwei Xu', 'Wenyu Jiao', 'Xunchuan Liu', 'Li Chen', 'Qiu-yi Luo', 'Xiaofeng Mai', 'Zi-yang Li', 'Dongting Yang']","We present Atacama Large Millimeter/submillimeter Array (ALMA) Band 6 (1.3 mm) observations of dense cores in three massive molecular clouds within the Central Molecular Zone (CMZ) of the Milky Way, including the Dust Ridge cloud e, Sgr C, and the 20 km s-1 cloud, at a spatial resolution of 2000 au. Among the 834 cores identified from the 1.3 mm continuum, we constrain temperatures and linewidths of 253 cores using local thermodynamic equilibrium (LTE) methods to fit the H2CO and/or CH3CN spectra. We determine their masses using the 1.3 mm dust continuum and derived temperatures, and then evaluate their virial parameters using the H2CO and/or CH3CN linewidths and construct the core mass functions (CMFs). We find that the contribution of external pressure is crucial for the virial equilibrium of the dense cores in the three clouds, which contrasts with the environment in the Galactic disk where dense cores are already bound even without the contribution of external pressure. We also find that the CMFs show a Salpeter-like slope in the high-mass (>~3-6 Msun) end, a change from previous works with our new temperature estimates. Combined with the possible top-heavy initial mass functions (IMFs) in the CMZ, our result suggests that gas accretion and further fragmentation may play important roles in transforming the CMF to the IMF.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01593
NCDD: Nearest Centroid Distance Deficit for Out-Of-Distribution Detection in Gastrointestinal Vision,['Computer Vision and Pattern Recognition'],"['Sandesh Pokhrel', 'Sanjay Bhandari', 'Sharib Ali', 'Tryphon Lambrou', 'Anh Nguyen', 'Yash Raj Shrestha', 'Angus Watson', 'Danail Stoyanov', 'Prashnna Gyawali', 'Binod Bhattarai']","The integration of deep learning tools in gastrointestinal vision holds the potential for significant advancements in diagnosis, treatment, and overall patient care. A major challenge, however, is these tools' tendency to make overconfident predictions, even when encountering unseen or newly emerging disease patterns, undermining their reliability.
  We address this critical issue of reliability by framing it as an out-of-distribution (OOD) detection problem, where previously unseen and emerging diseases are identified as OOD examples. However, gastrointestinal images pose a unique challenge due to the overlapping feature representations between in- Distribution (ID) and OOD examples. Existing approaches often overlook this characteristic, as they are primarily developed for natural image datasets, where feature distinctions are more apparent. Despite the overlap, we hypothesize that the features of an in-distribution example will cluster closer to the centroids of their ground truth class, resulting in a shorter distance to the nearest centroid. In contrast, OOD examples maintain an equal distance from all class centroids. Based on this observation, we propose a novel nearest-centroid distance deficit (NCCD) score in the feature space for gastrointestinal OOD detection.
  Evaluations across multiple deep learning architectures and two publicly available benchmarks, Kvasir2 and Gastrovision, demonstrate the effectiveness of our approach compared to several state-of-the-art methods. The code and implementation details are publicly available at: https://github.com/bhattarailab/NCDD△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01590
Emission and detection of ultra high frequency gravitational waves from highly eccentric orbits of compact binary systems,['General Relativity and Quantum Cosmology'],"['Pierre Jamet', 'Aurélien Barrau', 'Killian Martineau']","The ultra high frequency emission of gravitational waves by binary systems of black holes has recently been investigated in details in the framework of new experimental ideas around resonant cavities. In this article, we consider the case of elliptic trajectories. At fixed masses and frequency, we conclude that the total amount of energy radiated by the system within the bandwidth of the detector can be significantly higher than for circular orbits. However, due to subtle experimental effects, the signal-to-noise ratio is, overall, a decreasing function of the eccentricity. Limits on the maximum distance at which a merging system of black holes can be detected derived for circular orbits are therefore not improved by considering elliptic trajectories. The article is written as pedagogically as possible so as to be accessible to the non-familiar reader.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01582
Beyond quasi-particle self-consistent $GW$ for molecules with vertex corrections,['Chemical Physics'],['Arno Förster'],"We introduce the $Σ^{\text{BSE}}@L^{\text{BSE}}$ self-energy in the quasi-particle self-consistent $GW$ (qs$GW$) framework (qs$Σ^{\text{BSE}}@L^{\text{BSE}}$). Here, $L$ is the two-particle response function which we calculate by solving the Bethe-Salpeter equation with the static, first-order $GW$ kernel. The same kernel is added to $Σ$ directly. For a set of medium organic molecules, we show that including the vertex both in $L$ and $Σ$ is crucial. This approach retains the good performance of qs$GW$ for predicting first ionization potentials and fundamental gaps, while it greatly improves the description of electron affinities. Its good performance places qs$Σ^{\text{BSE}}@L^{\text{BSE}}$ among the best-performing electron propagator methods for charged excitations. Adding the vertex in $L$ only, as commonly done in the solid state community, leads to devastating results for electron affinities and fundamental gaps. We also test the performance of BSE@qs$GW$ and qs$Σ^{\text{BSE}}@L^{\text{BSE}}$ for neutral charge-transfer excitation and find both methods to perform similar. We conclude that $Σ^{\text{BSE}}@L^{\text{BSE}}$ is a promising approximation to the electronic self-energy beyond $GW$. We hope that future research on dynamical vertex effects, second-order vertex corrections, and full self-consistency will improve the accuracy of this method, both for charged and neutral excitation energies.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01581
A homotopy theorem for incremental stability,['Optimization and Control'],"['Thomas Chaffey', 'Andrey Kharitenko', 'Fulvio Forni', 'Rodolphe Sepulchre']","A theorem is proved to verify incremental stability of a feedback system via a homotopy from a known incrementally stable system. A first corollary of that result is that incremental stability may be verified by separation of Scaled Relative Graphs, correcting two assumptions in [1, Theorem 2]. A second corollary provides an incremental version of the classical IQC stability theorem.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01580
Double-edged Role of Interactions in Superconducting Twisted Bilayer Graphene,['Mesoscale and Nanoscale Physics'],"['Xueshi Gao', 'Alejandro Jimeno-Pozo', 'Pierre A. Pantaleon', 'Emilio Codecido', 'Daria L. Sharifi', 'Zheneng Zhang', 'Youwei Liu', 'Kenji Watanabe', 'Takashi Taniguchi', 'Marc W. Bockrath', 'Francisco Guinea', 'Chun Ning Lau']","For the unconventional superconducting phases in moire materials, a critical question is the role played by electronic interactions in the formation of Cooper pairs. In twisted bilayer graphene (tBLG), the strength of electronic interactions can be reduced by increasing the twist angle or screening provided by the dielectric medium. In this work, we place tBLG at 3-4 nm above bulk SrTiO3 substrates, which have a large yet tunable dielectric constant. By raising the dielectric constant in situ in a magic angle device, we observe suppression of both the height and the width of the entire superconducting dome, thus demonstrating that, unlike conventional superconductors, the pairing mechanism in tBLG is strongly dependent on electronic interactions. Interestingly, in contrast to the absence of superconductivity in devices on SiO2 with angle>1.3 deg, we observe a superconducting pocket in a large-angle (angle=1.4 deg) tBLG/STO device while the correlated insulating states are absent. These experimental results are in qualitative agreement with a theoretical model in which the pairing mechanism arises from Coulomb interactions that are screened by plasmons, electron-hole pairs, and longitudinal acoustic phonons. Our results highlight the unconventional nature of the superconductivity in tBLG, the double-edged role played by electronic interactions in its formation, as well as their complex interplay with the correlated insulating states.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01578
Coulomb screening of superconductivity in magic-angle twisted bilayer graphene,['Mesoscale and Nanoscale Physics'],"['Julien Barrier', 'Liangtao Peng', 'Shuigang Xu', ""V. I. Fal'ko"", 'K. Watanabe', 'T. Tanigushi', 'A. K. Geim', 'S. Adam', 'Alexey I. Berdyugin']","The origin of superconductivity in magic-angle twisted bilayer graphene has been a subject of intense debate. While some experimental evidence indicated an unconventional pairing mechanism, efforts to tune the critical temperature by screening the Coulomb interactions have been unsuccessful, possibly indicating a conventional phonon-mediated pairing. Here we study a double-layer electronic system consisting of two twisted graphene bilayers in immediate proximity of each other but remaining electronically decoupled. By increasing the carrier density in one bilayer, we completely suppressed both the superconductivity and the correlated-insulator state in the adjacent magic-angle graphene. The observation of such a screening effect offers strong support for an unconventional mechanism of Cooper pairing in magic-angle twisted bilayer graphene, shedding new light on the underlying physics governing their properties.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01577
"Single-Photon Generation: Materials, Techniques, and the Rydberg Exciton Frontier",['Quantum Physics'],"['Arya Keni', 'Kinjol Barua', 'Khabat Heshami', 'Alisa Javadi', 'Hadiseh Alaeian']","Due to their quantum nature, single-photon emitters generate individual photons in bursts or streams. They are paramount in emerging quantum technologies such as quantum key distribution, quantum repeaters, and measurement-based quantum computing. Many such systems have been reported in the last three decades, from Rubidium atoms coupled to cavities to semiconductor quantum dots and color centers implanted in waveguides. This review article highlights different material systems with deterministic and controlled single photon generation. We discuss and compare the performance metrics, such as purity and indistinguishability, for these sources and evaluate their potential for different applications. Finally, a new potential single-photon source, based on the Rydberg exciton in solid state metal oxide thin films, is introduced, briefly discussing its promising qualities and advantages in fabricating quantum chips for quantum photonic applications.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01573
Enhanced Time Division Duplexing Slot Allocation and Scheduling in Non-Terrestrial Networks,['Networking and Internet Architecture'],"['Alessandro Traspadini', 'Marco Giordani', 'Michele Zorzi']","The integration of non-terrestrial networks (NTNs) and terrestrial networks (TNs) is fundamental for extending connectivity to rural and underserved areas that lack coverage from traditional cellular infrastructure. However, this integration presents several challenges. For instance, TNs mainly operate in Time Division Duplexing (TDD). However, for NTN via satellites, TDD is complicated due to synchronization problems in large cells, and the significant impact of guard periods and long propagation delays. In this paper, we propose a novel slot allocation mechanism to enable TDD in NTN. This approach permits to allocate additional transmissions during the guard period between a downlink slot and the corresponding uplink slot to reduce the overhead, provided that they do not interfere with other concurrent transmissions. Moreover, we propose two scheduling methods to select the users that transmit based on considerations related to the Signal-to-Noise Ratio (SNR) or the propagation delay. Simulations demonstrate that our proposal can increase the network capacity compared to a benchmark scheme that does not schedule transmissions in guard periods.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01570
Physical Characteristics of Jupiter's Trojan (1437) Diomedes from a Tri-chord Stellar Occultation in 2020 and Dimensionless 3D Model,['Earth and Planetary Astrophysics'],"['H. Dutra', 'M. Assafin', 'B. Sicardy', 'J. L. Ortiz', 'A. R. Gomes-Júnior', 'B. E. Morgado', 'G. Benedetti-Rossi', 'F. Braga-Ribas', 'G. Margoti', 'E. Gradovski', 'J. I. B. Camargo', 'R. Boufleur', 'R. Vieira-Martins', 'J. Desmars', 'D. Oesper', 'K. Bender', 'C. Kitting', 'R. Nolthenius']","Jupiter Trojans preserve primitive formation characteristics due to their collisionless stable orbits. Determination of their shapes and size-frequency distribution constrains the collisional evolution of their parent population which also originated the Kuiper Belt. We started a program to find precise sizes/shapes for Trojans, combining stellar occultations and DAMIT 3D shape models. We report results for Diomedes, by fitting its dimensionless 3D model to 3 chords of a stellar occultation observed in 2020, using iterative $χ^{2}$ procedures. The pole coordinates, rotation period, volume-equivalent radius and geometric albedo were: $λ$ = 153.73$^{o}$ $\pm$ 2.5$^{o}$, $β$ = 12.69$^{o}$ $\pm$ 2.6$^{o}$, $P$ = 24.4984 $\pm$ 0.0002 h, $R_{eq}$ = 59.4 $\pm$ 0.3 km and $p_{V}$ = 0.030 $\pm$ 0.004. A precise position was obtained too.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01568
Estimation during Design Phases of Suitable SRAM Cells for PUF Applications Using Separatrix and Mismatch Metrics,['Cryptography and Security'],"['Abdel Alheyasat', 'Gabriel Torrens', 'Sebastia A. Bota', 'Bartomeu Alorda']","Physically unclonable functions (PUFs) are used as low-cost cryptographic primitives in device authentication and secret key creation. SRAM-PUFs are well-known as entropy sources; nevertheless, due of non-deterministic noise environment during the power-up process, they are subject to low challenge-response repeatability. The dependability of SRAM-PUFs is usually accomplished by combining complex error correcting codes (ECCs) with fuzzy extractor structures resulting in an increase in power consumption, area, cost, and design complexity. In this study, we established effective metrics on the basis of the separatrix concept and cell mismatch to estimate the percentage of cells that, due to the effect of variability, will tend to the same initial state during power-up. The effects of noise and temperature in cell start-up processes were used to validate the proposed metrics. The presented metrics may be applied at the SRAM-PUF design phases to investigate the impact of different design parameters on the percentage of reliable cells for PUF applications.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01560
Adaptive High-Pass Kernel Prediction for Efficient Video Deblurring,['Computer Vision and Pattern Recognition'],"['Bo Ji', 'Angela Yao']","State-of-the-art video deblurring methods use deep network architectures to recover sharpened video frames. Blurring especially degrades high-frequency (HF) information, yet this aspect is often overlooked by recent models that focus more on enhancing architectural design. Recovering these fine details is challenging, partly due to the spectral bias of neural networks, which are inclined towards learning low-frequency functions. To address this, we enforce explicit network structures to capture the fine details and edges. We dynamically predict adaptive high-pass kernels from a linear combination of high-pass basis kernels to extract high-frequency features. This strategy is highly efficient, resulting in low-memory footprints for training and fast run times for inference, all while achieving state-of-the-art when compared to low-budget models. The code is available at https://github.com/jibo27/AHFNet.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01559
A note on indefinite matrix splitting and preconditioning,['Numerical Analysis'],['Andy Wathen'],"The solution of systems of linear(ized) equations lies at the heart of many problems in Scientific Computing. In particular for systems of large dimension, iterative methods are a primary approach. Stationary iterative methods are generally based on a matrix splitting, whereas for polynomial iterative methods such as Krylov subspace iteration, the splitting matrix is the preconditioner. The smoother in a multigrid method is generally a stationary or polynomial iteration. Here we consider real symmetric indefinite and complex Hermitian indefinite coefficient matrices and prove that no splitting matrix can lead to a contractive stationary iteration unless the inertia is exactly preserved. This has consequences for preconditioning for indefinite systems and smoothing for multigrid as we further describe.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01554
SfM-Free 3D Gaussian Splatting via Hierarchical Training,['Computer Vision and Pattern Recognition'],"['Bo Ji', 'Angela Yao']","Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera poses and a sparse point cloud, obtained from structure-from-motion (SfM) preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free 3DGS (SFGS) method for video input, eliminating the need for known camera poses and SfM preprocessing. Our approach introduces a hierarchical training strategy that trains and merges multiple 3D Gaussian representations -- each optimized for specific scene regions -- into a single, unified 3DGS model representing the entire scene. To compensate for large camera motions, we leverage video frame interpolation models. Additionally, we incorporate multi-source supervision to reduce overfitting and enhance representation. Experimental results reveal that our approach significantly surpasses state-of-the-art SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with a top gain of 3.90dB. The code is available at https://github.com/jibo27/3DGS_Hierarchical_Training.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01553
Cosmology and general relativity (GR) in upper secondary school through new targeted teaching materials: a study on student learning and motivation,['Physics Education'],"['Alice Gasparini', 'Andreas Mueller', 'Florian Stern', 'Laura Weiss']","Cosmology and GR remain largely inaccessible to high-school teaching due to the advanced prerequisites to master these topics. Integrating them into upper secondary teaching is a significant challenge that remains unresolved. This contribution reports on an implementation study of a GR and cosmology course for upper secondary school students as part of an educational project launched during the centenary of GR and tested ever since for several years. The course aimed to expand students' knowledge to include current physics topics while highlighting their foundations in areas of classical physics such as Newtonian mechanics, electromagnetism, and waves. Targeted teaching and learning materials are focused on conceptual and qualitative understanding, while systematically combined with a mathematical treatment accessible at the upper secondary level, avoiding oversimplification. A key element is an active learning approach, incorporating activities and tasks such as engaging applications related to current research, reflective exercises, thought experiments, and hands-on tasks. The main research objective was to explore whether a conceptually deep and educationally effective GR and cosmology course could be successfully implemented for non-specialist upper secondary students. A pre-post study assessed both conceptual learning and affective outcomes, including interest, curiosity, self-concept, and perceived relevance of science. Results showed encouraging gains in both learning and motivation, with large to very large effect sizes for conceptual learning of core principles. Additionally, no or small effects of predictors such as gender were observed. We conclude that the integration of GR and cosmology into upper secondary physics teaching, in the form of courses and materials that are engaging, comprehensible, and impactful, is feasible.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01551
Silenced Voices: Exploring Social Media Polarization and Women's Participation in Peacebuilding in Ethiopia,['Computers and Society'],"['Adem Chanie Ali', 'Seid Muhie Yimam', 'Martin Semmann', 'Abinew Ali Ayele', 'Chris Biemann']","This exploratory study highlights the significant threats of social media polarization and weaponization in Ethiopia, analyzing the Northern Ethiopia (Tigray) War (November 2020 to November 2022) as a case study. It further uncovers the lack of effective digital peacebuilding initiatives. These issues particularly impact women, who bear a disproportionate burden in the armed conflict. These repercussions extend beyond the digital sphere, affecting women's socio-economic conditions, safety, and well-being. This reality was starkly evident during the war, where women faced gender-based and sexual violence. The research findings disclose the interface between social media polarization, conflict, and gender based violence. It also reveals the marginalization of women's voice in peacebuilding initiatives. This marginalization in peacebuilding efforts can be attributed to hostile online environments, the digital divide, cultural and societal norms, as well as top-down peace initiatives. The study highlights substantial gaps in leveraging digital media for sustainable peace and empowering women's participation. The unregulated landscape of social media in Ethiopia exacerbates these problems, necessitating heightened demands for accountability, especially from major social media platforms. The study recommends enhanced moderation and ethical considerations in algorithmic design gains traction, underlining the urgency for transparent and responsible social media frameworks. It is also recommended that digital peacebuilding initiatives should adopt a gender-sensitive and inclusive approach to address these complexities effectively and sustainably.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01549
Measurement of off-shell Higgs boson production in the $H^*\rightarrow ZZ\rightarrow 4\ell$ decay channel using a neural simulation-based inference technique in 13 TeV $pp$ collisions with the ATLAS detector,['High Energy Physics - Experiment'],['ATLAS Collaboration'],"A measurement of off-shell Higgs boson production in the $H^*\to ZZ\to 4\ell$ decay channel is presented. The measurement uses 140 fb$^{-1}$ of proton-proton collisions at $\sqrt{s}=13$ TeV collected by the ATLAS detector at the Large Hadron Collider and supersedes the previous result in this decay channel using the same dataset. The data analysis is performed using a neural simulation-based inference method, which builds per-event likelihood ratios using neural networks. The observed (expected) off-shell Higgs boson production signal strength in the $ZZ\to 4\ell$ decay channel at 68% CL is $0.87^{+0.75}_{-0.54}$ ($1.00^{+1.04}_{-0.95}$). The evidence for off-shell Higgs boson production using the $ZZ\to 4\ell$ decay channel has an observed (expected) significance of $2.5σ$ ($1.3σ$). The expected result represents a significant improvement relative to that of the previous analysis of the same dataset, which obtained an expected significance of $0.5σ$. When combined with the most recent ATLAS measurement in the $ZZ\to 2\ell 2ν$ decay channel, the evidence for off-shell Higgs boson production has an observed (expected) significance of $3.7σ$ ($2.4σ$). The off-shell measurements are combined with the measurement of on-shell Higgs boson production to obtain constraints on the Higgs boson total width. The observed (expected) value of the Higgs boson width at 68% CL is $4.3^{+2.7}_{-1.9}$ ($4.1^{+3.5}_{-3.4}$) MeV.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01548
Violation of the Wiedemann-Franz law and ultra-low thermal conductivity of Ti$_3$C$_2$T$_x$ MXene,['Materials Science'],"['Yubin Huang', 'Jean Spiece', 'Tetiana Parker', 'Asaph Lee', 'Yury Gogotsi', 'Pascal Gehring']","The high electrical conductivity and good chemical stability of MXenes offer hopes for their use in many applications, such as wearable electronics, energy storage, or electromagnetic interference shielding. While their optical, electronic and electrochemical properties have been widely studied, the information on thermal properties of MXenes is scarce. In this study, we investigate the heat transport properties of Ti$_3$C$_2$T$_x$ MXene single flakes using scanning thermal microscopy and find exceptionally low anisotropic thermal conductivities within the Ti$_3$C$_2$T$_x$ flakes, leading to an effective thermal conductivity of 0.78$\pm$0.21 W m$^{-1}$ K$^{-1}$. This observation is in stark contrast to the predictions of the Wiedemann-Franz law, as the estimated Lorenz number is only 0.25 of the classical value. Due to the combination of low thermal conductivity and low emissivity of Ti$_3$C$_2$T$_x$, the heat loss from it is two orders of magnitude smaller than that from common metals. Our study explores the heat transport mechanisms of MXenes and highlights a promising approach for developing thermal insulation, two-dimensional thermoelectric, or infrared stealth materials.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01546
Effectiveness of L2 Regularization in Privacy-Preserving Machine Learning,['Machine Learning'],"['Nikolaos Chandrinos', 'Iliana Loi', 'Panagiotis Zachos', 'Ioannis Symeonidis', 'Aristotelis Spiliotis', 'Maria Panou', 'Konstantinos Moustakas']","Artificial intelligence, machine learning, and deep learning as a service have become the status quo for many industries, leading to the widespread deployment of models that handle sensitive data. Well-performing models, the industry seeks, usually rely on a large volume of training data. However, the use of such data raises serious privacy concerns due to the potential risks of leaks of highly sensitive information. One prominent threat is the Membership Inference Attack, where adversaries attempt to deduce whether a specific data point was used in a model's training process. An adversary's ability to determine an individual's presence represents a significant privacy threat, especially when related to a group of users sharing sensitive information. Hence, well-designed privacy-preserving machine learning solutions are critically needed in the industry. In this work, we compare the effectiveness of L2 regularization and differential privacy in mitigating Membership Inference Attack risks. Even though regularization techniques like L2 regularization are commonly employed to reduce overfitting, a condition that enhances the effectiveness of Membership Inference Attacks, their impact on mitigating these attacks has not been systematically explored.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01541
"The Bare Necessities: Designing Simple, Effective Open-Vocabulary Scene Graphs",['Computer Vision and Pattern Recognition'],"['Christina Kassab', 'Matías Mattamala', 'Sacha Morin', 'Martin Büchner', 'Abhinav Valada', 'Liam Paull', 'Maurice Fallon']","3D open-vocabulary scene graph methods are a promising map representation for embodied agents, however many current approaches are computationally expensive. In this paper, we reexamine the critical design choices established in previous works to optimize both efficiency and performance. We propose a general scene graph framework and conduct three studies that focus on image pre-processing, feature fusion, and feature selection. Our findings reveal that commonly used image pre-processing techniques provide minimal performance improvement while tripling computation (on a per object view basis). We also show that averaging feature labels across different views significantly degrades performance. We study alternative feature selection strategies that enhance performance without adding unnecessary computational costs. Based on our findings, we introduce a computationally balanced approach for 3D point cloud segmentation with per-object features. The approach matches state-of-the-art classification accuracy while achieving a threefold reduction in computation.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01539
Hydrogen Utilization as a Plasma Source for Magnetohydrodynamic Direct Power Extraction (MHD-DPE),['Plasma Physics'],['Osama A. Marzouk'],"This study explores the suitability of hydrogen-based plasma in direct power extraction (DPE) as a non-conventional electricity generation method. We apply computational modeling and principles in physics and chemistry to estimate different thermal and electric properties of a water-vapor/nitrogen/cesium-vapor (H2O/N2/Cs) gas mixture with different levels of cesium (Cs) at a fixed temperature of 2300 K (2026.85 °C). This gas mixture and temperature are selected because they resemble the stoichiometric combustion of hydrogen with air, followed by the addition of the alkali metal element cesium to allow ionization, thus converting the gas mixture into electrically conducting plasma. We vary the cesium mole fraction in the gas mixture by two orders of magnitude, from a minute amount of 0.0625% (1/1600) to a major amount of 16% (0.16). We use these results to further estimate the theoretical upper limit of the electric power output from a unit volume of a high-speed magnetohydrodynamic (MHD) channel, with the plasma accelerated inside it to twice the local speed of sound (Mach number 2) while subject to an applied magnetic field of 5 T (5 teslas). We report that there is an optimum cesium mole fraction of 3%, at which the power output is maximized. Per 1 m3 of plasma volume, the estimated theoretical electric power generation at 1 atm (101.325 kPa) pressure of the hydrogen-combustion mixture is extraordinarily high at 360 MW/m3, and the plasma electric conductivity is 17.5 S/m. This estimated power generation even reaches an impressive level of 1.15 GW/m3 (11500 MW/m3) if the absolute pressure can be decreased to 0.0625 atm (6.333 kPa), at which the electric conductivity exceeds 55 S/m (more than 10 times the electric conductivity of seawater).△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01534
The Future of Document Verification: Leveraging Blockchain and Self-Sovereign Identity for Enhanced Security and Transparency,['Cryptography and Security'],"['Swapna Krishnakumar Radha', 'Andrey Kuehlkamp', 'Jarek Nabrzyski']","Attestation of documents like legal papers, professional qualifications, medical records, and commercial documents is crucial in global transactions, ensuring their authenticity, integrity, and trustworthiness. Companies expanding operations internationally need to submit attested financial statements and incorporation documents to foreign governments or business partners to prove their businesses and operations' authenticity, legal validity, and regulatory compliance. Attestation also plays a critical role in education, overseas employment, and authentication of legal documents such as testaments and medical records. The traditional attestation process is plagued by several challenges, including time-consuming procedures, the circulation of counterfeit documents, and concerns over data privacy in the attested records. The COVID-19 pandemic brought into light another challenge: ensuring physical presence for attestation, which caused a significant delay in the attestation process. Traditional methods also lack real-time tracking capabilities for attesting entities and requesters. This paper aims to propose a new strategy using decentralized technologies such as blockchain and self-sovereign identity to overcome the identified hurdles and provide an efficient, secure, and user-friendly attestation ecosystem.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01531
Generative AI-based data augmentation for improved bioacoustic classification in noisy environments,['Sound'],"['Anthony Gibbons', 'Emma King', 'Ian Donohue', 'Andrew Parnell']","1. Obtaining data to train robust artificial intelligence (AI)-based models for species classification can be challenging, particularly for rare species. Data augmentation can boost classification accuracy by increasing the diversity of training data and is cheaper to obtain than expert-labelled data. However, many classic image-based augmentation techniques are not suitable for audio spectrograms. 2. We investigate two generative AI models as data augmentation tools to synthesise spectrograms and supplement audio data: Auxiliary Classifier Generative Adversarial Networks (ACGAN) and Denoising Diffusion Probabilistic Models (DDPMs). The latter performed particularly well in terms of both realism of generated spectrograms and accuracy in a resulting classification task. 3. Alongside these new approaches, we present a new audio data set of 640 hours of bird calls from wind farm sites in Ireland, approximately 800 samples of which have been labelled by experts. Wind farm data are particularly challenging for classification models given the background wind and turbine noise. 4. Training an ensemble of classification models on real and synthetic data combined gave 92.6% accuracy (and 90.5% with just the real data) when compared with highly confident BirdNET predictions. 5. Our approach can be used to augment acoustic signals for more species and other land-use types, and has the potential to bring about a step-change in our capacity to develop reliable AI-based detection of rare species. Our code is available at https://github.com/gibbona1/ SpectrogramGenAI.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01530
Higher topological complexity of planar polygon spaces having small genetic codes,['Algebraic Topology'],"['Sutirtha Datta', 'Navnath Daundkar', 'Abhishek Sarkar']","In this paper, we compute sharp bounds for the higher topological complexity of many planar polygon spaces having small genetic codes. In most of these cases we show that the $k$-th higher topological complexity of planar polygon spaces is either $km$ or $km+1$, where $m$ is their dimension.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01529
CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models,['Artificial Intelligence'],"['Zhixiang Guo', 'Siyuan Liang', 'Aishan Liu', 'Dacheng Tao']","The diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01528
Outstanding framework for simulating and generating anchor trajectory in wireless sensor networks,['Networking and Internet Architecture'],['Abdelhady Naguib'],"This paper proposes a framework that has the ability to animate and generate different scenarios for the mobility of a movable anchor which can follow various paths in wireless sensor networks (WSNs). When the researchers use NS-2 to simulate a single anchor-assisted localization model, they face the problem of creating the movement file of the movable anchor. The proposed framework solved this problem by allowing them to create the movement scenario regarding different trajectories. The proposed framework lets the researcher set the needed parameters for simulating various static path models, which can be displayed through the graphical user interface. The researcher can also view the mobility of the movable anchor with control of its speed and communication range. The proposed framework has been validated by comparing its results to NS-2 outputs plus comparing it against existing tools. Finally, this framework has been published on the Code Project website and downloaded by many users.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01520
Magnetic-Transition-Induced Colossal Magnetoresistance in the Ferrimagnetic Semiconductor Mn$_3$Si$_2$Te$_6$,['Materials Science'],"['Yiyue Zhang', 'ZeYu Li', 'Kunya Yang', 'Linlin Wei', 'Xinrun Mi', 'Aifeng Wang', 'Xiaoyuan Zhou', 'Xiaolong Yang', 'Yisheng Chai', 'Mingquan He']","In the ferrimagnetic semiconductor Mn$_3$Si$_2$Te$_6$, a colossal magnetoresistance (CMR) is observed only when a magnetic field is applied along the magnetic hard axis ($\mathbf{H}\parallel c$). This phenomenon suggests an unconventional CMR mechanism potentially driven by the interplay between magnetism, topological band structure, and/or chiral orbital currents (COC). By comparing electrical resistance measurements using continuous direct currents and pulse currents, we found that the current-induced insulator-metal transition, supporting the COC-driven CMR mechanism, is likely a consequence of Joule heating effects. Additionally, multiple magnetic field-induced metamagnetic transitions were identified through AC magnetostriction coefficient experiments, but only when $\mathbf{H}\parallel c$. Importantly, the transition at $\sim$ 5 T marks the boundary between the low-field CMR and high-field weak MR. These findings suggest that field-induced metamagnetic transition combined with partial polarization of magnetic moments are the primary causes of the band gap closure, leading to the observed CMR in Mn$_3$Si$_2$Te$_6$.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01518
Performance of the LABDOS01 spectrometer in dosimetric measurements,['Instrumentation and Detectors'],"['Silvia Vernetto', 'Carlo Francesco Vigorito', 'Martin Kakona', 'Roman Dvorak', 'Satoshi Kodaira', 'Stefano Cirilli', 'Fabio Romanelli', 'Daniele Savietto', 'Alba Zanini']","This paper describes the performance of the LABDOS01, a silicon diode-based spectrometer suitable for dose measurements in mixed radiation fields. The instrument is currently being used in two high-altitude environmental dose monitoring projects: SAMADHA (South Atlantic Magnetic Anomaly Dosimetry at High Altitude) at Chacaltaya (Bolivia, 5240 m a.s.l.) and CORDIAL (COsmic Rays Dosimetry In Antarctic Latitudes) at the Concordia station (Antarctica, 3233 m a.s.l.). Before installing two of these devices at the measurement sites, the detectors were tested on flight routes covering a wide range of geomagnetic latitudes. The collected dosimetric data were compared with the expectations derived by the CARI-7A software, which provides the absorbed dose rate in silicon due to cosmic ray secondaries at a given position on the Earth. The measured dose rates along the flights at variable altitude and rigidity cutoff agree well with the simulated ones. By analyzing the spectrum of the energy deposited in the silicon layer, we derive an empirical method to approximately evaluate the ambient dose equivalent $H^{*}(10)$, a quantity directly related to the biological damage caused by environmental radiation.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01515
ArtBrain: An Explainable end-to-end Toolkit for Classification and Attribution of AI-Generated Art and Style,['Artificial Intelligence'],"['Ravidu Suien Rammuni Silva', 'Ahmad Lotfi', 'Isibor Kennedy Ihianle', 'Golnaz Shahtahmassebi', 'Jordan J. Bird']","Recently, the quality of artworks generated using Artificial Intelligence (AI) has increased significantly, resulting in growing difficulties in detecting synthetic artworks. However, limited studies have been conducted on identifying the authenticity of synthetic artworks and their source. This paper introduces AI-ArtBench, a dataset featuring 185,015 artistic images across 10 art styles. It includes 125,015 AI-generated images and 60,000 pieces of human-created artwork. This paper also outlines a method to accurately detect AI-generated images and trace them to their source model. This work proposes a novel Convolutional Neural Network model based on the ConvNeXt model called AttentionConvNeXt. AttentionConvNeXt was implemented and trained to differentiate between the source of the artwork and its style with an F1-Score of 0.869. The accuracy of attribution to the generative model reaches 0.999. To combine the scientific contributions arising from this study, a web-based application named ArtBrain was developed to enable both technical and non-technical users to interact with the model. Finally, this study presents the results of an Artistic Turing Test conducted with 50 participants. The findings reveal that humans could identify AI-generated images with an accuracy of approximately 58%, while the model itself achieved a significantly higher accuracy of around 99%.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01512
Hanle Effect for Lifetime Analysis: Li-like Ions,['Atomic Physics'],"['Jan Richter', 'Moto Togawa', 'José R. Crespo López-Urrutia', 'Andrey Surzhykov']","Accurate lifetime measurements of excited states of highly charged ions (HCIs) are essential for advancing diagnostics in both laboratory and astrophysical plasmas, especially in the X-ray regime. The Hanle effect, which utilizes external magnetic fields to modify photon scattering patterns, provides a powerful technique for these measurements. Previously, this method has been successfully employed for He-like ions. Here, we present a theoretical study of the prospects of the Hanle effect for lifetime determinations of Li-like ions. Our results highlight the potential for plasma diagnostics and X-ray spectral analysis.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01509
HaGRIDv2: 1M Images for Static and Dynamic Hand Gesture Recognition,['Computer Vision and Pattern Recognition'],"['Anton Nuzhdin', 'Alexander Nagaev', 'Alexander Sautin', 'Alexander Kapitanov', 'Karina Kvanchiani']","This paper proposes the second version of the widespread Hand Gesture Recognition dataset HaGRID -- HaGRIDv2. We cover 15 new gestures with conversation and control functions, including two-handed ones. Building on the foundational concepts proposed by HaGRID's authors, we implemented the dynamic gesture recognition algorithm and further enhanced it by adding three new groups of manipulation gestures. The ``no gesture"" class was diversified by adding samples of natural hand movements, which allowed us to minimize false positives by 6 times. Combining extra samples with HaGRID, the received version outperforms the original in pre-training models for gesture-related tasks. Besides, we achieved the best generalization ability among gesture and hand detection datasets. In addition, the second version enhances the quality of the gestures generated by the diffusion model. HaGRIDv2, pre-trained models, and a dynamic gesture recognition algorithm are publicly available.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01508
"Resonant states of muonic three-particle systems with lithium, helium and hydrogen nuclei",['High Energy Physics - Phenomenology'],"['A. V. Eskin', 'V. I. Korobov', 'A. P. Martynenko', 'F. A. Martynenko']","We study the energy spectrum of three-particle systems (He-p-μ), (He-d-μ), (Li-p-μ) and (Li-d-μ) on the basis of variational approach with exponential and Gaussian basis. Using the Complex Coordinate Rotation (CCR) method we calculate energies of resonant states of listed molecules.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01507
3D Spine Shape Estimation from Single 2D DXA,['Image and Video Processing'],"['Emmanuelle Bourigault', 'Amir Jamaludin', 'Andrew Zisserman']","Scoliosis is traditionally assessed based solely on 2D lateral deviations, but recent studies have also revealed the importance of other imaging planes in understanding the deformation of the spine. Consequently, extracting the spinal geometry in 3D would help quantify these spinal deformations and aid diagnosis. In this study, we propose an automated general framework to estimate the 3D spine shape from 2D DXA scans. We achieve this by explicitly predicting the sagittal view of the spine from the DXA scan. Using these two orthogonal projections of the spine (coronal in DXA, and sagittal from the prediction), we are able to describe the 3D shape of the spine. The prediction is learnt from over 30k paired images of DXA and MRI scans. We assess the performance of the method on a held out test set, and achieve high accuracy.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01504
Windowed Dictionary Design for Delay-Aware OMP Channel Estimation under Fractional Doppler,['Signal Processing'],"['Hanning Wang', 'Xiang Huang', 'Rong-Rong Chen', 'Arman Farhang']","Delay-Doppler (DD) signal processing has emerged as a powerful tool for analyzing multipath and time-varying channel effects. Due to the inherent sparsity of the wireless channel in the DD domain, compressed sensing (CS) based techniques, such as orthogonal matching pursuit (OMP), are commonly used for channel estimation. However, many of these methods assume integer Doppler shifts, which can lead to performance degradation in the presence of fractional Doppler. In this paper, we propose a windowed dictionary design technique while we develop a delay-aware orthogonal matching pursuit (DA-OMP) algorithm that mitigates the impact of fractional Doppler shifts on DD domain channel estimation. First, we apply receiver windowing to reduce the correlation between the columns of our proposed dictionary matrix. Second, we introduce a delay-aware interference block to quantify the interference caused by fractional Doppler. This approach removes the need for a pre-determined stopping criterion, which is typically based on the number of propagation paths, in conventional OMP algorithm. Our simulation results confirm the effective performance of our proposed DA-OMP algorithm using the proposed windowed dictionary in terms of normalized mean square error (NMSE) of the channel estimate. In particular, our proposed DA-OMP algorithm demonstrates substantial gains compared to standard OMP algorithm in terms of channel estimation NMSE with and without windowed dictionary.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01498
RaD: A Metric for Medical Image Distribution Comparison in Out-of-Domain Detection and Other Applications,['Computer Vision and Pattern Recognition'],"['Nicholas Konz', 'Yuwen Chen', 'Hanxue Gu', 'Haoyu Dong', 'Yaqian Chen', 'Maciej A. Mazurowski']","Determining whether two sets of images belong to the same or different domain is a crucial task in modern medical image analysis and deep learning, where domain shift is a common problem that commonly results in decreased model performance. This determination is also important to evaluate the output quality of generative models, e.g., image-to-image translation models used to mitigate domain shift. Current metrics for this either rely on the (potentially biased) choice of some downstream task such as segmentation, or adopt task-independent perceptual metrics (e.g., FID) from natural imaging which insufficiently capture anatomical consistency and realism in medical images. We introduce a new perceptual metric tailored for medical images: Radiomic Feature Distance (RaD), which utilizes standardized, clinically meaningful and interpretable image features. We show that RaD is superior to other metrics for out-of-domain (OOD) detection in a variety of experiments. Furthermore, RaD outperforms previous perceptual metrics (FID, KID, etc.) for image-to-image translation by correlating more strongly with downstream task performance as well as anatomical consistency and realism, and shows similar utility for evaluating unconditional image generation. RaD also offers additional benefits such as interpretability, as well as stability and computational efficiency at low sample sizes. Our results are supported by broad experiments spanning four multi-domain medical image datasets, nine downstream tasks, six image translation models, and other factors, highlighting the broad potential of RaD for medical image analysis.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01496
Understanding complex crowd dynamics with generative neural simulators,['Physics and Society'],"['Koen Minartz', 'Fleur Hendriks', 'Simon Martinus Koop', 'Alessandro Corbetta', 'Vlado Menkovski']","Understanding the dynamics of pedestrian crowds is an outstanding challenge crucial for designing efficient urban infrastructure and ensuring safe crowd management. To this end, both small-scale laboratory and large-scale real-world measurements have been used. However, these approaches respectively lack statistical resolution and parametric controllability, both essential to discovering physical relationships underlying the complex stochastic dynamics of crowds. Here, we establish an investigation paradigm that offers laboratory-like controllability, while ensuring the statistical resolution of large-scale real-world datasets. Using our data-driven Neural Crowd Simulator (NeCS), which we train on large-scale data and validate against key statistical features of crowd dynamics, we show that we can perform effective surrogate crowd dynamics experiments without training on specific scenarios. We not only reproduce known experimental results on pairwise avoidance, but also uncover the vision-guided and topological nature of N-body interactions. These findings show how virtual experiments based on neural simulation enable data-driven scientific discovery.△ Less",v1,https://arxiv.org/pdf/2412.01491
FastRM: An efficient and automatic explainability framework for multimodal generative models,['Artificial Intelligence'],"['Gabriela Ben-Melech Stan', 'Estelle Aflalo', 'Man Luo', 'Shachar Rosenman', 'Tiep Le', 'Sayak Paul', 'Shao-Yen Tseng', 'Vasudev Lal']","While Large Vision Language Models (LVLMs) have become masterly capable in reasoning over human prompts and visual inputs, they are still prone to producing responses that contain misinformation. Identifying incorrect responses that are not grounded in evidence has become a crucial task in building trustworthy AI. Explainability methods such as gradient-based relevancy maps on LVLM outputs can provide an insight on the decision process of models, however these methods are often computationally expensive and not suited for on-the-fly validation of outputs. In this work, we propose FastRM, an effective way for predicting the explainable Relevancy Maps of LVLM models. Experimental results show that employing FastRM leads to a 99.8% reduction in compute time for relevancy map generation and an 44.4% reduction in memory footprint for the evaluated LVLM, making explainable AI more efficient and practical, thereby facilitating its deployment in real-world applications.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01487
Schauder Estimates for Germs by Scaling,['Analysis of PDEs'],"['Jonas Sauer', 'Scott A. Smith']","In this expository note, we show that the blow-up arguments of L. Simon adapt well to the corresponding Schauder theory of germs used in the study of singular SPDEs. We illustrate this through some representative examples. As in the classical PDE framework, the argument relies only on the scaling properties of the germ semi-norms and the Liouville principle for the operator.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01486
Definite Descriptions and Hybrid Tense Logic,['Logic in Computer Science'],"['Andrzej Indrzejczak', 'Michał Zawidzki']","We provide a version of first-order hybrid tense logic with predicate abstracts and definite descriptions as the only non-rigid terms. It is formalised by means of a tableau calculus working on sat-formulas. A particular theory of DD exploited here is essentially based on the approach of Russell, but with descriptions treated as genuine terms. However, the reductionist aspect of the Russellian approach is retained in several ways. Moreover, a special form of tense definite descriptions is formally developed. A constructive proof of the interpolation theorem for this calculus is given, which is an extension of the result provided by Blackburn and Marx.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01484
Discovery of a PRS associated with FRB 20240114A,['High Energy Astrophysical Phenomena'],"['G. Bruni', 'L. Piro', 'Y. -P. Yang', 'E. Palazzi', 'L. Nicastro', 'A. Rossi', 'S. Savaglio', 'E. Maiorano', 'B. Zhang']","We present the discovery of the fourth persistent radio source (PRS) associated with a fast radio burst (FRB). Following previous indications of a candidate PRS associated with FRB20240114A, we performed deep VLBA observations at 5 GHz to test the presence of a compact radio source within the uncertainty position of this FRB ($\pm$200 mas). We detect a component $\sim$50 mas northwards the nominal position provided by the PRECISE collaboration. The corresponding radio luminosity, together with the Faraday rotation measure provided by previous observations of the FRB, locate this PRS in the expected region of the $L$ vs |RM| relation for the nebular model, further supporting it. The comparison of the measured flux density with the respect to the values collected at lower frequency by previous studies, indicates a steepening of the radio spectrum in the 1-3 GHz range and the presence of a possible synchrotron peak at $\sim$1 GHz. Optical observations performed with the LBT could reveal that the FRB and its PRS lie at $\sim$1 kpc from the centre of the host galaxy, which is a dwarf sub-solar metallicity starburst galaxy with SFR $\sim 1 M_\odot\;\mathrm{yr^{-1}}$ and stellar mass $M\sim10^8 M_\odot$.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01478
Dynamic Evolution of Quantum Fisher and Skew Information under Decoherence in Three-Qubit X-States,['Quantum Physics'],"['A. Naimy', 'A. Slaoui', 'A. Ali', 'H. El Hadfi', 'R. Ahl Laamara', 'S. Al-Kuwari']","Quantum metrology leverages quantum effects such as squeezing, entanglement, and other quantum correlations to boost precision in parameter estimation by saturating quantum Cramer Rao bound, which can be achieved by optimizing quantum Fisher information or Wigner-Yanase skew information. This work provides analytical expressions for quantum Fisher and skew information in a general three-qubit X-state and examines their evolution under phase damping, depolarization, and phase-flip decoherence channels. To illustrate the validity of our method, we investigate their dynamics for a three-qubit Greenberger-Horne-Zeilinger (GHZ) state subjected to various memoryless decoherence channels. Closed-form expressions for QFI and SQI are derived for each channel. By comparing these metrics with the entanglement measure of concurrence, we demonstrate the impact of decoherence on measurement precision for quantum metrology. Our results indicate that phase damping and phase-flip channels generally allow for better parameter estimation compared to depolarization. This study provides insights into the optimal selection of noise channels for enhancing precision in quantum metrological tasks involving multi-qubit entangled states.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01473
Time Resolved Absorption of Six Chemical Species With MAROON-X Points to Strong Drag in the Ultra Hot Jupiter TOI-1518 b,['Earth and Planetary Astrophysics'],"['A. Simonnin', 'V. Parmentier', 'J. P. Wardenier', 'G. Chauvin', 'A. Chiavassa', ""M. N'Diaye"", 'X. Tan', 'J. Bean', 'M. Line', 'D. Kitzmann', 'D. Kasper', 'A. Seifhart', 'M. Brogi', 'E. K. H. Lee', 'S. Pelletier', 'L. Pino', 'B. Prinoth', 'J. V. Seidel', 'M. Weiner Mansfield', 'B. Benneke', 'J-M. Désert', 'S. Gandhi', 'M. Hammond', 'P. Palma-Bifani', 'E. Rauscher']","Wind dynamics play a pivotal role in governing transport processes within planetary atmospheres, influencing atmospheric chemistry, cloud formation, and the overall energy budget. Understanding the strength and patterns of winds is crucial for comprehensive insights into the physics of ultra-hot Jupiter atmospheres. Current research has proposed two contrasting mechanisms that limit wind speeds in these atmospheres, each predicting a different scaling of wind speed with planet temperature. However, the sparse nature of existing observations hinders the determination of population trends and the validation of these proposed mechanisms. This study focuses on unraveling the wind dynamics and the chemical composition in the atmosphere of the ultra-hot Jupiter TOI-1518 b. Two transit observations using the high-resolution (Rλ = 85 000), optical (spectral coverage between 490 and 920 nm) spectrograph MAROON-X were obtained and analyzed to explore the chemical composition and wind dynamics using the cross-correlation techniques, global circulating models, and atmospheric retrieval. We report the detection of 14 species in the atmosphere of TOI-1518 b through cross-correlation analysis. Additionally, we measure the time-varying cross-correlation trails for 6 different species, compare them with predictions from General Circulation Models (GCM) and conclude that a strong drag is present in TOI-1518b's atmosphere. The ionized species require stronger drags than neutral species, likely due to the increased magnetic effects in the upper atmosphere. Furthermore, we detect vanadium oxide (VO) using the most up-to-date line list. This result is promising in detecting VO in other systems where inaccuracies in previous line lists have hindered detection. We use a retrieval analysis to further characterize the abundances of the different species detected.△ Less",v1,https://arxiv.org/pdf/2412.01472
Optical excitation of bulk plasmons in n-doped InAsSb thin films : investigating the second viscosity in electron gas,['Optics'],"['Antoine Moreau', 'Émilie Sakat', 'Jean-Paul Hugonin', 'Téo Mottin', 'Aidan Costard', 'Denis Langevin', 'Patricia Loren', 'Laurent Cerutti', 'Fernando Gonzalez Posada Flores', 'Thierry Taliercio']","We demonstrate that including the second viscosity of an electron gas in the hydrodynamic model allows for highly accurate modeling of the optical response of heavily doped semiconductors. In our setup, which improves resonance visibility compared to previous approaches, plasmon resonances become more distinct, allowing for detailed analysis of the underlying physics. With advanced fitting techniques based on a physics-informed cost function and a tailored optimization algorithm, we obtain close agreement between simulations and experimental data across different sample thicknesses. This enhanced resonance visibility, combined with our integrated approach, shows that key parameters such as doping level and effective electron mass can be retrieved from a single optical measurement. The spatial dispersion taken into account in the hydrodynamic framework is essential for accurately describing the optical response of plasmonic materials in this frequency range and is likely to become a standard modeling approach.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01466
A comprehensive review of datasets and deep learning techniques for vision in Unmanned Surface Vehicles,['Computer Vision and Pattern Recognition'],"['Linh Trinh', 'Siegfried Mercelis', 'Ali Anwar']","Unmanned Surface Vehicles (USVs) have emerged as a major platform in maritime operations, capable of supporting a wide range of applications. USVs can help reduce labor costs, increase safety, save energy, and allow for difficult unmanned tasks in harsh maritime environments. With the rapid development of USVs, many vision tasks such as detection and segmentation become increasingly important. Datasets play an important role in encouraging and improving the research and development of reliable vision algorithms for USVs. In this regard, a large number of recent studies have focused on the release of vision datasets for USVs. Along with the development of datasets, a variety of deep learning techniques have also been studied, with a focus on USVs. However, there is a lack of a systematic review of recent studies in both datasets and vision techniques to provide a comprehensive picture of the current development of vision on USVs, including limitations and trends. In this study, we provide a comprehensive review of both USV datasets and deep learning techniques for vision tasks. Our review was conducted using a large number of vision datasets from USVs. We elaborate several challenges and potential opportunities for research and development in USV vision based on a thorough analysis of current datasets and deep learning techniques.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01461
Phaseformer: Phase-based Attention Mechanism for Underwater Image Restoration and Beyond,['Computer Vision and Pattern Recognition'],"['MD Raqib Khan', 'Anshul Negi', 'Ashutosh Kulkarni', 'Shruti S. Phutke', 'Santosh Kumar Vipparthi', 'Subrahmanyam Murala']","Quality degradation is observed in underwater images due to the effects of light refraction and absorption by water, leading to issues like color cast, haziness, and limited visibility. This degradation negatively affects the performance of autonomous underwater vehicles used in marine applications. To address these challenges, we propose a lightweight phase-based transformer network with 1.77M parameters for underwater image restoration (UIR). Our approach focuses on effectively extracting non-contaminated features using a phase-based self-attention mechanism. We also introduce an optimized phase attention block to restore structural information by propagating prominent attentive features from the input. We evaluate our method on both synthetic (UIEB, UFO-120) and real-world (UIEB, U45, UCCS, SQUID) underwater image datasets. Additionally, we demonstrate its effectiveness for low-light image enhancement using the LOL dataset. Through extensive ablation studies and comparative analysis, it is clear that the proposed approach outperforms existing state-of-the-art (SOTA) methods.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01456
Bio-Inspired Adaptive Neurons for Dynamic Weighting in Artificial Neural Networks,['Neural and Evolutionary Computing'],"['Ashhadul Islam', 'Abdesselam Bouzerdoum', 'Samir Brahim Belhaouari']","Traditional neural networks employ fixed weights during inference, limiting their ability to adapt to changing input conditions, unlike biological neurons that adjust signal strength dynamically based on stimuli. This discrepancy between artificial and biological neurons constrains neural network flexibility and adaptability. To bridge this gap, we propose a novel framework for adaptive neural networks, where neuron weights are modeled as functions of the input signal, allowing the network to adjust dynamically in real-time. Importantly, we achieve this within the same traditional architecture of an Artificial Neural Network, maintaining structural familiarity while introducing dynamic adaptability. In our research, we apply Chebyshev polynomials as one of the many possible decomposition methods to achieve this adaptive weighting mechanism, with polynomial coefficients learned during training. Out of the 145 datasets tested, our adaptive Chebyshev neural network demonstrated a marked improvement over an equivalent MLP in approximately 8\% of cases, performing strictly better on 121 datasets. In the remaining 24 datasets, the performance of our algorithm matched that of the MLP, highlighting its ability to generalize standard neural network behavior while offering enhanced adaptability. As a generalized form of the MLP, this model seamlessly retains MLP performance where needed while extending its capabilities to achieve superior accuracy across a wide range of complex tasks. These results underscore the potential of adaptive neurons to enhance generalization, flexibility, and robustness in neural networks, particularly in applications with dynamic or non-linear data dependencies.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01454
PLD+: Accelerating LLM inference by leveraging Language Model Artifacts,['Computation and Language'],"['Shwetha Somasundaram', 'Anirudh Phukan', 'Apoorv Saxena']","To reduce the latency associated with autoretrogressive LLM inference, speculative decoding has emerged as a novel decoding paradigm, where future tokens are drafted and verified in parallel. However, the practical deployment of speculative decoding is hindered by its requirements for additional computational resources and fine-tuning, which limits its out-of-the-box usability. To address these challenges, we present PLD+, a suite of novel algorithms developed to accelerate the inference process of LLMs, particularly for input-guided tasks. These tasks, which include code editing, text editing, summarization, etc., often feature outputs with substantial overlap with their inputs-an attribute PLD+ is designed to exploit. PLD+ also leverages the artifacts (attention and hidden states) generated during inference to accelerate inference speed. We test our approach on five input-guided tasks and through extensive experiments we find that PLD+ outperforms all tuning-free approaches. In the greedy setting, it even outperforms the state-of-the-art tuning-dependent approach EAGLE on four of the tasks. (by a margin of upto 2.31 in terms of avg. speedup). Our approach is tuning free, does not require any additional compute and can easily be used for accelerating inference of any LLM.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01447
The fractional Helly number for separable convexity spaces,['Combinatorics'],"['Andreas F. Holmsen', 'Zuzana Patáková']","A convex lattice set in $\mathbb{Z}^d$ is the intersection of a convex set in $\mathbb{R}^d$ and the integer lattice $\mathbb{Z}^d$. A well-known theorem of Doignon states that the Helly number of $d$-dimensional convex lattice sets equals $2^d$, while a remarkable theorem of Bárány and Matoušek states that the fractional Helly number is only $d+1$. In this paper we generalize their result to abstract convexity spaces that are equipped with a suitable separation property. We also disprove a conjecture of Bárány and Kalai about an existence of fractional Helly property for a family of solutions of bounded-degree polynomial inequalities.△ Less",v1,https://arxiv.org/pdf/2412.01445
LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations,['Artificial Intelligence'],"['Anian Ruoss', 'Fabio Pardo', 'Harris Chan', 'Bonnie Li', 'Volodymyr Mnih', 'Tim Genewein']","Today's largest foundation models have increasingly general capabilities, yet when used as agents, they often struggle with simple reasoning and decision-making tasks, even though they possess good factual knowledge of the task and how to solve it. In this paper, we present a benchmark to pressure-test these models' multimodal decision-making capabilities in the very long-context regime (up to one million tokens) and investigate whether they can learn from a large number of expert demonstrations in their context. We evaluate a wide range of state-of-the-art frontier models as policies across a battery of simple interactive decision-making tasks: playing tic-tac-toe, chess, and Atari, navigating grid worlds, solving crosswords, and controlling a simulated cheetah. We measure the performance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, o1-mini, and o1-preview under increasing amounts of expert demonstrations in the context $\unicode{x2013}$ from no demonstrations up to 512 full episodes, pushing these models' multimodal long-context reasoning capabilities to their limits. Across our tasks, today's frontier models rarely manage to fully reach expert performance, showcasing the difficulty of our benchmark. Presenting more demonstrations often has little effect, but some models steadily improve with more demonstrations on a few tasks. We investigate the effect of encoding observations as text or images and the impact of chain-of-thought prompting. Overall, our results suggest that even today's most capable models often struggle to imitate desired behavior by generalizing purely from in-context demonstrations. To help quantify the impact of other approaches and future innovations aiming to tackle this problem, we open source our benchmark that covers the zero-, few-, and many-shot regimes in a unified evaluation.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01441
Maximal equicontinuous factor and minimal map on finitely suslinean continua,['Dynamical Systems'],['Aymen Daghar'],"In this paper, we introduce the notion of negatively regionally proximal pairs of onto maps which coincides with the set of regionally proximal pair of $f^{-1}$, whenever $f$ is an homeomorphism and we prove the maximal equicontinoues factor for any onto map on a locally connected continuum is monotone. Using this, we prove that if $f$ is a minimal map on a finitely suslinean continua $X$, then $X$ must be a topological circle and $f$ some irrational rotation of circle.△ Less",v1,https://arxiv.org/pdf/2412.01437
Feasibility of Logical Bell State Generation in Memory Assisted Quantum Networks,['Quantum Physics'],"['Vladlen Galetsky', 'Nilesh Vyas', 'Alberto Comin', 'Janis Nötzel']","This study explores the feasibility of utilizing quantum error correction (QEC) to generate and store logical Bell states in heralded quantum entanglement protocols, crucial for quantum repeater networks. Two novel lattice surgery-based protocols (local and non-local) are introduced to establish logical Bell states between distant nodes using an intermediary node. In the local protocol, the intermediary node creates and directly transmits the logical Bell states to quantum memories. In contrast, the non-local protocol distributes auxiliary Bell states, merging boundaries between pre-existing codes in the quantum memories.
  We simulate the protocols using realistic experimental parameters, including cavity-enhanced atomic frequency comb quantum memories and multimode fiber-optic noisy channels. The study evaluates rotated and planar surface codes alongside Bacon-Shor codes for small code distances $(d = 3, 5)$ under standard and realistic noise models. We observe pseudo-thresholds, indicating that when physical error rates exceed approximately $p_{\text{err}} \sim 10^{-3}$, QEC codes do not provide any benefit over using unencoded Bell states. Moreover, to achieve an advantage over unencoded Bell states for a distance of $1 \, \mathrm{km}$ between the end node and the intermediary, gate error rates must be reduced by an order of magnitude $(0.1p_{\text{err}_H}$, $0.1p_{\text{err}_{CX}}$, and $0.1p_{\text{err}_M}$), highlighting the need for significant hardware improvements to implement logical Bell state protocols with quantum memories. Finally, both protocols were analyzed for their achieved rates, with the non-local protocol showing higher rates, ranging from $6.64 \, \mathrm{kHz}$ to $1.91 \, \mathrm{kHz}$, over distances of $1$ to $9 \, \mathrm{km}$ between the end node and the intermediary node.△ Less",v1,https://arxiv.org/pdf/2412.01434
Revisiting Tilt and Tensor-to-Scalar Ratio in the Multi-Scalar Field Inflation,['General Relativity and Quantum Cosmology'],"['Fereshteh Felegary', 'Seyed Ali Hosseini Mansoori', 'Tahere Fallahi Serish', 'Phongpichit Channuie']","The present work investigates the possible range of the spectral index $n_s$ and the tensor-to-scalar ratio $r$ for a sub-class of the generalized multi-scalar field inflation, which includes a linear coupling term between the multi-scalar field potential and the canonical Lagrangian. This coupling influences the slow-roll parameters and also alters our predictions for $n_{s}$ and $r$, which directly depend on those parameters. More precisely, compared to standard multi-field inflation, the values of $n_{s}$ and $r$ decrease to levels consistent with the recent Planck+BICEP/Keck constraint. Interestingly, this validates the chaotic-type potential $V=\sum_{i} μ_{i} φ_{i}^{p}$, which were previously ruled out in the light of the current observations.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01428
Task Adaptation of Reinforcement Learning-based NAS Agents through Transfer Learning,['Machine Learning'],"['Amber Cassimon', 'Siegfried Mercelis', 'Kevin Mets']","Recently, a novel paradigm has been proposed for reinforcement learning-based NAS agents, that revolves around the incremental improvement of a given architecture. We assess the abilities of such reinforcement learning agents to transfer between different tasks. We perform our evaluation using the Trans-NASBench-101 benchmark, and consider the efficacy of the transferred agents, as well as how quickly they can be trained. We find that pretraining an agent on one task benefits the performance of the agent in another task in all but 1 task when considering final performance. We also show that the training procedure for an agent can be shortened significantly by pretraining it on another task. Our results indicate that these effects occur regardless of the source or target task, although they are more pronounced for some tasks than for others. Our results show that transfer learning can be an effective tool in mitigating the computational cost of the initial training procedure for reinforcement learning-based NAS agents.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01420
Chaos Engineering: A Multi-Vocal Literature Review,['Software Engineering'],"['Joshua Owotogbe', 'Indika Kumara', 'Willem-Jan Van Den Heuvel', 'Damian Andrew Tamburri']","Organizations, particularly medium and large enterprises, typically today rely heavily on complex, distributed systems to deliver critical services and products. However, the growing complexity of these systems poses challenges in ensuring service availability, performance, and reliability. Traditional resilience testing methods often fail to capture modern systems' intricate interactions and failure modes. Chaos Engineering addresses these challenges by proactively testing how systems in production behave under turbulent conditions, allowing developers to uncover and resolve potential issues before they escalate into outages. Though chaos engineering has received growing attention from researchers and practitioners alike, we observed a lack of a comprehensive literature review. Hence, we performed a Multivocal Literature Review (MLR) on chaos engineering to fill this research gap by systematically analyzing 88 academic and grey literature sources published from January 2019 to April 2024. We first used the selected sources to derive a unified definition of chaos engineering and to identify key capabilities, components, and adoption drivers. We also developed a taxonomy for chaos engineering and compared the relevant tools using it. Finally, we analyzed the state of the current chaos engineering research and identified several open research issues.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01416
Excitation of quasi-monochromotic waves by a high-voltage pulse in a ferrite coaxial line with the periodic structure,['Accelerator Physics'],"['A. B. Batrakov', 'S. Yu. Karelin', 'O. M. Lebedenko', 'V. S. Mukhin', 'I. N. Onishchenko', 'O. L. Rak', 'V. G. Sinitsin', 'M. V. Volovenko']","Experimental data and results of numerical simulations are presented, concerning excitation of narrowband gigahertz-range wave trains in coaxial guiding structures that are partially filled with ferromagnetic material and may involve periodically arranged metal inserts. The experiments performed confirm the possibility of exciting weakly damped electromagnetic waves by feeding high voltage, unilateral electromagnetic pulses of short duration into the line. The coax line was of outer diameter 50.5 mm, filled with an isotropic dielectric (relative dielectric constant ε = 2.25) and a set of ferrite rings with ε=16 and saturated-state μ about 4 to 5. With a peak voltage of the primary pulse close to 160 kV and a magnetizing field of 17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency 1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01415
Towards Proof-Theoretic Formulation of the General Theory of Term-Forming Operators,['Logic'],['Andrzej Indrzejczak'],"Term-forming operators (tfos), like iota- or epsilon-operator, are technical devices applied to build complex terms in formal languages. Although they are very useful in practice their theory is not well developed. In the paper we provide a proof-theoretic formulation of the general approach to tfos provided independently by several authors like Scott, Hatcher, Corcoran, and compare it with an approach proposed later by Tennant. Eventually it is shown how the general theory can be applied to specific areas like Quine's set theory NF.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01414
Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning,['Computation and Language'],"['Aditya Narayan Sankaran', 'Reza Farahbakhsh', 'Noel Crespi']","Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.△ Less",v1,https://arxiv.org/pdf/2412.01408
Source of powerful x-ray radiation based on a high current pulsed electron accelerator,['Accelerator Physics'],"['A. B. Batrakov', 'I. N. Onishchenko', 'S. I. Fedotov', 'E. G. Glushko', 'O. L. Rak', 'A. O. Zinchenko']",Enhancement of the bremsstrahlung X-ray radiation (BSXR) power generated by the high-current relativistic electron beam (REB) of a pulsed direct-action accelerator TEMP-B is being curried out to use in particular for studying the radiation resistance of walls material of radioactive waste containers. For this the magnetic field in which REB is transported in the drift chamber is taken higher at some distance before the converter to provide REB diameter on the converter the same as in the drift chamber. Scheme of power supply and creation of the required magnetic field for the REB current transportation through the whole distance from the diode to the converter is developed. Higher BSXR dose per REB current pulse at the same energy resource is obtained.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01403
ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting with Multi-View Geometric Consistency,['Computer Vision and Pattern Recognition'],"['Zhuoxiao Li', 'Shanliang Yao', 'Qizhong Gao', 'Angel F. Garcia-Fernandez', 'Yong Yue', 'Xiaohui Zhu']","While Gaussian Splatting (GS) demonstrates efficient and high-quality scene rendering and small area surface extraction ability, it falls short in handling large-scale aerial image surface extraction tasks. To overcome this, we present ULSR-GS, a framework dedicated to high-fidelity surface extraction in ultra-large-scale scenes, addressing the limitations of existing GS-based mesh extraction methods. Specifically, we propose a point-to-photo partitioning approach combined with a multi-view optimal view matching principle to select the best training images for each sub-region. Additionally, during training, ULSR-GS employs a densification strategy based on multi-view geometric consistency to enhance surface extraction details. Experimental results demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on large-scale aerial photogrammetry benchmark datasets, significantly improving surface extraction accuracy in complex urban environments. Project page: https://ulsrgs.github.io.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01402
"Linear stimulus reconstruction works on the KU Leuven audiovisual, gaze-controlled auditory attention decoding dataset",['Signal Processing'],"['Simon Geirnaert', 'Iustina Rotaru', 'Tom Francart', 'Alexander Bertrand']","In a recent paper, we presented the KU Leuven audiovisual, gaze-controlled auditory attention decoding (AV-GC-AAD) dataset, in which we recorded electroencephalography (EEG) signals of participants attending to one out of two competing speakers under various audiovisual conditions. The main goal of this dataset was to disentangle the direction of gaze from the direction of auditory attention, in order to reveal gaze-related shortcuts in existing spatial AAD algorithms that aim to decode the (direction of) auditory attention directly from the EEG. Various methods based on spatial AAD do not achieve significant above-chance performances on our AV-GC-AAD dataset, indicating that previously reported results were mainly driven by eye gaze confounds in existing datasets. Still, these adverse outcomes are often discarded for reasons that are attributed to the limitations of the AV-GC-AAD dataset, such as the limited amount of data to train a working model, too much data heterogeneity due to different audiovisual conditions, or participants allegedly being unable to focus their auditory attention under the complex instructions. In this paper, we present the results of the linear stimulus reconstruction AAD algorithm and show that high AAD accuracy can be obtained within each individual condition and that the model generalizes across conditions, across new subjects, and even across datasets. Therefore, we eliminate any doubts that the inadequacy of the AV-GC-AAD dataset is the primary reason for the (spatial) AAD algorithms failing to achieve above-chance performance when compared to other datasets. Furthermore, this report provides a simple baseline evaluation procedure (including source code) that can serve as the minimal benchmark for all future AAD algorithms evaluated on this dataset.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01401
Navigating Challenges in Spatio-temporal Modelling of Antarctic Krill Abundance: Addressing Zero-inflated Data and Misaligned Covariates,['Applications'],"['André Victor Ribeiro Amaral', 'Adam M. Sykulski', 'Emma Cavan', 'Sophie Fielding']","Antarctic krill (Euphausia superba) are among the most abundant species on our planet and serve as a vital food source for many marine predators in the Southern Ocean. In this paper, we utilise statistical spatio-temporal methods to combine data from various sources and resolutions, aiming to accurately model krill abundance. Our focus lies in fitting the model to a dataset comprising acoustic measurements of krill biomass. To achieve this, we integrate climate covariates obtained from satellite imagery and from drifting surface buoys (also known as drifters). Additionally, we use sparsely collected krill biomass data obtained from net fishing efforts (KRILLBASE) for validation. However, integrating these multiple heterogeneous data sources presents significant modelling challenges, including spatio-temporal misalignment and inflated zeros in the observed data. To address these challenges, we fit a Hurdle-Gamma model to jointly describe the occurrence of zeros and the krill biomass for the non-zero observations, while also accounting for misaligned and heterogeneous data sources, including drifters. Therefore, our work presents a comprehensive framework for analysing and predicting krill abundance in the Southern Ocean, leveraging information from various sources and formats. This is crucial due to the impact of krill fishing, as understanding their distribution is essential for informed management decisions and fishing regulations aimed at protecting the species.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01399
Holistic Understanding of 3D Scenes as Universal Scene Description,['Computer Vision and Pattern Recognition'],"['Anna-Maria Halacheva', 'Yang Miao', 'Jan-Nico Zaech', 'Xi Wang', 'Luc Van Gool', 'Danda Pani Paudel']","3D scene understanding is a long-standing challenge in computer vision and a key component in enabling mixed reality, wearable computing, and embodied AI. Providing a solution to these applications requires a multifaceted approach that covers scene-centric, object-centric, as well as interaction-centric capabilities. While there exist numerous datasets approaching the former two problems, the task of understanding interactable and articulated objects is underrepresented and only partly covered by current works. In this work, we address this shortcoming and introduce (1) an expertly curated dataset in the Universal Scene Description (USD) format, featuring high-quality manual annotations, for instance, segmentation and articulation on 280 indoor scenes; (2) a learning-based model together with a novel baseline capable of predicting part segmentation along with a full specification of motion attributes, including motion type, articulated and interactable parts, and motion parameters; (3) a benchmark serving to compare upcoming methods for the task at hand. Overall, our dataset provides 8 types of annotations - object and part segmentations, motion types, movable and interactable parts, motion parameters, connectivity, and object mass annotations. With its broad and high-quality annotations, the data provides the basis for holistic 3D scene understanding models. All data is provided in the USD format, allowing interoperability and easy integration with downstream tasks. We provide open access to our dataset, benchmark, and method's source code.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01398
Extracting millimeter-wavelength radio emission from a relativistic magnetron,['Accelerator Physics'],"['A. B. Batrakov', 'S. I. Fedotov', 'V. G. Korenev', 'O. M. Lebedenko', 'V. S. Mukhin', 'I. N. Onishchenko', 'O. L. Rak', 'A. O. Shtanko', 'V. G. Sinitsin', 'M. V. Volovenko']","Results of research are presented concerning operative modes of a high-voltage (relativistic) pulsed magnetron for the 8 mm wavelength range. Technical solutions are proposed for improving the output system of the device, such as to increase the efficiency of power extraction from the field-particle interaction space. The stability of the magnetron operation has been increased, enhanced quantitative indices of the output power to 381 kW in each of the two linear polarizations. And a stabilized frequency spectrum from 35.7 to 40 GHz of the EHF radiation generated.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01395
Quasilifting of hulls and depth of tensor product of modules,['Commutative Algebra'],"['Sutapa Dey', 'Amit Tripathi']","We use a quasilift type construction to obtain some bounds on the depth of the tensor product of certain modules over a local $\mathcal{TE}$ ring. We recover a result of Celikbas, Sadeghi, and Takahashi for local complete intersection rings. Some general conditions for lifting hulls and approximations are also studied.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01392
Refined Analysis of Federated Averaging's Bias and Federated Richardson-Romberg Extrapolation,['Machine Learning'],"['Paul Mangold', 'Alain Durmus', 'Aymeric Dieuleveut', 'Sergey Samsonov', 'Eric Moulines']","In this paper, we present a novel analysis of FedAvg with constant step size, relying on the Markov property of the underlying process. We demonstrate that the global iterates of the algorithm converge to a stationary distribution and analyze its resulting bias and variance relative to the problem's solution. We provide a first-order expansion of the bias in both homogeneous and heterogeneous settings. Interestingly, this bias decomposes into two distinct components: one that depends solely on stochastic gradient noise and another on client heterogeneity. Finally, we introduce a new algorithm based on the Richardson-Romberg extrapolation technique to mitigate this bias.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01389
Harnessing Preference Optimisation in Protein LMs for Hit Maturation in Cell Therapy,['Machine Learning'],"['Katarzyna Janocha', 'Annabel Ling', 'Alice Godson', 'Yulia Lampi', 'Simon Bornschein', 'Nils Y. Hammerla']","Cell and immunotherapy offer transformative potential for treating diseases like cancer and autoimmune disorders by modulating the immune system. The development of these therapies is resource-intensive, with the majority of drug candidates failing to progress beyond laboratory testing. While recent advances in machine learning have revolutionised areas such as protein engineering, applications in immunotherapy remain limited due to the scarcity of large-scale, standardised datasets and the complexity of cellular systems. In this work, we address these challenges by leveraging a high-throughput experimental platform to generate data suitable for fine-tuning protein language models. We demonstrate how models fine-tuned using a preference task show surprising correlations to biological assays, and how they can be leveraged for few-shot hit maturation in CARs. This proof-of-concept presents a novel pathway for applying ML to immunotherapy and could generalise to other therapeutic modalities.△ Less",v1,https://arxiv.org/pdf/2412.01388
"Uncertainty propagation and covariance analysis of 181Ta(n,γ)182Ta nuclear reaction",['Nuclear Experiment'],"['Namrata Singh', 'Mahesh Choudhary', 'A. Gandhi', 'Aman Sharma', 'Mahima Upadhyay', 'Punit Dubey', 'Akash Hingu', 'G. Mishra', 'Sukanya De', 'A. Mitra', 'L. S. Danu', 'Ajay Kumar', 'R. G. Thomas', 'Saurav Sood', 'Sajin Prasad', 'A. Kumar']","The neutron capture cross-section for the $^{181}$Ta(n,$γ$)$^{182}$Ta reaction has been experimentally measured at the neutron energies 0.53 and 1.05 MeV using off-line $γ$-ray spectrometry. $^{115}$In(n,n'$γ$)$^{115m}$In is used as a reference monitor reaction cross-section. The neutron was produced via the $^{7}$Li(p,n)$^{7}$Be reaction. The present study measures the cross-sections with their uncertainties and correlation matrix. The self-attenuation process, $γ$-ray correction factor, and low background neutron energy contribution have been calculated. The measured neutron spectrum averaged cross-sections of $^{181}$Ta(n,$γ$)$^{182}$Ta are discussed and compared with the existing data from the EXFOR database and also with the ENDF/B-VIII.0, TENDL-2019, JENDL-5, JEFF-3.3 evaluated data libraries.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01385
Second FRCSyn-onGoing: Winning Solutions and Post-Challenge Analysis to Improve Face Recognition with Synthetic Data,['Computer Vision and Pattern Recognition'],"['Ivan DeAndres-Tame', 'Ruben Tolosana', 'Pietro Melzi', 'Ruben Vera-Rodriguez', 'Minchul Kim', 'Christian Rathgeb', 'Xiaoming Liu', 'Luis F. Gomez', 'Aythami Morales', 'Julian Fierrez', 'Javier Ortega-Garcia', 'Zhizhou Zhong', 'Yuge Huang', 'Yuxi Mi', 'Shouhong Ding', 'Shuigeng Zhou', 'Shuai He', 'Lingzhi Fu', 'Heng Cong', 'Rongyu Zhang', 'Zhihong Xiao', 'Evgeny Smirnov', 'Anton Pimenov', 'Aleksei Grigorev', 'Denis Timoshenko']","Synthetic data is gaining increasing popularity for face recognition technologies, mainly due to the privacy concerns and challenges associated with obtaining real data, including diverse scenarios, quality, and demographic groups, among others. It also offers some advantages over real data, such as the large amount of data that can be generated or the ability to customize it to adapt to specific problem-solving needs. To effectively use such data, face recognition models should also be specifically designed to exploit synthetic data to its fullest potential. In order to promote the proposal of novel Generative AI methods and synthetic data, and investigate the application of synthetic data to better train face recognition systems, we introduce the 2nd FRCSyn-onGoing challenge, based on the 2nd Face Recognition Challenge in the Era of Synthetic Data (FRCSyn), originally launched at CVPR 2024. This is an ongoing challenge that provides researchers with an accessible platform to benchmark i) the proposal of novel Generative AI methods and synthetic data, and ii) novel face recognition systems that are specifically proposed to take advantage of synthetic data. We focus on exploring the use of synthetic data both individually and in combination with real data to solve current challenges in face recognition such as demographic bias, domain adaptation, and performance constraints in demanding situations, such as age disparities between training and testing, changes in the pose, or occlusions. Very interesting findings are obtained in this second edition, including a direct comparison with the first one, in which synthetic databases were restricted to DCFace and GANDiffFace.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01383
Magnonic Fabry-Pérot resonators as programmable phase shifters,['Materials Science'],"['Anton Lutsenko', 'Kevin G. Fripp', 'Lukáš Flajšman', 'Andrey V. Shytov', 'Volodymyr V. Kruglyak', 'Sebastiaan van Dijken']","We explore the use of magnonic Fabry-Pérot resonators as programmable phase shifters for spin-wave computing. The resonator, composed of a yttrium iron garnet (YIG) film coupled with a CoFeB nanostripe, operates through dynamic dipolar coupling, leading to wavelength downconversion and the formation of a magnonic cavity. Using super-Nyquist sampling magneto-optical Kerr effect (SNS-MOKE) microscopy and micromagnetic simulations, we demonstrate that these resonators can induce a $π$ phase shift in the transmitted spin wave. The phase shift is highly sensitive to the magnetization alignment within the resonator, allowing for on-demand control via magnetic switching. This feature, combined with low-loss transmission, positions the magnonic Fabry-Pérot resonator as a promising component for reconfigurable magnonic circuits and spin-wave computing devices.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01382
Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking,['Machine Learning'],"['Marco Federici', 'Davide Belli', 'Mart van Baalen', 'Amir Jalalirad', 'Andrii Skliar', 'Bence Major', 'Markus Nagel', 'Paul Whatmough']","While mobile devices provide ever more compute power, improvements in DRAM bandwidth are much slower. This is unfortunate for large language model (LLM) token generation, which is heavily memory-bound. Previous work has proposed to leverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce effective DRAM bandwidth per token. However, more recent LLMs use SwiGLU instead of ReLU, which result in little inherent sparsity. While SwiGLU activations can be pruned based on magnitude, the resulting sparsity patterns are difficult to predict, rendering previous approaches ineffective. To circumvent this issue, our work introduces Dynamic Input Pruning (DIP): a predictor-free dynamic sparsification approach, which preserves accuracy with minimal fine-tuning. DIP can further use lightweight LoRA adapters to regain some performance lost during sparsification. Lastly, we describe a novel cache-aware masking strategy, which considers the cache state and activation magnitude to further increase cache hit rate, improving LLM token rate on mobile devices. DIP outperforms other methods in terms of accuracy, memory and throughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP achieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1 loss in perplexity.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01380
A Survey on Deep Neural Networks in Collaborative Filtering Recommendation Systems,['Artificial Intelligence'],"['Pang Li', 'Shahrul Azman Mohd Noah', 'Hafiz Mohd Sarim']","This survey provides an examination of the use of Deep Neural Networks (DNN) in Collaborative Filtering (CF) recommendation systems. As the digital world increasingly relies on data-driven approaches, traditional CF techniques face limitations in scalability and flexibility. DNNs can address these challenges by effectively modeling complex, non-linear relationships within the data. We begin by exploring the fundamental principles of both collaborative filtering and deep neural networks, laying the groundwork for understanding their integration. Subsequently, we review key advancements in the field, categorizing various deep learning models that enhance CF systems, including Multilayer Perceptrons (MLP), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Graph Neural Networks (GNN), autoencoders, Generative Adversarial Networks (GAN), and Restricted Boltzmann Machines (RBM). The paper also discusses evaluation protocols, various publicly available auxiliary information, and data features. Furthermore, the survey concludes with a discussion of the challenges and future research opportunities in enhancing collaborative filtering systems with deep learning.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01378
Convolutional Transformer Neural Collaborative Filtering,['Artificial Intelligence'],"['Pang Li', 'Shahrul Azman Mohd Noah', 'Hafiz Mohd Sarim']","In this study, we introduce Convolutional Transformer Neural Collaborative Filtering (CTNCF), a novel approach aimed at enhancing recommendation systems by effectively capturing high-order structural information in user-item interactions. CTNCF represents a significant advancement over the traditional Neural Collaborative Filtering (NCF) model by seamlessly integrating Convolutional Neural Networks (CNNs) and Transformer layers. This sophisticated integration enables the model to adeptly capture and understand complex interaction patterns inherent in recommendation systems. Specifically, CNNs are employed to extract local features from user and item embeddings, allowing the model to capture intricate spatial dependencies within the data. Furthermore, the utilization of Transformer layers enables the model to capture long-range dependencies and interactions among user and item features, thereby enhancing its ability to understand the underlying relationships in the data. To validate the effectiveness of our proposed CTNCF framework, we conduct extensive experiments on two real-world datasets. The results demonstrate that CTNCF significantly outperforms state-of-the-art approaches, highlighting its efficacy in improving recommendation system performance.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01376
The chromatic number of random graphs: an approach using a recurrence relation,['Combinatorics'],"['Yayoi Abe', 'Ayuna Setoh', 'Gen Yoneda']","Finding the chromatic number of large graphs is known to be NP-hard. Although various algorithms have been developed to efficiently compute chromatic numbers, they still take an enormous amount of time for large graphs. In this paper, we propose the recurrence relation to obtain the expected value of the chromatic number of random graphs in a short time.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01374
Hierarchical VAE with a Diffusion-based VampPrior,['Machine Learning'],"['Anna Kuzina', 'Jakub M. Tomczak']","Deep hierarchical variational autoencoders (VAEs) are powerful latent variable generative models. In this paper, we introduce Hierarchical VAE with Diffusion-based Variational Mixture of the Posterior Prior (VampPrior). We apply amortization to scale the VampPrior to models with many stochastic layers. The proposed approach allows us to achieve better performance compared to the original VampPrior work and other deep hierarchical VAEs, while using fewer parameters. We empirically validate our method on standard benchmark datasets (MNIST, OMNIGLOT, CIFAR10) and demonstrate improved training stability and latent space utilization.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01373
An overview of diffusion models for generative artificial intelligence,['Machine Learning'],"['Davide Gallon', 'Arnulf Jentzen', 'Philippe von Wurstemberger']","This article provides a mathematically rigorous introduction to denoising diffusion probabilistic models (DDPMs), sometimes also referred to as diffusion probabilistic models or diffusion models, for generative artificial intelligence. We provide a detailed basic mathematical framework for DDPMs and explain the main ideas behind training and generation procedures. In this overview article we also review selected extensions and improvements of the basic framework from the literature such as improved DDPMs, denoising diffusion implicit models, classifier-free diffusion guidance models, and latent diffusion models.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01371
Understanding the World's Museums through Vision-Language Reasoning,['Computer Vision and Pattern Recognition'],"['Ada-Astrid Balauca', 'Sanjana Garai', 'Stefan Balauca', 'Rasesh Udayakumar Shetty', 'Naitik Agrawal', 'Dhwanil Subhashbhai Shah', 'Yuqian Fu', 'Xi Wang', 'Kristina Toutanova', 'Danda Pani Paudel', 'Luc Van Gool']","Museums serve as vital repositories of cultural heritage and historical artifacts spanning diverse epochs, civilizations, and regions, preserving well-documented collections. Data reveal key attributes such as age, origin, material, and cultural significance. Understanding museum exhibits from their images requires reasoning beyond visual features. In this work, we facilitate such reasoning by (a) collecting and curating a large-scale dataset of 65M images and 200M question-answer pairs in the standard museum catalog format for exhibits from all around the world; (b) training large vision-language models on the collected dataset; (c) benchmarking their ability on five visual question answering tasks. The complete dataset is labeled by museum experts, ensuring the quality as well as the practical significance of the labels. We train two VLMs from different categories: the BLIP model, with vision-language aligned embeddings, but lacking the expressive power of large language models, and the LLaVA model, a powerful instruction-tuned LLM enriched with vision-language reasoning capabilities. Through exhaustive experiments, we provide several insights on the complex and fine-grained understanding of museum exhibits. In particular, we show that some questions whose answers can often be derived directly from visual features are well answered by both types of models. On the other hand, questions that require the grounding of the visual features in repositories of human knowledge are better answered by the large vision-language models, thus demonstrating their superior capacity to perform the desired reasoning. Find our dataset, benchmarks, and source code at: https://github.com/insait-institute/Museum-65△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01370
Observation of an Extraordinary Type V Solar Radio Burst: Nonlinear Evolution of the Electron Two-Stream Instability,['Solar and Stellar Astrophysics'],"['Arnold O. Benz', 'Clemens R. Huber', 'Vincenzo Timmel', 'Christian Monstein']","Solar type V radio bursts are associated with type III bursts. Several processes have been proposed to interpret the association, electron distribution, and emission. We present the observation of a unique type V event observed by e-CALLISTO on 7 May 2021. The type V radio emission follows a group of U bursts. Unlike the unpolarized U bursts, the type V burst is circularly polarized, leaving room for a different emission process. Its starting edge drifts to higher frequency four times slower than the descending branch of the associated U burst. The type V processes seem to be ruled by electrons of lower energy. The observations conform to a coherent scenario where a dense electron beam drives the two-stream instability (causing type III emission) and, in the nonlinear stage, becomes unstable to another instability, previously known as the electron firehose instability (EFI). The secondary instability scatters some beam electrons into velocities perpendicular to the magnetic field and produces, after particle loss, a trapped distribution prone to electron cyclotron masering (ECM). A reduction in beaming and the formation of an isotropic halo are predicted for electron beams continuing to interplanetary space, possibly observable by Parker Solar Probe and Solar Orbiter.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01366
A Bottom-Up Approach to Optimizing the Solar Organic Rankine Cycle for Transactive Energy Trading,['Optimization and Control'],"['Silvia Anna Cordieri', 'Chiara Bordin', 'Sambeet Mishra']","Solar Organic Rankine Cycle (ORC)-based power generation plants leverage solar irradiation to produce thermal energy, offering a highly compatible renewable technology due to the alignment between solar irradiation temperatures and ORC operating requirements. Their superior performance compared to steam Rankine cycles in small-scale applications makes them particularly relevant within the smart grid and microgrid contexts. This study explores the role of ORC in peer-to-peer (P2P) energy trading within renewable-based community microgrids, where consumers become prosumers, simultaneously producing and consuming energy while engaging in virtual trading at the distribution system level. Focusing on a microgrid integrating solar ORC with a storage system to meet consumer demand, the paper highlights the importance of combining these technologies with storage to enhance predictability and competitiveness with conventional energy plants, despite management challenges. A methodology based on operations research techniques is developed to optimize system performance. Furthermore, the impact of various technological parameters of the solar ORC on the system's performance is examined. The study concludes by assessing the value of solar ORC within the transactive energy trading framework across different configurations and scenarios. Results demonstrate an average 16\% reduction in operational costs, showcasing the benefits of implementing a predictable and manageable system in P2P transactive energy trading.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01359
The ionizing photon production efficiency of star-forming galaxies at $z\sim 4-10$,['Astrophysics of Galaxies'],"['M. Llerena', 'L. Pentericci', 'L. Napolitano', 'S. Mascia', 'R. Amorín', 'A. Calabrò', 'M. Castellano', 'N. J. Cleri', 'M. Giavalisco', 'N. A. Grogin', 'N. P. Hathi', 'M. Hirschmann', 'A. M. Koekemoer', 'T. Nanayakkara', 'F. Pacucci', 'L. Shen', 'S. M. Wilkins', 'I. Yoon', 'L. Y. A. Yung', 'R. Bhatawdekar', 'R. A. Lucas', 'X. Wang', 'P. Arrabal Haro', 'M. B. Bagley', 'S. L. Finkelstein']","Investigating the ionizing emission of star-forming galaxies is critical to understanding their contribution to reionization and their impact on the surrounding environment. The number of ionizing photons available to reionize the intergalactic medium (IGM) depends not only on the abundance of galaxies but also on their efficiency in producing ionizing photons ($ξ_{ion}$). We aim to estimate the $ξ_{ion}$ using Balmer lines in a sample of 731 galaxies at $4\leq z \leq 10$ selected from different JWST surveys. We used the available HST and JWST photometry to perform a SED fitting in the sample to determine their physical properties. We used the BAGPIPES code and assumed a delayed exponential model for the star formation history. We used the NIRSpec spectra from prism or grating configurations to estimate Balmer luminosities and then constrained $ξ_{ion}$ values after dust correction. We find a mean value of 10$^{25.23}$Hz erg$^{-1}$ for $ξ_{ion}$ in the sample with an observed scatter of 0.42dex. We find an increase in the median values of $ξ_{ion}$ which confirms the redshift evolution of $ξ_{ion}$ found in other works. Regarding the relation with physical properties, we find a decrease of $ξ_{ion}$ with increasing stellar mass, indicating that low-mass galaxies are efficient producers of ionizing photons. We also find an increase of $ξ_{ion}$ with increasing specific star formation rate (sSFR) and increasing UV absolute magnitude, which indicates that faint galaxies and with high sSFR are also efficient producers. We also investigated the relation of $ξ_{ion}$ with the EW([OIII]$λ$5007) and find that galaxies with the higher EW([OIII]) are the more efficient producers of ionizing photons. Similarly, we find that galaxies with higher O32 ratios and lower gas-phase metallicities (based on the R23 calibration) show higher $ξ_{ion}$ values.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01358
Integrative CAM: Adaptive Layer Fusion for Comprehensive Interpretation of CNNs,['Computer Vision and Pattern Recognition'],"['Aniket K. Singh', 'Debasis Chaudhuri', 'Manish P. Singh', 'Samiran Chattopadhyay']","With the growing demand for interpretable deep learning models, this paper introduces Integrative CAM, an advanced Class Activation Mapping (CAM) technique aimed at providing a holistic view of feature importance across Convolutional Neural Networks (CNNs). Traditional gradient-based CAM methods, such as Grad-CAM and Grad-CAM++, primarily use final layer activations to highlight regions of interest, often neglecting critical features derived from intermediate layers. Integrative CAM addresses this limitation by fusing insights across all network layers, leveraging both gradient and activation scores to adaptively weight layer contributions, thus yielding a comprehensive interpretation of the model's internal representation. Our approach includes a novel bias term in the saliency map calculation, a factor frequently omitted in existing CAM techniques, but essential for capturing a more complete feature importance landscape, as modern CNNs rely on both weighted activations and biases to make predictions. Additionally, we generalize the alpha term from Grad-CAM++ to apply to any smooth function, expanding CAM applicability across a wider range of models. Through extensive experiments on diverse and complex datasets, Integrative CAM demonstrates superior fidelity in feature importance mapping, effectively enhancing interpretability for intricate fusion scenarios and complex decision-making tasks. By advancing interpretability methods to capture multi-layered model insights, Integrative CAM provides a valuable tool for fusion-driven applications, promoting the trustworthy and insightful deployment of deep learning models.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01354
Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk through Social Media using Base Language Models,['Human-Computer Interaction'],"['Chayan Tank', 'Shaina Mehta', 'Sarthak Pol', 'Vinayak Katoch', 'Avinash Anand', 'Raj Jaiswal', 'Rajiv Ratn Shah']","In recent times, more and more people are posting about their mental states across various social media platforms. Leveraging this data, AI-based systems can be developed that help in assessing the mental health of individuals, such as suicide risk. This paper is a study done on suicidal risk assessments using Reddit data leveraging Base language models to identify patterns from social media posts. We have demonstrated that using smaller language models, i.e., less than 500M parameters, can also be effective in contrast to LLMs with greater than 500M parameters. We propose Su-RoBERTa, a fine-tuned RoBERTa on suicide risk prediction task that utilized both the labeled and unlabeled Reddit data and tackled class imbalance by data augmentation using GPT-2 model. Our Su-RoBERTa model attained a 69.84% weighted F1 score during the Final evaluation. This paper demonstrates the effectiveness of Base language models for the analysis of the risk factors related to mental health with an efficient computation pipeline△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01353
A multi-criteria decision support system to evaluate the effectiveness of training courses on citizens' employability,['Computers and Society'],"['Maria C. Bas', 'Vicente J. Bolos', 'Alvaro E. Prieto', 'Roberto Rodriguez-Echeverria', 'Fernando Sanchez-Figueroa']","This study examines the impact of lifelong learning on the professional lives of employed and unemployed individuals. Lifelong learning is a crucial factor in securing employment or enhancing one's existing career prospects. To achieve this objective, this study proposes the implementation of a multi-criteria decision support system for the evaluation of training courses in accordance with their capacity to enhance the employability of the students. The methodology is delineated in four stages. Firstly, a `working life curve' was defined to provide a quantitative description of an individual's working life. Secondly, an analysis based on K-medoids clustering defined a control group for each individual for comparison. Thirdly, the performance of a course according to each of the four predefined criteria was calculated using a t-test to determine the mean performance value of those who took the course. Ultimately, the unweighted TOPSIS method was used to evaluate the efficacy of the various training courses in relation to the four criteria. This approach effectively addresses the challenge of using extensive datasets within a system while facilitating the application of a multi-criteria unweighted TOPSIS method. The results of the multi-criteria TOPSIS method indicated that training courses related to the professional fields of administration and management, hostel and tourism and community and sociocultural services have positive impact on employability and improving the working conditions of citizens. However, courses that demonstrate the greatest effectiveness in ranking are the least demanded by citizens. The results will help policymakers evaluate the effectiveness of each training course offered by the regional government.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01351
Enhancing multiscale simulations for spark plasma sintering with a novel Direct FE$^2$ framework,['Computational Physics'],"['A. Kumar', 'Z. Zhang', 'M. Bambach', 'M. Afrasiabi']","The spark plasma sintering (SPS) process, a key technology for advanced material manufacturing, demands accurate and efficient simulation tools to capture the complex electro-thermal-mechanical interactions inherent in powder materials. This paper introduces a novel concurrent multiscale framework employing the Direct FE$^2$ method, designed for fully coupled electro-thermal-mechanical simulations in SPS. The model integrates microscale powder characteristics into a macroscopic analysis through multi-point constraints within a 3D finite element (FE) solver. This approach enables, for the first time, a direct and seamless coupling of micro- and macroscale physical phenomena, enhancing both accuracy and computational efficiency by capturing interactions across scales. The proposed method achieves a temperature and displacement error margin below 1% compared to full FE analysis while reducing computational degrees of freedom by a factor of 8, resulting in a 70-fold acceleration in simulation time. Additionally, the methodology provides robust flexibility in accommodating diverse powder morphologies without compromising precision, enabling degree-of-freedom reductions of up to 44 times. This combination of enhanced efficiency and accuracy establishes the proposed Direct FE$^2$ approach as a highly effective tool for realistic and scalable simulations of the SPS process.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01350
Gauge invariant perturbations in teleparallel Horndeski gravity,['General Relativity and Quantum Cosmology'],"['Bobomurat Ahmedov', 'Maria Caruana', 'Jackson Levi Said', 'Konstantinos F. Dialektopoulos', 'Abdurakhmon Nosirov', 'Zinovia Oikonomopoulou', 'Odil Yunusov']","We present in the form of a catalogue of the cosmological perturbations within the Bahamonde- Dialektopoulos-Levi Said (BDLS) theory, which serves as the teleparallel counterpart of Horndeski gravity. To understand structure formation in cosmological models, it is essential to study both the background and perturbative aspects of their cosmology. While extensive analysis of both Horndeski gravity and its teleparallel analog exists in the literature, a quantitative understanding requires a detailed examination of their cosmological perturbations. We review here all the different gauges for the scalar, vector and tensor perturbations of a cosmological background up to second order and we hope this will help people who work with observations, to incorporate it in existing codes.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01349
Hierarchical Object-Oriented POMDP Planning for Object Rearrangement,['Machine Learning'],"['Rajesh Mangannavar', 'Alan Fern', 'Prasad Tadepalli']","We present an online planning framework for solving multi-object rearrangement problems in partially observable, multi-room environments. Current object rearrangement solutions, primarily based on Reinforcement Learning or hand-coded planning methods, often lack adaptability to diverse challenges. To address this limitation, we introduce a novel Hierarchical Object-Oriented Partially Observed Markov Decision Process (HOO-POMDP) planning approach. This approach comprises of (a) an object-oriented POMDP planner generating sub-goals, (b) a set of low-level policies for sub-goal achievement, and (c) an abstraction system converting the continuous low-level world into a representation suitable for abstract planning. We evaluate our system on varying numbers of objects, rooms, and problem types in AI2-THOR simulated environments with promising results.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01348
Data-Driven and Stealthy Deactivation of Safety Filters,['Systems and Control'],"['Daniel Arnström', 'André M. H. Teixeira']","Safety filters ensure that control actions that are executed are always safe, no matter the controller in question. Previous work has proposed a simple and stealthy false-data injection attack for deactivating such safety filters. This attack injects false sensor measurements to bias state estimates toward the interior of a safety region, making the safety filter accept unsafe control actions. The attack does, however, require the adversary to know the dynamics of the system, the safety region used in the safety filter, and the observer gain. In this work we relax these requirements and show how a similar data-injection attack can be performed when the adversary only observes the input and output of the observer that is used by the safety filter, without any a priori knowledge about the system dynamics, safety region, or observer gain. In particular, the adversary uses the observed data to identify a state-space model that describes the observer dynamics, and then approximates a safety region in the identified embedding. We exemplify the data-driven attack on an inverted pendulum, where we show how the attack can make the system leave a safe set, even when a safety filter is supposed to stop this from happening.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01346
See What You Seek: Semantic Contextual Integration for Cloth-Changing Person Re-Identification,['Computer Vision and Pattern Recognition'],"['Xiyu Han', 'Xian Zhong', 'Wenxin Huang', 'Xuemei Jia', 'Wenxuan Liu', 'Xiaohan Yu', 'Alex Chichung Kot']","Cloth-changing person re-identification (CC-ReID) aims to match individuals across multiple surveillance cameras despite variations in clothing. Existing methods typically focus on mitigating the effects of clothing changes or enhancing ID-relevant features but often struggle to capture complex semantic information. In this paper, we propose a novel prompt learning framework, Semantic Contextual Integration (SCI), for CC-ReID, which leverages the visual-text representation capabilities of CLIP to minimize the impact of clothing changes and enhance ID-relevant features. Specifically, we introduce Semantic Separation Enhancement (SSE) module, which uses dual learnable text tokens to separately capture confounding and clothing-related semantic information, effectively isolating ID-relevant features from distracting clothing semantics. Additionally, we develop a Semantic-Guided Interaction Module (SIM) that uses orthogonalized text features to guide visual representations, sharpening the model's focus on distinctive ID characteristics. This integration enhances the model's discriminative power and enriches the visual context with high-dimensional semantic insights. Extensive experiments on three CC-ReID datasets demonstrate that our method outperforms state-of-the-art techniques. The code will be released at github.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01345
A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls,['Computation and Language'],"['Sheikh Shafayat', 'Dongkeun Yoon', 'Woori Jang', 'Jiwoo Choi', 'Alice Oh', 'Seohyon Jung']","In this work, we propose and evaluate the feasibility of a two-stage pipeline to evaluate literary machine translation, in a fine-grained manner, from English to Korean. The results show that our framework provides fine-grained, interpretable metrics suited for literary translation and obtains a higher correlation with human judgment than traditional machine translation metrics. Nonetheless, it still fails to match inter-human agreement, especially in metrics like Korean Honorifics. We also observe that LLMs tend to favor translations generated by other LLMs, and we highlight the necessity of developing more sophisticated evaluation methods to ensure accurate and culturally sensitive machine translation of literary works.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01340
Radiative neutron capture cross section of $^{242}$Pu measured at n_TOF-EAR1 in the unresolved resonance region up to 600 keV,['Nuclear Experiment'],"['J. Lerendegui-Marco', 'C. Guerrero', 'E. Mendoza', 'J. M. Quesada', 'K. Eberhardt', 'A. R. Junghans', 'V. Alcayne', 'V. Babiano', 'O. Aberle', 'J. Andrzejewski', 'L. Audouin', 'V. Becares', 'M. Bacak', 'J. Balibrea-Correa', 'M. Barbagallo', 'S. Barros', 'F. Becvar', 'C. Beinrucker', 'E. Berthoumieux', 'J. Billowes', 'D. Bosnar', 'M. Brugger', 'M. Caamaño', 'F. Calviño', 'M. Calviani']","The design of fast reactors burning MOX fuels requires accurate capture and fission cross sections. For the particular case of neutron capture on 242Pu, the NEA recommends that an accuracy of 8-12% should be achieved in the fast energy region (2 keV-500 keV) compared to their estimation of 35% for the current uncertainty. Integral irradiation experiments suggest that the evaluated cross section of the JEFF-3.1 library overestimates the 242Pu(n,γ) cross section by 14% in the range between 1 keV and 1 MeV. In addition, the last measurement at LANSCE reported a systematic reduction of 20-30% in the 1-40 keV range relative to the evaluated libraries and previous data sets. In the present work this cross section has been determined up to 600 keV in order to solve the mentioned discrepancies. A 242Pu target of 95(4) mg enriched to 99.959% was irradiated at the n TOF-EAR1 facility at CERN. The capture cross section of 242Pu has been obtained between 1 and 600 keV with a systematic uncertainty (dominated by background subtraction) between 8 and 12%, reducing the current uncertainties of 35% and achieving the accuracy requested by the NEA in a large energy range. The shape of the cross section has been analyzed in terms of average resonance parameters using the FITACS code as implemented in SAMMY, yielding results compatible with our recent analysis of the resolved resonance region.The results are in good agreement with the data of Wisshak and Käppeler and on average 10-14% below JEFF-3.2 from 1 to 250 keV, which helps to achieve consistency between integral experiments and cross section data. At higher energies our results show a reasonable agreement within uncertainties with both ENDF/B-VII.1 and JEFF-3.2. Our results indicate that the last experiment from DANCE underestimates the capture cross section of 242Pu by as much as 40% above a few keV.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01332
Exploring Long-Term Prediction of Type 2 Diabetes Microvascular Complications,['Machine Learning'],"['Elizabeth Remfry', 'Rafael Henkin', 'Michael R Barnes', 'Aakanksha Naik']","Electronic healthcare records (EHR) contain a huge wealth of data that can support the prediction of clinical outcomes. EHR data is often stored and analysed using clinical codes (ICD10, SNOMED), however these can differ across registries and healthcare providers. Integrating data across systems involves mapping between different clinical ontologies requiring domain expertise, and at times resulting in data loss. To overcome this, code-agnostic models have been proposed. We assess the effectiveness of a code-agnostic representation approach on the task of long-term microvascular complication prediction for individuals living with Type 2 Diabetes. Our method encodes individual EHRs as text using fine-tuned, pretrained clinical language models. Leveraging large-scale EHR data from the UK, we employ a multi-label approach to simultaneously predict the risk of microvascular complications across 1-, 5-, and 10-year windows. We demonstrate that a code-agnostic approach outperforms a code-based model and illustrate that performance is better with longer prediction windows but is biased to the first occurring complication. Overall, we highlight that context length is vitally important for model performance. This study highlights the possibility of including data from across different clinical ontologies and is a starting point for generalisable clinical models.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01331
Remote Sensing with High Spatial Resolution,['Signal Processing'],"['André Sandmann', 'Florian Azendorf', 'Michael Eiselt']",Distributed fiber sensing based on correlation-aided phase-sensitive optical time domain reflectometry is presented. The focus is on correlation as an enabler for high spatial resolution. Results from different applications are presented.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01325
Pressure Wave Detection and Localization in Deployed Underground Fiber using Coherent Correlation OTDR,['Signal Processing'],"['Florian Azendorf', 'André Sandmann', 'Michael Eiselt']",A deployed fiber with in-house and underground sections is interrogated with a coherent correlation OTDR. The origin and propagation speed of a hammer-generated pressure wave in the underground section is detected and acoustic signals are monitored.△ Less,"2 December, 2024;",https://arxiv.org/pdf/2412.01320
Can ChatGPT pass a physics degree? Making a case for reformation of assessment of undergraduate degrees,['Physics Education'],"['Kevin A. Pimbblet', 'Lesley J. Morrell']","The emergence of conversational natural language processing models presents a significant challenge for Higher Education. In this work, we use the entirety of a UK physics undergraduate (BSc with Honours) degree including all examinations and coursework to test if ChatGPT (GPT-4) can pass a degree. We adopt a ""maximal cheating"" approach wherein we permit ourselves to modify questions for clarity, split questions up into smaller sub-components, expand on answers given - especially for long form written responses, obtaining references, and use of advanced coaching, plug-ins and custom instructions to optimize outputs. In general, there are only certain parts of the degree in question where GPT-4 fails. Explicitly these include compulsory laboratory elements, and the final project which is assessed by a viva. If these were no issue, then GPT-4 would pass with a grade of an upper second class overall. In general, coding tasks are performed exceptionally well, along with simple single-step solution problems. Multiple step problems and longer prose are generally poorer along with interdisciplinary problems. We strongly suggest that there is now a necessity to urgently re-think and revise assessment practice in physics - and other disciplines - due to the existence of AI such as GPT-4. We recommend close scrutiny of assessment tasks: only invigilated in-person examinations, vivas, laboratory skills testing (or ""performances"" in other disciplines), and presentations are not vulnerable to GPT-4, and urge consideration of how AI can be embedded within the disciplinary context.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01312
XY Factorization Bias in Luminosity Measurements,['High Energy Physics - Experiment'],"['Anna Fehérkuti', 'Péter Major', 'Gabriella Pásztor']","For most high-precision experiments in particle physics, it is essential to know the luminosity at highest accuracy. The luminosity is determined by the convolution of particle densities of the colliding beams. In special van der Meer transverse beam separation scans, the convolution function is sampled along the horizontal and vertical axes with the purpose of determining the beam convolution and getting an absolute luminosity calibration. For this purpose, the van der Meer data of luminometer rates are separately fitted in the two directions with analytic functions giving the best description. With the assumption that the 2D convolution shape is factorizable, one can calculate it from the two 1D fits. The task of XY factorization analyses is to check this assumption and give a quantitative measure of the effect of nonfactorizability on the calibration constant to improve the accuracy of luminosity measurements. \newline We perform a dedicated analysis to study XY non-factorization on proton-proton data collected in 2022 at $\sqrt{s} = 13.6$~TeV by the CMS experiment. A detailed examination of the shape of the bunch convolution function is presented, studying various biases, and choosing the best-fit analytic 2D functions to finally obtain the correction and its uncertainty.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01310
Monitoring of food spoilage by high resolution THz analysis,['Instrumentation and Detectors'],"['Francis Hindle', 'Lotta Kuuliala', 'Meriem Mouelhi', 'Arnaud Cuisset', 'Cédric Bray', 'Mathias Vanwolleghem', 'Frank Devlieghere', 'Gael Mouret', 'Robin Bocquet']","High resolution rotational Terahertz (THz) spectroscopy has been widely applied to the studies of numerous polar gas phase molecules, in particular volatile organic compounds (VOCs). During the storage of foodstuffs packed under a protective atmosphere, microbial activity will lead to the generation of a complex mixture of trace gases that could be used as food spoilage indicators. Here we have demonstrated that the THz instrumentation presently available provides sufficient sensitivity and selectivity to monitor the generation of hydrogen sulfide (H2S) in the headspace of packed Atlantic salmon (Salmo salar) fillet portions. A comprehensive comparison was made by selective-ion flow-tube mass spectrometry (SIFT-MS) in order to validate the THz measurements and protocol. The detectivity of a range of alternative compounds for this application is also provided, based on the experimental detection limit observed and molecular spectroscopic properties. Molecules like ethanol, methyl mercaptan and ammonia are suitable indicators with the presently available sensitivity levels, while dimethyl sulfide, acetone and butanone may be considered with a sensitivity improvement of 2 orders of magnitude.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01309
Terahertz Frequency Comb High-Resolution Heterodyne Spectrometer,['Instrumentation and Detectors'],"['Francis Hindle', 'Alexandra Khabbaz', 'Anthony Roucou', 'Jean-Francois Lampin', 'Gaël Mouret']","We demonstrate the advantages of THz frequency combs for high-resolution spectroscopy. This benefits from wide spectral coverage and the exact knowledge of the frequency position of each comb component. Heterodyne detection combined with a fast Fourier spectrometer enables rapid and simultaneous measurement of more than 80 frequency comb modes covering a 7.5 GHz bandwidth. A spectrum is obtained in under 20 minutes yielding a uniform resolution of 70 kHz. This new setup has been validated by recording more than 150 lines of methanol around 723 GHz, and represents a new solution to exploit THz frequency combs for high-resolution spectroscopy.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01308
"Full 3D Model of Modulation Efficiency of Complementary Metal Oxide Semiconductor (CMOS) Compatible, Submicron, Interleaved Junction Optical Phase Shifters",['Optics'],"['Abdurrahman Javid Shaikh', 'Fauzi Packeer', 'Mirza Muhammad Ali Baig', 'Othman Sidek']","Performance optimization associated with optical modulators requires reasonably accurate predictive models for key figures of merit. Interleaved PN-junction topology offers the maximum mode/junction overlap and is the most efficient modulator in depletion-mode of operation. Due to its structure, the accurate modelling process must be fully three-dimensional, which is a nontrivial computational problem. This paper presents a rigorous 3D model for the modulation efficiency of silicon-on-insulator interleaved junction optical phase modulators with submicron dimensions. Solution of Drift-Diffusion and Poisson equations were carried out on 3D finite-element-mesh and Maxwell equations were solved using Finite-Difference-Time-Domain (FDTD) method on 3D Yee-cells. Whole of the modelling process has been detailed and all the coefficients required in the model are presented. Model validation suggests < 10% RMS error.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01305
Learning Smooth Distance Functions via Queries,['Machine Learning'],"['Akash Kumar', 'Sanjoy Dasgupta']","In this work, we investigate the problem of learning distance functions within the query-based learning framework, where a learner is able to pose triplet queries of the form: ``Is $x_i$ closer to $x_j$ or $x_k$?'' We establish formal guarantees on the query complexity required to learn smooth, but otherwise general, distance functions under two notions of approximation: $ω$-additive approximation and $(1 + ω)$-multiplicative approximation. For the additive approximation, we propose a global method whose query complexity is quadratic in the size of a finite cover of the sample space. For the (stronger) multiplicative approximation, we introduce a method that combines global and local approaches, utilizing multiple Mahalanobis distance functions to capture local geometry. This method has a query complexity that scales quadratically with both the size of the cover and the ambient space dimension of the sample space.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01290
Self Phase Modulation and Cross Phase Modulation in Nonlinear Silicon Waveguides for On-Chip Optical Networks -- A Tutorial,['Optics'],"['Abdurrahman Javid Shaikh', 'Othman Sidek', 'Fauzi Packeer']","Silicon is a nonlinear material and optics based on silicon makes use of these nonlinearities to realize various functionalities required for on-chip communications. This article describes foundations of these nonlinearities in silicon at length. Particularly, self phase modulation and cross phase modulation in the context of integrated on-board and on-chip communications are presented. Important published results and principles of working of these nonlinearities are presented in considerable detail for non-expert readers.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01286
Self organisation of invasive breast cancer driven by the interplay of active and passive nematic dynamics,['Biological Physics'],"['Pablo Gottheil', 'Saraswat Bhattacharyya', 'Kolya Lettl', 'Philip Friedrich', 'Kilian Roth', 'Salvador Rivera-Moreno', 'Mario Merkel', 'Bahriye Aktas', 'Igor Sauer', 'Assal Daneshgar', 'Jonas Wieland', 'Hans Kubitschke', 'Anne-Sophie Wegscheider', 'Julia M. Yeomans', 'Josef A. Käs']","In invasive breast cancer, cell clusters of varying sizes and shapes are embedded in the fibrous extracellular matrix (ECM). Although the prevailing view attributes this structure to increasing disorder resulting from loss of function and dedifferentiation, our findings reveal that it arises through a process of active self-organization driven by cancer cell motility. Simulations and histological analyses of tumours from over 2,000 breast cancer patients reveal that motile, aligned cancer cells within clusters move as active nematic aggregates through the surrounding highly aligned ECM fibres, which form a confining, passive nematic phase. Cellular motion leads to cluster splitting and coalescence. The degree of cluster activity, combined with heterogeneity in cell motility, is reflected in specific scaling behaviours for cluster shape, size distribution, and the distance between cluster boundaries and nematic defects in ECM alignment. Increased activity estimates correlate with tumour progression and are associated with a poorer prognosis for patients.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01285
Big data approach to Kazhdan-Lusztig polynomials,['Representation Theory'],"['Abel Lacabanne', 'Daniel Tubbenhauer', 'Pedro Vaz']","We investigate the structure of Kazhdan-Lusztig polynomials of the symmetric group by leveraging computational approaches from big data, including exploratory and topological data analysis, applied to the polynomials for symmetric groups of up to 11 strands.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01283
Fast and efficient Bayesian method to search for strongly lensed gravitational waves,['General Relativity and Quantum Cosmology'],"['A. Barsode', 'S. Goyal', 'P. Ajith']","A small fraction of the gravitational-wave (GW) signals from binary black holes observable by ground-based detectors will be strongly lensed by intervening objects such as galaxies and clusters. Strong lensing will produce nearly identical copies of the GW signals separated in time. These lensed signals must be identified against a background of unlensed pairs GW events, some of which may appear similar by accident. This is usually done using fast, but approximate methods that, for example, check for the overlap between the posterior distributions of a subset of binary parameters, or using slow, but accurate joint Bayesian parameter estimation. In this work, we present a modified version of the posterior overlap method dubbed ""PO2.0"" that is mathematically equivalent to joint parameter estimation while still remaining fast. We achieve a significant gain in efficiency by incorporating informative priors about the binary and lensing populations, selection effects, and all the inferred parameters of the binary. For binary black hole signals lensed by galaxies, our improved method can detect 65% lensed events at a pair-wise false alarm probability of $\sim 2\times 10^{-6}$. Consequently, we have a 13% probability of detecting a strongly lensed event above $2.25σ$ significance during 18 months of observation by the LIGO-Virgo detectors at their current sensitivity. We also show how we can compute the joint posteriors of the lens and source parameters from a pair of lensed events by reweighting the posteriors of individual events in a computationally inexpensive way.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01278
PASTA-4-PHT: A Pipeline for Automated Security and Technical Audits for the Personal Health Train,['Cryptography and Security'],"['Sascha Welten', 'Karl Kindermann', 'Ahmet Polat', 'Martin Görz', 'Maximilian Jugl', 'Laurenz Neumann', 'Alexander Neumann', 'Johannes Lohmöller', 'Jan Pennekamp', 'Stefan Decker']","With the introduction of data protection regulations, the need for innovative privacy-preserving approaches to process and analyse sensitive data has become apparent. One approach is the Personal Health Train (PHT) that brings analysis code to the data and conducts the data processing at the data premises. However, despite its demonstrated success in various studies, the execution of external code in sensitive environments, such as hospitals, introduces new research challenges because the interactions of the code with sensitive data are often incomprehensible and lack transparency. These interactions raise concerns about potential effects on the data and increases the risk of data breaches. To address this issue, this work discusses a PHT-aligned security and audit pipeline inspired by DevSecOps principles. The automated pipeline incorporates multiple phases that detect vulnerabilities. To thoroughly study its versatility, we evaluate this pipeline in two ways. First, we deliberately introduce vulnerabilities into a PHT. Second, we apply our pipeline to five real-world PHTs, which have been utilised in real-world studies, to audit them for potential vulnerabilities. Our evaluation demonstrates that our designed pipeline successfully identifies potential vulnerabilities and can be applied to real-world studies. In compliance with the requirements of the GDPR for data management, documentation, and protection, our automated approach supports researchers using in their data-intensive work and reduces manual overhead. It can be used as a decision-making tool to assess and document potential vulnerabilities in code for data processing. Ultimately, our work contributes to an increased security and overall transparency of data processing activities within the PHT framework.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01275
AR-Facilitated Safety Inspection and Fall Hazard Detection on Construction Sites,['Human-Computer Interaction'],"['Jiazhou Liu', 'Aravinda S. Rao', 'Fucai Ke', 'Tim Dwyer', 'Benjamin Tag', 'Pari Delir Haghighi']","Together with industry experts, we are exploring the potential of head-mounted augmented reality to facilitate safety inspections on high-rise construction sites. A particular concern in the industry is inspecting perimeter safety screens on higher levels of construction sites, intended to prevent falls of people and objects. We aim to support workers performing this inspection task by tracking which parts of the safety screens have been inspected. We use machine learning to automatically detect gaps in the perimeter screens that require closer inspection and remediation and to automate reporting. This work-in-progress paper describes the problem, our early progress, concerns around worker privacy, and the possibilities to mitigate these.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01273
Embryo 2.0: Merging Synthetic and Real Data for Advanced AI Predictions,['Image and Video Processing'],"['Oriana Presacan', 'Alexandru Dorobantiu', 'Vajira Thambawita', 'Michael A. Riegler', 'Mette H. Stensen', 'Mario Iliceto', 'Alexandru C. Aldea', 'Akriti Sharma']","Accurate embryo morphology assessment is essential in assisted reproductive technology for selecting the most viable embryo. Artificial intelligence has the potential to enhance this process. However, the limited availability of embryo data presents challenges for training deep learning models. To address this, we trained two generative models using two datasets, one we created and made publicly available, and one existing public dataset, to generate synthetic embryo images at various cell stages, including 2-cell, 4-cell, 8-cell, morula, and blastocyst. These were combined with real images to train classification models for embryo cell stage prediction. Our results demonstrate that incorporating synthetic images alongside real data improved classification performance, with the model achieving 97% accuracy compared to 95% when trained solely on real data. Notably, even when trained exclusively on synthetic data and tested on real data, the model achieved a high accuracy of 94%. Furthermore, combining synthetic data from both generative models yielded better classification results than using data from a single generative model. Four embryologists evaluated the fidelity of the synthetic images through a Turing test, during which they annotated inaccuracies and offered feedback. The analysis showed the diffusion model outperformed the generative adversarial network model, deceiving embryologists 66.6% versus 25.3% and achieving lower Frechet inception distance scores.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01255
Yi-Lightning Technical Report,['Computation and Language'],"['01. AI', ':', 'Alan Wake', 'Albert Wang', 'Bei Chen', 'C. X. Lv', 'Chao Li', 'Chengen Huang', 'Chenglin Cai', 'Chujie Zheng', 'Daniel Cooper', 'Ethan Dai', 'Fan Zhou', 'Feng Hu', 'Heng Ji', 'Howard Qiu', 'Jiangcheng Zhu', 'Jun Tian', 'Katherine Su', 'Lihuan Zhang', 'Liying Li', 'Ming Song', 'Mou Li', 'Peng Liu', 'Qicheng Hu']","This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.△ Less",v1,https://arxiv.org/pdf/2412.01253
Light-matter interactions in layered materials and heterostructures: from moiré physics and magneto-optical effects to ultrafast dynamics and hybrid meta-photonics,['Materials Science'],"['Luca Sortino', 'Marcos H. D. Guimarães', 'Alejandro Molina-Sánchez', 'Jiamin Quan', 'Denis Garoli', 'Nicolò Maccaferri']","Layered two-dimensional (2D) materials have revolutionized how we approach light-matter interactions, offering unprecedented optical and electronic properties with the potential for vertical heterostructures and manipulation of spin-valley degrees of freedom. The discovery of moiré physics in twisted heterostructures has further unlocked new possibilities for controlling the band structure of tailored semiconductor heterostructures. In parallel, the integration of 2D materials with hybrid photonic structures and ultrafast studies on their optical and spin-valley properties has revealed a wealth of novel physical phenomena. This perspective highlights the recent advances in our understanding of light-matter interactions in moiré and 2D systems, with a particular emphasis on ultrafast processes and the integration of these materials into photonic platforms. We explore the implications for optoelectronics and emerging photonic technologies, positioning 2D materials as a transformative tool for next-generation devices.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01252
Multi-Functional RIS Integrated Sensing and Communications for 6G Networks,['Signal Processing'],"['Dongsheng Han', 'Peng Wang', 'Wanli Ni', 'Wen Wang', 'Ailing Zheng', 'Dusit Niyato', 'Naofal Al-Dhahir']","In this paper, we propose a novel multi-functional reconfigurable intelligent surface (MF-RIS) that supports signal reflection, refraction, amplification, and target sensing simultaneously. Our MF-RIS aims to enhance integrated communication and sensing (ISAC) systems, particularly in multi-user and multi-target scenarios. Equipped with reflection and refraction components (i.e., amplifiers and phase shifters), MF-RIS is able to adjust the amplitude and phase shift of both communication and sensing signals on demand. Additionally, with the assistance of sensing elements, MF-RIS is capable of capturing the echo signals from multiple targets, thereby mitigating the signal attenuation typically associated with multi-hop links. We propose a MF-RIS-enabled multi-user and multi-target ISAC system, and formulate an optimization problem to maximize the signal-to-interference-plus-noise ratio (SINR) of sensing targets. This problem involves jointly optimizing the transmit beamforming and MF-RIS configurations, subject to constraints on the communication rate, total power budget, and MF-RIS coefficients. We decompose the formulated non-convex problem into three sub-problems, and then solve them via an efficient iterative algorithm. Simulation results demonstrate that: 1) The performance of MF-RIS varies under different operating protocols, and energy splitting (ES) exhibits the best performance in the considered MF-RIS-enabled multi-user multi-target ISAC system; 2) Under the same total power budget, the proposed MF-RIS with ES protocol attains 52.2%, 73.5% and 60.86% sensing SINR gains over active RIS, passive RIS, and simultaneously transmitting and reflecting RIS (STAR-RIS), respectively; 3) The number of sensing elements will no longer improve sensing performance after exceeding a certain number.△ Less",v1,https://arxiv.org/pdf/2412.01251
Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input,['Artificial Intelligence'],"['Francesco Taioli', 'Edoardo Zorzi', 'Gianni Franchi', 'Alberto Castellini', 'Alessandro Farinelli', 'Marco Cristani', 'Yiming Wang']","Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01250
Multimodal Fusion Learning with Dual Attention for Medical Imaging,['Computer Vision and Pattern Recognition'],"['Joy Dhar', 'Nayyar Zaidi', 'Maryam Haghighat', 'Puneet Goyal', 'Sudipta Roy', 'Azadeh Alavi', 'Vikas Kumar']","Multimodal fusion learning has shown significant promise in classifying various diseases such as skin cancer and brain tumors. However, existing methods face three key limitations. First, they often lack generalizability to other diagnosis tasks due to their focus on a particular disease. Second, they do not fully leverage multiple health records from diverse modalities to learn robust complementary information. And finally, they typically rely on a single attention mechanism, missing the benefits of multiple attention strategies within and across various modalities. To address these issues, this paper proposes a dual robust information fusion attention mechanism (DRIFA) that leverages two attention modules, i.e. multi-branch fusion attention module and the multimodal information fusion attention module. DRIFA can be integrated with any deep neural network, forming a multimodal fusion learning framework denoted as DRIFA-Net. We show that the multi-branch fusion attention of DRIFA learns enhanced representations for each modality, such as dermoscopy, pap smear, MRI, and CT-scan, whereas multimodal information fusion attention module learns more refined multimodal shared representations, improving the network's generalization across multiple tasks and enhancing overall performance. Additionally, to estimate the uncertainty of DRIFA-Net predictions, we have employed an ensemble Monte Carlo dropout strategy. Extensive experiments on five publicly available datasets with diverse modalities demonstrate that our approach consistently outperforms state-of-the-art methods. The code is available at https://github.com/misti1203/DRIFA-Net.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01248
Optional participation only provides a narrow scope for sustaining cooperation,['Dynamical Systems'],"['Khadija Khatun', 'Chen Shen', 'Jun Tanimoto', 'Interdisciplinary Graduate School of Engineering Sciences', 'Kyushu University', 'Japan', 'Department of Applied Mathematics', 'University of Dhaka', 'Bangladesh', 'Faculty of Engineering Sciences', 'Kyushu University', 'Japan']","Understanding how cooperation emerges in public goods games is crucial for addressing societal challenges. While optional participation can establish cooperation without identifying cooperators, it relies on specific assumptions -- that individuals abstain and receive a non-negative payoff, or that non-participants cause damage to public goods -- which limits our understanding of its broader role. We generalize this mechanism by considering non-participants' payoffs and their potential direct influence on public goods, allowing us to examine how various strategic motives for non-participation affect cooperation. Using replicator dynamics, we find that cooperation thrives only when non-participants are motivated by individualistic or prosocial values, with individualistic motivations yielding optimal cooperation. These findings are robust to mutation, which slightly enlarges the region where cooperation can be maintained through cyclic dominance among strategies. Our results suggest that while optional participation can benefit cooperation, its effectiveness is limited and highlights the limitations of bottom-up schemes in supporting public goods.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01247
Class Distance Weighted Cross Entropy Loss for Classification of Disease Severity,['Computer Vision and Pattern Recognition'],"['Gorkem Polat', 'Ümit Mert Çağlar', 'Alptekin Temizel']","Assessing disease severity involving ordinal classes, where each class represents increasing levels of severity, benefit from loss functions that account for this ordinal structure. Traditional categorical loss functions, like Cross-Entropy (CE), often perform suboptimally in these scenarios. To address this, we propose a novel loss function, Class Distance Weighted Cross-Entropy (CDW-CE), which penalizes misclassifications more harshly when classes are farther apart. We evaluated CDW-CE on the Labeled Images for Ulcerative Colitis (LIMUC) dataset using various deep architectures. Its performance was compared against several categorical and ordinal loss functions. To analyze the quality of latent representations, we used t-distributed stochastic neighbor embedding (t-SNE) visualizations and quantified their clustering with the Silhouette Score. We also compared Class Activation Maps (CAM) generated by models trained with CDW-CE and CE loss, incorporating domain expert feedback to evaluate alignment with expert knowledge. Our results show that CDW-CE consistently improves performance in ordinal image classification tasks. It achieves higher Silhouette Scores, indicating better differentiation of class representations, and its CAM visualizations demonstrate a stronger focus on clinically significant regions, as confirmed by domain experts.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01246
A Bayesian Hierarchical Framework for Capturing Preference Heterogeneity in Migration Flows,['Applications'],"['Aric Cutuli', 'Upmanu Lall', 'Michael J. Puma', 'Émile Esmaili', 'Rachata Muneepeerakul']","Understanding and predicting human migration patterns is a central challenge in population dynamics research. Traditional physics-inspired gravity and radiation models represent migration flows as functions of attractiveness using socio-economic features as proxies. They assume that the relationship between features and migration is spatially invariant, regardless of the origin and destination locations of migrants. We use Bayesian hierarchical models to demonstrate that migrant preferences likely vary based on geographical context, specifically the origin-destination pair. By applying these models to U.S. interstate migration data, we show that incorporating heterogeneity in a single latent migration parameter significantly improves the ability to explain variations in migrant flows. Accounting for such heterogeneity enables it to outperform classical methods and recent machine-learning approaches. A clustering analysis of spatially varying parameters reveals two distinct groups of migration paths. Individuals migrating along low-flow paths (typically between smaller populations or over larger distances) exhibit more nuanced decision-making. Their choices are less directly influenced by specific destination characteristics such as housing costs, land area, and climate-related disaster costs. High-flow path migrants appear to respond more directly to these destination attributes. Our results challenge assumptions of uniform preferences and underscore the value of capturing heterogeneity in migration models and policymaking.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01242
Quantum Pointwise Convolution: A Flexible and Scalable Approach for Neural Network Enhancement,['Machine Learning'],"['An Ning', 'Tai-Yue Li', 'Nan-Yow Chen']","In this study, we propose a novel architecture, the Quantum Pointwise Convolution, which incorporates pointwise convolution within a quantum neural network framework. Our approach leverages the strengths of pointwise convolution to efficiently integrate information across feature channels while adjusting channel outputs. By using quantum circuits, we map data to a higher-dimensional space, capturing more complex feature relationships. To address the current limitations of quantum machine learning in the Noisy Intermediate-Scale Quantum (NISQ) era, we implement several design optimizations. These include amplitude encoding for data embedding, allowing more information to be processed with fewer qubits, and a weight-sharing mechanism that accelerates quantum pointwise convolution operations, reducing the need to retrain for each input pixels. In our experiments, we applied the quantum pointwise convolution layer to classification tasks on the FashionMNIST and CIFAR10 datasets, where our model demonstrated competitive performance compared to its classical counterpart. Furthermore, these optimizations not only improve the efficiency of the quantum pointwise convolutional layer but also make it more readily deployable in various CNN-based or deep learning models, broadening its potential applications across different architectures.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01241
Multi-Electrode Dielectric Barrier Discharge Actuators: Geometrical Optimization of High Power Density Array,['Plasma Physics'],"['Anthony Tang', 'Alexander Mamishev', 'Igor Novosselov']","Dielectric barrier discharge (DBD) plasma actuator arrays have been suggested as active flow control devices due to the robust electrohydrodynamic (EHD) force generation in variable atmospheric conditions. DBD plasma augmentation schemes allow for significant performance improvements. However, the transitions to sliding discharge or counter-flow discharge limit their use in high-power arrays. Here, we experimentally demonstrate the performance of a scalable DBD array for two alternating phases of air-exposed electrode configuration. Plasma emissions, direct thrust, velocity profiles, and power consumption measurements of the DBD array reveal that cross-talk between DBD stages can be eliminated to create high-power density actuators. AC augmentation of plasma provides additional gains in thrust; however, the transition to sliding and filamentary discharge reveals geometric limits when increasing the array power density. Introducing a segmented electrode with a resistor delays the onset of adverse sliding and filamentary discharge, allowing it to operate at higher voltage inputs. An optimized four-stage DBD array generated thrust > 250 mN/m with a wall jet thickness > 15 mm, enabling a broader range of flow control applications.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01237
Confinement Specific Design of SOI Rib Waveguides with Submicron Dimensions and Single Mode Operation,['Optics'],"['Abdurrahman Javid Shaikh', 'Abdul Ghani Abro', 'Mirza Muhammad Ali Baig', 'Muhammad Adeel Ahmad Siddiqui', 'Syed Mohsin Abbas']","Full-vectorial finite difference method with perfectly matched layers boundaries is used to identify the single mode operation region of submicron rib waveguides fabricated using sili-con-on-insulator material system. Achieving high mode power confinement factors is emphasized while maintaining the single mode operation. As opposed to the case of large cross-section rib waveguides, theoretical single mode conditions have been demonstrated to hold for sub-micron waveguides with accuracy approaching 100%. Both, the deeply and the shallowly etched rib waveguides have been considered and the single mode condition for entire sub-micrometer range is presented while adhering to design specific mode confinement requirements.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01236
Best Practices for Large Language Models in Radiology,['Artificial Intelligence'],"['Christian Bluethgen', 'Dave Van Veen', 'Cyril Zakka', 'Katherine Link', 'Aaron Fanous', 'Roxana Daneshjou', 'Thomas Frauenfelder', 'Curtis Langlotz', 'Sergios Gatidis', 'Akshay Chaudhari']","At the heart of radiological practice is the challenge of integrating complex imaging data with clinical information to produce actionable insights. Nuanced application of language is key for various activities, including managing requests, describing and interpreting imaging findings in the context of clinical data, and concisely documenting and communicating the outcomes. The emergence of large language models (LLMs) offers an opportunity to improve the management and interpretation of the vast data in radiology. Despite being primarily general-purpose, these advanced computational models demonstrate impressive capabilities in specialized language-related tasks, even without specific training. Unlocking the potential of LLMs for radiology requires basic understanding of their foundations and a strategic approach to navigate their idiosyncrasies. This review, drawing from practical radiology and machine learning expertise and recent literature, provides readers insight into the potential of LLMs in radiology. It examines best practices that have so far stood the test of time in the rapidly evolving landscape of LLMs. This includes practical advice for optimizing LLM characteristics for radiology practices along with limitations, effective prompting, and fine-tuning strategies.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01233
Variational formulation based on duality to solve partial differential equations: Use of B-splines and machine learning approximants,['Numerical Analysis'],"['N. Sukumar', 'Amit Acharya']","Many partial differential equations (PDEs) such as Navier--Stokes equations in fluid mechanics, inelastic deformation in solids, and transient parabolic and hyperbolic equations do not have an exact, primal variational structure. Recently, a variational principle based on the dual (Lagrange multiplier) field was proposed. The essential idea in this approach is to treat the given PDE as constraints, and to invoke an arbitrarily chosen auxiliary potential with strong convexity properties to be optimized. This leads to requiring a convex dual functional to be minimized subject to Dirichlet boundary conditions on dual variables, with the guarantee that even PDEs that do not possess a variational structure in primal form can be solved via a variational principle. The vanishing of the first variation of the dual functional is, up to Dirichlet boundary conditions on dual fields, the weak form of the primal PDE problem with the dual-to-primal change of variables incorporated. We derive the dual weak form for the linear, one-dimensional, transient convection-diffusion equation. A Galerkin discretization is used to obtain the discrete equations, with the trial and test functions chosen as linear combination of either RePU activation functions (shallow neural network) or B-spline basis functions; the corresponding stiffness matrix is symmetric. For transient problems, a space-time Galerkin implementation is used with tensor-product B-splines as approximating functions. Numerical results are presented for the steady-state and transient convection-diffusion equation, and transient heat conduction. The proposed method delivers sound accuracy for ODEs and PDEs and rates of convergence are established in the $L^2$ norm and $H^1$ seminorm for the steady-state convection-diffusion problem.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01232
Self-energy correction to the E1 transition amplitudes in hydrogen-like ions,['Atomic Physics'],"['M. G. Kozlov', 'M. Y. Kaygorodov', 'Yu. A. Demidov', 'V. A. Yerokhin']","We present calculations of the self-energy correction to the $E1$ transition amplitudes in hydrogen-like ions, performed to all orders in the nuclear binding strength parameter. Our results for the $1s$-$2p_{1/2}$ transition for the hydrogen isoelectronic sequence show that the perturbed-orbital part of the self-energy correction provides the dominant contribution, accounting for approximately 99\% of the total correction for this transition. Detailed calculations were performed for $ns$-$n'p$ and $np$-$n'd$ transitions in H-like caesium. We conclude that the perturbed-orbital part remains dominant also for other $ns$-$n'p$ transitions, whereas for the $np$-$n'd$ matrix elements this dominance no longer holds. Consequently, the self-energy corrections for the $np$-$n'd$ one-electron matrix elements cannot be well reproduced by means of effective QED operators constructed for energy levels.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01231
The Subparsec-scale Structure and Evolution of Centaurus A. III. A Multi-Epoch Spectral And Polarimetric VLBA Study,['High Energy Astrophysical Phenomena'],"['Steve Prabu', 'Steven J Tingay', 'Arash Bahramian', 'James C. A. Miller-Jones', 'Callan M. Wood', ""Shane P. O'Sullivan""]","The Centaurus A radio galaxy, due to its proximity, presents itself as one of the few systems that allow the study of relativistic jet outflows at sub-parsec distances from the central supermassive black holes, with high signal to noise. We present the results from the first multi-epoch spectropolarimetric observations of Centaurus A at milliarcsecond resolution, with a continuous frequency coverage of $4.59-7.78$\,GHz. Using a Bayesian framework, we perform a comprehensive study of the jet kinematics, and discuss aspects of the jet geometry including the jet inclination angle, jet opening angle, and the jet expansion profile. We calculate an upper limit on the jet's inclination to the line of sight to be $<25^{\circ}$, implying the lower limit on the intrinsic jet speed to be $0.2$\,c. On the observed VLBA scales we detect new jet components launched by the central engine since our previous study. Using the observed frequency-dependent core shift in Centaurus A, we find the jet to have reached constant bulk speed and conical outflow at the regions probed by the base of the jet at $7.78- 4.59$\,GHz, and we also estimate the location of the central black hole further upstream. Through polarimetric analysis (by applying RM synthesis for the first time on VLBI data), we find evidence to suggest the possible onset of acceleration towards the leading edge of Centaurus A's subparsec-scale jet studied here.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01222
Assessing GPT Model Uncertainty in Mathematical OCR Tasks via Entropy Analysis,['Information Theory'],['Alexei Kaltchenko'],"This paper investigates the uncertainty of Generative Pre-trained Transformer (GPT) models in extracting mathematical equations from images of varying resolutions and converting them into LaTeX code. We employ concepts of entropy and mutual information to examine the recognition process and assess the model's uncertainty in this Optical Character Recognition (OCR) task. By analyzing the conditional entropy of the output token sequences, we provide both theoretical insights and practical measurements of the GPT model's performance given different image qualities.
  Our experimental results, obtained using a Python implementation available on GitHub, demonstrate a clear relationship between image resolution and GPT model uncertainty. Higher-resolution images lead to lower entropy values, indicating reduced uncertainty and improved accuracy in the recognized LaTeX code. Conversely, lower-resolution images result in increased entropy, reflecting higher uncertainty and a higher likelihood of recognition errors. These findings highlight the practical importance of considering image quality in GPT-based mathematical OCR applications and demonstrate how entropy analysis, grounded in information-theoretic concepts, can effectively quantify model uncertainty in real-world tasks.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01221
FD-LLM: Large Language Model for Fault Diagnosis of Machines,['Artificial Intelligence'],"['Hamzah A. A. M. Qaid', 'Bo Zhang', 'Dan Li', 'See-Kiong Ng', 'Wei Li']","Large language models (LLMs) are effective at capturing complex, valuable conceptual representations from textual data for a wide range of real-world applications. However, in fields like Intelligent Fault Diagnosis (IFD), incorporating additional sensor data-such as vibration signals, temperature readings, and operational metrics-is essential but it is challenging to capture such sensor data information within traditional text corpora. This study introduces a novel IFD approach by effectively adapting LLMs to numerical data inputs for identifying various machine faults from time-series sensor data. We propose FD-LLM, an LLM framework specifically designed for fault diagnosis by formulating the training of the LLM as a multi-class classification problem. We explore two methods for encoding vibration signals: the first method uses a string-based tokenization technique to encode vibration signals into text representations, while the second extracts statistical features from both the time and frequency domains as statistical summaries of each signal. We assess the fault diagnosis capabilities of four open-sourced LLMs based on the FD-LLM framework, and evaluate the models' adaptability and generalizability under various operational conditions and machine components, namely for traditional fault diagnosis, cross-operational conditions, and cross-machine component settings. Our results show that LLMs such as Llama3 and Llama3-instruct demonstrate strong fault detection capabilities and significant adaptability across different operational conditions, outperforming state-of-the-art deep learning (DL) approaches in many cases.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01218
The MeerKAT Pulsar Timing Array: Maps of the gravitational-wave sky with the 4.5 year data release,['High Energy Astrophysical Phenomena'],"['Kathrin Grunthal', 'Rowina S. Nathan', 'Eric Thrane', 'David J. Champion', 'Matthew T. Miles', 'Ryan M. Shannon', 'Atharva D. Kulkarni', 'Federico Abbate', 'Sarah Buchner', 'Andrew D. Cameron', 'Marisa Geyer', 'Pratyasha Gitika', 'Michael J. Keith', 'Michael Kramer', 'Paul D. Lasky', 'Aditya Parthasarathy', 'Daniel J. Reardon', 'Jaikhomba Singha', 'Vivek Venkatraman Krishnan']","In an accompanying publication, the MeerKAT Pulsar Timing Array (MPTA) collaboration reports tentative evidence for the presence of a stochastic gravitational-wave background, following observations of similar signals from the European and Indian Pulsar Timing Arrays, NANOGrav, the Parkes Pulsar Timing Array and the Chinese Pulsar Timing Array. If such a gravitational-wave background signal originates from a population of inspiraling supermassive black-hole binaries, the signal may be anisotropically distributed on the sky. In this Letter we evaluate the anisotropy of the MPTA signal using a spherical harmonic decomposition. We discuss complications arising from the covariance between pulsar pairs and regularisation of the Fisher matrix. Applying our method to the 4.5 yr dataset, we obtain two forms of sky maps for the three most sensitive MPTA frequency bins between 7 -21 nHz. Our ""clean maps'' estimate the distribution of gravitational-wave strain power with minimal assumptions. Our radiometer maps answer the question: is there a statistically significant point source? We find a noteworthy hotspot in the 7 nHz clean map with a $p$-factor of $p=0.015$ (not including trial factors). Future observations are required to determine if this hotspot is of astrophysical origin.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01214
GeoTP: Latency-aware Geo-Distributed Transaction Processing in Database Middlewares (Extended Version),['Databases'],"['Qiyu Zhuang', 'Xinyue Shi', 'Shuang Liu', 'Wei Lu', 'Zhanhao Zhao', 'Yuxing Chen', 'Tong Li', 'Anqun Pan', 'Xiaoyong Du']","The widespread adoption of database middleware for supporting distributed transaction processing is prevalent in numerous applications, with heterogeneous data sources deployed across national and international boundaries. However, transaction processing performance significantly drops due to the high network latency between the middleware and data sources and the long lock contention span, where transactions may be blocked while waiting for the locks held by concurrent transactions. In this paper, we propose GeoTP, a latency-aware geo-distributed transaction processing approach in database middlewares. GeoTP incorporates three key techniques to enhance geo-distributed transaction performance. First, we propose a decentralized prepare mechanism, which diminishes the requirement of network round trips for distributed transactions. Second, we design a latency-aware scheduler to minimize the lock contention span by strategically postponing the lock acquisition time point. Third, heuristic optimizations are proposed for the scheduler to reduce the lock contention span further. We implemented GeoTP on Apache Shardingsphere, a state-of-the-art middleware, and extended it into Apache ScalarDB. Experimental results on YCSB and TPC-C demonstrate that GeoTP achieves up to 17.7x performance improvement over Shardingsphere.△ Less",v1,https://arxiv.org/pdf/2412.01213
Smoothing effect and quantum-classical correspondence for the Schr{ö}dinger equation with confining potential,['Analysis of PDEs'],['Antoine Prouff'],"The smoothing effect states that solutions to the Schr{ö}dinger equation in the Euclidean space have, for almost-every time, a local-in-space improved regularity (gain of half a derivative in Sobolev spaces). In this note, we show that, for the Schr{ö}dinger equation with a sub-quadratic confining potential, the smoothing effect is equivalent to an escape rate estimate on the associated classical flow. The proof relies on an Egorov theorem proved in~\cite{P:24Egorovinprep}.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01209
"The Simons Observatory: Design, Optimization, and Performance of Low Frequency Detectors",['Instrumentation and Methods for Astrophysics'],"['Aashrita Mangu', 'Benjamin Westbrook', 'Shawn Beckman', 'Lance Corbett', 'Kevin T. Crowley', 'Daniel Dutcher', 'Bradley R. Johnson', 'Adrian T. Lee', 'Varun Kabra', 'Bhoomija Prasad', 'Suzanne T. Staggs', 'Aritoki Suzuki', 'Yuhan Wang', 'Kaiwen Zheng']","The Simons Observatory (SO) is a cosmic microwave background (CMB) experiment located in the Atacama Desert in Chile that will make precise temperature and polarization measurements over six spectral bands ranging from 27 to 285 GHz. Three small aperture telescopes (SATs) and one large aperture telescope (LAT) will house $\sim$60,000 detectors and cover angular scales between one arcminute and tens of degrees. We present the performance of the dichroic, low-frequency (LF) lenslet-coupled sinuous antenna transition-edge sensor (TES) bolometer arrays with bands centered at 27 and 39 GHz. The LF focal plane will primarily characterize Galactic synchrotron emission as a critical part of foreground subtraction from CMB data. We will discuss the design, optimization, and current testing status of these pixels.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01204
"The Simons Observatory: Design, Integration, and Current Status of Small Aperture Telescopes",['Instrumentation and Methods for Astrophysics'],"['Aashrita Mangu', 'Lance Corbett', 'Sanah Bhimani', 'Fred Carl', 'Samuel Day-Weiss', 'Brooke DiGia', 'Josquin Errard', 'Nicholas Galitzki', 'Masashi Hazumi', 'Shawn W. Henderson', 'Varun Kabra', 'Amber Miller', 'Jenna Moore', 'Xue Song', 'Tran Tsan', 'Yuhan Wang', 'Andrea Zonca']","The Simons Observatory (SO) is a cosmic microwave background (CMB) survey experiment located in the Atacama Desert in Chile at an elevation of 5200 meters, nominally consisting of an array of three 0.42-meter small aperture telescopes (SATs) and one 6-meter large aperture telescope (LAT). SO will make accurate measurements of the CMB temperature and polarization spanning six frequency bands ranging from 27 to 280 GHz, fielding a total of $\sim$68,000 detectors covering angular scales between one arcminute to tens of degrees. In this paper, we focus on the SATs, which are tailored to search for primordial gravitational waves, with the primary science goal of measuring the primordial tensor-to-scalar ratio \textit{r} at a target level of $σ(r) \approx 0.003$. We discuss the design drivers, scientific impact, and current deployment status of the three SATs, which are scheduled to start taking data in the coming year. The SATs aim to map 10\% of the sky at a 2 $μ$K-arcmin noise level observing at mid-frequencies (93/145 GHz), with additional ultra-high-frequency (225/280 GHz) and low-frequency (27/39 GHz) targets to yield galactic foreground-subtracted measurements.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01200
Divergent Ensemble Networks: Enhancing Uncertainty Estimation with Shared Representations and Independent Branching,['Machine Learning'],"['Arnav Kharbanda', 'Advait Chandorkar']","Ensemble learning has proven effective in improving predictive performance and estimating uncertainty in neural networks. However, conventional ensemble methods often suffer from redundant parameter usage and computational inefficiencies due to entirely independent network training. To address these challenges, we propose the Divergent Ensemble Network (DEN), a novel architecture that combines shared representation learning with independent branching. DEN employs a shared input layer to capture common features across all branches, followed by divergent, independently trainable layers that form an ensemble. This shared-to-branching structure reduces parameter redundancy while maintaining ensemble diversity, enabling efficient and scalable learning.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01193
MiningGPT -- A Domain-Specific Large Language Model for the Mining Industry,['Computation and Language'],['Kurukulasooriya Fernando ana Gianluca Demartini'],"Recent advancements of generative LLMs (Large Language Models) have exhibited human-like language capabilities but have shown a lack of domain-specific understanding. Therefore, the research community has started the development of domain-specific LLMs for many domains. In this work we focus on discussing how to build mining domain-specific LLMs, as the global mining industry contributes significantly to the worldwide economy. We report on MiningGPT, a mining domain-specific instruction-following 7B parameter LLM model which showed a 14\% higher mining domain knowledge test score as compared to its parent model Mistral 7B instruct.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01189
MeasureNet: Measurement Based Celiac Disease Identification,['Computer Vision and Pattern Recognition'],"['Aayush Kumar Tyagi', 'Vaibhav Mishra', 'Ashok Tiwari', 'Lalita Mehra', 'Prasenjit Das', 'Govind Makharia', 'Prathosh AP', 'Mausam']","Celiac disease is an autoimmune disorder triggered by the consumption of gluten. It causes damage to the villi, the finger-like projections in the small intestine that are responsible for nutrient absorption. Additionally, the crypts, which form the base of the villi, are also affected, impairing the regenerative process. The deterioration in villi length, computed as the villi-to-crypt length ratio, indicates the severity of celiac disease. However, manual measurement of villi-crypt length can be both time-consuming and susceptible to inter-observer variability, leading to inconsistencies in diagnosis. While some methods can perform measurement as a post-hoc process, they are prone to errors in the initial stages. This gap underscores the need for pathologically driven solutions that enhance measurement accuracy and reduce human error in celiac disease assessments.
  Our proposed method, MeasureNet, is a pathologically driven polyline detection framework incorporating polyline localization and object-driven losses specifically designed for measurement tasks. Furthermore, we leverage segmentation model to provide auxiliary guidance about crypt location when crypt are partially visible. To ensure that model is not overdependent on segmentation mask we enhance model robustness through a mask feature mixup technique. Additionally, we introduce a novel dataset for grading celiac disease, consisting of 750 annotated duodenum biopsy images. MeasureNet achieves an 82.66% classification accuracy for binary classification and 81% accuracy for multi-class grading of celiac disease. Code: https://github.com/dair-iitd/MeasureNet△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01182
OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows,['Multimedia'],"['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Zichun Liao', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover']","We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01169
HumekaFL: Automated Detection of Neonatal Asphyxia Using Federated Learning,['Machine Learning'],"['Pamely Zantou', 'Blessed Guda', 'Bereket Retta', 'Gladys Inabeza', 'Carlee Joe-Wong', 'Assane Gueye']","Birth Apshyxia (BA) is a severe condition characterized by insufficient supply of oxygen to a newborn during the delivery. BA is one of the primary causes of neonatal death in the world. Although there has been a decline in neonatal deaths over the past two decades, the developing world, particularly sub-Saharan Africa, continues to experience the highest under-five (<5) mortality rates. While evidence-based methods are commonly used to detect BA in African healthcare settings, they can be subject to physician errors or delays in diagnosis, preventing timely interventions. Centralized Machine Learning (ML) methods demonstrated good performance in early detection of BA but require sensitive health data to leave their premises before training, which does not guarantee privacy and security. Healthcare institutions are therefore reluctant to adopt such solutions in Africa. To address this challenge, we suggest a federated learning (FL)-based software architecture, a distributed learning method that prioritizes privacy and security by design. We have developed a user-friendly and cost-effective mobile application embedding the FL pipeline for early detection of BA. Our Federated SVM model outperformed centralized SVM pipelines and Neural Networks (NN)-based methods in the existing literature△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01167
Double-Directional V2V Channel Measurement using ReRoMA at 60 GHz,['Information Theory'],"['Hussein Hammoud', 'Yuning Zhang', 'Zihang Cheng', 'Seun Sangodoyin', 'Markus Hofer', 'Faruk Pasic', 'Thomas M. Pohl', 'Radek Závorka', 'Ales Prokes', 'Thomas Zemen', 'Christoph F. Mecklenbräuker', 'Andreas F. Molisch']","The coordination of vehicles is a crucial element of autonomous driving, as it enhances the efficiency, convenience, and safety of road traffic. In order to fully exploit the capabilities of such coordination, communication with high data rate and low latency is required. It can be reasonably argued that millimeter-wave (mm-wave) vehicle-to-vehicle (V2V) systems are capable of fulfilling the aforementioned requirements. Nevertheless, in order to develop a system that can be deployed in real-world scenarios and to gain an understanding of the various effects of mm-wave propagation, it is necessary to perform radio propagation measurements and to derive radio channel models from them across a range of scenarios and environments. To this end, we have conducted measurement campaigns at 60\,GHz in a variety of situations, including driving in a convoy, driving in opposite direction on a six-lane road, and overtaking. These measurements employ a channel sounder based on ReRoMA, a recently introduced concept that enables the real-time measurement of dynamic double-directional radio channels. The evaluations presented herein encompass key channel parameters, including the path loss (path loss coefficient of approximately 1.9), the root mean square (RMS) delay spread (within a range of 5\,ns to 110\,ns), the angular spreads (in a range of 0.05 to 0.4), the power distribution among multipath components, and the channel stationarity time (multiple seconds).△ Less",v1,https://arxiv.org/pdf/2412.01165
"Probing Dark Energy Properties in $f Q,C)$ Gravity with FLRW Cosmological Models",['General Relativity and Quantum Cosmology'],"['N. Myrzakulov', 'Anirudh Pradhan', 'A. Dixit', 'S. H. Shekh']","This study delves into the cosmological implications of the $f(Q,C)$ modified gravity framework within the context of the FLRW spacetime which offers a dynamic alternative to the standard $Λ$CDM cosmology. Here, we define the transit form of Hubble's parameter to explain several geometrical and physical aspects. The chosen parametric form of the Hubble parameter represents a smooth transition from the decelerating early universe to the accelerating present and late-time evolution. Employing observational datasets such as the Hubble parameter, Type Ia supernovae, Baryon Acoustic Oscillations (BAO), and Standard Candles (SC), we constrain the model parameters using the Markov Chain Monte Carlo (MCMC) method. The isotropic pressure, energy density, equation of state parameter, and energy conditions were analyzed to explore the physical viability of the $f(Q,C)$ framework. The results highlight the model's ability to replicate key cosmological behaviors, including the accelerated expansion driven by dark energy.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01164
Minimally Deformed Regular Bardeen Black Hole Solutions in Rastall Theory,['General Relativity and Quantum Cosmology'],"['M. Sharif', 'and Malick Sallah']","In this study, we utilize the minimal geometric deformation technique of gravitational decoupling to extend the regular Bardeen black hole, leading to the derivation of new black hole solutions within the framework of Rastall theory. By decoupling the field equations associated with an extended matter source into two subsystems, we address the first subsystem using the metric components of the regular Bardeen black hole. The second subsystem, incorporating the effects of the additional source, is solved through a constraint imposed by a linear equation of state. By linearly combining the solutions of these subsystems, we obtain two extended models. We then explore the distinct physical properties of these models for specific values of the Rastall and decoupling parameters. Our investigations encompass effective thermodynamic variables such as density and anisotropic pressure, asymptotic flatness, energy conditions, and thermodynamic properties including Hawking temperature, entropy, and specific heat. The results reveal that both models violate asymptotic flatness of the resulting spacetimes. The violation of energy conditions indicate the presence of exotic matter, for both models. Nonetheless, the energy density, radial pressure, as well as the Hawking temperature exhibit acceptable behavior, while the specific heat and Hessian matrix suggest thermodynamic stability.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01158
The MeerKAT Pulsar Timing Array: The first search for gravitational waves with the MeerKAT radio telescope,['High Energy Astrophysical Phenomena'],"['Matthew T. Miles', 'Ryan M. Shannon', 'Daniel J. Reardon', 'Matthew Bailes', 'David J. Champion', 'Marisa Geyer', 'Pratyasha Gitika', 'Kathrin Grunthal', 'Michael J. Keith', 'Michael Kramer', 'Atharva D. Kulkarni', 'Rowina S. Nathan', 'Aditya Parthasarathy', 'Jaikhomba Singha', 'Gilles Theureau', 'Eric Thrane', 'Federico Abbate', 'Sarah Buchner', 'Andrew D. Cameron', 'Fernando Camilo', 'Beatrice E. Moreschi', 'Golam Shaifullah', 'Mohsen Shamohammadi', 'Andrea Possenti', 'Vivek Venkatraman Krishnan']","Pulsar Timing Arrays search for nanohertz-frequency gravitational waves by regularly observing ensembles of millisecond pulsars over many years to look for correlated timing residuals. Recently the first evidence for a stochastic gravitational wave background has been presented by the major Arrays, with varying levels of significance ($\sim$2-4$σ$). In this paper we present the results of background searches with the MeerKAT Pulsar Timing Array. Although of limited duration (4.5 yr), the $\sim$ 250,000 arrival times with a median error of just $3 μ$s on 83 pulsars make it very sensitive to spatial correlations. Detection of a gravitational wave background requires careful modelling of noise processes to ensure that any correlations represent a fit to the underlying background and not other misspecified processes. Under different assumptions about noise processes we can produce either what appear to be compelling Hellings-Downs correlations of high significance (3-3.4$σ$) with a spectrum close to that which is predicted, or surprisingly, under slightly different assumptions, ones that are insignificant. This appears to be related to the fact that many of the highest precision MeerKAT Pulsar Timing Array pulsars are in close proximity and dominate the detection statistics. The sky-averaged characteristic strain amplitude of the correlated signal in our most significant model is $h_{c, {\rm yr}} = 7.5^{+0.8}_{-0.9} \times 10^{-15}$ measured at a spectral index of $α=-0.26$, decreasing to $h_{c, {\rm yr}} = 4.8^{+0.8}_{-0.9} \times 10^{-15}$ when assessed at the predicted $α=-2/3$. These data will be valuable as the International Pulsar Timing Array project explores the significance of gravitational wave detections and their dependence on the assumed noise models.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01153
The MeerKAT Pulsar Timing Array: The $4.5$-year data release and the noise and stochastic signals of the millisecond pulsar population,['High Energy Astrophysical Phenomena'],"['Matthew T. Miles', 'Ryan M. Shannon', 'Daniel J. Reardon', 'Matthew Bailes', 'David J. Champion', 'Marisa Geyer', 'Pratyasha Gitika', 'Kathrin Grunthal', 'Michael J. Keith', 'Michael Kramer', 'Atharva D. Kulkarni', 'Rowina S. Nathan', 'Aditya Parthasarathy', 'Nataliya K. Porayko', 'Jaikhomba Singha', 'Gilles Theureau', 'Federico Abbate', 'Sarah Buchner', 'Andrew D. Cameron', 'Fernando Camilo', 'Beatrice E. Moreschi', 'Golam Shaifullah', 'Mohsen Shamohammadi', 'Vivek Venkatraman Krishnan']","Pulsar timing arrays are ensembles of regularly observed millisecond pulsars timed to high precision. Each pulsar in an array could be affected by a suite of noise processes, most of which are astrophysically motivated. Analysing them carefully can be used to understand these physical processes. However, the primary purpose of these experiments is to detect signals that are common to all pulsars, in particular signals associated with a stochastic gravitational wave background. To detect this, it is paramount to appropriately characterise other signals that may otherwise impact array sensitivity or cause a spurious detection. Here we describe the second data release and first detailed noise analysis of the pulsars in the MeerKAT Pulsar Timing Array, comprising high-cadence and high-precision observations of $83$ millisecond pulsars over $4.5$ years. We use this analysis to search for a common signal in the data, finding a process with an amplitude of $\log_{10}\mathrm{A_{CURN}} = -14.25^{+0.21}_{-0.36}$ and spectral index $γ_\mathrm{CURN} = 3.60^{+1.31}_{-0.89}$. Fixing the spectral index at the value predicted for a background produced by the inspiral of binary supermassive black holes, we measure the amplitude to be $\log_{10}\mathrm{A_{CURN}} = -14.28^{+0.21}_{-0.21}$ at a significance expressed as a Bayes factor of $\ln(\mathcal{B}) = 4.46$. Under both assumptions, the amplitude that we recover is larger than those reported by other PTA experiments. We use the results of this analysis to forecast our sensitivity to a gravitational wave background possessing the spectral properties of the common signal we have measured.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01148
Space Complexity of Minimum Cut Problems in Single-Pass Streams,['Data Structures and Algorithms'],"['Matthew Ding', 'Alexandro Garces', 'Jason Li', 'Honghao Lin', 'Jelani Nelson', 'Vihan Shah', 'David P. Woodruff']","We consider the problem of finding a minimum cut of a weighted graph presented as a single-pass stream. While graph sparsification in streams has been intensively studied, the specific application of finding minimum cuts in streams is less well-studied. To this end, we show upper and lower bounds on minimum cut problems in insertion-only streams for a variety of settings, including for both randomized and deterministic algorithms, for both arbitrary and random order streams, and for both approximate and exact algorithms. One of our main results is an $\widetilde{O}(n/\varepsilon)$ space algorithm with fast update time for approximating a spectral cut query with high probability on a stream given in an arbitrary order. Our result breaks the $Ω(n/\varepsilon^2)$ space lower bound required of a sparsifier that approximates all cuts simultaneously. Using this result, we provide streaming algorithms with near optimal space of $\widetilde{O}(n/\varepsilon)$ for minimum cut and approximate all-pairs effective resistances, with matching space lower-bounds. The amortized update time of our algorithms is $\widetilde{O}(1)$, provided that the number of edges in the input graph is at least $(n/\varepsilon^2)^{1+o(1)}$. We also give a generic way of incorporating sketching into a recursive contraction algorithm to improve the post-processing time of our algorithms. In addition to these results, we give a random-order streaming algorithm that computes the {\it exact} minimum cut on a simple, unweighted graph using $\widetilde{O}(n)$ space. Finally, we give an $Ω(n/\varepsilon^2)$ space lower bound for deterministic minimum cut algorithms which matches the best-known upper bound up to polylogarithmic factors.△ Less",v1,https://arxiv.org/pdf/2412.01143
Eyes on the Road: State-of-the-Art Video Question Answering Models Assessment for Traffic Monitoring Tasks,['Computer Vision and Pattern Recognition'],"['Joseph Raj Vishal', 'Divesh Basina', 'Aarya Choudhary', 'Bharatesh Chakravarthi']","Recent advances in video question answering (VideoQA) offer promising applications, especially in traffic monitoring, where efficient video interpretation is critical. Within ITS, answering complex, real-time queries like ""How many red cars passed in the last 10 minutes?"" or ""Was there an incident between 3:00 PM and 3:05 PM?"" enhances situational awareness and decision-making. Despite progress in vision-language models, VideoQA remains challenging, especially in dynamic environments involving multiple objects and intricate spatiotemporal relationships. This study evaluates state-of-the-art VideoQA models using non-benchmark synthetic and real-world traffic sequences. The framework leverages GPT-4o to assess accuracy, relevance, and consistency across basic detection, temporal reasoning, and decomposition queries. VideoLLaMA-2 excelled with 57% accuracy, particularly in compositional reasoning and consistent answers. However, all models, including VideoLLaMA-2, faced limitations in multi-object tracking, temporal coherence, and complex scene interpretation, highlighting gaps in current architectures. These findings underscore VideoQA's potential in traffic monitoring but also emphasize the need for improvements in multi-object tracking, temporal reasoning, and compositional capabilities. Enhancing these areas could make VideoQA indispensable for incident detection, traffic flow management, and responsive urban planning. The study's code and framework are open-sourced for further exploration: https://github.com/joe-rabbit/VideoQA_Pilot_Study△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01132
Independence complexes of circle graphs,['Geometric Topology'],"['Rhea Palak Bakshi', 'Ali Guo', 'Dionne Ibarra', 'Gabriel Montoya-Vega', 'Sujoy Mukherjee', 'Marithania Silvero', 'Jonathan Spreer']","Independence complexes of circle graphs are purely combinatorial objects. However, when constructed from some diagram of a link $L$, they reveal topological properties of $L$, more specifically, of its Khovanov homology. We analyze the homotopy type of independence complexes of circle graphs, with a focus on those arising when the graph is bipartite. Moreover, we compute (real) extreme Khovanov homology of a $4$-strand pretzel knot using chord diagrams and independence complexes.△ Less","2 December, 2024;",https://arxiv.org/pdf/2412.01125
LoyalDiffusion: A Diffusion Model Guarding Against Data Replication,['Computer Vision and Pattern Recognition'],"['Chenghao Li', 'Yuke Zhang', 'Dake Chen', 'Jingqi Xu', 'Peter A. Beerel']","Diffusion models have demonstrated significant potential in image generation. However, their ability to replicate training data presents a privacy risk, particularly when the training data includes confidential information. Existing mitigation strategies primarily focus on augmenting the training dataset, leaving the impact of diffusion model architecture under explored. In this paper, we address this gap by examining and mitigating the impact of the model structure, specifically the skip connections in the diffusion model's U-Net model. We first present our observation on a trade-off in the skip connections. While they enhance image generation quality, they also reinforce the memorization of training data, increasing the risk of replication. To address this, we propose a replication-aware U-Net (RAU-Net) architecture that incorporates information transfer blocks into skip connections that are less essential for image quality. Recognizing the potential impact of RAU-Net on generation quality, we further investigate and identify specific timesteps during which the impact on memorization is most pronounced. By applying RAU-Net selectively at these critical timesteps, we couple our novel diffusion model with a targeted training and inference strategy, forming a framework we refer to as LoyalDiffusion. Extensive experiments demonstrate that LoyalDiffusion outperforms the state-of-the-art replication mitigation method achieving a 48.63% reduction in replication while maintaining comparable image quality.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01118
"Look Ma, No Ground Truth! Ground-Truth-Free Tuning of Structure from Motion and Visual SLAM",['Computer Vision and Pattern Recognition'],"['Alejandro Fontan', 'Javier Civera', 'Tobias Fischer', 'Michael Milford']","Evaluation is critical to both developing and tuning Structure from Motion (SfM) and Visual SLAM (VSLAM) systems, but is universally reliant on high-quality geometric ground truth -- a resource that is not only costly and time-intensive but, in many cases, entirely unobtainable. This dependency on ground truth restricts SfM and SLAM applications across diverse environments and limits scalability to real-world scenarios. In this work, we propose a novel ground-truth-free (GTF) evaluation methodology that eliminates the need for geometric ground truth, instead using sensitivity estimation via sampling from both original and noisy versions of input images. Our approach shows strong correlation with traditional ground-truth-based benchmarks and supports GTF hyperparameter tuning. Removing the need for ground truth opens up new opportunities to leverage a much larger number of dataset sources, and for self-supervised and online tuning, with the potential for a data-driven breakthrough analogous to what has occurred in generative AI.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01116
Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning,['Computation and Language'],"['Keito Kudo', 'Yoichi Aoki', 'Tatsuki Kuribayashi', 'Shusaku Sone', 'Masaya Taniguchi', 'Ana Brassard', 'Keisuke Sakaguchi', 'Kentaro Inui']","This study investigates the internal reasoning mechanism of language models during symbolic multi-step reasoning, motivated by the question of whether chain-of-thought (CoT) outputs are faithful to the model's internals. Specifically, we inspect when they internally determine their answers, particularly before or after CoT begins, to determine whether models follow a post-hoc ""think-to-talk"" mode or a step-by-step ""talk-to-think"" mode of explanation. Through causal probing experiments in controlled arithmetic reasoning tasks, we found systematic internal reasoning patterns across models; for example, simple subproblems are solved before CoT begins, and more complicated multi-hop calculations are performed during CoT.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01113
Multi-Scale Representation Learning for Protein Fitness Prediction,['Machine Learning'],"['Zuobai Zhang', 'Pascal Notin', 'Yining Huang', 'Aurélie Lozano', 'Vijil Chenthamarakshan', 'Debora Marks', 'Payel Das', 'Jian Tang']","Designing novel functional proteins crucially depends on accurately modeling their fitness landscape. Given the limited availability of functional annotations from wet-lab experiments, previous methods have primarily relied on self-supervised models trained on vast, unlabeled protein sequence or structure datasets. While initial protein representation learning studies solely focused on either sequence or structural features, recent hybrid architectures have sought to merge these modalities to harness their respective strengths. However, these sequence-structure models have so far achieved only incremental improvements when compared to the leading sequence-only approaches, highlighting unresolved challenges effectively leveraging these modalities together. Moreover, the function of certain proteins is highly dependent on the granular aspects of their surface topology, which have been overlooked by prior models. To address these limitations, we introduce the Sequence-Structure-Surface Fitness (S3F) model - a novel multimodal representation learning framework that integrates protein features across several scales. Our approach combines sequence representations from a protein language model with Geometric Vector Perceptron networks encoding protein backbone and detailed surface topology. The proposed method achieves state-of-the-art fitness prediction on the ProteinGym benchmark encompassing 217 substitution deep mutational scanning assays, and provides insights into the determinants of protein function. Our code is at https://github.com/DeepGraphLearning/S3F.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01108
Personalized Coupled Tensor Decomposition for Multimodal Data Fusion: Uniqueness and Algorithms,['Machine Learning'],"['Ricardo Augusto Borsoi', 'Konstantin Usevich', 'David Brie', 'Tülay Adali']","Coupled tensor decompositions (CTDs) perform data fusion by linking factors from different datasets. Although many CTDs have been already proposed, current works do not address important challenges of data fusion, where: 1) the datasets are often heterogeneous, constituting different ""views"" of a given phenomena (multimodality); and 2) each dataset can contain personalized or dataset-specific information, constituting distinct factors that are not coupled with other datasets. In this work, we introduce a personalized CTD framework tackling these challenges. A flexible model is proposed where each dataset is represented as the sum of two components, one related to a common tensor through a multilinear measurement model, and another specific to each dataset. Both the common and distinct components are assumed to admit a polyadic decomposition. This generalizes several existing CTD models. We provide conditions for specific and generic uniqueness of the decomposition that are easy to interpret. These conditions employ uni-mode uniqueness of different individual datasets and properties of the measurement model. Two algorithms are proposed to compute the common and distinct components: a semi-algebraic one and a coordinate-descent optimization method. Experimental results illustrate the advantage of the proposed framework compared with the state of the art approaches.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01102
Prediction and observation of topological modes in fractal nonlinear optics,['Optics'],['Boris A. Malomed'],"This item from the News & Views category, to be published in Light: Science & Applications, aims to provide a summary of theoretical and experimental results recently published in Ref. [24], which demonstrate the creation of corner modes in nonlinear optical waveguides of the higher-order topological-insulator (HOTI) type. Actually, these are second-order HOTIs, in which the transverse dimension of the topologically protected edge modes is smaller than the bulk dimension (it is 2, in the case of optical waveguide) by 2, implying zero dimension of the protected modes, that are actually realized as corner or defect ones. Work [24] reports prediction and creation of various forms of the corner modes in a HOTI with a fractal transverse structure, represented by the Sierpinski gasket (SG). The self-focusing nonlinearity of the waveguide's material transforms the corner modes into corner solitons, almost all of which are stable. The solitons may be attached to external or internal corners created by the underlying SG. This N&V item offers an overview of these new findings reported in Ref. [24] and other recent works, and a brief discussion of directions for the further work on this topic.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01097
How the use of feature selection methods influences the efficiency and accuracy of complex network simulations,['Artificial Intelligence'],"['Katarzyna Musial', 'Jiaqi Wen', 'Andreas Gwyther-Gouriotis']","Complex network systems' models are designed to perfectly emulate real-world networks through the use of simulation and link prediction. Complex network systems are defined by nodes and their connections where both have real-world features that result in a heterogeneous network in which each of the nodes has distinct characteristics. Thus, incorporating real-world features is an important component to achieve a simulation which best represents the real-world. Currently very few complex network systems implement real-world features, thus this study proposes feature selection methods which utilise unsupervised filtering techniques to rank real-world node features alongside a wrapper function to test combinations of the ranked features. The chosen method was coined FS-SNS which improved 8 out of 10 simulations of real-world networks. A consistent threshold of included features was also discovered which saw a threshold of 4 features to achieve the most accurate simulation for all networks. Through these findings the study also proposes future work and discusses how the findings can be used to further the Digital Twin and complex network system field.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01096
On the zeros of certain composite polynomials and an operator preserving inequalities,['Complex Variables'],"['N. A. Rather', 'Ishfaq Dar', 'Suhail Gulzar']","If all the zeros of $n$th degree polynomials $f(z)$ and $g(z) = \sum_{k=0}^{n}λ_k\binom{n}{k}z^k$ respectively lie in the cricular regions $|z|\leq r$ and $|z| \leq s|z-σ|$, $s>0$, then it was proved by Marden \cite[p. 86]{mm} that all the zeros of the polynomial $h(z)= \sum_{k=0}^{n}λ_k f^{(k)}(z) \frac{(σz)^k}{k!}$ lie in the circle $|z| \leq r ~ \max(1,s)$. In this paper, we relax the condition that $f(z)$ and $g(z)$ are of the same degree and instead assume that $f(z)$ and $g(z)$ are polynomials of arbitrary degree $n$ and $m$ respectively, $m\leq n,$ and obtain a generalization of this result. As an application, we also introduce a linear operator which preserve Bernstein type polynomial inequalities.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01088
Effect of 2009 major SSW event on the mesospheric CO2 cooling,['Space Physics'],"['Akash Kumar', 'MV Sunil Krishna', 'Alok K Ranjan']","Carbon dioxide (CO2), an important trace species that is gradually increasing in the atmosphere due to anthropogenic activities, causes enhanced warming in the lower atmosphere. The increased concentration of CO2 in the upper atmosphere results in enhanced radiative cooling rates leading to the contraction of the upper atmosphere. Due to its long lifetime and large vertical gradient, CO2 concentration is also influenced by large dynamic events. We report a startling case of variability in CO2 density and its infrared radiative cooling rates in the mesosphere and lower thermospher during a major sudden stratospheric warming (SSW) event. A counter-intuitive connection between CO2 density and resulting CO2 radiative cooling has been observed during the 2009 major SSW event. The behaviour of CO2 cooling rates during such a dramatic events draw attention to our current understanding of CO2 infrared cooling variation and its connection to changes in CO2 concentration. The significance of temperature and atomic oxygen variability in the observed cooling patterns despite changes in CO2 concentration, is also highlighted.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01081
Quantum criticality and universality in stationary state of long-range Kitaev model,['Quantum Physics'],"['Akash Mitra', 'Sanku Paul', 'Shashi C. L. Srivastava']","We investigate the signature of quantum criticality in the long-time stationary state of the long-range Kitaev chain by performing various quench protocols. In this model, the pairing interaction decays with distance according to a power law with exponent $α$. Using quantum information-theoretic measures, such as mutual information and logarithmic negativity, we show that, irrespective of the values of $α$, critical-to-critical quench displays quantum criticality even in the stationary state. Remarkably, in the presence of long-range pairing interactions, where fermionic correlators decay algebraically even at non-critical points, signature of quantum criticality persists in the stationary state. Furthermore, the effective central charge, calculated from both mutual information and logarithmic negativity of stationary state following a critical-to-critical quench, agrees with the central charge of the corresponding ground states for both $α= 0$ and $α= 2$. Therefore, information of the universality class can be inferred from the stationary state.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01076
On the solutions to $Ax^p+By^p+Cz^p=0$ over quadratic fields,['Number Theory'],"['Alejandro Argáez-García', 'Luis Elí Pech-Moreno']","We provide the necessary conditions for the existence of solutions to $Ax^p+By^p+Cz^p=0$ over any quadratic field $K$ with $A,B,C$ pairwise coprime $p$th powerfree integer numbers. Moreover, we prove that there are no solutions $(x,y,z)$ with $x,y,z\in\mathcal{O}_K$ satisfying $xyz\neq 0$.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01068
Harmonic analysis in Dunkl settings,['Classical Analysis and ODEs'],['The Anh Bui'],"Let $L$ be the Dunkl Laplacian on the Euclidean space $\mathbb R^N$ associated with a normalized root $R$ and a multiplicity function $k(ν)\ge 0, ν\in R$. In this paper, we first prove that the Besov and Triebel-Lizorkin spaces associated with the Dunkl Laplacian $L$ are identical to the Besov and Triebel-Lizorkin spaces defined in the space of homogeneous type $(\mathbb R^N, \|\cdot\|, dw)$, where $dw({\rm x})=\prod_{ν\in R}\langle ν,{\rm x}\rangle^{k(ν)}d{\rm x}$. Next, consider the Dunkl transform denoted by $\mathcal{F}$. We introduce the multiplier operator $T_m$, defined as $T_mf = \mathcal{F}^{-1}(m\mathcal{F}f)$, where $m$ is a bounded function defined on $\mathbb{R}^N$. Our second aim is to prove multiplier theorems, including the Hörmander multiplier theorem, for $T_m$ on the Besov and Tribel-Lizorkin spaces in the space of homogeneous type $(\mathbb R^N, \|\cdot\|, dw)$. Importantly, our findings present novel results, even in the specific case of the Hardy spaces.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01067
Adaptive cancellation of mains power interference in continuous gravitational wave searches with a hidden Markov model,['General Relativity and Quantum Cosmology'],"['Tom Kimpson', 'Sofia Suvorova', 'Hannah Middleton', 'Changrong Liu', 'Andrew Melatos', 'Robin J. Evans', 'William Moran']","Continuous gravitational wave searches with terrestrial, long-baseline interferometers are hampered by long-lived, narrowband features in the power spectral density of the detector noise, known as lines. Candidate GW signals which overlap spectrally with known lines are typically vetoed. Here we demonstrate a line subtraction method based on adaptive noise cancellation, using a recursive least squares algorithm, a common approach in electrical engineering applications such as audio and biomedical signal processing. We validate the line subtraction method by combining it with a hidden Markov model (HMM), a standard continuous wave search tool, to detect an injected continuous wave signal with an unknown and randomly wandering frequency, which overlaps with the mains power line at $60 \, {\rm Hz}$ in the Laser Interferometer Gravitational Wave Observatory (LIGO). The performance of the line subtraction method is tested on an injected continuous wave signal obscured by (a) synthetic noise data with both Gaussian and non-Gaussian components, and (b) real noise data obtained from the LIGO Livingston detector. In both cases, before applying the line subtraction method the HMM does not detect the injected continuous wave signal. After applying the line subtraction method the mains power line is suppressed by 20--40 dB, and the HMM detects the underlying signal, with a time-averaged root-mean-square error in the frequency estimate of $\sim 0.05 $ Hz. The performance of the line subtraction method with respect to the characteristics of the 60 Hz line and the control parameters of the recursive least squares algorithm is quantified in terms of receiver operating characteristic curves.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01058
An Efficient Unsupervised Framework for Convex Quadratic Programs via Deep Unrolling,['Optimization and Control'],"['Linxin Yang', 'Bingheng Li', 'Tian Ding', 'Jianghua Wu', 'Akang Wang', 'Yuyi Wang', 'Jiliang Tang', 'Ruoyu Sun', 'Xiaodong Luo']","Quadratic programs (QPs) arise in various domains such as machine learning, finance, and control. Recently, learning-enhanced primal-dual hybrid gradient (PDHG) methods have shown great potential in addressing large-scale linear programs; however, this approach has not been extended to QPs. In this work, we focus on unrolling ""PDQP"", a PDHG algorithm specialized for convex QPs. Specifically, we propose a neural network model called ""PDQP-net"" to learn optimal QP solutions. Theoretically, we demonstrate that a PDQP-net of polynomial size can align with the PDQP algorithm, returning optimal primal-dual solution pairs. We propose an unsupervised method that incorporates KKT conditions into the loss function. Unlike the standard learning-to-optimize framework that requires optimization solutions generated by solvers, our unsupervised method adjusts the network weights directly from the evaluation of the primal-dual gap. This method has two benefits over supervised learning: first, it helps generate better primal-dual gap since the primal-dual gap is in the objective function; second, it does not require solvers. We show that PDQP-net trained in this unsupervised manner can effectively approximate optimal QP solutions. Extensive numerical experiments confirm our findings, indicating that using PDQP-net predictions to warm-start PDQP can achieve up to 45% acceleration on QP instances. Moreover, it achieves 14% to 31% acceleration on out-of-distribution instances.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01051
Circle bundles with PSC over some four manifolds,['Differential Geometry'],"['Aditya Kumar', 'Balarka Sen']","We construct infinitely many examples of four manifolds with macroscopic dimension 4 equipped with circle bundles whose total spaces admit metrics of positive scalar curvature. Further, we verify that these bundles have macroscopic dimension at most 3. Our constructions are based on techniques from symplectic geometry.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01047
Detecting Spoof Voices in Asian Non-Native Speech: An Indonesian and Thai Case Study,['Audio and Speech Processing'],"['Aulia Adila', 'Candy Olivia Mawalim', 'Masashi Unoki']","This study focuses on building effective spoofing countermeasures (CMs) for non-native speech, specifically targeting Indonesian and Thai speakers. We constructed a dataset comprising both native and non-native speech to facilitate our research. Three key features (MFCC, LFCC, and CQCC) were extracted from the speech data, and three classic machine learning-based classifiers (CatBoost, XGBoost, and GMM) were employed to develop robust spoofing detection systems using the native and combined (native and non-native) speech data. This resulted in two types of CMs: Native and Combined. The performance of these CMs was evaluated on both native and non-native speech datasets. Our findings reveal significant challenges faced by Native CM in handling non-native speech, highlighting the necessity for domain-specific solutions. The proposed method shows improved detection capabilities, demonstrating the importance of incorporating non-native speech data into the training process. This work lays the foundation for more effective spoofing detection systems in diverse linguistic contexts.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01040
Using Reinforcement Learning to Guide Graph State Generation for Photonic Quantum Computers,['Quantum Physics'],"['Yingheng Li', 'Yue Dai', 'Aditya Pawar', 'Rongchao Dong', 'Jun Yang', 'Youtao Zhang', 'Xulong Tang']","Photonic quantum computer (PQC) is an emerging and promising quantum computing paradigm that has gained momentum in recent years. In PQC, which leverages the measurement-based quantum computing (MBQC) model, computations are executed by performing measurements on photons in graph states (i.e., sets of entangled photons) that are generated before measurements. The graph state in PQC is generated deterministically by quantum emitters. The generation process is achieved by applying a sequence of quantum gates to quantum emitters. In this process, i) the time required to complete the process, ii) the number of quantum emitters used, and iii) the number of CZ gates performed between emitters greatly affect the fidelity of the generated graph state. However, prior work for determining the generation sequence only focuses on optimizing the number of quantum emitters. Moreover, identifying the optimal generation sequence has vast search space. To this end, we propose RLGS, a novel compilation framework to identify optimal generation sequences that optimize the three metrics. Experimental results show that RLGS achieves an average reduction in generation time of 31.1%, 49.6%, and 57.5% for small, medium, and large graph states compared to the baseline.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01038
"Using Binary Population Synthesis to Examine the Impact of Binary Evolution on the C, N, O, and $S$-Process Yields of Solar-Metallicity Low- and Intermediate-Mass Stars",['Solar and Stellar Astrophysics'],"['Zara Osborn', 'Amanda I. Karakas', 'Alex J. Kemp', 'Robert Izzard', 'Devika Kamath', 'Maria Lugaro']","Asymptotic giant branch (AGB) stars play a significant role in our understanding of the origin of the elements. They contribute to the abundances of C, N, and approximately $50\%$ of the abundances of the elements heavier than iron. An aspect often neglected in studies of AGB stars is the impact of a stellar companion on AGB stellar evolution and nucleosynthesis. In this study, we update the stellar abundances of AGB stars in the binary population synthesis code \textsc{binary\_c} and calibrate our treatment of the third dredge-up using observations of Galactic carbon stars. We model stellar populations of low- to intermediate-mass stars at solar-metallicity and examine the stellar wind contributions to C, N, O, Sr, Ba, and Pb yields at binary fractions between 0 and 1. For a stellar population with a binary fraction of 0.7, we find $\sim 20-25\%$ less C and $s$-process elements ejected than from a population composed of only single stars, and we find little change in the N and O yields. We also compare our models with observed abundances from Ba stars and find our models can reproduce most Ba star abundances, but our population estimates a higher frequency of Ba stars with a surface [Ce/Y] > $+0.2\,$dex. Our models also predict the rare existence of Ba stars with masses $> 10 \text{M}\,_\odot$.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01025
Learning Structured Representations with Hyperbolic Embeddings,['Machine Learning'],"['Aditya Sinha', 'Siqi Zeng', 'Makoto Yamada', 'Han Zhao']","Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work [Zeng et al., 2022] proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context [Chen et al., 2013]. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss, and can be combined with any standard task loss to learn hierarchy-informed features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance especially under low dimensional scenarios. For a better understanding of structured representation, we perform eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at \url{https://github.com/uiuctml/HypStructure}.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01023
On the Feature Learning in Diffusion Models,['Machine Learning'],"['Andi Han', 'Wei Huang', 'Yuan Cao', 'Difan Zou']","The predominant success of diffusion models in generative modeling has spurred significant interest in understanding their theoretical foundations. In this work, we propose a feature learning framework aimed at analyzing and comparing the training dynamics of diffusion models with those of traditional classification models. Our theoretical analysis demonstrates that, under identical settings, diffusion models, due to the denoising objective, are encouraged to learn more balanced and comprehensive representations of the data. In contrast, neural networks with a similar architecture trained for classification tend to prioritize learning specific patterns in the data, often focusing on easy-to-learn components. To support these theoretical insights, we conduct several experiments on both synthetic and real-world datasets, which empirically validate our findings and highlight the distinct feature learning dynamics in diffusion models compared to classification.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01021
Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces,['Machine Learning'],"['Tobias Schröder', 'Zijing Ou', 'Yingzhen Li', 'Andrew B. Duncan']","Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on data in discrete or mixed state spaces poses significant challenges due to the lack of robust and fast sampling methods. In this work, we propose to train discrete EBMs with Energy Discrepancy, a loss function which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for Markov chain Monte Carlo. We introduce perturbations of the data distribution by simulating a diffusion process on the discrete state space endowed with a graph structure. This allows us to inform the choice of perturbation from the structure of the modelled discrete variable, while the continuous time parameter enables fine-grained control of the perturbation. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including the estimation of discrete densities with non-binary vocabulary and binary image modelling. Finally, we train EBMs on tabular data sets with applications in synthetic data generation and calibrated classification.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01019
"Transistors based on Novel 2-D Monolayer Semiconductors Bi2O2Se, InSe, and MoSi2N4 for Enhanced Logic Density Scaling",['Mesoscale and Nanoscale Physics'],"['Keshari Nandan', 'Ateeb Naseer', 'Amit Agarwal', 'Somnath Bhowmick', 'Yogesh S. Chauhan']","Making ultra-short gate-length transistors significantly contributes to scaling the contacted gate pitch. This, in turn, plays a vital role in achieving smaller standard logic cells for enhanced logic density scaling. As we push the boundaries of miniaturization, it is intriguing to consider that the ultimate limit of contacted gate pitch could be reached with remarkable 1 nm gate-length transistors. Here, we identify InSe, Bi2O2Se, and MoSi2N4 as potential two-dimensional semiconductors for 1 nm transistors with low contact resistance and outstanding interface properties. We employ a fully self-consistent ballistic quantum transport model starting from first-principle calculations. Our simulations show that the interplay between electrostatics and quantum tunneling influences the performance of these devices over the device design space. MoSi2N4 channels have the best immunity to quantum tunneling, and Bi2O2Se channel devices have the best electrostatics. We show that for a channel length of 12 nm, all the devices can deliver I_$ON$/I_$OFF$ > 10^3 , suitable for electronic applications, and Bi2O2Se is the best-performing channel material.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01016
Optimal transport and regularity of weak Kantorovich potentials on a globally hyperbolic spacetime,['Optimization and Control'],['Alec Metsch'],"We consider the optimal transportation problem on a globally hyperbolic spacetime for some Lorentzian cost function, which corresponds to the optimal transportation problem on a complete Riemannian manifold where the cost function is the Riemannian distance squared. We establish existence and uniqueness results for the optimal transport map and we investigate the regularity of weak Kantorovich potentials.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01012
Computationally-guided discovery and synthesis of the amorphous nitride Y2WN4,['Materials Science'],"['O. V. Pshyk', 'S. Zhuk', 'J. Patidar', 'A. Wieczorek', 'A. Sharma', 'J. Michler', 'C. Cancellieri', 'V. Stevanovic', 'S. Siol']","Amorphous materials offer unique functional characteristics, which are often not observed in their crystalline counterparts. This makes them invaluable for many technological applications, such as diffusion barriers in semiconductor devices. However, the computationally guided search for new functional amorphous materials with attractive properties represents a major challenge. In this work, we combine theory and experiment to discover and synthesize the amorphous ternary nitride Y2WN4. We show how computational random structure sampling offers a route to robustly identify chemistries which are hard to crystallize. Experiments prove that the predicted nitride is easily synthesized in amorphous phase with no detectable precipitates. The material exhibits remarkable stability against crystallization at high temperature and as well as excellent oxidation resistance and stability against Cu diffusion. Moreover, Y2WN4 exhibits a sharp onset of optical absorption and an indirect band gap of 2.24 eV. These properties make this material promising for the integration in electronic devices as a high-performance diffusion barrier with adjustable band edges.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01009
CoRNStack: High-Quality Contrastive Data for Better Code Ranking,['Computation and Language'],"['Tarun Suresh', 'Revanth Gangi Reddy', 'Yifei Xu', 'Zach Nussbaum', 'Andriy Mulyar', 'Brandon Duderstadt', 'Heng Ji']","Effective code retrieval plays a crucial role in advancing code generation, bug fixing, and software maintenance, particularly as software systems increase in complexity. While current code embedding models have demonstrated promise in retrieving code snippets for small-scale, well-defined tasks, they often underperform in more demanding real-world applications such as bug localization within GitHub repositories. We hypothesize that a key issue is their reliance on noisy and inconsistent datasets for training, which impedes their ability to generalize to more complex retrieval scenarios. To address these limitations, we introduce CoRNStack, a large-scale, high-quality contrastive training dataset for code that spans multiple programming languages. This dataset is curated using consistency filtering to eliminate noisy positives and is further enriched with mined hard negatives, thereby facilitating more effective learning. We demonstrate that contrastive training of embedding models using CoRNStack leads to state-of-the-art performance across a variety of code retrieval tasks. Furthermore, the dataset can be leveraged for training code reranking models, a largely underexplored area compared to text reranking. Our finetuned code reranking model significantly improves the ranking quality over the retrieved results. Finally, by employing our code retriever and reranker together, we demonstrate significant improvements in function localization for GitHub issues, an important component of real-world software development.△ Less",v1,https://arxiv.org/pdf/2412.01007
Characterizing and Modeling AI-Driven Animal Ecology Studies at the Edge,['Systems and Control'],"['Jenna Kline', ""Austin O'Quinn"", 'Tanya Berger-Wolf', 'Christopher Stewart']","Platforms that run artificial intelligence (AI) pipelines on edge computing resources are transforming the fields of animal ecology and biodiversity, enabling novel wildlife studies in animals' natural habitats. With emerging remote sensing hardware, e.g., camera traps and drones, and sophisticated AI models in situ, edge computing will be more significant in future AI-driven animal ecology (ADAE) studies. However, the study's objectives, the species of interest, its behaviors, range, habitat, and camera placement affect the demand for edge resources at runtime. If edge resources are under-provisioned, studies can miss opportunities to adapt the settings of camera traps and drones to improve the quality and relevance of captured data. This paper presents salient features of ADAE studies that can be used to model latency, throughput objectives, and provision edge resources. Drawing from studies that span over fifty animal species, four geographic locations, and multiple remote sensing methods, we characterized common patterns in ADAE studies, revealing increasingly complex workflows involving various computer vision tasks with strict service level objectives (SLO). ADAE workflow demands will soon exceed individual edge devices' compute and memory resources, requiring multiple networked edge devices to meet performance demands. We developed a framework to scale traces from prior studies and replay them offline on representative edge platforms, allowing us to capture throughput and latency data across edge configurations. We used the data to calibrate queuing and machine learning models that predict performance on unseen edge configurations, achieving errors as low as 19%.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.01000
Decoupling Optical and Thermal Responses: Thermo-optical Nonlinearities Unlock MHz Transmission Modulation in Dielectric Metasurfaces,['Optics'],"['Omer Can Karaman', 'Gopal Narmada Naidu', 'Alan R. Bowman', 'Elif Nur Dayi', 'Giulia Tagliabue']","Thermo-optical nonlinearities (TONL) in metasurfaces enable dynamic control of optical properties like transmission, reflection, and absorption through external stimuli such as laser irradiation or temperature. As slow thermal dynamics of extended systems are expected to limit modulation speeds ultimately, research has primarily focused on steady-state effects. In this study, we investigate photo-driven TONL in amorphous silicon (a-Si) metasurfaces both under steady-state and, most importantly, dynamic conditions (50 kHz modulation) using a 488 nm continuous-wave pump laser. First, we show that a non-monotonic change in the steady-state transmission occurs at wavelengths longer than the electric-dipole resonance (800 nm). In particular, at 815 nm transmission first decreases by 30% and then increases by 30% as the laser intensity is raised to 5 mW/μm2. Next, we demonstrate that TONL decouple the thermal and optical characteristic times, the latter being up to 7 times shorter in the tested conditions (i.e τopt =0.5 μs vs τth =3.5 μs). Most remarkably, we experimentally demonstrate that combining these two effects enables optical modulation at twice the speed (100 kHz) of the excitation laser modulation. We finally show how to achieve all-optical transmission modulation at MHz speeds with large amplitudes (85%). Overall, these results show that photo-driven TONL produce large and fully reversible transmission modulation in dielectric metasurfaces with fast and adjustable speeds. Therefore, they open completely new opportunities toward exploiting TONL in dynamically reconfigurable systems, from optical switching to wavefront manipulation.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00996
Secondary terms in the first moment of $|{\rm Sel}_2(E)|$,['Number Theory'],"['Arul Shankar', 'Takashi Taniguchi']","We prove the existence of secondary terms of order $X^{3/4}$, with power saving error terms, in the counting functions of $|{\rm Sel}_2(E)|$, the 2-Selmer group of E, for elliptic curves E having height bounded by X. This is the first improvement on the error term of $o(X^{5/6})$, proved by Bhargava--Shankar, where the primary term of order $X^{5/6}$ for this counting function was obtained.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00995
DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis,['Machine Learning'],"['Ahmad Mohammadshirazi', 'Ali Nosratifiroozsalari', 'Rajiv Ramnath']","Time series forecasting is a crucial yet challenging task in machine learning, requiring domain-specific knowledge due to its wide-ranging applications. While recent Transformer models have improved forecasting capabilities, they come with high computational costs. Linear-based models have shown better accuracy than Transformers but still fall short of ideal performance. To address these challenges, we introduce the Decomposition State-Space Recurrent Neural Network (DSSRNN), a novel framework designed for both long-term and short-term time series forecasting. DSSRNN uniquely combines decomposition analysis to capture seasonal and trend components with state-space models and physics-based equations. We evaluate DSSRNN's performance on indoor air quality datasets, focusing on CO2 concentration prediction across various forecasting horizons. Results demonstrate that DSSRNN consistently outperforms state-of-the-art models, including transformer-based architectures, in terms of both Mean Squared Error (MSE) and Mean Absolute Error (MAE). For example, at the shortest horizon (T=96) in Office 1, DSSRNN achieved an MSE of 0.378 and an MAE of 0.401, significantly lower than competing models. Additionally, DSSRNN exhibits superior computational efficiency compared to more complex models. While not as lightweight as the DLinear model, DSSRNN achieves a balance between performance and efficiency, with only 0.11G MACs and 437MiB memory usage, and an inference time of 0.58ms for long-term forecasting. This work not only showcases DSSRNN's success but also establishes a new benchmark for physics-informed machine learning in environmental forecasting and potentially other domains.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00994
LOFAR Deep Fields: Probing the sub-mJy regime of polarized extragalactic sources in ELAIS-N1. II. Analysis,['Astrophysics of Galaxies'],"['S. Piras', 'C. Horellou', 'J. Conway', 'M. Thomasson', 'T. W. Shimwell', ""S. P. O'Sullivan"", 'E. Carretti', 'V. Vacca', 'A. Bonafede', 'I. Prandoni']","Deep polarization surveys at low radio frequencies are key to cosmic magnetism studies: Larger catalogs of polarized extragalactic sources and increased precision on Faraday rotation measures (RMs) make it possible to probe the magneto-ionic medium along the lines of sight of the sources and to construct denser RM grids. In a first paper, we presented a search for polarized sources in deep observations of the 25 square degree area of the European Large Area ISO Survey North 1 (ELAIS-N1) field with the LOw Frequency ARray (LOFAR) in the range 114.9 to 177.4 MHz. In this paper, we investigate the properties of the polarized radio galaxies and use the catalog to produce an RM grid of the field. After identifying the host galaxies and collecting redshift information, we characterized the radio galaxies in terms of their radio morphologies, rest frame radio luminosities, and linear sizes. We calculated residual rotation measures (RRMs) by removing the Galactic RM and studied the variation in the RRMs with redshift and degree of polarization. We produced an RRM grid of the field and compared the positions of the polarized sources with those of galaxy clusters and superclusters. The radio galaxies show a variety of morphologies, including diffuse emission; Fanaroff Riley type II sources make up about half of the sample. Using available multiband catalogs, we found redshifts for the hosts of all polarized sources in the range of 0.06 to 1.9. Polarized emission is detected mainly from large radio galaxies. The RRM values have a median close to zero, and they appear to be independent of redshift and degree of polarization. The sources in the lines of sight of clusters of galaxies and of a supercluster are indistinguishable in their polarization and RRM properties from the population of sources that are not behind these structures.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00988
Provable Partially Observable Reinforcement Learning with Privileged Information,['Machine Learning'],"['Yang Cai', 'Xiangyu Liu', 'Argyris Oikonomou', 'Kaiqing Zhang']","Partial observability of the underlying states generally presents significant challenges for reinforcement learning (RL). In practice, certain \emph{privileged information}, e.g., the access to states from simulators, has been exploited in training and has achieved prominent empirical successes. To better understand the benefits of privileged information, we revisit and examine several simple and practically used paradigms in this setting. Specifically, we first formalize the empirical paradigm of \emph{expert distillation} (also known as \emph{teacher-student} learning), demonstrating its pitfall in finding near-optimal policies. We then identify a condition of the partially observable environment, the \emph{deterministic filter condition}, under which expert distillation achieves sample and computational complexities that are \emph{both} polynomial. Furthermore, we investigate another useful empirical paradigm of \emph{asymmetric actor-critic}, and focus on the more challenging setting of observable partially observable Markov decision processes. We develop a belief-weighted asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, in which one key component is a new provable oracle for learning belief states that preserve \emph{filter stability} under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL (MARL) with privileged information. We develop algorithms featuring \emph{centralized-training-with-decentralized-execution}, a popular framework in empirical MARL, with polynomial sample and (quasi-)polynomial computational complexities in both paradigms above. Compared with a few recent related theoretical studies, our focus is on understanding practically inspired algorithmic paradigms, without computationally intractable oracles.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00985
Dual-Use Commercial and Military Communications on a Single Platform using RAN Domain Specific Language,['Systems and Control'],"['Alan Gatherer', 'Chaitali Sengupta', 'Sudipta Sen', 'Jeffery H. Reed']","Despite the success of the O-RAN Alliance in developing a set of interoperable interfaces, development of unique Radio Access Network (RAN) deployments remains challenging. This is especially true for military communications, where deployments are highly specialized with limited volume. The construction and maintenance of the RAN, which is a real time embedded system, is an ill-defined NP problem requiring teams of specialized system engineers, with specialized knowledge of the hardware platform. In this paper, we introduce a RAN Domain Specific Language (RDSL(TM)) to formally describe use cases, constraints, and multi-vendor hardware/software abstraction to allow automation of RAN construction. In this DSL, system requirements are declarative, and performance constraints are guaranteed by construction using an automated system solver. Using our RAN system solver platform, Gabriel(TM) we show how a system engineer can confidently modify RAN functionality without knowledge of the underlying hardware. We show benefits for specific system requirements when compared to the manually optimized, default configuration of the Intel FlexRAN(TM), and conclude that DSL/automation driven construction of the RAN can lead to significant power and latency benefits when the deployment constraints are tuned for a specific case. We give examples of how constraints and requirements can be formatted in a ""Kubernetes style"" YAML format which allows the use of other tools, such as Ansible, to integrate the generation of these requirements into higher level automation flows such as Service Management and Orchestration (SMO).△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00983
Finite-time quantum equilibration for continuous variables,['Quantum Physics'],"['Alberto Acevedo', 'Antonio Falco']","We develop the theory of equilibration in quantum dynamics for the case were the dynamics-generating Hamiltonians have continuous spectrum. The main goal of this paper will be to propose a framework to extend the results obtained by Short in [11], where estimates for equilibration on average and effective equilibration are derived. We will primarily focus on the case where the quantum dynamics are generated by a semi-group whose generator, i.e. the Hamiltonian, has purely absolutely continuous spectrum, and show that for such a case it is compulsory to constrain ourselves to finite time equilibration; we then develop estimates analogous to the main results in the proposed setting△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00982
Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled D2D Communications,['Signal Processing'],"['Aditya Powari', 'Daniel K. C. So']","Non-orthogonal multiple access (NOMA) is widely viewed as a potential candidate for providing enhanced multiple access in future mobile networks by eliminating the orthogonal distribution of radio resources amongst the users. Nevertheless, the performance of NOMA can be significantly improved by combining it with other sophisticated technologies such as wireless data caching and device-to-device (D2D) communications. In this letter, we propose a novel cellular system model which integrates uplink NOMA with cache based device-to-device (D2D) communications. The proposed system would enable a cellular user to upload data file to base station while simultaneously exchanging useful cache content with another nearby user. We maximize the system sum rate by deriving closed form solutions for optimal power allocation. Simulation results demonstrate the superior performance of our proposed model over other potential combinations of uplink NOMA and D2D communications.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00977
Aggregation of hydrophobic-amphiphilic block copolymers,['Soft Condensed Matter'],"['S. A. Pavlenko', 'E. N. Govorun']","We analyze the aggregation of locally amphiphilic copolymers with blocky architecture and uniformly distributed amphiphilic moieties in terms of a mean-field theory. Locally amphiphilic structure is characteristic of many thermoresponsive polymers, both linear and grafted, which endows them with local surface activity. Self-assembly of such copolymers exhibits a rich diversity of morphologies, which are analyzed in the present work in the limit of high surface activity of amphiphilic dimers with solvophilic/polar pendants. Depending on the composition and architecture of the copolymer and sizes and interaction parameters of the solvophilic/polar pendants, we build morphological diagrams of copolymer solutions. Copolymers with small volume pendants form precipitates or large aggregates with internal voids containing these pendants. For moderate volume pendants, a lamellar structure (or large vesicles) is observed at smaller fractions of amphiphilic monomer units in the chain and micelles are formed at larger fractions of these units. The sizes and shapes of micelles depend on the monomer distribution along the chain, and the blocky architecture favors the existence of single spherical micelles or spherical particles that constitute compound micelles. For sufficiently large polar pendants, copolymers of both types mainly form granular branched compound micelles. The distance at which adjacent hydrophobic and polar groups are located in a chain determines the size of domains and particles formed by locally amphiphilic blocks or chains. Our predictions explain the existence of dense multicore complex micelles consisting of beads of several tens of nanometers size and other structures such as disc-shaped micelles.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00972
From Priest to Doctor: Domain Adaptaion for Low-Resource Neural Machine Translation,['Computation and Language'],"['Ali Marashian', 'Enora Rice', 'Luke Gessler', 'Alexis Palmer', 'Katharina von der Wense']","Many of the world's languages have insufficient data to train high-performing general neural machine translation (NMT) models, let alone domain-specific models, and often the only available parallel data are small amounts of religious texts. Hence, domain adaptation (DA) is a crucial issue faced by contemporary NMT and has, so far, been underexplored for low-resource languages. In this paper, we evaluate a set of methods from both low-resource NMT and DA in a realistic setting, in which we aim to translate between a high-resource and a low-resource language with access to only: a) parallel Bible data, b) a bilingual dictionary, and c) a monolingual target-domain corpus in the high-resource language. Our results show that the effectiveness of the tested methods varies, with the simplest one, DALI, being most effective. We follow up with a small human evaluation of DALI, which shows that there is still a need for more careful investigation of how to accomplish DA for low-resource NMT.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00966
Token Cropr: Faster ViTs for Quite a Few Tasks,['Computer Vision and Pattern Recognition'],"['Benjamin Bergner', 'Christoph Lippert', 'Aravindh Mahendran']","The adoption of Vision Transformers (ViTs) in resource-constrained applications necessitates improvements in inference throughput. To this end several token pruning and merging approaches have been proposed that improve efficiency by successively reducing the number of tokens. However, it remains an open problem to design a token reduction method that is fast, maintains high performance, and is applicable to various vision tasks. In this work, we present a token pruner that uses auxiliary prediction heads that learn to select tokens end-to-end based on task relevance. These auxiliary heads can be removed after training, leading to throughput close to that of a random pruner. We evaluate our method on image classification, semantic segmentation, object detection, and instance segmentation, and show speedups of 1.5 to 4x with small drops in performance. As a best case, on the ADE20k semantic segmentation benchmark, we observe a 2x speedup relative to the no-pruning baseline, with a negligible performance penalty of 0.1 median mIoU across 5 seeds.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00965
"The WALOP-North Instrument I: Optical Design, Filter Design, Calibration",['Instrumentation and Methods for Astrophysics'],"['John A. Kypriotakis', 'Siddharth Maharana', 'Ramya M. Anche', 'Chaitanya V. Rajarshi', 'A. N. Ramaprakash', 'Bhushan Joshi', 'Artem Basyrov', 'Dmitry Blinov', 'Tuhin Ghosh', 'Eirik Gjerlow', 'Sebastian Kiehlmann', 'Nikolaos Mandarakas', 'Georgia V. Panopoulou', 'Katerina Papadaki', 'Vasiliki Pavlidou', 'Timothy J. Pearson', 'Vincent Pelgrims', 'Stephen B. Potter', 'Anthony C. S. Readhead', 'Raphael Skalidis', 'Konstantinos Tassis']","The Wide Area Linear Optical Polarimeter North (WALOP-North) is an optical polarimeter designed for the needs of the PASIPHAE survey. It will be installed on the 1.3m telescope at the Skinakas Observatory in Crete, Greece. After commissioning, it will measure the polarization of millions of stars at high Galactic latitude, aiming to measure hundreds of stars per $deg^2$. The astronomical filter used in the instrument is a modified, polarimetrically-neutral broadband SDSS-r. This instrument will be pioneering one due to its large field-of-view (FoV) of $30\times 30$ $arcmin^2$ and high accuracy polarimetry measurements. The accuracy and sensitivity of the instrument in polarization fraction will be at the 0.1\% and 0.05\% level, respectively. Four separate 4k$\times$4k CCDs will be used as the instrument detectors, each imaging one of the $0°, 45°, 90°$ and $135°$ polarized FoV separately, therefore making the instrument a four-channel, one-shot polarimeter. Here, we present the overall optical design of the instrument, emphasizing on the aspects of the instrument that are different from WALOP-South. We also present a novel design of filters appropriate for polarimetry along with details on the management of the instrument size and its polarimetric calibration.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00964
LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics,['Artificial Intelligence'],"['Mijntje Meijer', 'Hadi Mohammadi', 'Ayoub Bagheri']","Large language models (LLMs) have become increasingly pivotal in various domains due the recent advancements in their performance capabilities. However, concerns persist regarding biases in LLMs, including gender, racial, and cultural biases derived from their training data. These biases raise critical questions about the ethical deployment and societal impact of LLMs. Acknowledging these concerns, this study investigates whether LLMs accurately reflect cross-cultural variations and similarities in moral perspectives. In assessing whether the chosen LLMs capture patterns of divergence and agreement on moral topics across cultures, three main methods are employed: (1) comparison of model-generated and survey-based moral score variances, (2) cluster alignment analysis to evaluate the correspondence between country clusters derived from model-generated moral scores and those derived from survey data, and (3) probing LLMs with direct comparative prompts. All three methods involve the use of systematic prompts and token pairs designed to assess how well LLMs understand and reflect cultural variations in moral attitudes. The findings of this study indicate overall variable and low performance in reflecting cross-cultural differences and similarities in moral values across the models tested, highlighting the necessity for improving models' accuracy in capturing these nuances effectively. The insights gained from this study aim to inform discussions on the ethical development and deployment of LLMs in global contexts, emphasizing the importance of mitigating biases and promoting fair representation across diverse cultural perspectives.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00962
AI Meets Antimatter: Unveiling Antihydrogen Annihilations,"['Data Analysis, Statistics and Probability']","['Ashley Ferreira', 'Mahip Singh', 'Andrea Capra', 'Ina Carli', 'Daniel Duque Quiceno', 'Wojciech T. Fedorko', 'Makoto M. Fujiwara', 'Muyan Li', 'Lars Martin', 'Yukiya Saito', 'Gareth Smith', 'Anqi Xu']","The ALPHA-g experiment at CERN aims to perform the first-ever direct measurement of the effect of gravity on antimatter, determining its weight to within 1% precision. This measurement requires an accurate prediction of the vertical position of annihilations within the detector. In this work, we present a novel approach to annihilation position reconstruction using an ensemble of models based on the PointNet deep learning architecture. The newly developed model, PointNet Ensemble for Annihilation Reconstruction (PEAR) outperforms the standard approach to annihilation position reconstruction, providing more than twice the resolution while maintaining a similarly low bias. This work may also offer insights for similar efforts applying deep learning to experiments that require high resolution and low bias.△ Less",v1,https://arxiv.org/pdf/2412.00961
Broadband study of the Be X-ray binary RX J0520.5-6932 during its outburst in 2024,['High Energy Astrophysical Phenomena'],"['H. N. Yang', 'C. Maitra', 'G. Vasilopoulos', 'F. Haberl', 'P. A. Jenke', 'A. S. Karaferias', 'R. Sharma', 'A. Beri', 'L. Ji', 'C. Jin', 'W. Yuan', 'Y. J. Zhang', 'C. Y. Wang', 'X. P. Xu', 'Y. Liu', 'W. D. Zhang', 'C. Zhang', 'Z. X. Ling', 'H. Y. Liu', 'H. Q. Cheng', 'H. W. Pan']","A new giant outburst of the Be X-ray binary RX J0520.5-6932 was detected and subsequently observed with several space-borne and ground-based instruments. This study presents a comprehensive analysis of the optical and X-ray data, focusing on the spectral and timing characteristics of selected X-ray observations. A joint fit of spectra from simultaneous observations performed by the X-ray telescope (XRT) on the Neil Gehrels Swift Observatory (Swift) and Nuclear Spectroscopic Telescope ARray (NuSTAR) provides broadband parameter constraints, including a cyclotron resonant scattering feature (CRSF) at 32.2(+0.8/-0.7) keV with no significant energy change since 2014, and a weaker Fe line. Independent spectral analyses of observations by the Lobster Eye Imager for Astronomy (LEIA), Einstein Probe (EP), Swift-XRT, and NuSTAR demonstrate the consistency of parameters across different bands. Luminosity variations during the current outburst were tracked. The light curve of the Optical Gravitational Lensing Experiment (OGLE) aligns with the X-ray data in both 2014 and 2024. Spin evolution over 10 years is studied after adding Fermi Gamma-ray Burst Monitor (GBM) data, improving the orbital parameters, with an estimated orbital period of 24.39 days, slightly differing from OGLE data. Despite intrinsic spin-up during outbursts, a spin-down of ~0.04s over 10.3 years is suggested. For the new outburst, the pulse profiles indicate a complicated energy-dependent shape, with decreases around 15 keV and 25 keV in the pulsed fraction, a first for an extragalactic source. Phase-resolved NuSTAR data indicate variations in parameters such as flux, photon index, and CRSF energy with rotation phase.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00960
Generative Language Models Potential for Requirement Engineering Applications: Insights into Current Strengths and Limitations,['Software Engineering'],"['Summra Saleem', 'Muhammad Nabeel Asim', 'Ludger Van Elst', 'Andreas Dengel']","Traditional language models have been extensively evaluated for software engineering domain, however the potential of ChatGPT and Gemini have not been fully explored. To fulfill this gap, the paper in hand presents a comprehensive case study to investigate the potential of both language models for development of diverse types of requirement engineering applications. It deeply explores impact of varying levels of expert knowledge prompts on the prediction accuracies of both language models. Across 4 different public benchmark datasets of requirement engineering tasks, it compares performance of both language models with existing task specific machine/deep learning predictors and traditional language models. Specifically, the paper utilizes 4 benchmark datasets; Pure (7,445 samples, requirements extraction),PROMISE (622 samples, requirements classification), REQuestA (300 question answer (QA) pairs) and Aerospace datasets (6347 words, requirements NER tagging). Our experiments reveal that, in comparison to ChatGPT, Gemini requires more careful prompt engineering to provide accurate predictions. Moreover, across requirement extraction benchmark dataset the state-of-the-art F1-score is 0.86 while ChatGPT and Gemini achieved 0.76 and 0.77,respectively. The State-of-the-art F1-score on requirements classification dataset is 0.96 and both language models 0.78. In name entity recognition (NER) task the state-of-the-art F1-score is 0.92 and ChatGPT managed to produce 0.36, and Gemini 0.25. Similarly, across question answering dataset the state-of-the-art F1-score is 0.90 and ChatGPT and Gemini managed to produce 0.91 and 0.88 respectively. Our experiments show that Gemini requires more precise prompt engineering than ChatGPT. Except for question-answering, both models under-perform compared to current state-of-the-art predictors across other tasks.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00959
Large Language Models as Mirrors of Societal Moral Standards,['Artificial Intelligence'],"['Evi Papadopoulou', 'Hadi Mohammadi', 'Ayoub Bagheri']","Prior research has demonstrated that language models can, to a limited extent, represent moral norms in a variety of cultural contexts. This research aims to replicate these findings and further explore their validity, concentrating on issues like 'homosexuality' and 'divorce'. This study evaluates the effectiveness of these models using information from two surveys, the WVS and the PEW, that encompass moral perspectives from over 40 countries. The results show that biases exist in both monolingual and multilingual models, and they typically fall short of accurately capturing the moral intricacies of diverse cultures. However, the BLOOM model shows the best performance, exhibiting some positive correlations, but still does not achieve a comprehensive moral understanding. This research underscores the limitations of current PLMs in processing cross-cultural differences in values and highlights the importance of developing culturally aware AI systems that better align with universal human values.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00956
STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft,['Machine Learning'],"['Nicholas Lenzen', 'Amogh Raut', 'Andrew Melnik']","Recently, the STEVE-1 approach has been introduced as a method for training generative agents to follow instructions in the form of latent CLIP embeddings. In this work, we present a methodology to extend the control modalities by learning a mapping from new input modalities to the latent goal space of the agent. We apply our approach to the challenging Minecraft domain, and extend the goal conditioning to include the audio modality. The resulting audio-conditioned agent is able to perform on a comparable level to the original text-conditioned and visual-conditioned agents. Specifically, we create an Audio-Video CLIP foundation model for Minecraft and an audio prior network which together map audio samples to the latent goal space of the STEVE-1 policy. Additionally, we highlight the tradeoffs that occur when conditioning on different modalities. Our training code, evaluation code, and Audio-Video CLIP foundation model for Minecraft are made open-source to help foster further research into multi-modal generalist sequential decision-making agents.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00949
Uhura: A Benchmark for Evaluating Scientific Question Answering and Truthfulness in Low-Resource African Languages,['Computation and Language'],"['Edward Bayes', 'Israel Abebe Azime', 'Jesujoba O. Alabi', 'Jonas Kgomo', 'Tyna Eloundou', 'Elizabeth Proehl', 'Kai Chen', 'Imaan Khadir', 'Naome A. Etori', 'Shamsuddeen Hassan Muhammad', 'Choice Mpanza', 'Igneciah Pocia Thete', 'Dietrich Klakow', 'David Ifeoluwa Adelani']","Evaluations of Large Language Models (LLMs) on knowledge-intensive tasks and factual accuracy often focus on high-resource languages primarily because datasets for low-resource languages (LRLs) are scarce. In this paper, we present Uhura -- a new benchmark that focuses on two tasks in six typologically-diverse African languages, created via human translation of existing English benchmarks. The first dataset, Uhura-ARC-Easy, is composed of multiple-choice science questions. The second, Uhura-TruthfulQA, is a safety benchmark testing the truthfulness of models on topics including health, law, finance, and politics. We highlight the challenges creating benchmarks with highly technical content for LRLs and outline mitigation strategies. Our evaluation reveals a significant performance gap between proprietary models such as GPT-4o and o1-preview, and Claude models, and open-source models like Meta's LLaMA and Google's Gemma. Additionally, all models perform better in English than in African languages. These results indicate that LMs struggle with answering scientific questions and are more prone to generating false claims in low-resource African languages. Our findings underscore the necessity for continuous improvement of multilingual LM capabilities in LRL settings to ensure safe and reliable use in real-world contexts. We open-source the Uhura Benchmark and Uhura Platform to foster further research and development in NLP for LRLs.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00948
Generalized spatial autoregressive model,['Methodology'],"['N. A. Cruz', 'J. D. Toloza-Delgado', 'O. O. Melo']","This paper presents the generalized spatial autoregression (GSAR) model, a significant advance in spatial econometrics for non-normal response variables belonging to the exponential family. The GSAR model extends the logistic SAR, probit SAR, and Poisson SAR approaches by offering greater flexibility in modeling spatial dependencies while ensuring computational feasibility. Fundamentally, theoretical results are established on the convergence, efficiency, and consistency of the estimates obtained by the model. In addition, it improves the statistical properties of existing methods and extends them to new distributions. Simulation samples show the theoretical results and allow a visual comparison with existing methods. An empirical application is made to Republican voting patterns in the United States. The GSAR model outperforms standard spatial models by capturing nuanced spatial autocorrelation and accommodating regional heterogeneity, leading to more robust inferences. These findings underline the potential of the GSAR model as an analytical tool for researchers working with categorical or count data or skewed distributions with spatial dependence in diverse domains, such as political science, epidemiology, and market research. In addition, the R codes for estimating the model are provided, which allows its adaptability in these scenarios.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00945
Bilinear Convolution Decomposition for Causal RL Interpretability,['Machine Learning'],"['Narmeen Oozeer', 'Sinem Erisken', 'Alice Rigg']","Efforts to interpret reinforcement learning (RL) models often rely on high-level techniques such as attribution or probing, which provide only correlational insights and coarse causal control. This work proposes replacing nonlinearities in convolutional neural networks (ConvNets) with bilinear variants, to produce a class of models for which these limitations can be addressed. We show bilinear model variants perform comparably in model-free reinforcement learning settings, and give a side by side comparison on ProcGen environments. Bilinear layers' analytic structure enables weight-based decomposition. Previous work has shown bilinearity enables quantifying functional importance through eigendecomposition, to identify interpretable low rank structure. We show how to adapt the decomposition to convolution layers by applying singular value decomposition to vectors of interest, to separate the channel and spatial dimensions. Finally, we propose a methodology for causally validating concept-based probes, and illustrate its utility by studying a maze-solving agent's ability to track a cheese object.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00944
Calibration through the Lens of Interpretability,['Machine Learning'],"['Alireza Torabian', 'Ruth Urner']","Calibration is a frequently invoked concept when useful label probability estimates are required on top of classification accuracy. A calibrated model is a function whose values correctly reflect underlying label probabilities. Calibration in itself however does not imply classification accuracy, nor human interpretable estimates, nor is it straightforward to verify calibration from finite data. There is a plethora of evaluation metrics (and loss functions) that each assess a specific aspect of a calibration model. In this work, we initiate an axiomatic study of the notion of calibration. We catalogue desirable properties of calibrated models as well as corresponding evaluation metrics and analyze their feasibility and correspondences. We complement this analysis with an empirical evaluation, comparing common calibration methods to employing a simple, interpretable decision tree.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00943
Limit-sure reachability for small memory policies in POMDPs is NP-complete,['Computational Complexity'],"['Ali Asadi', 'Krishnendu Chatterjee', 'Raimundo Saona', 'Ali Shafiee']","A standard model that arises in several applications in sequential decision making is partially observable Markov decision processes (POMDPs) where a decision-making agent interacts with an uncertain environment. A basic objective in such POMDPs is the reachability objective, where given a target set of states, the goal is to eventually arrive at one of them. The limit-sure problem asks whether reachability can be ensured with probability arbitrarily close to 1. In general, the limit-sure reachability problem for POMDPs is undecidable. However, in many practical cases the most relevant question is the existence of policies with a small amount of memory. In this work, we study the limit-sure reachability problem for POMDPs with a fixed amount of memory. We establish that the computational complexity of the problem is NP-complete.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00941
Control of sphalerite-chalcopyrite phase transition in CdSnAs2 for n-type thermoelectrics with high power factor,['Materials Science'],"['Shoki Kishida', 'Norihiko. L. Okamoto', 'Ryoji Katsube', 'Akira Nagaoka', 'Yoshitaro Nose']","Practical applications of thermoelectric (TE) materials are constrained by less developments of high-performance n-type materials compared to their p-type counterparts. Chalcopyrite CdSnAs2 is a promising n-type semiconductor for thermoelectrics from its narrow bandgap around 0.2 eV and exceptionally high electron mobility. In this study, we investigated the crystal growth, microstructure, and thermoelectric properties of CdSnAs2. Contrary to conventional theory of unidirectional melt growth, CdSnAs2 samples at higher cooling rates exhibited better crystallinity, while some cracks were observed in samples cooled more slowly. Thermal analyses clarified that a phase transition from sphalerite to chalcopyrite occurred after solidification in the case of slow cooling, leading to dislocations and cracks due to the lattice mismatch between phases. The analysis at rapid cooling suggested that supercooling lowers the solidification temperature and produces an appropriate microstructure. Consequently, the sample grown at the highest cooling rate (7.6 K/min) achieved an ultrahigh power factor of 3.18 mW/mK^2 at 600 K and a peak zT of 0.62 at 682 K. In the power factor, CdSnAs2 surpasses conventional binary n-type TE materials such as SnSe and PbTe, proving that CdSnAs2 is a high potential candidate for mid-temperature TE applications.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00940
Structured light and induced vorticity in superconductors II: Quantum Print with Laguerre-Gaussian beam,['Superconductivity'],"['Tien-Tien Yeh', 'Hennadii Yerzhakov', 'Logan Bishop-Van Horn', 'Srinivas Raghu', 'Alexander Balatsky']","Challenge to control the quantum states of matter via light have been at the forefront of modern research on driven quantum matter. We explore the imprinting effects of structured light on superconductors, demonstrating how the quantum numbers of light-specifically spin angular momentum, orbital angular momentum, and radial order-can be transferred to the superconducting order parameter and control vortex dynamics. Using Laguerre-Gaussian beams, we show that by tuning the quantum numbers and the amplitude of the electric field, it is possible to manipulate a variety of vortex behaviors, including breathing vortex pairs, braiding vortex pairs, vortex droplets, supervortices, and swirling 2D vortex rings. More complex structure of vortex-clusters, such as vortex-flake structures, and standing wave motions, also emerge under specific quantum numbers. These results demonstrate the ability to control SC vortex motion and phase structures through structured light, offering potential applications in quantum fluids and optical control of superconducting states. Our findings present a diagram that links light's quantum numbers to the resulting SC vortex behaviors, highlighting the capacity of light to transfer its symmetry onto superconducting condensates. We point that this approach represents the extension of the printing to quantum printing by light in a coherent state of electrons.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00935
Fuzzy Galaxies or Cirrus? Decomposition of Galactic Cirrus in Deep Wide-Field Images,['Astrophysics of Galaxies'],"['Qing Liu', 'Roberto Abraham', 'Peter G. Martin', 'William P. Bowman', 'Pieter van Dokkum', 'Shany Danieli', 'Ekta Patel', 'Steven R. Janssens', 'Zili Shen', 'Seery Chen', 'Ananthan Karunakaran', 'Michael A. Keim', 'Deborah Lokhorst', 'Imad Pasha', 'Douglas L. Welch']","Diffuse Galactic cirrus, or Diffuse Galactic Light (DGL), can be a prominent component in the background of deep wide-field imaging surveys. The DGL provides unique insights into the physical and radiative properties of dust grains in our Milky Way, and it also serves as a contaminant on deep images, obscuring the detection of background sources such as low surface brightness galaxies. However, it is challenging to disentangle the DGL from other components of the night sky. In this paper, we present a technique for the photometric characterization of Galactic cirrus, based on (1) extraction of its filamentary or patchy morphology and (2) incorporation of color constraints obtained from Planck thermal dust models. Our decomposition method is illustrated using a $\sim$10 deg$^2$ imaging dataset obtained by the Dragonfly Telephoto Array, and its performance is explored using various metrics which characterize the flatness of the sky background. As a concrete application of the technique, we show how removal of cirrus allows low surface brightness galaxies to be identified on cirrus-rich images. We also show how modeling the cirrus in this way allows optical DGL intensities to be determined with high radiometric precision.△ Less",v1,https://arxiv.org/pdf/2412.00933
Do anomalies break the momentum routing invariance?,['High Energy Physics - Theory'],['A. R. Vieira'],"The diagrammatic computation of anomalies is usually associated with the breaking of the momentum routing invariance. This is because the momentum routing is usually chosen to fulfill the desired Ward identity. In the case of the chiral anomaly, the momentum routing is chosen in order to fulfill the gauge Ward identity and break the chiral Ward identity. Although the chiral anomaly is physical because it is associated with the pion decay into two photons, this does not necessarily mean that the momentum routing invariance is broken because the momentum routing was chosen in the computation of the anomaly. In this work, we show that if gauge invariance is assumed, the chiral and the scale anomalies are independent of the momentum routing chosen and as a result they are momentum routing invariant. Thus, it turns out that momentum routing invariance might be violated when there is a gauge anomaly.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00930
A Deep Generative Model for the Design of Synthesizable Ionizable Lipids,['Machine Learning'],"['Yuxuan Ou', 'Jingyi Zhao', 'Austin Tripp', 'Morteza Rasoulianboroujeni', 'José Miguel Hernández-Lobato']","Lipid nanoparticles (LNPs) are vital in modern biomedicine, enabling the effective delivery of mRNA for vaccines and therapies by protecting it from rapid degradation. Among the components of LNPs, ionizable lipids play a key role in RNA protection and facilitate its delivery into the cytoplasm. However, designing ionizable lipids is complex. Deep generative models can accelerate this process and explore a larger candidate space compared to traditional methods. Due to the structural differences between lipids and small molecules, existing generative models used for small molecule generation are unsuitable for lipid generation. To address this, we developed a deep generative model specifically tailored for the discovery of ionizable lipids. Our model generates novel ionizable lipid structures and provides synthesis paths using synthetically accessible building blocks, addressing synthesizability. This advancement holds promise for streamlining the development of lipid-based delivery systems, potentially accelerating the deployment of new therapeutic agents, including mRNA vaccines and gene therapies.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00928
Enhanced $Q$ factor and robustness of photonic bound states in the continuum merging at locally bent trajectories,['Optics'],"['Huayu Bai', 'Andriy Shevchenko', 'Radoslaw Kolkowski']","Bound states in the continuum (BICs) in planar photonic structures have attracted broad scientific interest owing to their exceptional capability to confine light. Topological robustness of certain BICs allows them to be moved in the momentum space by tuning the geometric parameters of the structure. In this work, we study such a BIC in a one-dimensional periodic grating, and find that its momentum-space position can be made a non-monotonic function of a geometric parameter, forming a locally bent ''V''-shaped trajectory. We show that, near the turning point of this trajectory, the robustness of the BIC and its $Q$ factor can be greatly enhanced. We tune such ''V-BICs'' to almost merge with a symmetry-protected BIC at the $Γ$-point. This creates a ''K''-shaped ultrahigh-$Q$ region containing a BIC with a much higher and more stable $Q$ factor compared to the ordinary merging BICs. The ''K-BICs'' are also found to provide a strong enhancement of the $Q$ factor in finite gratings over an extremely wide range of geometric parameters. Our findings enable further advancements in the development of ultrahigh-$Q$ BICs and their applications.△ Less",v1,https://arxiv.org/pdf/2412.00925
Online convex optimization for constrained control of nonlinear systems,['Systems and Control'],"['Marko Nonhoff', 'Johannes Köhler', 'Matthias A. Müller']","This paper investigates the problem of controlling nonlinear dynamical systems subject to state and input constraints while minimizing time-varying and a priori unknown cost functions. We propose a modular approach that combines the online convex optimization framework and reference governors to solve this problem. Our method is general in the sense that we do not limit our analysis to a specific choice of online convex optimization algorithm or reference governor. We show that the dynamic regret of the proposed framework is bounded linearly in both the dynamic regret and the path length of the chosen online convex optimization algorithm, even though the online convex optimization algorithm does not account for the underlying dynamics. We prove that a linear bound with respect to the online convex optimization algorithm's dynamic regret is optimal, i.e., cannot be improved upon. Furthermore, for a standard class of online convex optimization algorithms, our proposed framework attains a bound on its dynamic regret that is linear only in the variation of the cost functions, which is known to be an optimal bound. Finally, we demonstrate implementation and flexibility of the proposed framework by comparing different combinations of online convex optimization algorithms and reference governors to control a nonlinear chemical reactor in a numerical experiment.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00922
Floquet driven long-range interactions induce super-extensive scaling in quantum battery,['Quantum Physics'],"['Stavya Puri', 'Tanoy Kanti Konar', 'Leela Ganesh Chandra Lakkaraju', 'Aditi Sen De']","Achieving quantum advantage in energy storage and power extraction is a primary objective in the design of quantum-based batteries. We explore how long-range (LR) interactions in conjunction with Floquet driving can improve the performance of quantum batteries, particularly when the battery is initialized in a fully polarized state. In particular, we exhibit that by optimizing the driving frequency, the maximum average power scales super extensively with system-size which is not achievable through next-nearest neighbor interactions or traditional unitary charging, thereby gaining genuine quantum advantage. We illustrate that the inclusion of either two-body or many-body interaction terms in the LR charging Hamiltonian leads to a scaling benefit. Furthermore, we discover that a super-linear scaling in power results from increasing the strength of interaction compared to the transverse magnetic field and the range of interaction with low fall-off rate, highlighting the advantageous role of long-range interactions in optimizing quantum battery charging.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00921
Classical elliptic integrable systems from the moduli space of instantons,['Mathematical Physics'],['Andrei Grekov'],"This paper is intended to serve as a review of a series of papers with Nikita Nekrasov, where we achieved several important results concerning the relation between the moduli space of instantons and classical integrable systems. We derive I. Krichever's Lax matrix for the elliptic Calogero-Moser system from the equivariant cohomology of the moduli space of instantons. This result also has K-theoretic and elliptic cohomology counterparts. Our methods rely upon the so-called $θ$-transform of the $qq$-characters vev's, defined as integrals of certain classes in these cohomology theories. The key step is the non-commutative Jacobi-like product formula for them. We also obtained a natural answer for the eigenvector of the Lax matrix and the horizontal section for the associated isomonodromic connection in terms of the partition function of folded instantons. As an application of our formula, we demonstrate some progress towards the spectral duality of the many-body systems in question, as well as give a new look at the quantum-classical duality between their trigonometric version and the corresponding spin chains.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00912
"Optomechanical system with nonlinear interactions: photon blockade, collapse and revival of optical oscillations",['Quantum Physics'],"['A. P. Saiko', 'G. A. Rusetsky', 'S. A. Markevich', 'R. Fedaruk']","Closed-form expressions for the average amplitude of the optical field in the optomechanical system are obtained, in which, in addition to the linear interaction, quadratic and cubic interactions of the vibrational mode of the mechanical resonator with the mode of the optical resonator are considered. It is shown that the effects of photon blockade, collapse and revival of optical oscillations in such system can be realized.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00908
Symbolic Quantitative Information Flow for Probabilistic Programs,['Cryptography and Security'],"['Philipp Schröer', 'Francesca Randone', 'Raúl Pardo', 'Andrzej Wąsowski']","It is of utmost importance to ensure that modern data intensive systems do not leak sensitive information. In this paper, the authors, who met thanks to Joost-Pieter Katoen, discuss symbolic methods to compute information-theoretic measures of leakage: entropy, conditional entropy, Kullback-Leibler divergence, and mutual information. We build on two semantic frameworks for symbolic execution of probabilistic programs. For discrete programs, we use weakest pre-expectation calculus to compute exact symbolic expressions for the leakage measures. Using Second Order Gaussian Approximation (SOGA), we handle programs that combine discrete and continuous distributions. However, in the SOGA setting, we approximate the exact semantics using Gaussian mixtures and compute bounds for the measures. We demonstrate the use of our methods in two widely used mechanisms to ensure differential privacy: randomized response and the Gaussian mechanism.△ Less",v1,https://arxiv.org/pdf/2412.00907
Towards a Proof System for Probabilistic Dynamic Logic,['Logic in Computer Science'],"['Einar Broch Johnsen', 'Eduard Kamburjan', 'Raúl Pardo', 'Erik Voogd', 'Andrzej Wąsowski']","Whereas the semantics of probabilistic languages has been extensively studied, specification languages for their properties have received less attention -- with the notable exception of recent and on-going efforts by Joost-Pieter Katoen and collaborators. In this paper, we revisit probabilistic dynamic logic (pDL), a specification logic for programs in the probabilistic guarded command language (pGCL) of McIver and Morgan. Building on dynamic logic, pDL can express both first-order state properties and probabilistic reachability properties. In this paper, we report on work in progress towards a deductive proof system for pDL. This proof system, in line with verification systems for dynamic logic such as KeY, is based on forward reasoning by means of symbolic execution.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00906
Ref-GS: Directional Factorization for 2D Gaussian Splatting,['Computer Vision and Pattern Recognition'],"['Youjia Zhang', 'Anpei Chen', 'Yumin Wan', 'Zikai Song', 'Junqing Yu', 'Yawei Luo', 'Wei Yang']","In this paper, we introduce Ref-GS, a novel approach for directional light factorization in 2D Gaussian splatting, which enables photorealistic view-dependent appearance rendering and precise geometry recovery. Ref-GS builds upon the deferred rendering of Gaussian splatting and applies directional encoding to the deferred-rendered surface, effectively reducing the ambiguity between orientation and viewing angle. Next, we introduce a spherical Mip-grid to capture varying levels of surface roughness, enabling roughness-aware Gaussian shading. Additionally, we propose a simple yet efficient geometry-lighting factorization that connects geometry and lighting via the vector outer product, significantly reducing renderer overhead when integrating volumetric attributes. Our method achieves superior photorealistic rendering for a range of open-world scenes while also accurately recovering geometry.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00905
Tomographic SAR Reconstruction for Forest Height Estimation,['Computer Vision and Pattern Recognition'],"['Grace Colverd', 'Jumpei Takami', 'Laura Schade', 'Karol Bot', 'Joseph A. Gallego-Mejia']","Tree height estimation serves as an important proxy for biomass estimation in ecological and forestry applications. While traditional methods such as photogrammetry and Light Detection and Ranging (LiDAR) offer accurate height measurements, their application on a global scale is often cost-prohibitive and logistically challenging. In contrast, remote sensing techniques, particularly 3D tomographic reconstruction from Synthetic Aperture Radar (SAR) imagery, provide a scalable solution for global height estimation. SAR images have been used in earth observation contexts due to their ability to work in all weathers, unobscured by clouds. In this study, we use deep learning to estimate forest canopy height directly from 2D Single Look Complex (SLC) images, a derivative of SAR. Our method attempts to bypass traditional tomographic signal processing, potentially reducing latency from SAR capture to end product. We also quantify the impact of varying numbers of SLC images on height estimation accuracy, aiming to inform future satellite operations and optimize data collection strategies. Compared to full tomographic processing combined with deep learning, our minimal method (partial processing + deep learning) falls short, with an error 16-21\% higher, highlighting the continuing relevance of geometric signal processing.△ Less",v1,https://arxiv.org/pdf/2412.00903
An AMReX-based Compressible Reacting Flow Solver for High-speed Reacting Flows relevant to Hypersonic Propulsion,['Fluid Dynamics'],"['Shivank Sharma', 'Ral Bielawski', 'Oliver Gibson', 'Shuzhi Zhang', 'Vansh Sharma', 'Andreas H. Rauch', 'Jagmohan Singh', 'Sebastian Abisleiman', 'Michael Ullman', 'Shivam Barwey', 'Venkat Raman']","This work presents a comprehensive framework for the efficient implementation of finite-volume-based reacting flow solvers, specifically tailored for high speed propulsion applications. Using the exascale computing project (ECP) based AMReX framework, a compressible flow solver for handling high-speed reacting flows is developed. This work is complementary to the existing PeleC solver, emphasizing specific applications that include confined shock-containing flows, stationary and moving shocks and detonations. The framework begins with a detailed exposition of the numerical methods employed, emphasizing their application to complex geometries and their effectiveness in ensuring accurate and stable numerical simulations. Subsequently, an in-depth analysis evaluates the solver's performance across canonical and practical geometries, with particular focus on computational cost and efficiency. The solver's scalability and robustness are demonstrated through practical test cases, including flow path simulations of scramjet engines and detailed analysis of various detonation phenomena.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00900
Toric Multivariate Gaussian Models from Symmetries in a Tree,['Algebraic Geometry'],"['Emma Cardwell', 'Aida Maraj', 'Alvaro Ribot']","Given a rooted tree $T$ on $n$ non-root leaves with colored and zeroed nodes, we construct a linear space $L_T$ of $n\times n$ symmetric matrices with constraints determined by the combinatorics of the tree. When $L_T$ represents the covariance matrices of a Gaussian model, it provides natural generalizations of Brownian motion tree (BMT) models in phylogenetics. When $L_T$ represents a space of concentration matrices of a Gaussian model, it gives certain colored Gaussian graphical models, which we refer to as BMT derived models. We investigate conditions under which the reciprocal variety $L_T^{-1}$ is toric. Relying on the birational isomorphism of the inverse matrix map, we show that if the BMT derived graph of $T$ is vertex-regular and a block graph, under the derived Laplacian transformation, $L_T^{-1}$ is the vanishing locus of a toric ideal. This ideal is given by the sum of the toric ideal of the Gaussian graphical model on the block graph, the toric ideal of the original BMT model, and binomial linear conditions coming from vertex-regularity. To this end, we provide monomial parametrizations for these toric models realized through paths among leaves in $T$.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00895
DPE-Net: Dual-Parallel Encoder Based Network for Semantic Segmentation of Polyps,['Image and Video Processing'],"['Malik Abdul Manan', 'Feng Jinchao', 'Shahzad Ahmed', 'Abdul Raheem']","In medical imaging, efficient segmentation of colon polyps plays a pivotal role in minimally invasive solutions for colorectal cancer. This study introduces a novel approach employing two parallel encoder branches within a network for polyp segmentation. One branch of the encoder incorporates the dual convolution blocks that have the capability to maintain feature information over increased depths, and the other block embraces the single convolution block with the addition of the previous layer's feature, offering diversity in feature extraction within the encoder, combining them before transpose layers with a depth-wise concatenation operation. Our model demonstrated superior performance, surpassing several established deep-learning architectures on the Kvasir and CVC-ClinicDB datasets, achieved a Dice score of 0.919, a mIoU of 0.866 for the Kvasir dataset, and a Dice score of 0.931 and a mIoU of 0.891 for the CVC-ClinicDB. The visual and quantitative results highlight the efficacy of our model, potentially setting a new model in medical image segmentation.△ Less",v1,https://arxiv.org/pdf/2412.00888
Ionized Gas Outflows in the Galaxy And Mass Assembly (GAMA) Survey: Signatures of AGN Feedback in Low-Mass Galaxies,['Astrophysics of Galaxies'],"['Sheyda Salehirad', 'Amy E. Reines', 'Mallory Molina']","We present a sample of 398 galaxies with ionized gas outflow signatures in their spectra from the Galaxy and Mass Assembly (GAMA) Survey Data Release 4, including 45 low-mass galaxies with stellar masses $M_*<10^{10}$ $M_\odot$. We assemble our sample by systematically searching for the presence of a second velocity component in the [O III]$λλ4959, 5007$ doublet emission line in 39,612 galaxies with redshifts $z<0.3$. The host galaxies are classified using the BPT diagram, with $\sim$89% identified as AGNs and composites and 11% as star-forming (SF) galaxies. The outflows are typically faster in AGNs with a median velocity of 936 km s$^{-1}$ compared to 655 km s$^{-1}$ in the SF objects. Of particular interest are the 45 galaxies in the low-mass range, of which a third are classified as AGNs/composites. The outflows from the low-mass AGNs are also faster and more blueshifted compared to those in the low-mass SF galaxies. This indicates that black hole outflows can affect host galaxies in the low-mass range and that AGN feedback in galaxies with $M_*<10^{10}$ $ M_\odot$ should be considered in galaxy evolution models.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00880
Tracing Hierarchical Star Formation out to Kiloparsec Scales in Nearby Spiral Galaxies with UVIT,['Astrophysics of Galaxies'],"['Shashank Gairola', 'Smitha Subramanian', 'Sreedevi M.', 'Shyam H Menon', 'Chayan Mondal', 'Sriram Krishna', 'Mousumi Das', 'Annapurni Subramaniam']","Molecular clouds fragment under the action of supersonic turbulence & gravity which results in a scale-free hierarchical distribution of star formation (SF) within galaxies. Recent studies suggest that the hierarchical distribution of SF in nearby galaxies shows a dependence on host galaxy properties. In this context, we study the nature of hierarchical SF from a few tens of pc up to several kpc in 4 nearby spiral galaxies NGC1566, NGC5194, NGC5457 & NGC7793, by leveraging the large FoV & high resolution FUV+NUV observations from the UltraViolet Imaging Telescope (UVIT). Using the two-point correlation function, we infer that the young star-forming clumps (SFCs) in the galaxies are arranged in a fractal-like hierarchical distribution, but only up to a maximum scale ($l_{corr}$) & it ranges from 0.5 kpc to 3.1 kpc. The flocculent spiral NGC7793 has $\sim$5 times smaller $l_{corr}$ than the 3 grand design spirals, possibly due to its lower mass, low pressure environment & lack of strong spiral arms. $l_{corr}$ being much smaller than the galaxy size suggests that the SF hierarchy does not extend to the full galaxy size & it is likely an effect set by multiple physical mechanisms in the galaxy. The hierarchical distribution of SFCs dissipates within 10 to 50 Myr, signifying their migration away from their birthplaces over time. Our results suggest that the global hierarchical properties of SF in galaxies are not universal & significant variations exist in the local & global hierarchy parameters of a galaxy. This study also demonstrates the capabilities of UVIT in characterizing the SF hierarchy in nearby galaxies. In the future, a bigger sample can be employed to further understand the role of large-scale galaxy properties (morphology, environment) & physical processes (feedback, turbulence, shear & ISM conditions) on determining the non-universal hierarchical properties of SF in galaxies.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00872
Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting,['Computation and Language'],"['Thilini Wijesiriwardene', 'Ruwan Wickramarachchi', 'Sreeram Vennam', 'Vinija Jain', 'Aman Chadha', 'Amitava Das', 'Ponnurangam Kumaraguru', 'Amit Sheth']","Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like ""Oxygen is to Gas as <blank> is to <blank>"" requires identifying the semantic relationship (e.g., ""type of"") between the first pair of terms (""Oxygen"" and ""Gas"") and finding a second pair that shares the same relationship (e.g., ""Aluminum"" and ""Metal""). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00869
Numerical approach to compressible shallow-water dynamics of neutron-star spreading layers,['High Energy Astrophysical Phenomena'],"['Aleksandr Rusakov', 'Pavel Abolmasov', 'Omer Bromberg']","A weakly magnetized neutron star (NS) undergoing disk accretion should release about a half of its power in a compact region known as the accretion boundary layer. Latitudinal spread of the accreted matter and efficient radiative cooling justify the approach to this flow as a two-dimensional spreading layer (SL) on the surface of the star. Numerical simulations of SLs are challenging because of the curved geometry and supersonic nature of the problem. We develop a new two-dimensional hydrodynamics code that uses the multislope second-order MUSCL scheme in combination with an HLLC+ Riemann solver on an arbitrary irregular mesh on a spherical surface. The code is suitable and accurate for Mach numbers at least up to 5-10. Adding sinks and sources to the conserved variables, we simulate constant-rate accretion onto a spherical NS. During the early stages of accretion, heating in the equatorial region triggers convective instability that causes rapid mixing in latitudinal direction. One of the outcomes of the instability is the development of a two-armed `tennis ball' pattern rotating as a rigid body. From the point of view of a high-inclination observer, its contribution to the light curve is seen as a high-quality-factor quasi-periodic oscillation mode with a frequency considerably smaller than the rotation frequency of the matter in the SL. Other variability modes seen in the simulated light curves are probably associated with low-azimuthal-number Rossby waves.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00867
Explicit and data-Efficient Encoding via Gradient Flow,['Machine Learning'],"['Kyriakos Flouris', 'Anna Volokitin', 'Gustav Bredell', 'Ender Konukoglu']","The autoencoder model typically uses an encoder to map data to a lower dimensional latent space and a decoder to reconstruct it. However, relying on an encoder for inversion can lead to suboptimal representations, particularly limiting in physical sciences where precision is key. We introduce a decoder-only method using gradient flow to directly encode data into the latent space, defined by ordinary differential equations (ODEs). This approach eliminates the need for approximate encoder inversion. We train the decoder via the adjoint method and show that costly integrals can be avoided with minimal accuracy loss. Additionally, we propose a $2^{nd}$ order ODE variant, approximating Nesterov's accelerated gradient descent for faster convergence. To handle stiff ODEs, we use an adaptive solver that prioritizes loss minimization, improving robustness. Compared to traditional autoencoders, our method demonstrates explicit encoding and superior data efficiency, which is crucial for data-scarce scenarios in the physical sciences. Furthermore, this work paves the way for integrating machine learning into scientific workflows, where precise and efficient encoding is critical. \footnote{The code for this work is available at \url{https://github.com/k-flouris/gfe}.}△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00864
Thermal Vision: Pioneering Non-Invasive Temperature Tracking in Congested Spaces,['Computer Vision and Pattern Recognition'],"['Arijit Samal', 'Haroon R Lone']","Non-invasive temperature monitoring of individuals plays a crucial role in identifying and isolating symptomatic individuals. Temperature monitoring becomes particularly vital in settings characterized by close human proximity, often referred to as dense settings. However, existing research on non-invasive temperature estimation using thermal cameras has predominantly focused on sparse settings. Unfortunately, the risk of disease transmission is significantly higher in dense settings like movie theaters or classrooms. Consequently, there is an urgent need to develop robust temperature estimation methods tailored explicitly for dense settings.
  Our study proposes a non-invasive temperature estimation system that combines a thermal camera with an edge device. Our system employs YOLO models for face detection and utilizes a regression framework for temperature estimation. We evaluated the system on a diverse dataset collected in dense and sparse settings. Our proposed face detection model achieves an impressive mAP score of over 84 in both in-dataset and cross-dataset evaluations. Furthermore, the regression framework demonstrates remarkable performance with a mean square error of 0.18$^{\circ}$C and an impressive $R^2$ score of 0.96. Our experiments' results highlight the developed system's effectiveness, positioning it as a promising solution for continuous temperature monitoring in real-world applications. With this paper, we release our dataset and programming code publicly.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00863
Deep evolving semi-supervised anomaly detection,['Machine Learning'],"['Jack Belham', 'Aryan Bhosale', 'Samrat Mukherjee', 'Biplab Banerjee', 'Fabio Cuzzolin']","The aim of this paper is to formalise the task of continual semi-supervised anomaly detection (CSAD), with the aim of highlighting the importance of such a problem formulation which assumes as close to real-world conditions as possible. After an overview of the relevant definitions of continual semi-supervised learning, its components, anomaly detection extension, and the training protocols; the paper introduces a baseline model of a variational autoencoder (VAE) to work with semi-supervised data along with a continual learning method of deep generative replay with outlier rejection. The results show that such a use of extreme value theory (EVT) applied to anomaly detection can provide promising results even in comparison to an upper baseline of joint training. The results explore the effects of how much labelled and unlabelled data is present, of which class, and where it is located in the data stream. Outlier rejection shows promising initial results where it often surpasses a baseline method of Elastic Weight Consolidation (EWC). A baseline for CSAD is put forward along with the specific dataset setups used for reproducability and testability for other practitioners. Future research directions include other CSAD settings and further research into efficient continual hyperparameter tuning.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00860
Magnetically tuned topological phase in graphene nanoribbon heterojunctions,['Mesoscale and Nanoscale Physics'],"['Wei-Jian Li', 'Da-Fei Sun', 'Sheng Ju', 'Ai-Lei He', 'Yuan Zhou']","The interplay between topology and magnetism often triggers the exotic quantum phases. Here, we report an accessible scheme to engineer the robust $\mathbb{Z}_{2}$ topology by intrinsic magnetism, originating from the zigzag segment connecting two armchair segments with different width, in one-dimensional graphene nanoribbon heterojunctions. Our first-principle and model simulations reveal that the emergent spin polarization substantially modifies the dimerization between junction states, forming the special SSH mechanism depending on the magnetic configurations. Interestingly, the topological phase in magnetic state is only determined by the width of the narrow armchair segment, in sharp contrast with that in the normal state. In addition, the emergent magnetism increases the bulk energy band gap by an order of magnitude than that in the nonmagnetic state. We also discuss the $\mathbb{Z}$ topology of the junction states and the termination-dependent of topological end states. Our results bring new way to tune the topology in graphene nanoribbon heterostructure, providing a new platform for future one-dimensional topological devices and molecular-scale spintronics.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00859
A new way to find symbiotic stars: accretion disc detection with optical survey photometry,['Solar and Stellar Astrophysics'],"['A. B. Lucy', 'J. L. Sokoloski', 'G. J. M. Luna', 'K. Mukai', 'N. E. Nuñez', 'D. A. H. Buckley', 'H. Breytenbach', 'B. Paul', 'S. B. Potter', 'R. Manick', 'D. A. Howell', 'C. Wolf', 'C. A. Onken']","Symbiotic stars are binaries in which a cool and evolved star of luminosity class I-III accretes onto a smaller companion. However, direct accretion signatures like disc flickering and boundary layer X-rays are typically outshone or suppressed by the luminous giant, shell burning on the accreting white dwarf, and the illuminated wind nebula. We present a new way to find symbiotics that is less biased against directly-detectable accretion discs than methods based on narrow-band H$α$ photometry or objective prism plate surveys. We identified outliers in SkyMapper survey photometry, using reconstructed uvg snapshot colours and rapid variability among the three exposures of each 20-minute SkyMapper Main Survey filter sequence, from a sample of 366,721 luminous red objects. We found that SkyMapper catalog colours of large-amplitude pulsating giants must be corrected for variability, and that flickering is detectable with only three data points. Our methods probed a different region of parameter space than a recent search for accreting-only symbiotics in the GALAH survey, while being surprisingly concordant with another survey's infrared detection algorithm. We discovered 12 new symbiotics, including four with optical accretion disc flickering. Two of the optical flickerers exhibited boundary-layer hard X-rays. We also identified 10 symbiotic candidates, and discovered likely optical flickering in the known symbiotic V1044 Cen (CD-36 8436). We conclude that at least 20% of the true population of symbiotics exhibit detectable optical flickering from the inner accretion disc, the majority of which do not meet the H$α$ detection thresholds used to find symbiotics in typical narrow-band surveys.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00855
"Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And Multistep Scoring",['Artificial Intelligence'],"['Avinash Anand', 'Raj Jaiswal', 'Abhishek Dharmadhikari', 'Atharva Marathe', 'Harsh Parimal Popat', 'Harshil Mital', 'Kritarth Prasad', 'Rajiv Ratn Shah', 'Roger Zimmermann']","This paper presents GPSM4K, a comprehensive geometry multimodal dataset tailored to augment the problem-solving capabilities of Large Vision Language Models (LVLMs). GPSM4K encompasses 2157 multimodal question-answer pairs manually extracted from mathematics textbooks spanning grades 7-12 and is further augmented to 5340 problems, consisting of both numerical and theorem-proving questions. In contrast to PGPS9k, Geometry3K, and Geo170K which feature only objective-type questions, GPSM4K offers detailed step-by-step solutions in a consistent format, facilitating a comprehensive evaluation of problem-solving approaches. This dataset serves as an excellent benchmark for assessing the geometric reasoning capabilities of LVLMs. Evaluation of our test set shows that there is scope for improvement needed in open-source language models in geometry problem-solving. Finetuning on our training set increases the geometry problem-solving capabilities of models. Further, We also evaluate the effectiveness of techniques such as image captioning and Retrieval Augmentation generation (RAG) on model performance. We leveraged LLM to automate the task of final answer evaluation by providing ground truth and predicted solutions. This research will help to assess and improve the geometric reasoning capabilities of LVLMs.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00846
Some properties of general-$λ$-matrix polynomials: an umbral approach,['Classical Analysis and ODEs'],"['Ghazala Yasmin', 'Aditi Sharma']","The ""2-variable general-$λ$-matrix polynomials (2VG$λ$MP)"" is a new family of matrix polynomials, introduced and studied in this article. These matrix polynomials are constructed using umbral and symbolic methods. We delve into the generating function, explicit series representation, differential equation, quasi-monomiality, summation formula, determinant representation, and various identities satisfied by these polynomials. Furthermore, a thorough investigation is conducted to ascertain the outcomes for the members of the general-$λ$-matrix family. Additionally, 3D graphs and figures showing the zeros distribution, real zeros, and stacks of zeros for a few members of the family of these polynomials are also presented in the article.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00844
Wireless Electronic-free Mechanical Metamaterial Implants,['Medical Physics'],"['Jianzhe Luo', 'Wenyun Lu', 'Pengcheng Jiao', 'Daeik Jang', 'Kaveh Barri', 'Jiajun Wang', 'Wenxuan Meng', 'Rohit Prem Kumar', 'Nitin Agarwal', 'D. Kojo Hamilton', 'Zhong Lin Wang', 'Amir H. Alavi']","Despite significant advancements in wireless smart implants over the last two decades, current implantable devices still operate passively and require additional electronic modules for wireless transmission of the stored biological data. To address these challenges, we propose an innovative wireless force sensing paradigm for implantable systems through the integration of mechanical metamaterials and nano energy harvesting technologies. We demonstrate composite mechanical metamaterial implants capable of serving as all-in-one wireless force sensing units, incorporating functions for power generation, sensing and transmission with ultra-low power requirements. In this alternative communication approach, the electrical signals harvested by the implants from mechanical stimuli are utilized directly for the wireless transmission of the sensed data. We conduct experimental and theoretical studies to demonstrate the wireless detection of the generated strain-induced polarization electric field using electrodes. The feasibility of the proposed wireless force sensing approach is evaluated through a proof-of-concept orthopedic implant in the form of a total knee replacement. The findings indicate that the created wireless, electronic-free metamaterial implants with a power output as low as 0.1 picowatts enable direct, self-powered wireless communication during force sensing across air, simulated body fluid and animal tissue. We validate the functionality of the proposed implants through a series of experiments conducted on an ex vivo human cadaver knee specimen. Furthermore, the effect of electrode size and placement on the strength of the received signals is examined. Finally, we highlight the potential of our approach to create a diverse array of mechanically-tunable wireless force sensing implants without relying on any external power sources.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00843
McKean-Vlasov stochastic equations with Hölder coefficients,['Probability'],"['Andrea Pascucci', 'Alessio Rondelli']","This work revisits the well-posedness of non-degenerate McKean-Vlasov stochastic differential equations with Hölder continuous coefficients, recently established by Chaudru de Raynal. We provide a streamlined and direct proof that leverages standard Gaussian estimates for uniformly parabolic PDEs, bypassing the need for derivatives with respect to the measure argument and extending applicability to hypoelliptic PDEs under weaker assumptions.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00834
Ab-initio Approach for Constructing Inverse Potentials for Resonant States of α-3H and α-3He Scattering,['Nuclear Theory'],"['Ishwar Kant', 'Ayushi Awasthi', 'Arushi Sharma', 'Shikha Awasthi', 'O. S. K. S. Sastri', 'M. R. Ganesh Kumar']","In this paper, the inverse potentials for the resonant f states of α-3H and α-3He are constructed using the phase function method by utilizing an ab-initio approach. A combination of three Morse functions are joined smoothly to prepare the reference potential. While the regular Morse function captures the nuclear and Coulomb interactions at short and medium ranges, an inverse Morse function is chosen to obtain the Coulomb barrier that arises because of the long-range Coulomb interaction. This reference potential is representative of a large family of curves consisting of eight distinct model parameters and two intermediate points that define the boundaries that exist between the three regions. The phase equation is solved using the Runge-Kutta 5th order method for the input reference potential to obtain the scattering phase shifts at various center of mass energies. The model parameters are then adjusted using the genetic algorithm in an iterative fashion to minimize the mean square error between the simulated and expected phase shift values. Our approach successfully constructed the inverse potentials for the resonant f states of the α-3H and α-3He systems, achieving convergence with a minimized mean square error. The resonance energies and widths for the α-3H system for the f-5/2 and f-7/2 states are determined to be [4.19 (4.14), 1.225 (0.918)] MeV and [2.20 (2.18), 0.099 (0.069)] MeV, respectively. For the f-5/2 and f-7/2 states of the α-3He system, the resonance energies and widths are [5.03 (5.14), 1.6 (1.2)] MeV and [2.99 (2.98), 0.182(0.175)] MeV, respectively. Our ab-initio approach to solve the phase equation utilizing a combination of smoothly joined Morse functions effectively captures both short-range nuclear and long-range Coulomb interactions, providing an accurate model for nuclear scattering involving charged particles.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00824
Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents,['Artificial Intelligence'],"['Raj Jaiswal', 'Dhruv Jain', 'Harsh Parimal Popat', 'Avinash Anand', 'Abhishek Dharmadhikari', 'Atharva Marathe', 'Rajiv Ratn Shah']","Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding. When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors. While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously. To address this, we introduce Mixture of Refinement Agents (MoRA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs. Our approach aims to bridge the gap between opensource LLMs and GPT-4o by utilizing the latter as error identifier to guide these refinement agents. We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA). MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00821
Formation Energy Prediction of Material Crystal Structures using Deep Learning,['Materials Science'],"['V. Torlao', 'E. A. Fajardo']","Determining the stability of chemical compounds is essential for advancing material discovery. In this study, we introduce a novel deep neural network model designed to predict a crystal's formation energy, which identifies its stability property. Our model leverages elemental fractions derived from material composition and incorporates the symmetry classification as an additional input feature. The materials' symmetry classifications represent the crystal polymorphs and are crucial for understanding phase transitions in materials. Our findings demonstrate that the integration of crystal system, point group, or space group symmetry information significantly enhances the predictive performance of the developed deep learning architecture, where the highest accuracy was achieved when space group classification was incorporated. In addition, we use the same model architecture to predict the energy above hull, an indicator to material stability, with formation energy as an additional input feature.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00819
Vid-Morp: Video Moment Retrieval Pretraining from Unlabeled Videos in the Wild,['Computer Vision and Pattern Recognition'],"['Peijun Bao', 'Chenqi Kong', 'Zihao Shao', 'Boon Poh Ng', 'Meng Hwa Er', 'Alex C. Kot']","Given a natural language query, video moment retrieval aims to localize the described temporal moment in an untrimmed video. A major challenge of this task is its heavy dependence on labor-intensive annotations for training. Unlike existing works that directly train models on manually curated data, we propose a novel paradigm to reduce annotation costs: pretraining the model on unlabeled, real-world videos. To support this, we introduce Video Moment Retrieval Pretraining (Vid-Morp), a large-scale dataset collected with minimal human intervention, consisting of over 50K videos captured in the wild and 200K pseudo annotations. Direct pretraining on these imperfect pseudo annotations, however, presents significant challenges, including mismatched sentence-video pairs and imprecise temporal boundaries. To address these issues, we propose the ReCorrect algorithm, which comprises two main phases: semantics-guided refinement and memory-consensus correction. The semantics-guided refinement enhances the pseudo labels by leveraging semantic similarity with video frames to clean out unpaired data and make initial adjustments to temporal boundaries. In the following memory-consensus correction phase, a memory bank tracks the model predictions, progressively correcting the temporal boundaries based on consensus within the memory. Comprehensive experiments demonstrate ReCorrect's strong generalization abilities across multiple downstream settings. Zero-shot ReCorrect achieves over 75% and 80% of the best fully-supervised performance on two benchmarks, while unsupervised ReCorrect reaches about 85% on both. The code, dataset, and pretrained models are available at https://github.com/baopj/Vid-Morp.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00811
Non-perturbative Effects in Attosecond Four-Wave Mixing Spectra,['Atomic Physics'],"['Sergio Yanez-Pagans', 'Nathan Harkema', 'Arvinder Sandhu', 'Coleman Cariker', 'Luca Argenti']","We study the nonlinear optical response of argon to a four-wave-mixing pulse sequence consisting of an extreme ultraviolet pulse, an overlapping collinear IR and an non-collinear delayed IR pulses. Absorption of an extreme ultraviolet and an IR photon from the collinear beams excites, sequentially, the $3s^{-1}4p$ bright state and the {$3s^{-1}3d/5s$} dark states. The subsequent absorption of an IR photon from the non-collinear beam results in an angled extreme ultraviolet emission whose variation with delay encodes coupling between autoionizing-states, dark-state lifetimes, and non-perturbative effects. Both our measurements and \emph{ab initio} simulations of the angled four-wave-mixing signal show a double-peak structure in delay dependence, in excellent agreement with each other. We attribute the minimum at the center of the signal to the rapid Rabi cycling, driven by the IR pulse, between dark states and the $3s^{-1}4p$ resonance, which results in the destructive interference in the final transition amplitude.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00808
Generative Model for Synthesizing Ionizable Lipids: A Monte Carlo Tree Search Approach,['Machine Learning'],"['Jingyi Zhao', 'Yuxuan Ou', 'Austin Tripp', 'Morteza Rasoulianboroujeni', 'José Miguel Hernández-Lobato']","Ionizable lipids are essential in developing lipid nanoparticles (LNPs) for effective messenger RNA (mRNA) delivery. While traditional methods for designing new ionizable lipids are typically time-consuming, deep generative models have emerged as a powerful solution, significantly accelerating the molecular discovery process. However, a practical challenge arises as the molecular structures generated can often be difficult or infeasible to synthesize. This project explores Monte Carlo tree search (MCTS)-based generative models for synthesizable ionizable lipids. Leveraging a synthetically accessible lipid building block dataset and two specialized predictors to guide the search through chemical space, we introduce a policy network guided MCTS generative model capable of producing new ionizable lipids with available synthesis pathways.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00807
"Percent-level timing of reionization: self-consistent, implicit-likelihood inference from XQR-30+ Ly$α$ forest data",['Cosmology and Nongalactic Astrophysics'],"['Yuxiang Qin', 'Andrei Mesinger', 'David Prelogović', 'George Becker', 'Manuela Bischetti', 'Sarah E. I. Bosman', 'Frederick B. Davies', ""Valentina D'Odorico"", 'Prakash Gaikwad', 'Martin G. Haehnelt', 'Laura Keating', 'Samuel Lai', 'Emma Ryan-Weber', 'Sindhu Satyavolu', 'Fabian Walter', 'Yongda Zhu']","The Lyman alpha (Lya) forest in the spectra of z>5 quasars provides a powerful probe of the late stages of the Epoch of Reionization (EoR). With the recent advent of exquisite datasets such as XQR-30, many models have struggled to reproduce the observed large-scale fluctuations in the Lya opacity. Here we introduce a Bayesian analysis framework that forward-models large-scale lightcones of IGM properties, and accounts for unresolved sub-structure in the Lya opacity by calibrating to higher-resolution hydrodynamic simulations. Our models directly connect physically-intuitive galaxy properties with the corresponding IGM evolution, without having to tune ""effective"" parameters or calibrate out the mean transmission. The forest data, in combination with UV luminosity functions and the CMB optical depth, are able to constrain global IGM properties at percent level precision in our fiducial model. Unlike many other works, we recover the forest observations without evoking a rapid drop in the ionizing emissivity from z~7 to 5.5, which we attribute to our sub-grid model for recombinations. In this fiducial model, reionization ends at $z=5.44\pm0.02$ and the EoR mid-point is at $z=7.7\pm0.1$. The ionizing escape fraction increases towards faint galaxies, showing a mild redshift evolution at fixed UV magnitude, Muv. Half of the ionizing photons are provided by galaxies fainter than Muv~-12, well below direct detection limits of optical/NIR instruments including JWST. We also show results from an alternative galaxy model that does not allow for a redshift evolution in the ionizing escape fraction. Despite being decisively disfavored by the Bayesian evidence, the posterior of this model is in qualitative agreement with that from our fiducial model. We caution however that our conclusions regarding the early stages of the EoR and which sources reionized the Universe are more model-dependent.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00799
A negative index metamaterial driven by phonons on a ZnO platform,['Optics'],"['Julia Ingles-Cerrillo', 'Pablo Ibanez-Romero', 'Rajveer Fandan', 'Jorge Pedros', 'Nolwenn Le Biavan', 'Jean-Michel Chauveau', 'Miguel Montes Bajo', 'Adrian Hierro']","Negative index metamaterials (NIMs) can be achieved with uniaxial hyperbolic metamaterials (HMMs) featuring $ε_{parallel}>0$ and $ε_{perpendicular}<0$. This type of approach has been traditionally realized using stacked doped/undoped semiconductor layers. Only recently surface phonon polaritons (SPhPs) have emerged as a promising low-loss alternative to surface plasmon polaritons (SPPs). Despite this advantage, the SPhP-based approach has been underexplored due to the challenges associated with ensuring high crystal quality in the heterostructure when using alloys with different phonon frequencies. In this work, we design a phononic-driven NIM using a ZnO/(Zn,Mg)O heterostructure, demonstrating control over its hyperbolic behavior through the precise selection of the Mg content and the relative layer thicknesses. Our study shows that increasing the Mg content in the ternary layers enhances the type I behavior, and that the optimal layer thickness varies depending on the Mg content. After analyzing the conditions for achieving type I hyperbolic dispersion, we experimentally demonstrate this concept with a sample featuring equal layer thicknesses and a 32% Mg concentration. We characterize the structure by means of polarized reflectance spectroscopy and use attenuated total reflectance spectroscopy to report the presence of a SPhP mode located within the type I hyperbolic region. By employing the transfer matrix method, we demonstrate that this mode exhibits negative frequency dispersion, a hallmark of type I hyperbolic modes, and isofrequency curve calculations further confirm this behavior. Controlling the design of a phononic hyperbolic type I metamaterial lays the groundwork for exploring its potential applications in attaining low-loss, sub-diffraction-limited optical modes using SPhP excitations.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00794
FeynKrack: A continuum model for quasi-brittle damage through Feynman-Kac killed diffusion,['Computational Physics'],"['Ved Prakash', 'Upadhyayula M. M. A. Sai Gopal', 'Sanhita Das', 'Ananth Ramaswamy', 'Debasish Roy']","Continuum damage mechanics (CDM) is a popular framework for modelling crack propagation in solids. The CDM uses a damage parameter to quantitatively assess what one loosely calls `material degradation'. While this parameter is sometimes given a physical meaning, the mathematical equations for its evolution are generally not consistent with such physical interpretations. Curiously, degradation in the CDM may be viewed as a change of measures, wherein the damage variable appears as the Radon-Nikodym derivative. We adopt this point of view and use a probabilistic measure-valued description for the random microcracks underlying quasi-brittle damage. We show that the evolution of the underlying density may be described via killed diffusion as in the Feynman-Kac theory. Damage growth is then interpreted as the reduction in this measure over a region, which in turn quantifies the disruption of bonds through a loss of force-transmitting mechanisms between nearby material points. Remarkably, the evolution of damage admits an approximate closed-form solution. This brings forth substantive computational ease, facilitating fast yet accurate simulations of large dimensional problems. By selecting an appropriate killing rate, one accounts for the irreversibility of damage and thus eliminates the need for ad-hoc history-dependent routes typically employed, say, in phase field modelling of damage. Our proposal FeynKrack (a short form for Feynman-Kac crack propagator) is validated and demonstrated for its efficacy through several simulations on quasi-brittle damage. It also offers a promising stochastic route for future explorations of non-equilibrium thermodynamic aspects of damage.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00791
Detection of the Long Period Variable Stars of And II Dwarf Satellite galaxy,['Astrophysics of Galaxies'],"['Hedieh Abdollahi', 'Atefeh Javadi', 'Jacco Th. van Loon', 'Iain McDonald', 'Mahdi Abdollahi', 'Elham Saremi', 'Habib G. Khosroshahi', 'Hamidreza Mahani']","We conducted an extensive study of the spheroidal dwarf satellite galaxies around the Andromeda galaxy to produce an extensive catalog of LPV stars. The optical monitoring project consists of 55 dwarf galaxies and four globular clusters that are members of the Local Group. We have made observations of these galaxies using the WFC mounted on the 2.5 m INT in nine different periods, both in the i-band filter Sloan and in the filter V-band Harris. We aim to select AGB stars with brightness variations larger than 0.2 mag to investigate the evolutionary processes in these dwarf galaxies. The resulting catalog of LPV stars in Andromeda's satellite galaxies offers updated information on features like half-light radii, TRGB magnitudes, and distance moduli. This manuscript will review the results obtained for And II galaxy. Using the Sobel filter, we have calculated the distance modulus for this satellite galaxy, which ranges from 23.90 to 24.11 mag.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00790
A Cognac shot to forget bad memories: Corrective Unlearning in GNNs,['Machine Learning'],"['Varshita Kolipaka', 'Akshit Sinha', 'Debangan Mishra', 'Sumit Kumar', 'Arvindh Arun', 'Shashwat Goel', 'Ponnurangam Kumaraguru']","Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. As graph data does not follow the independently and identically distributed (i.i.d) assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, deteriorating the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of Corrective Unlearning. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method, Cognac, which can unlearn the effect of the manipulation set even when only 5% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set while being 8x more efficient. We hope our work guides GNN developers in fixing harmful effects due to issues in real-world data post-training.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00789
Landis' conjecture: a survey,['Analysis of PDEs'],"['Aingeru Fernández-Bertolin', 'Diana Stan', 'Luz Roncal']","We survey Kondrat'ev--Landis' conjecture, providing an up-to-date account of the main advances and describing the techniques developed. We complement the overview with references and formulations of the problem in further closely connected contexts.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00788
Memories of Forgotten Concepts,['Computer Vision and Pattern Recognition'],"['Matan Rusanovsky', 'Shimon Malnick', 'Amir Jevnisek', 'Ohad Fried', 'Shai Avidan']","Diffusion models dominate the space of text-to-image generation, yet they may produce undesirable outputs, including explicit content or private data. To mitigate this, concept ablation techniques have been explored to limit the generation of certain concepts. In this paper, we reveal that the erased concept information persists in the model and that erased concept images can be generated using the right latent. Utilizing inversion methods, we show that there exist latent seeds capable of generating high quality images of erased concepts. Moreover, we show that these latents have likelihoods that overlap with those of images outside the erased concept. We extend this to demonstrate that for every image from the erased concept set, we can generate many seeds that generate the erased concept. Given the vast space of latents capable of generating ablated concept images, our results suggest that fully erasing concept information may be intractable, highlighting possible vulnerabilities in current concept ablation techniques.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00782
Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher Quality Maps,['Computer Vision and Pattern Recognition'],"['Girmaw Abebe Tadesse', 'Caleb Robinson', 'Charles Mwangi', 'Esther Maina', 'Joshua Nyakundi', 'Luana Marotti', 'Gilles Quentin Hacheme', 'Hamed Alemohammad', 'Rahul Dodhia', 'Juan M. Lavista Ferres']","Approximately 20% of Africa's population suffered from undernourishment, and 868 million people experienced moderate to severe food insecurity in 2022. Land-use and land-cover maps provide crucial insights for addressing food insecurity, e.g., by mapping croplands. The development of global land-cover maps has been facilitated by the increasing availability of earth observation data and advancements in geospatial machine learning. However, these global maps exhibit lower accuracy and inconsistencies in Africa, partly due to the lack of representative training data. To address this issue, we propose a data-centric framework with a teacher-student model setup, which uses diverse data sources of satellite images and label examples to produce local land-cover maps. Our method trains a high-resolution teacher model on images with a resolution of 0.331 m/pixel and a low-resolution student model on publicly available images with a resolution of 10 m/pixel. The student model also utilizes the teacher model's output as its weak label examples through knowledge distillation. We evaluated our framework using Murang'a County, Kenya, as a use case and achieved significant improvements, i.e., 0.14 in the F1 score and 0.21 in Intersection-over-Union, compared to the best global map. Our evaluation also revealed inconsistencies in existing global maps, with a maximum agreement rate of 0.30 among themselves. Insights obtained from our cross-collaborative work can provide valuable guidance to local and national policymakers in making informed decisions to improve resource utilization and food security.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00777
Post-Vaccination COVID-19 Data Analysis: Privacy and Ethics,['Cryptography and Security'],"['Sankha Das', 'Amit Dua']","The COVID-19 pandemic has severely affected the world in terms of health, economy and peace. Fortunately, the countries are trying to overcome the situation by actively carrying out vaccinations. However, like any other massive operation involving humans such as human resource management, elections, surveys, etc., the vaccination process raises several questions about citizen privacy and misuse of personal data. In most of the countries, few attempts have been made to verify the vaccination statistics as reported by the health centers. These issues collectively require the solutions of anonymity of citizens' personal information, immutability of vaccination data and easy yet restricted access by adversarial bodies such as the government for the verification and analysis of the data. This paper introduces a blockchain-based application to simulate and monitor the vaccination process. The structure of data model used in the proposed system is based on the IEEE Standard for Data Format for Blockchain Systems 2418.2TM-2020. The proposed system enables authorized stakeholders to share and access relevant information for vaccination process chain while preserving citizen privacy and accountability of the system. It is implemented on the Ethereum blockchain and uses a Python API for the simulation and validation of each step of the vaccination process.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00774
Messenger size optimality in cellular communications,['Biological Physics'],"['Arash Tirandaz', 'Abolfazl Ramezanpour', 'Vivi Rottschäfer', 'Mehrad Babaei', 'Andrei Zinovyev', 'Alireza Mashaghi']","Living cells presumably employ optimized information transfer methods, enabling efficient communication even in noisy environments. As expected, the efficiency of chemical communications between cells depends on the properties of the molecular messenger. Evidence suggests that proteins from narrow ranges of molecular masses have been naturally selected to mediate cellular communications, yet the underlying communication design principles are not understood. Using a simple physical model that considers the cost of chemical synthesis, diffusion, molecular binding, and degradation, we show that optimal mass values exist that ensure efficient communication of various types of signals. Our findings provide insights into the design principles of biological communications and can be used to engineer chemically communicating biomimetic systems.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00771
Periodical orbits and waveforms with spontaneous Lorentz symmetry-breaking in Kalb-Ramond gravity,['General Relativity and Quantum Cosmology'],"['Ednaldo L. B. Junior', 'José Tarciso S. S. Junior', 'Francisco S. N. Lobo', 'Manuel E. Rodrigues', 'Diego Rubiera-Garcia', 'Luís F. Dias da Silva', 'Henrique A. Vieira']","In this paper, we study time-like geodesics around a spherically symmetric black hole in Kalb-Ramond (KR) gravity, characterized by the parameter $l$, which induces spontaneous Lorentz symmetry breaking. The geodesic equations and effective potential are derived to investigate the influence of $l$. We calculate the marginally bound orbits and innermost stable circular orbits, analyzing the parameter's impact. Periodic orbits are computed numerically and classified within the standard taxonomy, revealing significant effects of $l$ on their momentum and energy. Additionally, we explore an extreme mass ratio inspiral system under the adiabatic approximation to derive gravitational waveforms emitted by an object orbiting a supermassive black hole in KR gravity. These waveforms reflect the distinctive characteristics of periodic orbits and highlight the influence of $l$. With advancements in gravitational wave detection, these results offer insights into black holes influenced by Lorentz symmetry-breaking fields.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00769
EnFed: An Energy-aware Opportunistic Federated Learning in Resource Constrained Environments for Human Activity Recognition,"['Distributed, Parallel, and Cluster Computing']","['Anwesha Mukherjee', 'Rajkumar Buyya']","This paper proposes an energy-efficient federated learning method and its application in human activity monitoring and recognition. In the proposed approach, the device that needs a model for an application requests its nearby devices for collaboration. The nearby devices that accept the request, send their model updates to the requesting device. The device receives the model updates from the collaborators and performs aggregation to build its model. As mobile devices have limited battery life, the number of rounds is decided based on the desired accuracy level and battery level of the requesting device. The performance of the proposed approach is evaluated with respect to prediction accuracy, training time, training energy consumption of the device, and response time. We have used two different datasets for performance evaluation. The first dataset contains different types of physical activities and the respective calorie burn. The second dataset is a human activity recognition dataset that considers six types of physical activities. The experimental results show that using the proposed method the training time and training energy consumption of the device are reduced by approximately 59% and 19% for the first and second datasets respectively, than the decentralized federated learning approach, while using LSTM as the underlying data analysis model. The results also present that the proposed method reduces the training time and energy consumption by approximately 55% and 72% for the first and second datasets respectively, than the decentralized federated learning approach while using MLP as the underlying data analysis model.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00768
SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts,['Computation and Language'],"['Aihua Pei', 'Zehua Yang', 'Shunan Zhu', 'Ruoxi Cheng', 'Ju Jia']","Traditional methods for evaluating the robustness of large language models (LLMs) often rely on standardized benchmarks, which can escalate costs and limit evaluations across varied domains. This paper introduces a novel framework designed to autonomously evaluate the robustness of LLMs by incorporating refined adversarial prompts and domain-constrained knowledge guidelines in the form of knowledge graphs. Our method systematically generates descriptive sentences from domain-constrained knowledge graph triplets to formulate adversarial prompts, enhancing the relevance and challenge of the evaluation. These prompts, generated by the LLM itself and tailored to evaluate its own robustness, undergo a rigorous filtering and refinement process, ensuring that only those with high textual fluency and semantic fidelity are used. This self-evaluation mechanism allows the LLM to evaluate its robustness without the need for external benchmarks. We assess the effectiveness of our framework through extensive testing on both proprietary models like ChatGPT and open-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that our approach not only reduces dependency on conventional data but also provides a targeted and efficient means of evaluating LLM robustness in constrained domains.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00765
Symmetric Truncated Freud polynomials,['Classical Analysis and ODEs'],"['Edmundo J. Huertas', 'Alberto Lastra', 'Francisco Marcellán', 'Víctor Soto-Larrosa']","We define the family of symmetric truncated Freud polynomials $P_n(x;z)$, orthogonal with respect to the linear functional $\mathbf{u}$ defined by \begin{equation*} \langle \mathbf{u}, p(x)\rangle = \int_{-z}^z p(x)e^{-x^4}dx, \quad p\in \mathbb{P}, \quad z>0. \end{equation*} The semiclassical character of $P_n (x; z)$ as polynomials of class $4$ is stated. As a consequence, several properties of $P_n (x; z)$ concerning the coefficients $γ_n (z)$ in the three-term recurrence relation they satisfy as well as the moments and the Stieltjes function of $\mathbf{u}$ are studied. Ladder operators associated with such a linear functional and the holonomic equation that the polynomials $P_n (x; z)$ satisfy are deduced. Finally, an electrostatic interpretation of the zeros of such polynomials and their dynamics in terms of the parameter $z$ are given.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00764
"Automating Feedback Analysis in Surgical Training: Detection, Categorization, and Assessment",['Audio and Speech Processing'],"['Firdavs Nasriddinov', 'Rafal Kocielnik', 'Arushi Gupta', 'Cherine Yang', 'Elyssa Wong', 'Anima Anandkumar', 'Andrew Hung']","This work introduces the first framework for reconstructing surgical dialogue from unstructured real-world recordings, which is crucial for characterizing teaching tasks. In surgical training, the formative verbal feedback that trainers provide to trainees during live surgeries is crucial for ensuring safety, correcting behavior immediately, and facilitating long-term skill acquisition. However, analyzing and quantifying this feedback is challenging due to its unstructured and specialized nature. Automated systems are essential to manage these complexities at scale, allowing for the creation of structured datasets that enhance feedback analysis and improve surgical education. Our framework integrates voice activity detection, speaker diarization, and automated speech recaognition, with a novel enhancement that 1) removes hallucinations (non-existent utterances generated during speech recognition fueled by noise in the operating room) and 2) separates speech from trainers and trainees using few-shot voice samples. These aspects are vital for reconstructing accurate surgical dialogues and understanding the roles of operating room participants. Using data from 33 real-world surgeries, we demonstrated the system's capability to reconstruct surgical teaching dialogues and detect feedback instances effectively (F1 score of 0.79+/-0.07). Moreover, our hallucination removal step improves feedback detection performance by ~14%. Evaluation on downstream clinically relevant tasks of predicting Behavioral Adjustment of trainees and classifying Technical feedback, showed performances comparable to manual annotations with F1 scores of 0.82+/0.03 and 0.81+/0.03 respectively. These results highlight the effectiveness of our framework in supporting clinically relevant tasks and improving over manual methods.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00760
Plasmon-polaritons on a single electron,['High Energy Physics - Phenomenology'],"['I. M. Akimov', 'P. O. Kazinski', 'A. A. Sokolov']","The explicit expression for the photon polarization operator in the presence of a single electron is found in the $in$-$in$ formalism in the one-loop approximation out of the photon mass-shell. This polarization operator describes the dielectric permittivity of a single electron wave packet in coherent scattering processes. The plasmons and plasmon-polaritons supported by a single electron wave packet are described. The two limiting cases are considered: the wavelength of the external electromagnetic field is much smaller than the typical scale of variations of the electron wave packet and the wavelength of the external electromagnetic field is much larger than the size of the electron wave packet. In the former case, there are eight independent plasmon-polariton modes. In the latter case, the plasmons boil down to the dynamical dipole moment attached to a point electron. Thus, in the infrared limit, the electron possesses a dynamical electric dipole moment manifesting itself in coherent scattering processes.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00750
BDefects4NN: A Backdoor Defect Database for Controlled Localization Studies in Neural Networks,['Software Engineering'],"['Yisong Xiao', 'Aishan Liu', 'Xinwei Zhang', 'Tianyuan Zhang', 'Tianlin Li', 'Siyuan Liang', 'Xianglong Liu', 'Yang Liu', 'Dacheng Tao']","Pre-trained large deep learning models are now serving as the dominant component for downstream middleware users and have revolutionized the learning paradigm, replacing the traditional approach of training from scratch locally. To reduce development costs, developers often integrate third-party pre-trained deep neural networks (DNNs) into their intelligent software systems. However, utilizing untrusted DNNs presents significant security risks, as these models may contain intentional backdoor defects resulting from the black-box training process. These backdoor defects can be activated by hidden triggers, allowing attackers to maliciously control the model and compromise the overall reliability of the intelligent software. To ensure the safe adoption of DNNs in critical software systems, it is crucial to establish a backdoor defect database for localization studies. This paper addresses this research gap by introducing BDefects4NN, the first backdoor defect database, which provides labeled backdoor-defected DNNs at the neuron granularity and enables controlled localization studies of defect root causes. In BDefects4NN, we define three defect injection rules and employ four representative backdoor attacks across four popular network architectures and three widely adopted datasets, yielding a comprehensive database of 1,654 backdoor-defected DNNs with four defect quantities and varying infected neurons. Based on BDefects4NN, we conduct extensive experiments on evaluating six fault localization criteria and two defect repair techniques, which show limited effectiveness for backdoor defects. Additionally, we investigate backdoor-defected models in practical scenarios, specifically in lane detection for autonomous driving and large language models (LLMs), revealing potential threats and highlighting current limitations in precise defect localization.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00746
Overview of NR Enhancements for Extended Reality (XR) in 3GPP 5G-Advanced,['Networking and Internet Architecture'],"['Margarita Gapeyenko', 'Stefano Paris', 'Markus Isomaki', 'Boyan Yanakiev', 'Abolfazl Amiri', 'Benoist Sébire', 'Jorma Kaikkonen', 'Chunli Wu', 'Klaus I. Pedersen']","Extended reality (XR) is unlocking numerous possibilities and continues attracting individuals and larger groups across different business sectors. With Virtual reality (VR), Augmented reality (AR), or Mixed reality (MR) it is possible to improve the way we access, deliver and exchange information in education, health care, entertainment, and many other aspects of our daily lives. However, to fully exploit the potential of XR, it is important to provide reliable, fast and secure wireless connectivity to the users of XR and that requires refining existing solutions and tailoring those to support XR services. This article presents a tutorial on 3GPP 5G-Advanced Release 18 XR activities, summarizing physical as well as higher layer enhancements introduced for New Radio considering the specifics of XR. In addition, we also describe enhancements across 5G system architecture that impacted radio access network. Furthermore, the paper provides system-level simulation results for several Release 18 enhancements to show their benefits in terms of XR capacity and power saving gains. Finally, it concludes with an overview of future work in Release 19 that continues developing features to support XR services.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00741
Refine3DNet: Scaling Precision in 3D Object Reconstruction from Multi-View RGB Images using Attention,['Computer Vision and Pattern Recognition'],"['Ajith Balakrishnan', 'Sreeja S', 'Linu Shine']","Generating 3D models from multi-view 2D RGB images has gained significant attention, extending the capabilities of technologies like Virtual Reality, Robotic Vision, and human-machine interaction. In this paper, we introduce a hybrid strategy combining CNNs and transformers, featuring a visual auto-encoder with self-attention mechanisms and a 3D refiner network, trained using a novel Joint Train Separate Optimization (JTSO) algorithm. Encoded features from unordered inputs are transformed into an enhanced feature map by the self-attention layer, decoded into an initial 3D volume, and further refined. Our network generates 3D voxels from single or multiple 2D images from arbitrary viewpoints. Performance evaluations using the ShapeNet datasets show that our approach, combined with JTSO, outperforms state-of-the-art techniques in single and multi-view 3D reconstruction, achieving the highest mean intersection over union (IOU) scores, surpassing other models by 4.2% in single-view reconstruction.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00731
Omega theorem for fractional sigma function,['Number Theory'],"['Yuan Qiu', 'Alexander B. Kalmynin']","The research in the subfield of analytic number theory around error term of summation of sigma functions possesses a history which can be dated back to the mid-19th century when Dirichlet provided an $O(\sqrt{n})$ estimation of error term of summation of $d(n)$. Later, G. Voronoi, G. Kolesnik, and M.N. Huxley (to name just a few) contributed more on the upper bound on the error term of summation of sigma functions. As for $Ω$-theorems, G.H. Hardy was the first contributor. Later researchers on this topic include G.H. Hardy and T.H. Gronwall, but the amount of academic effort is much sparser than $O$-theorems. This research aims to provide a better $Ω$-bound for the error term of summation of fractional sigma function $σ_α(n)$ on the range $0 < α< \frac{1}{2}$, obtaining the result $Ω((x \ln x)^{\frac{1}{4}+\fracα{2}})$.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00723
Well log data generation and imputation using sequence-based generative adversarial networks,['Geophysics'],"['Abdulrahman Al-Fakih', 'A. Koeshidayatullah', 'Tapan Mukerji', 'Sadam Al-Azani', 'SanLinn I. Kaka']","Well log analysis is crucial for hydrocarbon exploration, providing detailed insights into subsurface geological formations. However, gaps and inaccuracies in well log data, often due to equipment limitations, operational challenges, and harsh subsurface conditions, can introduce significant uncertainties in reservoir evaluation. Addressing these challenges requires effective methods for both synthetic data generation and precise imputation of missing data, ensuring data completeness and reliability. This study introduces a novel framework utilizing sequence-based generative adversarial networks (GANs) specifically designed for well log data generation and imputation. The framework integrates two distinct sequence-based GAN models: Time Series GAN (TSGAN) for generating synthetic well log data and Sequence GAN (SeqGAN) for imputing missing data. Both models were tested on a dataset from the North Sea, Netherlands region, focusing on different sections of 5, 10, and 50 data points. Experimental results demonstrate that this approach achieves superior accuracy in filling data gaps compared to other deep learning models for spatial series analysis. The method yielded R^2 values of 0.921, 0.899, and 0.594, with corresponding mean absolute percentage error (MAPE) values of 8.320, 0.005, and 151.154, and mean absolute error (MAE) values of 0.012, 0.005, and 0.032, respectively. These results set a new benchmark for data integrity and utility in geosciences, particularly in well log data analysis.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00718
Data-Driven Solution Portfolios,['Data Structures and Algorithms'],"['Marina Drygala', 'Silvio Lattanzi', 'Andreas Maggiori', 'Miltiadis Stouras', 'Ola Svensson', 'Sergei Vassilvitskii']","In this paper, we consider a new problem of portfolio optimization using stochastic information. In a setting where there is some uncertainty, we ask how to best select $k$ potential solutions, with the goal of optimizing the value of the best solution. More formally, given a combinatorial problem $Π$, a set of value functions $V$ over the solutions of $Π$, and a distribution $D$ over $V$, our goal is to select $k$ solutions of $Π$ that maximize or minimize the expected value of the {\em best} of those solutions. For a simple example, consider the classic knapsack problem: given a universe of elements each with unit weight and a positive value, the task is to select $r$ elements maximizing the total value. Now suppose that each element's weight comes from a (known) distribution. How should we select $k$ different solutions so that one of them is likely to yield a high value?
  In this work, we tackle this basic problem, and generalize it to the setting where the underlying set system forms a matroid. On the technical side, it is clear that the candidate solutions we select must be diverse and anti-correlated; however, it is not clear how to do so efficiently. Our main result is a polynomial-time algorithm that constructs a portfolio within a constant factor of the optimal.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00717
GenTact Toolbox: A Computational Design Pipeline to Procedurally Generate Context-Driven 3D Printed Whole-Body Tactile Skins,['Robotics'],"['Carson Kohlbrenner', 'Caleb Escobedo', 'S. Sandra Bae', 'Alexander Dickhans', 'Alessandro Roncone']","Developing whole-body tactile skins for robots remains a challenging task, as existing solutions often prioritize modular, one-size-fits-all designs, which, while versatile, fail to account for the robot's specific shape and the unique demands of its operational context. In this work, we introduce the GenTact Toolbox, a computational pipeline for creating versatile whole-body tactile skins tailored to both robot shape and application domain. Our pipeline includes procedural mesh generation for conforming to a robot's topology, task-driven simulation to refine sensor distribution, and multi-material 3D printing for shape-agnostic fabrication. We validate our approach by creating and deploying six capacitive sensing skins on a Franka Research 3 robot arm in a human-robot interaction scenario. This work represents a shift from one-size-fits-all tactile sensors toward context-driven, highly adaptable designs that can be customized for a wide range of robotic systems and applications.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00711
On-Chain Credit Risk Score (OCCR Score) in DeFi,['Risk Management'],"['Rik Ghosh', 'Arka Datta', 'Vidhi Aggarwal', 'Sudipan Sinha', 'Abhimanyu Nag']","Decentralized Finance (DeFi) and smart contracts are the next generation and fast-growing market for quick and safe interaction between lenders and borrowers. However for maintaining a streamline ecosystem it is necessary to understand the risk associated with the particular user under consideration. In this paper we have developed 'On Chain Credit Risk Score' of a wallet is an answer to quantifying the risk of the particular wallet, namely the probability that the particular wallet may have a loan liquidated. 'On Chain Credit Risk Score (OCCR Score)' of wallets, will help lending borrowing protocols and other DeFi institutes to understand the risk involved in giving out loans to a wallet and may change the Loan-to-Value (LTV) Ratio and subsequently the Liquidation Threshold (LT) if required.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00710
The Forking Way: When TEEs Meet Consensus,['Cryptography and Security'],"['Annika Wilde', 'Tim Niklas Gruel', 'Claudio Soriente', 'Ghassan Karame']","An increasing number of distributed platforms combine Trusted Execution Environments (TEEs) with blockchains. Indeed, many hail the combination of TEEs and blockchains a good ""marriage"": TEEs bring confidential computing to the blockchain while the consensus layer could help defend TEEs from forking attacks.
  In this paper, we systemize how current blockchain solutions integrate TEEs and to what extent they are secure against forking attacks. To do so, we thoroughly analyze 29 proposals for TEE-based blockchains, ranging from academic proposals to production-ready platforms. We uncover a lack of consensus in the community on how to combine TEEs and blockchains. In particular, we identify four broad means to interconnect TEEs with consensus, analyze their limitations, and discuss possible remedies. Our analysis also reveals previously undocumented forking attacks on three production-ready TEE-based blockchains: Ten, Phala, and the Secret Network. We leverage our analysis to propose effective countermeasures against those vulnerabilities; we responsibly disclosed our findings to the developers of each affected platform.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00706
A Machine Learning Approach to Contact Localization in Variable Density Three-Dimensional Tactile Artificial Skin,['Robotics'],"['Carson Kohlbrenner', 'Mitchell Murray', 'Yutong Zhang', 'Caleb Escobedo', 'Thomas Dunnington', 'Nolan Stevenson', 'Nikolaus Correll', 'Alessandro Roncone']","Estimating the location of contact is a primary function of artificial tactile sensing apparatuses that perceive the environment through touch. Existing contact localization methods use flat geometry and uniform sensor distributions as a simplifying assumption, limiting their ability to be used on 3D surfaces with variable density sensing arrays. This paper studies contact localization on an artificial skin embedded with mutual capacitance tactile sensors, arranged non-uniformly in an unknown distribution along a semi-conical 3D geometry. A fully connected neural network is trained to localize the touching points on the embedded tactile sensors. The studied online model achieves a localization error of $5.7 \pm 3.0$ mm. This research contributes a versatile tool and robust solution for contact localization that is ambiguous in shape and internal sensor distribution.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00689
Towards Privacy-Preserving Medical Imaging: Federated Learning with Differential Privacy and Secure Aggregation Using a Modified ResNet Architecture,['Machine Learning'],"['Mohamad Haj Fares', 'Ahmed Mohamed Saad Emam Saad']","With increasing concerns over privacy in healthcare, especially for sensitive medical data, this research introduces a federated learning framework that combines local differential privacy and secure aggregation using Secure Multi-Party Computation for medical image classification. Further, we propose DPResNet, a modified ResNet architecture optimized for differential privacy. Leveraging the BloodMNIST benchmark dataset, we simulate a realistic data-sharing environment across different hospitals, addressing the distinct privacy challenges posed by federated healthcare data. Experimental results indicate that our privacy-preserving federated model achieves accuracy levels close to non-private models, surpassing traditional approaches while maintaining strict data confidentiality. By enhancing the privacy, efficiency, and reliability of healthcare data management, our approach offers substantial benefits to patients, healthcare providers, and the broader healthcare ecosystem.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00687
FlashSLAM: Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction with Gaussian Splatting,['Computer Vision and Pattern Recognition'],"['Phu Pham', 'Damon Conover', 'Aniket Bera']","We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based SLAM methods often fall short in sparse view settings and during large camera movements due to their reliance on gradient descent-based optimization, which is both slow and inaccurate. FlashSLAM addresses these limitations by combining 3DGS with a fast vision-based camera tracking technique, utilizing a pretrained feature matching model and point cloud registration for precise pose estimation in under 80 ms - a 90% reduction in tracking time compared to SplaTAM - without costly iterative rendering. In sparse settings, our method achieves up to a 92% improvement in average tracking accuracy over previous methods. Additionally, it accounts for noise in depth sensors, enhancing robustness when using unspecialized devices such as smartphones. Extensive experiments show that FlashSLAM performs reliably across both sparse and dense settings, in synthetic and real-world environments. Evaluations on benchmark datasets highlight its superior accuracy and efficiency, establishing FlashSLAM as a versatile and high-performance solution for SLAM, advancing the state-of-the-art in 3D reconstruction across diverse applications.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00682
MIMIC: Multimodal Islamophobic Meme Identification and Classification,['Computer Vision and Pattern Recognition'],"['S M Jishanul Islam', 'Sahid Hossain Mustakim', 'Sadia Ahmmed', 'Md. Faiyaz Abdullah Sayeedi', 'Swapnil Khandoker', 'Syed Tasdid Azam Dhrubo', 'Nahid Hossain']","Anti-Muslim hate speech has emerged within memes, characterized by context-dependent and rhetorical messages using text and images that seemingly mimic humor but convey Islamophobic sentiments. This work presents a novel dataset and proposes a classifier based on the Vision-and-Language Transformer (ViLT) specifically tailored to identify anti-Muslim hate within memes by integrating both visual and textual representations. Our model leverages joint modal embeddings between meme images and incorporated text to capture nuanced Islamophobic narratives that are unique to meme culture, providing both high detection accuracy and interoperability.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00681
Remote Estimation Games with Random Walk Processes: Stackelberg Equilibrium,['Information Theory'],"['Atahan Dokme', 'Raj Kiriti Velicheti', 'Melih Bastopcu', 'Tamer Başar']","Remote estimation is a crucial element of real time monitoring of a stochastic process. While most of the existing works have concentrated on obtaining optimal sampling strategies, motivated by malicious attacks on cyber-physical systems, we model sensing under surveillance as a game between an attacker and a defender. This introduces strategic elements to conventional remote estimation problems. Additionally, inspired by increasing detection capabilities, we model an element of information leakage for each player. Parameterizing the game in terms of uncertainty on each side, information leakage, and cost of sampling, we consider the Stackelberg Equilibrium (SE) concept where one of the players acts as the leader and the other one as the follower. By focusing our attention on stationary probabilistic sampling policies, we characterize the SE of this game and provide simulations to show the efficacy of our results.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00679
2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification,['Computer Vision and Pattern Recognition'],"['Jingwei Zhang', 'Anh Tien Nguyen', 'Xi Han', 'Vincent Quoc-Huy Trinh', 'Hong Qin', 'Dimitris Samaras', 'Mahdi S. Hosseini']","Efficiently modeling large 2D contexts is essential for various fields including Giga-Pixel Whole Slide Imaging (WSI) and remote sensing. Transformer-based models offer high parallelism but face challenges due to their quadratic complexity for handling long sequences. Recently, Mamba introduced a selective State Space Model (SSM) with linear complexity and high parallelism, enabling effective and efficient modeling of wide context in 1D sequences. However, extending Mamba to vision tasks, which inherently involve 2D structures, results in spatial discrepancies due to the limitations of 1D sequence processing. On the other hand, current 2D SSMs inherently model 2D structures but they suffer from prohibitively slow computation due to the lack of efficient parallel algorithms. In this work, we propose 2DMamba, a novel 2D selective SSM framework that incorporates the 2D spatial structure of images into Mamba, with a highly optimized hardware-aware operator, adopting both spatial continuity and computational efficiency. We validate the versatility of our approach on both WSIs and natural images. Extensive experiments on 10 public datasets for WSI classification and survival analysis show that 2DMamba~improves up to $2.48\%$ in AUC, $3.11\%$ in F1 score, $2.47\%$ in accuracy and $5.52\%$ in C-index. Additionally, integrating our method with VMamba for natural imaging yields $0.5$ to $0.7$ improvements in mIoU on the ADE20k semantic segmentation dataset, and $0.2\%$ accuracy improvement on ImageNet-1K classification dataset. Our code is available at https://github.com/AtlasAnalyticsLab/2DMamba.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00678
"A Sensor Position Localization Method for Flexible, Non-Uniform Capacitive Tactile Sensor Arrays",['Robotics'],"['Carson Kohlbrenner', 'Caleb Escobedo', 'Nataliya Nechyporenko', 'Alessandro Roncone']","Tactile sensing is used in robotics to obtain real-time feedback during physical interactions. Fine object manipulation is a robotic application that benefits from a high density of sensors to accurately estimate object pose, whereas a low sensing resolution is sufficient for collision detection. Introducing variable sensing resolution into a single tactile sensing array can increase the range of tactile use cases, but also invokes challenges in localizing internal sensor positions. In this work, we present a mutual capacitance sensor array with variable sensor density, VARSkin, along with a localization method that determines the position of each sensor in the non-uniform array. When tested on two distinct artificial skin patches with concealed sensor layouts, our method achieves a localization accuracy within $\pm 2mm$. We also provide a comprehensive error analysis, offering strategies for further precision improvement.△ Less","1 December, 2024;",https://arxiv.org/pdf/2412.00672
Superdeformed $\mathbb{CP}$ $σ$-model equivalence,['High Energy Physics - Theory'],['Anton Pribytok'],"We find the novel class of the supersymmetric deformation of the $\mathbb{CP}^{1}$ $σ$-model and its equivalence with the generalised chiral Gross-Neveu. This construction allows the use of field-theoretic techniques and particularly the study of renormalisability and $β$-function. Provided approach is useful in finding conformal limits and establishes relation between chiral (GN) and sigma model description (geometric), which is explicitly demonstrated for the case of $ \mathbb{R} \times S^{1} $/Super-Thirring models. We also provide discussion on its emergence in $\mathcal{N}=2$ Liouville and 4-dim Chern-Simons theory.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00670
"Mapping, modeling, and reprogramming cell-fate decision making systems",['Cell Behavior'],"['Lucy Ham', 'Taylor E. Woodford', 'Megan A. Coomer', 'Michael P. H. Stumpf']","Many cellular processes involve information processing and decision making. We can probe these processes at increasing molecular detail. The analysis of heterogeneous data remains a challenge that requires new ways of thinking about cells in quantitative, predictive, and mechanistic ways. We discuss the role of mathematical models in the context of cell-fate decision making systems across the tree of life. Complex multi-cellular organisms have been a particular focus, but single celled organisms also have to sense and respond to their environment. We center our discussion around the idea of design principles which we can learn from observations and modeling, and exploit in order to (re)-design or guide cellular behavior.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00667
Improving Vietnamese Legal Document Retrieval using Synthetic Data,['Information Retrieval'],"['Son Pham Tien', 'Hieu Nguyen Doan', 'An Nguyen Dai', 'Sang Dinh Viet']","In the field of legal information retrieval, effective embedding-based models are essential for accurate question-answering systems. However, the scarcity of large annotated datasets poses a significant challenge, particularly for Vietnamese legal texts. To address this issue, we propose a novel approach that leverages large language models to generate high-quality, diverse synthetic queries for Vietnamese legal passages. This synthetic data is then used to pre-train retrieval models, specifically bi-encoder and ColBERT, which are further fine-tuned using contrastive loss with mined hard negatives. Our experiments demonstrate that these enhancements lead to strong improvement in retrieval accuracy, validating the effectiveness of synthetic data and pre-training techniques in overcoming the limitations posed by the lack of large labeled datasets in the Vietnamese legal domain.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00657
Self-consistent microscopic calculations for electron captures on nuclei in core-collapse supernovae,['Nuclear Theory'],"['A. Ravlić', 'S. Giraud', 'N. Paar', 'R. G. T. Zegers']","Calculations for electron capture rates on nuclei with atomic numbers between $Z=20$ and $Z=52$ are performed in a self-consistent finite-temperature covariant energy density functional theory within the relativistic quasiparticle random-phase approximation. Electron captures on these nuclei contribute most to reducing the electron fraction during the collapse phase of core-collapse supernovae. The rates include contributions from allowed (Gamow-Teller) and first-forbidden (FF) transitions, and it is shown that the latter become dominant at high stellar densities and temperatures. Temperature-dependent effects such as Pauli unblocking and transitions from thermally excited states are also included. The new rates are implemented in a spherically symmetric 1D simulation of the core-collapse phase. The results indicate that the increase in electron capture rates, due to inclusion of FF transitions, leads to reductions of the electron fraction at nuclear saturation density, the peak neutrino luminosity, and enclosed mass at core bounce. The new rates reaffirm that the most relevant nuclei for the deleptonization situate around the $N = 50$ and $82$ shell closures, but compared to previous simulations, nuclei are less proton rich. The new rates developed in this work are available, and will be of benefit to improve the accuracy of multi-dimensional supernova simulations.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00650
Extreme Points in Multi-Dimensional Screening,['Theoretical Economics'],"['Patrick Lahr', 'Axel Niemeyer']","This paper characterizes extreme points of the set of incentive-compatible mechanisms for screening problems with linear utility. Extreme points are exhaustive mechanisms, meaning their menus cannot be scaled and translated to make additional feasibility constraints binding. In problems with one-dimensional types, extreme points admit a tractable description with a tight upper bound on their menu size. In problems with multi-dimensional types, every exhaustive mechanism can be transformed into an extreme point by applying an arbitrarily small perturbation. For mechanisms with a finite menu, this perturbation displaces the menu items into general position. Generic exhaustive mechanisms are extreme points with an uncountable menu. Similar results hold in applications to delegation, veto bargaining, and monopoly problems, where we consider mechanisms that are unique maximizers for specific classes of objective functionals. The proofs involve a novel connection between menus of extreme points and indecomposable convex bodies, first studied by Gale (1954).△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00649
Pessimistic Cardinality Estimation,['Databases'],"['Mahmoud Abo Khamis', 'Kyle Deeds', 'Dan Olteanu', 'Dan Suciu']","Cardinality Estimation is to estimate the size of the output of a query without computing it, by using only statistics on the input relations. Existing estimators try to return an unbiased estimate of the cardinality: this is notoriously difficult. A new class of estimators have been proposed recently, called ""pessimistic estimators"", which compute a guaranteed upper bound on the query output. Two recent advances have made pessimistic estimators practical. The first is the recent observation that degree sequences of the input relations can be used to compute query upper bounds. The second is a long line of theoretical results that have developed the use of information theoretic inequalities for query upper bounds. This paper is a short overview of pessimistic cardinality estimators, contrasting them with traditional estimators.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00642
Spectra of magnetic fields from electroweak symmetry breaking,['Cosmology and Nongalactic Astrophysics'],"['Tanmay Vachaspati', 'Axel Brandenburg']","We characterize magnetic fields produced during electroweak symmetry breaking by non-dynamical numerical simulations based on the Kibble mechanism. The generated magnetic fields were thought to have an energy spectrum $\propto k^3$ for small wavenumbers $k$, but here we show that it is actually a spectrum $\propto k^4$ along with characteristic fluctuations in the magnetic helicity. Using scaling results from MHD simulations for the evolution and assuming that the initial magnetic field is coherent on the electroweak Hubble scale, we estimate the magnetic field strength to be $\sim 10^{-13}\, {\rm G}$ on kpc scales at the present epoch for non-helical fields. For maximally helical fields we obtain $\sim 10^{-10}\, {\rm G}$ on Mpc scales. We also give scalings of these estimates for partially helical fields.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00641
Needle: A Generative-AI Powered Monte Carlo Method for Answering Complex Natural Language Queries on Multi-modal Data,['Information Retrieval'],"['Mahdi Erfanian', 'Mohsen Dehghankar', 'Abolfazl Asudeh']","Multi-modal data, such as image data sets, often miss the detailed descriptions that properly capture the rich information encoded in them. This makes answering complex natural language queries a major challenge in these domains. In particular, unlike the traditional nearest-neighbor search, where the tuples and the query are modeled as points in a data cube, the query and the tuples are of different natures, making the traditional query answering solutions not directly applicable for such settings. Existing literature addresses this challenge for image data through vector representations jointly trained on natural language and images. This technique, however, underperforms for complex queries due to various reasons.
  This paper takes a step towards addressing this challenge by introducing a Generative-AI (GenAI) powered Monte Carlo method that utilizes foundation models to generate synthetic samples that capture the complexity of the natural language query and transform it to the same space of the multi-modal data. Following this method, we develop a system for image data retrieval and propose practical solutions that enable leveraging future advancements in GenAI and vector representations for improving our system's performance. Our comprehensive experiments on various benchmark datasets verify that our system significantly outperforms state-of-the-art techniques.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00639
Optimization of Delivery Routes for Fresh E-commerce in Pre-warehouse Mode,['Econometrics'],"['Alice Harward', 'Junjie Lin', 'Yun Wang', 'Xiaoke Xie']","With the development of the economy, fresh food e-commerce has experienced rapid growth. One of the core competitive advantages of fresh food e-commerce platforms lies in selecting an appropriate logistics distribution model. This study focuses on the front warehouse model, aiming to minimize distribution costs. Considering the perishable nature and short shelf life of fresh food, a distribution route optimization model is constructed, and the saving mileage method is designed to determine the optimal distribution scheme. The results indicate that under certain conditions, different distribution schemes significantly impact the performance of fresh food e-commerce platforms. Based on a review of domestic and international research, this paper takes Dingdong Maicai as an example to systematically introduce the basic concepts of distribution route optimization in fresh food e-commerce platforms under the front warehouse model, analyze the advantages of logistics distribution, and thoroughly examine the importance of distribution routes for fresh products.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00634
Grey-body factors for gravitational and electromagnetic perturbations around Gibbons-Maeda-Garfinkle-Horovits-Strominger black holes,['General Relativity and Quantum Cosmology'],['Alexey Dubinsky'],"While grey-body factors for a test scalar field in stringy black holes described by the renowned Gibbons-Maeda-Garfinkle-Horowitz-Strominger (GMGHS) solution have been analyzed in the literature, no such analysis exists for gravitons, likely due to the complexity of the perturbation equations. In this study, we utilize known data on quasinormal modes and the relationship between quasinormal modes and grey-body factors to derive these factors for gravitational and electromagnetic perturbations of the dilaton black hole. Our findings indicate that grey-body factors are significantly suppressed by the dilaton parameter as the black hole's charge approaches its extreme value. The iso-spectrality between axial and polar channels of perturbations is broken in the presence of the dilaton field, which leads to different grey-body factors for different types of perturbations.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00625
Visual Modality Prompt for Adapting Vision-Language Object Detectors,['Computer Vision and Pattern Recognition'],"['Heitor R. Medeiros', 'Atif Belal', 'Srikanth Muralidharan', 'Eric Granger', 'Marco Pedersoli']","The zero-shot performance of object detectors degrades when tested on different modalities, such as infrared and depth. While recent work has explored image translation techniques to adapt detectors to new modalities, these methods are limited to a single modality and apply only to traditional detectors. Recently, vision-language detectors, such as YOLO-World and Grounding DINO, have shown promising zero-shot capabilities, however, they have not yet been adapted for other visual modalities. Traditional fine-tuning approaches tend to compromise the zero-shot capabilities of the detectors. The visual prompt strategies commonly used for classification with vision-language models apply the same linear prompt translation to each image making them less effective. To address these limitations, we propose ModPrompt, a visual prompt strategy to adapt vision-language detectors to new modalities without degrading zero-shot performance. In particular, an encoder-decoder visual prompt strategy is proposed, further enhanced by the integration of inference-friendly task residuals, facilitating more robust adaptation. Empirically, we benchmark our method for modality adaptation on two vision-language detectors, YOLO-World and Grounding DINO, and on challenging infrared (LLVIP, FLIR) and depth (NYUv2) data, achieving performance comparable to full fine-tuning while preserving the model's zero-shot capability. Our code is available at: https://github.com/heitorrapela/ModPrompt△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00622
Flow matching for stochastic linear control systems,['Optimization and Control'],"['Yuhang Mei', 'Mohammad Al-Jarrah', 'Amirhossein Taghvaei', 'Yongxin Chen']","This paper addresses the problem of steering an initial probability distribution to a target probability distribution through a deterministic or stochastic linear control system. Our proposed approach is inspired by the flow matching methodology, with the difference that we can only affect the flow through the given control channels. The motivation comes from applications such as robotic swarms and stochastic thermodynamics, where agents or particles can only be manipulated through control actions. The feedback control law that achieves the task is characterized as the conditional expectation of the control inputs for the stochastic bridges that respect the given control system dynamics. Explicit forms are derived for special cases, and a numerical procedure is presented to approximate the control law, illustrated with examples.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00617
The transcendence degree of the reals over certain set-theoretical subfields,['Logic'],"['Azul Fatalini', 'Ralf Schindler']","It is a well-known result that, after adding one Cohen real, the transcendence degree of the reals over the ground-model reals is continuum. We extend this result for a set $X$ of finitely many Cohen reals, by showing that, in the forcing extension, the transcendence degree of the reals over a combination of the reals in the extension given by each proper subset of $X$ is also maximal. This answers a question of Kanovei and Schindler.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00616
Phase-resolving spin-wave microscopy using infrared strobe light,['Materials Science'],"['Yuzan Xiong', 'Andrew Christy', 'Muntasir Mahdi', 'Rui Sun', 'Yi Li', 'Robert D. Geil', 'James F. Cahoon', 'Frank Tsui', 'Binbin Yang', 'Tae Hee Kim', 'Jia-Mian Hu', 'Dali Sun', 'Michael C. Hamilton', 'Valentine Novosad', 'Wei Zhang']","The needs for sensitively and reliably probing magnetization dynamics have been increasing in various contexts such as studying novel hybrid magnonic systems, in which the spin dynamics strongly and coherently couple to other excitations, including microwave photons, light photons, or phonons. Recent advances in quantum magnonics also highlight the need for employing magnon phase as quantum state variables, which is to be detected and mapped out with high precision in on-chip micro- and nano-scale magnonic devices. Here, we demonstrate a facile optical technique that can directly perform concurrent spectroscopic and imaging functionalities with spatial- and phase-resolutions, using infrared strobe light operating at 1550-nm wavelength. To showcase the methodology, we spectroscopically studied the phase-resolved spin dynamics in a bilayer of Permalloy and Y3Fe5O12 (YIG), and spatially imaged the backward volume spin wave modes of YIG in the dipolar spin wave regime. Using the strobe light probe, the detected precessional phase contrast can be directly used to construct the map of the spin wave wavefront, in the continuous-wave regime of spin-wave propagation and in the stationary state, without needing any optical reference path. By selecting the applied field, frequency, and detection phase, the spin wave images can be made sensitive to the precession amplitude and phase. Our results demonstrate that infrared optical strobe light can serve as a versatile platform for magneto-optical probing of magnetization dynamics, with potential implications in investigating hybrid magnonic systems.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00611
Exploration and Evaluation of Bias in Cyberbullying Detection with Machine Learning,['Machine Learning'],"['Andrew Root', 'Liam Jakubowski', 'Mounika Vanamala']","It is well known that the usefulness of a machine learning model is due to its ability to generalize to unseen data. This study uses three popular cyberbullying datasets to explore the effects of data, how it's collected, and how it's labeled, on the resulting machine learning models. The bias introduced from differing definitions of cyberbullying and from data collection is discussed in detail. An emphasis is made on the impact of dataset expansion methods, which utilize current data points to fetch and label new ones. Furthermore, explicit testing is performed to evaluate the ability of a model to generalize to unseen datasets through cross-dataset evaluation. As hypothesized, the models have a significant drop in the Macro F1 Score, with an average drop of 0.222. As such, this study effectively highlights the importance of dataset curation and cross-dataset testing for creating models with real-world applicability. The experiments and other code can be found at https://github.com/rootdrew27/cyberbullying-ml.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00609
Risk models from tree-structured Markov random fields following multivariate Poisson distributions,['Methodology'],"['Hélène Cossette', 'Benjamin Côté', 'Alexandre Dubeau', 'Etienne Marceau']","We propose risk models for a portfolio of risks, each following a compound Poisson distribution, with dependencies introduced through a family of tree-based Markov random fields with Poisson marginal distributions inspired in Côté et al. (2024b, arXiv:2408.13649). The diversity of tree topologies allows for the construction of risk models under several dependence schemes. We study the distribution of the random vector of risks and of the aggregate claim amount of the portfolio. We perform two risk management tasks: the assessment of the global risk of the portfolio and its allocation to each component. Numerical examples illustrate the findings and the efficiency of the computation methods developed throughout. We also show that the discussed family of Markov random fields is a subfamily of the multivariate Poisson distribution constructed through common shocks.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00607
Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions,['Artificial Intelligence'],"['Resmi Ramachandranpillai', 'Kishore Sampath', 'Ayaazuddin Mohammad', 'Malihe Alikhani']","Biases in automated clinical decision-making using Electronic Healthcare Records (EHR) impose significant disparities in patient care and treatment outcomes. Conventional approaches have primarily focused on bias mitigation strategies stemming from single attributes, overlooking intersectional subgroups -- groups formed across various demographic intersections (such as race, gender, ethnicity, etc.). Rendering single-attribute mitigation strategies to intersectional subgroups becomes statistically irrelevant due to the varying distribution and bias patterns across these subgroups. The multimodal nature of EHR -- data from various sources such as combinations of text, time series, tabular, events, and images -- adds another layer of complexity as the influence on minority groups may fluctuate across modalities. In this paper, we take the initial steps to uncover potential intersectional biases in predictions by sourcing extensive multimodal datasets, MIMIC-Eye1 and MIMIC-IV ED, and propose mitigation at the intersectional subgroup level. We perform and benchmark downstream tasks and bias evaluation on the datasets by learning a unified text representation from multimodal sources, harnessing the enormous capabilities of the pre-trained clinical Language Models (LM), MedBERT, Clinical BERT, and Clinical BioBERT. Our findings indicate that the proposed sub-group-specific bias mitigation is robust across different datasets, subgroups, and embeddings, demonstrating effectiveness in addressing intersectional biases in multimodal settings.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00606
DynRank: Improving Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification,['Computation and Language'],"['Abdelrahman Abdallah', 'Jamshid Mozafari', 'Bhawna Piryani', 'Mohammed M. Abdelgwad', 'Adam Jatowt']","This paper presents DynRank, a novel framework for enhancing passage retrieval in open-domain question-answering systems through dynamic zero-shot question classification. Traditional approaches rely on static prompts and pre-defined templates, which may limit model adaptability across different questions and contexts. In contrast, DynRank introduces a dynamic prompting mechanism, leveraging a pre-trained question classification model that categorizes questions into fine-grained types. Based on these classifications, contextually relevant prompts are generated, enabling more effective passage retrieval. We integrate DynRank into existing retrieval frameworks and conduct extensive experiments on multiple QA benchmark datasets.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00600
Gaussian Generating functionals on easy quantum groups,['Quantum Algebra'],"['Uwe Franz', 'Amaury Freslon', 'Adam Skalski']","We describe all Gaussian generating functionals on several easy quantum groups given by non-crossing partitions. This includes in particular the free unitary, orthogonal and symplectic quantum groups. We further characterize central Gaussian generating functionals and describe a centralization procedure yielding interesting (non-Gaussian) generating functionals.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00595
Audio Atlas: Visualizing and Exploring Audio Datasets,['Sound'],"['Luca A. Lanzendörfer', 'Florian Grötschla', 'Uzeyir Valizada', 'Roger Wattenhofer']","We introduce Audio Atlas, an interactive web application for visualizing audio data using text-audio embeddings. Audio Atlas is designed to facilitate the exploration and analysis of audio datasets using a contrastive embedding model and a vector database for efficient data management and semantic search. The system maps audio embeddings into a two-dimensional space and leverages DeepScatter for dynamic visualization. Designed for extensibility, Audio Atlas allows easy integration of new datasets, enabling users to better understand their audio data and identify both patterns and outliers. We open-source the codebase of Audio Atlas, and provide an initial implementation containing various audio and music datasets.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00591
Goldbach Theorems for Group Semidomains,['Commutative Algebra'],"['Eddy Li', 'Advaith Mopuri', 'Charles Zhang']","A semidomain is a subsemiring of an integral domain. We call a semidomain $S$ additively reduced if $0$ is the only invertible element of the monoid $(S, +)$, while we say that $S$ is additively Furstenberg if every non-invertible element of $(S,+)$ can be expressed as the sum of an atom and an element of $S$. In this paper, we study a variant of the Goldbach conjecture within the framework of group semidomains $S[G]$ and group series semidomains $S[\![G]\!]$, where $S$ is both an additively reduced and additively Furstenberg semidomain and $G$ is a torsion-free abelian group. In particular, we show that every non-constant polynomial expression in $S[G]$ can be written as the sum of at most two irreducibles if and only if the condition $\mathscr{A}_+(S) = S^\times$ holds.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00590
Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects,['Cryptography and Security'],"['Fred Heiding', 'Simon Lermen', 'Andrew Kao', 'Bruce Schneier', 'Arun Vishwanath']","In this paper, we evaluate the capability of large language models to conduct personalized phishing attacks and compare their performance with human experts and AI models from last year. We include four email groups with a combined total of 101 participants: A control group of arbitrary phishing emails, which received a click-through rate (recipient pressed a link in the email) of 12%, emails generated by human experts (54% click-through), fully AI-automated emails 54% (click-through), and AI emails utilizing a human-in-the-loop (56% click-through). Thus, the AI-automated attacks performed on par with human experts and 350% better than the control group. The results are a significant improvement from similar studies conducted last year, highlighting the increased deceptive capabilities of AI models. Our AI-automated emails were sent using a custom-built tool that automates the entire spear phishing process, including information gathering and creating personalized vulnerability profiles for each target. The AI-gathered information was accurate and useful in 88% of cases and only produced inaccurate profiles for 4% of the participants. We also use language models to detect the intention of emails. Claude 3.5 Sonnet scored well above 90% with low false-positive rates and detected several seemingly benign emails that passed human detection. Lastly, we analyze the economics of phishing, highlighting how AI enables attackers to target more individuals at lower cost and increase profitability by up to 50 times for larger audiences.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00586
Dynamics of a particle in the double-slit experiment with measurement,['Quantum Physics'],['Alexey A. Kryukov'],"Spontaneous collapse models use non-linear stochastic modifications of the Schroedinger equation to suppress superpositions of eigenstates of the measured observable and drive the state to an eigenstate. It was recently demonstrated that the Born rule for transition probabilities can be modeled using the linear Schroedinger equation with a Hamiltonian represented by a random matrix from the Gaussian unitary ensemble. The matrices representing the Hamiltonian at different time points throughout the observation period are assumed to be independent. Instead of suppressing superpositions, such Schroedinger evolution makes the state perform an isotropic random walk on the projective space of states. The relative frequency of reaching different eigenstates of an arbitrary observable in the random walk is shown to satisfy the Born rule. Here, we apply this methodology to investigate the behavior of a particle in the context of the double-slit experiment with measurement. Our analysis shows that, in this basic case, the evolution of the particle's state can be effectively captured through a random walk on a two-dimensional submanifold of the state space. This random walk reproduces the Born rule for the probability of finding the particle near the slits, conditioned on its arrival at one of them. To ensure that this condition is satisfied, we introduce a drift term representing a change in the variance of the position observable for the state. A drift-free model, based on equivalence classes of states indistinguishable by the detector, is also considered. The resulting random walk, with or without drift, serves as a suitable model for describing the transition from the initial state to an eigenstate of the measured observable in the experiment, offering new insights into its potential underlying mechanisms.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00584
Dynamics Modeling using Visual Terrain Features for High-Speed Autonomous Off-Road Driving,['Robotics'],"['Jason Gibson', 'Anoushka Alavilli', 'Erica Tevere', 'Evangelos A. Theodorou', 'Patrick Spieler']","Rapid autonomous traversal of unstructured terrain is essential for scenarios such as disaster response, search and rescue, or planetary exploration. As a vehicle navigates at the limit of its capabilities over extreme terrain, its dynamics can change suddenly and dramatically. For example, high-speed and varying terrain can affect parameters such as traction, tire slip, and rolling resistance. To achieve effective planning in such environments, it is crucial to have a dynamics model that can accurately anticipate these conditions. In this work, we present a hybrid model that predicts the changing dynamics induced by the terrain as a function of visual inputs. We leverage a pre-trained visual foundation model (VFM) DINOv2, which provides rich features that encode fine-grained semantic information. To use this dynamics model for planning, we propose an end-to-end training architecture for a projection distance independent feature encoder that compresses the information from the VFM, enabling the creation of a lightweight map of the environment at runtime. We validate our architecture on an extensive dataset (hundreds of kilometers of aggressive off-road driving) collected across multiple locations as part of the DARPA Robotic Autonomy in Complex Environments with Resiliency (RACER) program. https://www.youtube.com/watch?v=dycTXxEosMk△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00581
Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives,['Computer Vision and Pattern Recognition'],"['Alex Hanson', 'Allen Tu', 'Geng Lin', 'Vasu Singla', 'Matthias Zwicker', 'Tom Goldstein']","3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians. However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings. In this paper, we identify and address two key inefficiencies in 3D-GS, achieving substantial improvements in rendering speed, model size, and training time. First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity. Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed. Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $6.71\times$ across scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets with $10.6\times$ fewer primitives than 3D-GS.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00578
Asymmetric Colorings of Disjoint Unions of Graphs,['Combinatorics'],"['Bruno Aguilar', 'Daibik Barik', 'Jetharam Bhambhu', 'Evan Frankel', 'Nam Hung Tran Nguyen', 'Revathi Mandava', 'Aiden Marco', 'Kyle Pon', 'Tejas Shende', 'Yi Wang']","The asymmetric coloring number of a graph is the minimum number of colors needed to color its vertices, so that no non-trivial automorphism preserves the color classes. We investigate the asymmetric coloring number of graphs that are disjoint unions of graphs. We will derive a general relationship between the asymmetric coloring number of disjoint copies of graphs and the number of ways to color a single copy asymmetrically, and then look at particular cases such as disjoint copies of paths, stars, cycles, and hypercubes.△ Less",v1,https://arxiv.org/pdf/2412.00574
Contextual Bandits in Payment Processing: Non-uniform Exploration and Supervised Learning at Adyen,['Machine Learning'],"['Akhila Vangara', 'Alex Egg']","Uniform random exploration in decision-making systems supports off-policy learning via supervision but incurs high regret, making it impractical for many applications. Conversely, non-uniform exploration offers better immediate performance but lacks support for off-policy learning. Recent research suggests that regression oracles can bridge this gap by combining non-uniform exploration with supervised learning. In this paper, we analyze these approaches within a real-world industrial context at Adyen, a large global payments processor characterized by batch logged delayed feedback, short-term memory, and dynamic action spaces under the Empirical Risk Minimization (ERM) framework. Our analysis reveals that while regression oracles significantly improve performance, they introduce challenges due to rigid algorithmic assumptions. Specifically, we observe that as a policy improves, subsequent generations may perform worse due to shifts in the reward distribution and increased class imbalance in the training data. This degradation occurs de spite improvements in other aspects of the training data, leading to decreased performance in successive policy iterations. We further explore the long-term impact of regression oracles, identifying a potential ""oscillation effect."" This effect arises when regression oracles influence probability estimates and the realizability of subsequent policy models, leading to fluctuations in performance across iterations. Our findings highlight the need for more adaptable algorithms that can leverage the benefits of regression oracles without introducing instability in policy performance over time.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00569
The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,['Machine Learning'],"['Ruben Ohana', 'Michael McCabe', 'Lucas Meyer', 'Rudy Morel', 'Fruzsina J. Agocs', 'Miguel Beneitez', 'Marsha Berger', 'Blakesley Burkhart', 'Stuart B. Dalziel', 'Drummond B. Fielding', 'Daniel Fortunato', 'Jared A. Goldberg', 'Keiya Hirashima', 'Yan-Fei Jiang', 'Rich R. Kerswell', 'Suryanarayana Maddu', 'Jonah Miller', 'Payel Mukhopadhyay', 'Stefan S. Nixon', 'Jeff Shen', 'Romain Watteaux', 'Bruno Régaldo-Saint Blancard', 'François Rozet', 'Liam H. Parker', 'Miles Cranmer']","Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00568
Parameter estimation of microlensed gravitational waves with Conditional Variational Autoencoders,['General Relativity and Quantum Cosmology'],"['Roberto Bada Nerin', 'Oleg Bulashenko', 'Osvaldo Gramaxo Freitas', 'José A. Font']","Gravitational lensing of gravitational waves (GWs) provides a unique opportunity to study cosmology and astrophysics at multiple scales. Detecting microlensing signatures, in particular, requires efficient parameter estimation methods due to the high computational cost of traditional Bayesian inference. In this paper we explore the use of deep learning, namely Conditional Variational Autoencoders (CVAE), to estimate parameters of microlensed binary black hole (simulated) waveforms. We find that our CVAE model yields accurate parameter estimation and significant computational savings compared to Bayesian methods such as bilby (up to five orders of magnitude faster inferences). Moreover, the incorporation of CVAE-generated priors in bilby reduces the average runtime of the latter in about 48% with no penalty on its accuracy. Our results suggest that a CVAE model is a promising tool for future low-latency searches of lensed signals. Further applications to actual signals and integration with advanced pipelines could help extend the capabilities of GW observatories in detecting microlensing events.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00566
What is the weakest idempotent Maltsev condition that implies that abelian tolerances generate abelian congruences?,['Logic'],"['Keith A. Kearnes', 'Emil W. Kiss']","We answer the question in the title. In the process, we correct an error in our AMS Memoir The Shape of Congruence Lattices.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00565
A novel algorithm for the decomposition of non-stationary multidimensional and multivariate signals,['Numerical Analysis'],"['Roberto Cavassi', 'Antonio Cicone', 'Enza Pellegrino', 'Haomin Zhou']","The decomposition of a signal is a fundamental tool in many fields of research, including signal processing, geophysics, astrophysics, engineering, medicine, and many more. By breaking down complex signals into simpler oscillatory components we can enhance the understanding and processing of the data, unveiling hidden information contained in them. Traditional methods, such as Fourier analysis and wavelet transforms, which are effective in handling mono-dimensional stationary signals struggle with non-stationary data sets and they require, this is the case of the wavelet, the selection of predefined basis functions. In contrast, the Empirical Mode Decomposition (EMD) method and its variants, such as Iterative Filtering (IF), have emerged as effective nonlinear approaches, adapting to signals without any need for a priori assumptions. To accelerate these methods, the Fast Iterative Filtering (FIF) algorithm was developed, and further extensions, such as Multivariate FIF (MvFIF) and Multidimensional FIF (FIF2), have been proposed to handle higher-dimensional data.
  In this work, we introduce the Multidimensional and Multivariate Fast Iterative Filtering (MdMvFIF) technique, an innovative method that extends FIF to handle data that vary simultaneously in space and time. This new algorithm is capable of extracting Intrinsic Mode Functions (IMFs) from complex signals that vary in both space and time, overcoming limitations found in prior methods. The potentiality of the proposed method is demonstrated through applications to artificial and real-life signals, highlighting its versatility and effectiveness in decomposing multidimensional and multivariate nonstationary signals. The MdMvFIF method offers a powerful tool for advanced signal analysis across many scientific and engineering disciplines.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00553
Some remarks on Hayward-AdS black hole surrounded by a fluid of strings,['General Relativity and Quantum Cosmology'],"['F. F. Nascimento', 'V. B. Bezerra', 'J. M. Toledo', 'G. A. Marques']","We obtain a class of solutions corresponding to a generalization of the Hayward black hole by solving the Einstein equations coupled to a particular nonlinear electromagnetic field. The generalization is realized by considering, additionally, the presence of the cosmological constant and a source corresponding to an anisotropic fluid, namely, a fluid of strings, that surrounds the black hole. We show that the obtained class of solutions preserves or does not the regularity of the original Hayward black hole solution, depending on the values of the parameter $β$ which labels the different solutions. We discuss the characteristics of the solutions, from the point of view of the singularities of spacetime, by examining the behavior of the Kretschmann scalar as well as of the geodesics concerning their completeness. We analyze some aspects of thermodynamics, particularizing one of the solutions obtained, namely, for $β=-1/2$, in which case the regularity of the Hayward black hole is preserved. Some thermodynamic quantities are obtained and analyzed, for example, pressure, heat capacity, and the critical points, and we show how these quantities change for different values of the parameter $q$ associated with the original Hayward solution, as well as with the parameter $b$ associated with the presence of the fluid of strings. The phase transitions are also analyzed by using the equation of state and the Gibbs free energy.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00552
"On new regular charged black hole solutions: Limiting Curvature Condition, Quasinormal modes and Shadows",['General Relativity and Quantum Cosmology'],"['Leonardo Balart', 'Grigoris Panotopoulos', 'Angel Rincon']","We introduce two new static, spherically symmetric regular black hole solutions that can be obtained from non-linear electrodynamics models. For each solution, we investigate the dynamic stability with respect to arbitrary linear fluctuations of the metric and electromagnetic field, and also examine the energy conditions that those black holes satisfy. Moreover, based on those solutions, we present two additional ones that satisfy the Limiting Curvature Condition. Finally, we make a comparison between the two solutions exploring their null geodesics and circular photon orbits.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00550
SeQwen at the Financial Misinformation Detection Challenge Task: Sequential Learning for Claim Verification and Explanation Generation in Financial Domains,['Computation and Language'],"['Jebish Purbey', 'Siddhant Gupta', 'Nikhil Manali', 'Siddartha Pullakhandam', 'Drishti Sharma', 'Ashay Srivastava', 'Ram Mohan Rao Kadiyala']","This paper presents the system description of our entry for the COLING 2025 FMD challenge, focusing on misinformation detection in financial domains. We experimented with a combination of large language models, including Qwen, Mistral, and Gemma-2, and leveraged pre-processing and sequential learning for not only identifying fraudulent financial content but also generating coherent, and concise explanations that clarify the rationale behind the classifications. Our approach achieved competitive results with an F1-score of 0.8283 for classification, and ROUGE-1 of 0.7253 for explanations. This work highlights the transformative potential of LLMs in financial applications, offering insights into their capabilities for combating misinformation and enhancing transparency while identifying areas for future improvement in robustness and domain adaptation.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00549
"Rank It, Then Ask It: Input Reranking for Maximizing the Performance of LLMs on Symmetric Tasks",['Machine Learning'],"['Mohsen Dehghankar', 'Abolfazl Asudeh']","Large language models (LLMs) have quickly emerged as practical and versatile tools that provide new solutions for a wide range of domains. In this paper, we consider the application of LLMs on symmetric tasks where a query is asked on an (unordered) bag of elements. Examples of such tasks include answering aggregate queries on a database table. In general, when the bag contains a large number of elements, LLMs tend to overlook some elements, leading to challenges in generating accurate responses to the query. LLMs receive their inputs as ordered sequences. However, in this problem, we leverage the fact that the symmetric input is not ordered, and reordering should not affect the LLM's response.
  Observing that LLMs are less likely to miss elements at certain positions of the input, we introduce the problem of LLM input reranking: to find a ranking of the input that maximizes the LLM's accuracy for the given query without making explicit assumptions about the query. Finding the optimal ranking requires identifying (i) the relevance of each input element for answering the query and (ii) the importance of each rank position for the LLM's attention. We develop algorithms for estimating these values efficiently utilizing a helper LLM. We conduct comprehensive experiments on different synthetic and real datasets to validate our proposal and to evaluate the effectiveness of our proposed algorithms. Our experiments confirm that our reranking approach improves the accuracy of the LLMs on symmetric tasks by up to $99\%$ proximity to the optimum upper bound.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00546
Context-Based Echo State Networks with Prediction Confidence for Human-Robot Shared Control,['Robotics'],"['Negin Amirshirzad', 'Mehmet Arda Eren', 'Erhan Oztop']","In this paper, we propose a novel lightweight learning from demonstration (LfD) model based on reservoir computing that can learn and generate multiple movement trajectories with prediction intervals, which we call as Context-based Echo State Network with prediction confidence (CESN+). CESN+ can generate movement trajectories that may go beyond the initial LfD training based on a desired set of conditions while providing confidence on its generated output. To assess the abilities of CESN+, we first evaluate its performance against Conditional Neural Movement Primitives (CNMP), a comparable framework that uses a conditional neural process to generate movement primitives. Our findings indicate that CESN+ not only outperforms CNMP but is also faster to train and demonstrates impressive performance in generating trajectories for extrapolation cases. In human-robot shared control applications, the confidence of the machine generated trajectory is a key indicator of how to arbitrate control sharing. To show the usability of the CESN+ for human-robot adaptive shared control, we have designed a proof-of-concept human-robot shared control task and tested its efficacy in adapting the sharing weight between the human and the robot by comparing it to a fixed-weight control scheme. The simulation experiments show that with CESN+ based adaptive sharing the total human load in shared control can be significantly reduced. Overall, the developed CESN+ model is a strong lightweight LfD system with desirable properties such fast training and ability to extrapolate to the new task parameters while producing robust prediction intervals for its output.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00541
Prognostic Framework for Robotic Manipulators Operating Under Dynamic Task Severities,['Robotics'],"['Ayush Mohanty', 'Jason Dekarske', 'Stephen K. Robinson', 'Sanjay Joshi', 'Nagi Gebraeel']","Robotic manipulators are critical in many applications but are known to degrade over time. This degradation is influenced by the nature of the tasks performed by the robot. Tasks with higher severity, such as handling heavy payloads, can accelerate the degradation process. One way this degradation is reflected is in the position accuracy of the robot's end-effector. In this paper, we present a prognostic modeling framework that predicts a robotic manipulator's Remaining Useful Life (RUL) while accounting for the effects of task severity. Our framework represents the robot's position accuracy as a Brownian motion process with a random drift parameter that is influenced by task severity. The dynamic nature of task severity is modeled using a continuous-time Markov chain (CTMC). To evaluate RUL, we discuss two approaches -- (1) a novel closed-form expression for Remaining Lifetime Distribution (RLD), and (2) Monte Carlo simulations, commonly used in prognostics literature. Theoretical results establish the equivalence between these RUL computation approaches. We validate our framework through experiments using two distinct physics-based simulators for planar and spatial robot fleets. Our findings show that robots in both fleets experience shorter RUL when handling a higher proportion of high-severity tasks.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00538
FullStack Bench: Evaluating LLMs as Full Stack Coders,['Artificial Intelligence'],"['Siyao Liu', 'He Zhu', 'Jerry Liu', 'Shulin Xin', 'Aoyan Li', 'Rui Long', 'Li Chen', 'Jack Yang', 'Jinxiang Xia', 'Z. Y. Peng', 'Shukai Liu', 'Zhaoxiang Zhang', 'Jing Mai', 'Ge Zhang', 'Wenhao Huang', 'Kai Shen', 'Liang Xiang']","As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.△ Less",v1,https://arxiv.org/pdf/2412.00535
"ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance & Efficiency on a Specific Domain",['Computation and Language'],"['Ali Shiraee Kasmaee', 'Mohammad Khodadad', 'Mohammad Arshi Saloot', 'Nick Sherck', 'Stephen Dokas', 'Hamidreza Mahyar', 'Soheila Samiee']","Recent advancements in language models have started a new era of superior information retrieval and content generation, with embedding models playing an important role in optimizing data representation efficiency and performance. While benchmarks like the Massive Text Embedding Benchmark (MTEB) have standardized the evaluation of general domain embedding models, a gap remains in specialized fields such as chemistry, which require tailored approaches due to domain-specific challenges. This paper introduces a novel benchmark, the Chemical Text Embedding Benchmark (ChemTEB), designed specifically for the chemical sciences. ChemTEB addresses the unique linguistic and semantic complexities of chemical literature and data, offering a comprehensive suite of tasks on chemical domain data. Through the evaluation of 34 open-source and proprietary models using this benchmark, we illuminate the strengths and weaknesses of current methodologies in processing and understanding chemical information. Our work aims to equip the research community with a standardized, domain-specific evaluation framework, promoting the development of more precise and efficient NLP models for chemistry-related applications. Furthermore, it provides insights into the performance of generic models in a domain-specific context. ChemTEB comes with open-source code and data, contributing further to its accessibility and utility.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00532
Simon Says: Exploring the Importance of Notification Design Formats on User Engagement,['Human-Computer Interaction'],"['Hans Matthew Abello', 'Maxine Beatriz Badiola', 'Mark John Custer', 'Lorane Bernadeth Fausto', 'Patrick Josh Leonida', 'Denzel Bryan Yongco', 'Jordan Aiko Deja']","Push notifications are brief messages that users frequently encounter in their daily lives. However, the volume of notifications can lead to information overload, making it challenging for users to engage effectively. This study investigates how notification behavior and color influence user interaction and perception. To explore this, we developed an app prototype that tracks user interactions with notifications, categorizing them as accepted, dismissed, or ignored. After each interaction, users were asked to complete a survey regarding their perception of the notifications. The study focused on how different notification colors might affect the likelihood of acceptance and perceived importance. The results reveal that certain colors were more likely to be accepted and were perceived as more important compared to others, suggesting that both color and behavior play significant roles in shaping user engagement with notifications.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00531
The Schrijver system of the length polyhedron of an interval order,['Combinatorics'],"['André E. Kézdy', 'Jenő Lehel']","The length polyhedron of an interval order $P$ is the convex hull of integer vectors representing the interval lengths in possible interval representations of $P$ in which all intervals have integer endpoints. This polyhedron is an integral translation of a polyhedral cone, with its apex corresponding to the canonical interval representation of $P$ (also known as the minimal endpoint representation).
  In earlier work, we introduced an arc-weighted directed graph model, termed the key graph, inspired by this canonical representation. We showed that cycles in the key graph correspond, via Fourier-Motzkin elimination, to inequalities that describe supporting hyperplanes of the length polyhedron. These cycle inequalities derived from the key graph form a complete system of linear inequalities defining the length polyhedron. By applying a theorem due to Cook, we establish here that this system of inequalities is totally dual integral (TDI).
  Leveraging circulations, total dual integrality, and the special structure of the key graph, our main theorem demonstrates that a cycle inequality is a positive linear combination of other cycle inequalities if and only if it is a positive integral linear combination of smaller cycle inequalities (where `smaller' here refers a natural weak ordering among these cycle inequalities). This yields an efficient method to remove redundant cycle inequalities and ultimately construct the unique minimal TDI-system, also known as the Schrijver system, for the length polyhedron. Notably, if the key graph contains a polynomial number of cycles, this gives a polynomial-time algorithm to compute the Schrijver system for the length polyhedron.
  Lastly, we provide examples of interval orders where the Schrijver system has an exponential size.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00528
GloCOM: A Short Text Neural Topic Model via Global Clustering Context,['Computation and Language'],"['Quang Duc Nguyen', 'Tung Nguyen', 'Duc Anh Nguyen', 'Linh Ngo Van', 'Sang Dinh', 'Thien Huu Nguyen']","Uncovering hidden topics from short texts is challenging for traditional and neural models due to data sparsity, which limits word co-occurrence patterns, and label sparsity, stemming from incomplete reconstruction targets. Although data aggregation offers a potential solution, existing neural topic models often overlook it due to time complexity, poor aggregation quality, and difficulty in inferring topic proportions for individual documents. In this paper, we propose a novel model, GloCOM (Global Clustering COntexts for Topic Models), which addresses these challenges by constructing aggregated global clustering contexts for short documents, leveraging text embeddings from pre-trained language models. GloCOM can infer both global topic distributions for clustering contexts and local distributions for individual short texts. Additionally, the model incorporates these global contexts to augment the reconstruction loss, effectively handling the label sparsity issue. Extensive experiments on short text datasets show that our approach outperforms other state-of-the-art models in both topic quality and document representations.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00525
Shaping terahertz harmonic frequency combs with frequency dependent external reflectors,['Optics'],"['Carlo Silvestri', 'Xiaoqiong Qi', 'Thomas Taimre', 'Aleksandar D. Rakić']","We present a method for engineering harmonic frequency combs (HFCs) in the terahertz spectral region. This approach involves interfacing a quantum cascade laser (QCL) with an external reflector featuring frequency-dependent reflectivity. A notable advantage of this method over existing ones is its dual functionality in shaping HFCs, allowing for control over both the frequency offset and comb spacing based on the external reflectivity profile. Moreover, the resulting HFCs manifest as sequences of short pulses in the time domain. Consequently, our method enables the generation of ultrashort picosecond pulses passively, providing a distinct alternative to conventional pulse generation systems reliant on active bias current modulation, which struggle with modulation frequencies significantly higher than the first beatnote. This offers intriguing prospects for utilizing HFCs in pump and probe spectroscopy, a field already recognized in the literature as one of the most compelling applications of these states. Furthermore, we demonstrate that these HFCs can be triggered from an initial condition of free-running unlocked dynamics, eliminating the need for assuming free-running comb emission. Thus, the utilization of an external, frequency-dependent reflector is capable of enhancing the coherence of the QCL emission.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00524
Correlating EDMs and $A\to γγ$ in the 2HDM in light of the diphoton excesses at 95 GeV and 152 GeV,['High Energy Physics - Phenomenology'],"['Sumit Banik', 'Guglielmo Coloretti', 'Andreas Crivellin', 'Howard E. Haber']","We examine the correlations between Higgs decays to photons and electric dipole moments (EDMs) in the CP-violating flavor-aligned two-Higgs-doublet model (2HDM). It is convenient to work in the Higgs basis $\{H_1,H_2\}$ where only the first Higgs doublet field $H_1$ acquires a vacuum expectation value. In light of the LHC Higgs data, which agree well with Standard Model (SM) predictions, it follows that the parameters of the 2HDM are consistent with the Higgs alignment limit. In this parameter regime, the observed SM-like Higgs boson resides almost entirely in ${H}_1$, and the other two physical neutral scalars, which reside almost entirely in ${H}_2$, are approximate eigenstates of CP (denoted by the CP-even $H$ and the CP-odd $A$). In the Higgs basis, the scalar potential term $\bar{Z}_7 {H}_1^\dagger {H}_2 {H}_2^\dagger {H}_2+{\rm h.c.}$ governs the charged-Higgs loop contributions to the decay of $H$ and $A$ to photons. If $\Re \bar{Z}_7 \Im\ \bar{Z}_7 \neq 0$, then CP-violating effects are present and allow for an $H^+ H^- A$ coupling, which can yield a sizable branching ratio for $A\toγγ$. These CP-violating effects also generate non-zero EDMs for the electron, the neutron and the proton. We examine these correlations for the cases of $m_{A}=95$ GeV and $m_{A}=152$ GeV where interesting excesses in the diphoton spectrum have been observed at the LHC. These excesses can be explained via the decay of $A$ while being consistent with the experimental bound for the electron EDM in regions of parameter space that can be tested with future neutron and proton EDM measurements. This allows for the interesting possibility where the 95 GeV diphoton excess can be identified with $A$, while $m_H\simeq 98$ GeV can account for the best fit to the LEP excess in $e^+e^-\to ZH$ with $H\to b\bar b$.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00523
"Extending the atomic decomposition and many-body representation, a chemistry-motivated monomer-centered approach for machine learning potentials",['Chemical Physics'],"['Qi Yu', 'Ruitao Ma', 'Chen Qu', 'Riccardo Conte', 'Apurba Nandi', 'Priyanka Pandey', 'Paul L. Houston', 'Dong H. Zhang', 'Joel M. Bowman']","Most widely used machine learned (ML) potentials for condensed phase applications rely on many-body permutationally invariant polynomial (PIP) or atom-centered neural networks (NN). However, these approaches often lack chemical interpretability in atomistic energy decomposition and the computational efficiency of traditional force fields has not been fully achieved. Here, we present a novel method that combines aspects of both approaches, and achieves state-of-the-art balance of accuracy and force field-level speed. This method utilizes a monomer-centered representation, where the potential energy is decomposed into the sum of chemically meaningful monomeric energies. Without sophisticated neural network design, the structural descriptors of monomers are described by 1-body and 2-body effective interactions, enforced by appropriate sets of PIPs as inputs to the feed forward NN. We demonstrate the performance of this method through systematic assessments of models for gas-phase water trimer, liquid water, and also liquid CO2. The high accuracy, fast speed, and flexibility of this method provide a new route for constructing accurate ML potentials and enabling large-scale quantum and classical simulations for complex molecular systems.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00522
A Self-Explainable Heterogeneous GNN for Relational Deep Learning,['Machine Learning'],"['Francesco Ferrini', 'Antonio Longa', 'Andrea Passerini', 'Manfred Jaeger']","Recently, significant attention has been given to the idea of viewing relational databases as heterogeneous graphs, enabling the application of graph neural network (GNN) technology for predictive tasks. However, existing GNN methods struggle with the complexity of the heterogeneous graphs induced by databases with numerous tables and relations. Traditional approaches either consider all possible relational meta-paths, thus failing to scale with the number of relations, or rely on domain experts to identify relevant meta-paths. A recent solution does manage to learn informative meta-paths without expert supervision, but assumes that a node's class depends solely on the existence of a meta-path occurrence. In this work, we present a self-explainable heterogeneous GNN for relational data, that supports models in which class membership depends on aggregate information obtained from multiple occurrences of a meta-path. Experimental results show that in the context of relational databases, our approach effectively identifies informative meta-paths that faithfully capture the model's reasoning mechanisms. It significantly outperforms existing methods in both synthetic and real-world scenario.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00521
Terrestrial atmospheric ion implantation occurred in the nearside lunar regolith during the history of Earth's dynamo,['Earth and Planetary Astrophysics'],"['Shubhonkar Paramanick', 'Eric G. Blackman', 'John A. Tarduno', 'Jonathan Carroll-Nellenback']","Light volatile elements in lunar regolith are thought to be a mixture of the solar wind and Earth's atmosphere, the latter sourced in the absence of geomagnetic field. However, the extent to which both the current and primitive geodynamo influence the transport of terrestrial ions still remains unclear, and this uncertainty is further complicated by the enigmatic composition and poorly constrained location of the Eoarchean exosphere. Here we use 3-D MHD numerical simulations with present-day magnetized and Archean unmagnetized atmospheres to investigate how Earth's intrinsic magnetic field affects this transfer, aiming to constrain how and when the lunar isotopic signature provides a record of Earth's paleoatmosphere. We find that atmospheric transfer is efficient only when the Moon is within Earth's magnetotail. The non-solar contribution to the lunar soil is best explained by implantation during the long history of the geodynamo, rather than any short, putatively unmagnetized epoch of early Earth. This further suggests the history of the terrestrial atmosphere, spanning billions of years, could be preserved in buried lunar soils. Our results indicate that the elemental abundances of Apollo samples are very sensitive to Earth's exobase altitude, which, at the time of ion implantation, was never smaller than 190 km.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00519
Instant3dit: Multiview Inpainting for Fast Editing of 3D Objects,['Computer Vision and Pattern Recognition'],"['Amir Barda', 'Matheus Gadelha', 'Vladimir G. Kim', 'Noam Aigerman', 'Amit H. Bermano', 'Thibault Groueix']","We propose a generative technique to edit 3D shapes, represented as meshes, NeRFs, or Gaussian Splats, in approximately 3 seconds, without the need for running an SDS type of optimization. Our key insight is to cast 3D editing as a multiview image inpainting problem, as this representation is generic and can be mapped back to any 3D representation using the bank of available Large Reconstruction Models. We explore different fine-tuning strategies to obtain both multiview generation and inpainting capabilities within the same diffusion model. In particular, the design of the inpainting mask is an important factor of training an inpainting model, and we propose several masking strategies to mimic the types of edits a user would perform on a 3D shape. Our approach takes 3D generative editing from hours to seconds and produces higher-quality results compared to previous works.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00518
"Alexa, I Wanna See You: Envisioning Smart Home Assistants for the Deaf and Hard-of-Hearing",['Human-Computer Interaction'],"['Tyrone Justin Sta. Maria', 'Jordan Aiko Deja']","Smart Home Assistants (SHAs) have become ubiquitous in modern households, offering convenience and efficiency through its voice interface. However, for Deaf and Hard-of-Hearing (DHH) individuals, the reliance on auditory and textual feedback through a screen poses significant challenges. Existing solutions primarily focus on sign language input but overlook the need for seamless interaction and feedback modalities. This paper envisions SHAs designed specifically for DHH users, focusing on accessibility and inclusion. We discuss integrating augmented reality (AR) for visual feedback, support for multimodal input, including sign language and gestural commands, and context awareness through sound detection. Our vision highlights the importance of considering the diverse communication needs of the DHH community in developing SHA to ensure equitable access to smart home technology.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00514
Energy-Based Prior Latent Space Diffusion model for Reconstruction of Lumbar Vertebrae from Thick Slice MRI,['Image and Video Processing'],"['Yanke Wang', 'Yolanne Y. R. Lee', 'Aurelio Dolfini', 'Markus Reischl', 'Ender Konukoglu', 'Kyriakos Flouris']","Lumbar spine problems are ubiquitous, motivating research into targeted imaging for treatment planning and guided interventions. While high resolution and high contrast CT has been the modality of choice, MRI can capture both bone and soft tissue without the ionizing radiation of CT albeit longer acquisition time. The critical trade-off between contrast quality and acquisition time has motivated 'thick slice MRI', which prioritises faster imaging with high in-plane resolution but variable contrast and low through-plane resolution. We investigate a recently developed post-acquisition pipeline which segments vertebrae from thick-slice acquisitions and uses a variational autoencoder to enhance quality after an initial 3D reconstruction. We instead propose a latent space diffusion energy-based prior to leverage diffusion models, which exhibit high-quality image generation. Crucially, we mitigate their high computational cost and low sample efficiency by learning an energy-based latent representation to perform the diffusion processes. Our resulting method outperforms existing approaches across metrics including Dice and VS scores, and more faithfully captures 3D features.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00511
Graph-to-SFILES: Control structure prediction from process topologies using generative artificial intelligence,['Machine Learning'],"['Lukas Schulze Balhorn', 'Kevin Degens', 'Artur M. Schweidtmann']","Control structure design is an important but tedious step in P&ID development. Generative artificial intelligence (AI) promises to reduce P&ID development time by supporting engineers. Previous research on generative AI in chemical process design mainly represented processes by sequences. However, graphs offer a promising alternative because of their permutation invariance. We propose the Graph-to-SFILES model, a generative AI method to predict control structures from flowsheet topologies. The Graph-to-SFILES model takes the flowsheet topology as a graph input and returns a control-extended flowsheet as a sequence in the SFILES 2.0 notation. We compare four different graph encoder architectures, one of them being a graph neural network (GNN) proposed in this work. The Graph-to-SFILES model achieves a top-5 accuracy of 73.2% when trained on 10,000 flowsheet topologies. In addition, the proposed GNN performs best among the encoder architectures. Compared to a purely sequence-based approach, the Graph-to-SFILES model improves the top-5 accuracy for a relatively small training dataset of 1,000 flowsheets from 0.9% to 28.4%. However, the sequence-based approach performs better on a large-scale dataset of 100,000 flowsheets. These results highlight the potential of graph-based AI models to accelerate P&ID development in small-data regimes but their effectiveness on industry relevant case studies still needs to be investigated.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00508
Scalable nonlinear manifold reduced order model for dynamical systems,['Numerical Analysis'],"['Ivan Zanardi', 'Alejandro N. Diaz', 'Seung Whan Chung', 'Marco Panesi', 'Youngsoo Choi']","The domain decomposition (DD) nonlinear-manifold reduced-order model (NM-ROM) represents a computationally efficient method for integrating underlying physics principles into a neural network-based, data-driven approach. Compared to linear subspace methods, NM-ROMs offer superior expressivity and enhanced reconstruction capabilities, while DD enables cost-effective, parallel training of autoencoders by partitioning the domain into algebraic subdomains. In this work, we investigate the scalability of this approach by implementing a ""bottom-up"" strategy: training NM-ROMs on smaller domains and subsequently deploying them on larger, composable ones. The application of this method to the two-dimensional time-dependent Burgers' equation shows that extrapolating from smaller to larger domains is both stable and effective. This approach achieves an accuracy of 1% in relative error and provides a remarkable speedup of nearly 700 times.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00507
How Fitts' Fits in 3D: A Tangible Twist on Spatial Tasks,['Human-Computer Interaction'],"['Faith Griffin', 'Kevin Abelgas', 'Kriz Royce Tahimic', 'Andrei Kevin Chua', 'Jordan Aiko Deja', 'Tyrone Justin Sta. Maria']","Expanding Fitts' Law into a 3D context, we analyze PointARs, a mixed reality system that teaches pointer skills through an object manipulation task. Nine distinct configurations, varying in object sizes and distances, were explored to evaluate task complexity using metrics such as completion time, error rate, and throughput. Our results support Fitts' Law, showing that increased distances generally increase task difficulty. However, contrary to its predictions, larger objects also led to higher complexity, possibly due to the system's limitations in tracking them. Based on these findings, we suggest using tangible cubes between 1.5"" and 2"" in size and limiting the distance between objects to 2"" for optimal interaction in the system's 3D space. Future research should explore additional configurations and shapes to further validate Fitts' Law in the context of 3D object manipulation in systems like PointARs. This could help refine guidelines for designing mixed reality interfaces.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00506
Homeostazis and Sparsity in Transformer,['Machine Learning'],"['Leonid Kotyuzanskiy', 'Artem Klimov']","The transformer architecture has become an integral part of the field of modern neural networks, playing a crucial role in a variety of tasks, such as text generation, machine translation, image and audio processing, among others. There is also an alternative approach to building intelligent systems, proposed by Jeff Hawkins and inspired by the processes occurring in the neocortex. In our article we want to combine some of these ideas and to propose the use of homeostazis mechanisms, such as RFB-kWTA and ""Smart"" Inhibition, in the attention mechanism of the transformer and at the output of the transformer block, as well as conducting an experiment involving the introduction of sparse distributed representations of the transformer at various points. RFB-kWTA utilizes statistics of layer activations across time to adjust the entire layer, enhancing the values of rare activations while reducing those of frequent ones. ""Smart"" Inhibition also uses activation statistics to sample sparsity masks, with rarer activation times are more likely to be activated. Our proposed mechanisms significantly outperform the classical transformer 0.2768 BLEU and a model that only makes use of dropout in the attention mechanism and output of the transformer block 0.3007 BLEU, achieving a score of 0.3062 on the Multi30K dataset.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00503
Point n Move: Designing a Glove-Based Pointing Device,['Human-Computer Interaction'],"['Sealtiel B. Dy', 'Robert Joachim O. Encinas', 'Daphne Janelyn L. Go', 'Kyle Carlo C. Lasala', 'Bentley Andrew Y. Lu', 'Maria Monica Manlises', 'Jordan Aiko Deja']","In-person presentations commonly depend on projectors or screens, requiring input devices for slide transitions and laser pointing. This paper introduces a glove-based pointer device that integrates these functions, offering an alternative to conventional tools. The device leverages accelerometer and gyroscope technology to enhance precision and usability. We evaluated its performance by comparing it to the original CheerPod interface in hierarchical menu navigation tasks, involving participants aged 18 to 25. Results indicate task completion times ranging from 9 to 15 seconds with the proposed device, highlighting its efficiency and consistency. While the original CheerPod interface performed adequately, the glove-based pointer demonstrated advantages in reliability across tasks. These findings contribute to the design considerations for wearable input devices and suggest pathways for future improvements in presentation tools.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00501
Learning-Based Model Predictive Control for Piecewise Affine Systems with Feasibility Guarantees,['Systems and Control'],"['Samuel Mallick', 'Azita Dabiri', 'Bart De Schutter']","Online model predictive control (MPC) for piecewise affine (PWA) systems requires the online solution to an optimization problem that implicitly optimizes over the switching sequence of PWA regions, for which the computational burden can be prohibitive. Alternatively, the computation can be moved offline using explicit MPC; however, the online memory requirements and the offline computation can then become excessive. In this work we propose a solution in between online and explicit MPC, addressing the above issues by partially dividing the computation between online and offline. To solve the underlying MPC problem, a policy, learned offline, specifies the sequence of PWA regions that the dynamics must follow, thus reducing the complexity of the remaining optimization problem that solves over only the continuous states and control inputs. We provide a condition, verifiable during learning, that guarantees feasibility of the learned policy's output, such that an optimal continuous control input can always be found online. Furthermore, a method for iteratively generating training data offline allows the feasible policy to be learned efficiently, reducing the offline computational burden. A numerical experiment demonstrates the effectiveness of the method compared to both online and explicit MPC.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00490
Improved Cleanup and Decoding of Fractional Power Encodings,['Neural and Evolutionary Computing'],"['Alicia Bremer', 'Jeff Orchard']","High-dimensional vectors have been proposed as a neural method for representing information in the brain using Vector Symbolic Algebras (VSAs). While previous work has explored decoding and cleaning up these vectors under the noise that arises during computation, existing methods are limited. Cleanup methods are essential for robust computation within a VSA. However, cleanup methods for continuous-value encodings are not as effective. In this paper, we present an iterative optimization method to decode and clean up Fourier Holographic Reduced Representation (FHRR) vectors that are encoding continuous values. We combine composite likelihood estimation (CLE) and maximum likelihood estimation (MLE) to ensure convergence to the global optimum. We also demonstrate that this method can effectively decode FHRR vectors under different noise conditions, and show that it outperforms existing methods.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00488
Bimerons as Edge states in Thin Magnetic Strips,['Mesoscale and Nanoscale Physics'],"['Mario Castro', 'David Gálvez', 'Sebastián Castillo', 'Vagson L. Carvalho-Santos', 'Álvaro S. Núñez', 'Sebastián Allende']","Magnetic bimerons are potential information carriers in spintronic devices. Bimerons, topologically equivalent to skyrmions, manifest in chiral magnetic systems with in-plane magnetization due to anisotropies or external magnetic fields. Applications demanding their current-driven motion face significant challenges, notably the bimeron Hall effect, which causes transverse movement and annihilation at nanomagnet borders. This study addresses the problem of stabilizing bimeron propagation under current-driven conditions. We demonstrate that bimerons can propagate through thin ferromagnetic strips without annihilation when the easy-axis anisotropy and the electric current are orthogonal. Our findings indicate that below a threshold value of current, the repulsion between the bimeron and the strip boundary allows for stable soliton propagation, even in bent regions. This phenomenon extends to bimeron chains, which propagate parallel to the current flow. By enabling stable long-distance propagation, our results open new avenues for developing bimeron-based racetrack memory devices, enhancing the efficiency and reliability of future spintronic applications.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00483
Beam-cavity interactions in the rapid cycling synchrotron chain of the future muon collider,['Accelerator Physics'],"['Leonard Thiele', 'Fabian Batsch', 'Rama Calaga', 'Heiko Damerau', 'Alexej Grudiev', 'Ivan Karpov', 'Ursula van Rienen']","The International Muon Collider Collaboration (IMCC) is engaged in a design study for a future facility intended to collide muons. Subsequent to the initial linear acceleration, the counter-rotating muons and anti-muons are accelerated in a chain of rapid cycling synchrotrons (RCS) up to the multi-TeV collision energy. To maximise the number of muons available in the collider, it is essential to exploit the time dilation of the muon lifetime by employing a large accelerating gradient.
  The 1.3 GHz TESLA cavity serves as the baseline for the RCS chain. Considering the high bunch population and the small aperture of the cavity, the resulting beam-induced voltage per bunch passage is considerable, resulting in a substantial perturbation of the cavity voltage for subsequent bunch passages. In this contribution, the effects of beam loading during the acceleration cycle on the muons are calculated with the objective of determining the optimum parameters for minimising the cavity voltage transients. The interaction of the induced voltages, considering the counter-rotating beams, is studied.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00463
Nonlinear optical response and spontaneous polarization in layer-stacked gallenene using second harmonic generation,['Materials Science'],"['Muhammad Yunusa', 'Andrew K. Schulz', 'Tim Parker', 'Felix Schneider', 'Kenan Elibol', 'Marius Predel', 'Jana Dzíbelová', 'Michel Rebmann', 'Taylan Gorkan', 'Peter A. van Aken', 'Alfred J. Meixner', 'Engin Durgun', 'Jani Kotakoski', 'Dai Zhang', 'Metin Sitti']","Gallenene is a promising low-dimensional material with a structure down to the thickness of a single atom, similar to graphene. However, van der Waals stacking of two-dimensional (2D) gallenene under confinement remain poorly understood. In this study, we present evidence of the formation of parallel-stacked hexagonal gallenene (a100) structures in liquid gallium. The present study demonstrates the AB stacking of 2D gallenene a100 crystals in liquid gallium sandwiched between two graphene layers, as observed through transmission electron microscopy. A nonlinear optical response of the confined hexagonal gallenene was investigated through second harmonic generation (SHG) microscopy. The SHG signal exhibits periodic peak intensity shifts upon angular rotation up to 90 degrees and intensity dampening at elevated temperatures. These findings offer insights on device applications of 2D gallenene.△ Less",v1,https://arxiv.org/pdf/2412.00461
Personal Sound Zones and Shielded Localized Communication through Active Acoustic Control,['Sound'],"['Neil Jerome A. Egarguin', 'Daniel Onofrei']","In this paper, we present a time domain extension of our strategy on manipulating radiated scalar Helmholtz fields and discuss two important applied scenarios, namely (1) creating personal sound zones inside a bounded domain and (2) shielded localized communication. Our strategy is based on the authors' previous works establishing the possibility and stability of controlling acoustic fields using an array of almost non-radiating coupling sources and presents a detailed Fourier synthesis approach towards a time-domain effect. We require that the array of acoustic sources creates the desired fields on the control regions while maintaining a zero field beyond a larger circumscribed sphere. This paper recalls the main theoretical results then presents the underlying Fourier synthesis paradigm and show, through relevant simulations, the performance of our strategy.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00456
The Highly-Granular Time-of-Flight Neutron Detector for the BM@N experiment,['High Energy Physics - Experiment'],"['S. Morozov', 'V. Bocharnikov', 'D. Finogeev', 'M. Golubeva', 'F. Guber', 'A. Izvestnyy', 'N. Karpushkin', 'A. Makhnev', 'P. Parfenov', 'D. Serebryakov', 'A. Shabanov', 'V. Volkov', 'A. Zubankov']",A new Highly-Granular time-of-flight Neutron Detector (HGND) is being developed and constructed to measure azimuthal neutron flow and neutron yields in nucleus-nucleus interactions in heavy-ion collisions with energies up to 4A GeV in the fixed target experiment BM@N at JINR. Details of the detector design and results of performance studies for neutron identification and reconstruction are shown. Comparison of simulations for different options of the HGND layout at the BM@N is presented. Several proposed methods of neutron reconstruction including machine learning and cluster methods are discussed.△ Less,"30 November, 2024;",https://arxiv.org/pdf/2412.00455
A conditional Generative Adversarial network model for the Weather4Cast 2024 Challenge,['Computer Vision and Pattern Recognition'],"['Atharva Deshpande', 'Kaushik Gopalan', 'Jeet Shah', 'Hrishikesh Simu']","This study explores the application of deep learning for rainfall prediction, leveraging the Spinning Enhanced Visible and Infrared Imager (SEVIRI) High rate information transmission (HRIT) data as input and the Operational Program on the Exchange of weather RAdar information (OPERA) ground-radar reflectivity data as ground truth. We use the mean of 4 InfraRed frequency channels as the input. The radiance images are forecasted up to 4 hours into the future using a dense optical flow algorithm. A conditional generative adversarial network (GAN) model is employed to transform the predicted radiance images into rainfall images which are aggregated over the 4 hour forecast period to generate cumulative rainfall values. This model scored a value of approximately 7.5 as the Continuous Ranked Probability Score (CRPS) in the Weather4Cast 2024 competition and placed 1st on the core challenge leaderboard.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00451
Effect of Correlated Building Blockages on the Ergodic Capacity of mmWave Systems in Urban Scenarios,['Signal Processing'],"['Cristian García Ruiz', 'Olga Muñoz', 'Antonio Pascual-Iserte']","The mmWave bands, considered to support the forthcoming generation of mobile communications technologies, have a well-known vulnerability to blockages. Recent works in the literature analyze the blockage probability considering independence or correlation among the blocking elements of the different links. In this letter, we characterize the effect of blockages and their correlation on the ergodic capacity. We carry out the analysis for urban scenarios, where the considered blocking elements are buildings that are primarily parallel to the streets. We also present numerical simulations based on actual building features of the city of Chicago to validate the obtained expressions.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00449
Impact of nonextensivity on the transport coefficients of strongly interacting QCD matter,['High Energy Physics - Phenomenology'],"['Dhananjay Singh', 'Arvind Kumar']","Tsallis nonextensive statistics is applied to study the transport coefficients of strongly interacting matter within the Polyakov chiral SU(3) quark mean field model (PCQMF). Nonextensivity is introduced within the PCQMF model through a dimensionless $q$ parameter to examine the viscous properties such as shear viscosity ($η$), bulk viscosity ($ζ_b$), and conductive properties, including electrical conductivity ($σ_{el}$) and thermal conductivity ($κ$). Additionally, some key thermodynamic quantities relevant to the transport coefficients, like the speed of sound ($c_{sq}^2$) and specific heat at constant volume ($c_{vq}$), are calculated. The temperature dependence of the transport coefficients is explored through a kinetic theory approach with the relaxation time approximation. The results are compared to the extensive case where $q$ approaches 1. The nonextensive $q$ parameter is found to have a significant effect on all transport coefficients. We find that the nonextensive behaviour of the medium enhances both specific shear viscosity $η/s_q$ as well as conductive coefficients $σ_{el}/T$ and $κ/T^2$. In contrast, the normalised bulk viscosity $ζ_b/s_q$ is found to decrease as the nonextensivity of the medium increases. We have also studied the transport coefficients for finite values of chemical potentials. The magnitude of $η$, $σ_{el}$, and $κ$ increases at lower temperatures while $ζ$ is found to decrease for systems with non-zero chemical potential.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00444
Analysis of Blocking in mmWave Cellular Systems: Characterization of the LOS and NLOS Intervals in Urban Scenarios,['Signal Processing'],"['Cristian García Ruiz', 'Antonio Pascual-Iserte', 'Olga Muñoz-Medina']","In the millimeter waves (mmWave) bands considered for 5G and beyond, the use of very high frequencies results in the interruption of communication whenever there is no line of sight between the transmitter and the receiver. Blockages have been modeled in the literature so far using tools such as stochastic geometry and random shape theory. Using these tools, in this paper, we characterize the lengths of the segments in line-of-sight (LOS) and in non-line-of-sight (NLOS) statistically in an urban scenario where buildings (with random positions, lengths, and heights) are deployed in parallel directions configuring streets.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00442
Lefschetz principle-type theorems for curves semistable Higgs sheaves and applications,['Algebraic Geometry'],['Armando Capasso'],"I study Higgs bundles over smooth projective varieties defined on an algebraically closed field of characteristic $0$. I prove ``Lefschetz principle''-type theorems for semistable Higgs sheaves and curve semistable Higgs bundles. I give an application to variates whose canonical bundle is ample, showing stability of the so-called Simpson System. From all this I obtain another proof of the Guggenheimer-Yau inequality. Where this inequality is saturated, I prove that the discriminant class of the Simpson system vanishes. This follows from the study of the relations between these numerical properties of Higgs bundles and curve semistability.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00439
Amenability and skew-amenability of actions of topological groups,['Group Theory'],"['Vadim Alekseev', 'Hiroshi Ando', 'Friedrich Martin Schneider', 'Andreas Thom']",We define and study notions of amenability and skew-amenability of continuous actions of topological groups on compact topological spaces. Our main motivation is the question under what conditions amenability of a topological group passes to a closed subgroup. Other applications include the understanding of the universal minimal flow of various non-amenable groups.△ Less,"30 November, 2024;",https://arxiv.org/pdf/2412.00438
Multi-Agent System for Cosmological Parameter Analysis,['Instrumentation and Methods for Astrophysics'],"['Andrew Laverick', 'Kristen Surrao', 'Inigo Zubeldia', 'Boris Bolliet', 'Miles Cranmer', 'Antony Lewis', 'Blake Sherwin', 'Julien Lesgourgues']","Multi-agent systems (MAS) utilizing multiple Large Language Model agents with Retrieval Augmented Generation and that can execute code locally may become beneficial in cosmological data analysis. Here, we illustrate a first small step towards AI-assisted analyses and a glimpse of the potential of MAS to automate and optimize scientific workflows in Cosmology. The system architecture of our example package, that builds upon the autogen/ag2 framework, can be applied to MAS in any area of quantitative scientific research. The particular task we apply our methods to is the cosmological parameter analysis of the Atacama Cosmology Telescope lensing power spectrum likelihood using Monte Carlo Markov Chains. Our work-in-progress code is open source and available at https://github.com/CMBAgents/cmbagent.△ Less",v1,https://arxiv.org/pdf/2412.00431
"NUTs, Bolts, and Spindles",['High Energy Physics - Theory'],"['Matteo Kevin Crisafio', 'Alessio Fontanarossa', 'Dario Martelli']","We construct new infinite classes of Euclidean supersymmetric solutions of four dimensional minimal gauged supergravity comprising a $U (1) \times U (1)$-invariant, asymptotically locally hyperbolic, metric on the total space of orbifold line bundles over a spindle (bolt). The conformal boundary is generically a squashed, branched, lens space and the graviphoton gauge field can have either twist or anti-twist through the spindle bolt. Correspondingly, the boundary geometry inherits two types of rigid Killing spinors, that we refer to as twist and anti-twist for the three-dimensional Seifert orbifolds, as well as some specific flat connections for the background gauge field, determined by the data of the spindle bolt. For all our solutions we compute the holographically renormalized on-shell action and compare it to the expression obtained via equivariant localization, uncovering a markedly distinct behaviour in the cases of twist and anti-twist. Our results provide precise predictions for the large $N$ limit of the corresponding localized partition functions of three-dimensional $\mathcal{N}=2$ superconformal field theories placed on Seifert orbifolds.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00428
Few-Shot Domain Adaptation for Named-Entity Recognition via Joint Constrained k-Means and Subspace Selection,['Computation and Language'],"['Ayoub Hammal', 'Benno Uthayasooriyar', 'Caio Corro']","Named-entity recognition (NER) is a task that typically requires large annotated datasets, which limits its applicability across domains with varying entity definitions. This paper addresses few-shot NER, aiming to transfer knowledge to new domains with minimal supervision. Unlike previous approaches that rely solely on limited annotated data, we propose a weakly supervised algorithm that combines small labeled datasets with large amounts of unlabeled data. Our method extends the k-means algorithm with label supervision, cluster size constraints and domain-specific discriminative subspace selection. This unified framework achieves state-of-the-art results in few-shot NER on several English datasets.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00426
On autoregressive deep learning models for day-ahead wind power forecasting with irregular shutdowns due to redispatching,['Machine Learning'],"['Stefan Meisenbacher', 'Silas Aaron Selzer', 'Mehdi Dado', 'Maximilian Beichter', 'Tim Martin', 'Markus Zdrallek', 'Peter Bretschneider', 'Veit Hagenmeyer', 'Ralf Mikut']","Renewable energies and their operation are becoming increasingly vital for the stability of electrical power grids since conventional power plants are progressively being displaced, and their contribution to redispatch interventions is thereby diminishing. In order to consider renewable energies like Wind Power (WP) for such interventions as a substitute, day-ahead forecasts are necessary to communicate their availability for redispatch planning. In this context, automated and scalable forecasting models are required for the deployment to thousands of locally-distributed onshore WP turbines. Furthermore, the irregular interventions into the WP generation capabilities due to redispatch shutdowns pose challenges in the design and operation of WP forecasting models. Since state-of-the-art forecasting methods consider past WP generation values alongside day-ahead weather forecasts, redispatch shutdowns may impact the forecast. Therefore, the present paper highlights these challenges and analyzes state-of-the-art forecasting methods on data sets with both regular and irregular shutdowns. Specifically, we compare the forecasting accuracy of three autoregressive Deep Learning (DL) methods to methods based on WP curve modeling. Interestingly, the latter achieve lower forecasting errors, have fewer requirements for data cleaning during modeling and operation while being computationally more efficient, suggesting their advantages in practical applications.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00423
Topological magnetism in diluted artificial adatom lattices,['Mesoscale and Nanoscale Physics'],"['Amal Aldarawsheh', 'Samir Lounis']","The ability to control matter at the atomic scale has revolutionized our understanding of the physical world, opening doors to unprecedented technological advancements. Quantum technology, which harnesses the unique principles of quantum mechanics, enables us to construct and manipulate atomic structures with extraordinary precision.
  Here, we propose a bottom-up approach to create topological magnetic textures in diluted adatom lattices on the Nb(110) surface. By fine-tuning adatom spacing, previously inaccessible magnetic phases can emerge. Our findings reveal that interactions between magnetic adatoms, mediated by the Nb substrate, foster the formation of unique topological spin textures, such as skyrmions and anti-skyrmions, both ferromagnetic and antiferromagnetic. Since Nb can be superconducting, our findings present a novel platform with valuable insights into the interplay between topological magnetism and superconductivity, paving the way for broader exploration of topological superconductivity in conjunction with spintronics applications.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00421
TAROT: Targeted Data Selection via Optimal Transport,['Machine Learning'],"['Lan Feng', 'Fan Nie', 'Yuejiang Liu', 'Alexandre Alahi']","We propose TAROT, a targeted data selection framework grounded in optimal transport theory. Previous targeted data selection methods primarily rely on influence-based greedy heuristics to enhance domain-specific performance. While effective on limited, unimodal data (i.e., data following a single pattern), these methods struggle as target data complexity increases. Specifically, in multimodal distributions, these heuristics fail to account for multiple inherent patterns, leading to suboptimal data selection. This work identifies two primary factors contributing to this limitation: (i) the disproportionate impact of dominant feature components in high-dimensional influence estimation, and (ii) the restrictive linear additive assumptions inherent in greedy selection strategies. To address these challenges, TAROT incorporates whitened feature distance to mitigate dominant feature bias, providing a more reliable measure of data influence. Building on this, TAROT uses whitened feature distance to quantify and minimize the optimal transport distance between the selected data and target domains. Notably, this minimization also facilitates the estimation of optimal selection ratios. We evaluate TAROT across multiple tasks, including semantic segmentation, motion prediction, and instruction tuning. Results consistently show that TAROT outperforms state-of-the-art methods, highlighting its versatility across various deep learning tasks. Code is available at https://github.com/vita-epfl/TAROT.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00420
Mixture of Experts for Node Classification,['Social and Information Networks'],"['Yu Shi', 'Yiqi Wang', 'WeiXuan Lang', 'Jiaxin Zhang', 'Pan Dong', 'Aiping Li']","Nodes in the real-world graphs exhibit diverse patterns in numerous aspects, such as degree and homophily. However, most existent node predictors fail to capture a wide range of node patterns or to make predictions based on distinct node patterns, resulting in unsatisfactory classification performance. In this paper, we reveal that different node predictors are good at handling nodes with specific patterns and only apply one node predictor uniformly could lead to suboptimal result. To mitigate this gap, we propose a mixture of experts framework, MoE-NP, for node classification. Specifically, MoE-NP combines a mixture of node predictors and strategically selects models based on node patterns. Experimental results from a range of real-world datasets demonstrate significant performance improvements from MoE-NP.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00418
Cosmological Models in Lovelock Gravity: An Overview of Recent Progress,['General Relativity and Quantum Cosmology'],['Sergey Pavluchenko'],"In the current review, we provide a summary of the recent progress made in the cosmological aspect of extra-dimensional Lovelock gravity. Our review covers a wide variety of particular model/matter source combinations: Einstein--Gauss--Bonnet as well as cubic Lovelock gravities with vacuum, cosmological constant, perfect fluid, spatial curvature, and some of their combinations. Our analysis suggests that it is possible to set constraints on the parameters of the above-mentioned models from the simple requirement of the existence of a smooth transition from the initial singularity to a realistic low-energy regime. Initially, anisotropic space naturally evolves into a configuration with two isotropic subspaces, and if one of these subspaces is three-dimensional and is expanding while another is contracting, we call it realistic compactification. Of course, the process is not devoid of obstacles, and in our paper, we review the results of the compactification occurrence investigation for the above-mentioned models. In particular, for vacuum and $Λ$-term EGB models, compactification is not suppressed (but is not the only possible outcome either) if the number of extra dimensions is $D \geqslant 2$; for vacuum cubic Lovelock gravities it is always present (however, cubic Lovelock gravity is defined only for $D \geqslant 3$ number of extra dimensions); for the EGB model with perfect fluid it is present for $D=2$ (we have not considered this model in higher dimensions yet), and in the presence of spatial curvature, the realistic stabilization of extra dimensions is always present (however, such a model is well-defined only in $D \geqslant 4$ number of extra dimensions).△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00414
Irreducible representations of certain nilpotent groups of finite rank,['Representation Theory'],['Anatolii V. Tushev'],In the paper we study irreducible representations of some nilpotent groups of finite abelian total rank. The main result of the paper states that if a torsion-free minimax group $G$ of nilpotency class 2 admits a faithful irreducible representation $\varphi $ over a finitely generated field $k$ such that $chark \notin Sp(G)$ then there exist a subgroup $N$ and an irreducible primitive representation $ψ$ of the subgroup $N$ over $k$ such that the representation $\varphi $ is induced from $ψ$ and the quotient group $N/Kerψ$ is finitely generated.△ Less,"30 November, 2024;",https://arxiv.org/pdf/2412.00398
On Foundation Models for Dynamical Systems from Purely Synthetic Data,['Machine Learning'],"['Martin Ziegler', 'Andres Felipe Posada-Moreno', 'Friedrich Solowjow', 'Sebastian Trimpe']","Foundation models have demonstrated remarkable generalization, data efficiency, and robustness properties across various domains. In this paper, we explore the feasibility of foundation models for applications in the control domain. The success of these models is enabled by large-scale pretaining on Internet-scale datasets. These are available in fields like natural language processing and computer vision, but do not exist for dynamical systems. We address this challenge by pretraining a transformer-based foundation model exclusively on synthetic data and propose to sample dynamics functions from a reproducing kernel Hilbert space. Our pretrained model generalizes for prediction tasks across different dynamical systems, which we validate in simulation and hardware experiments, including cart-pole and Furuta pendulum setups. Additionally, the model can be fine-tuned effectively to new systems to increase performance even further. Our results demonstrate the feasibility of foundation models for dynamical systems that outperform specialist models in terms of generalization, data efficiency, and robustness.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00395
Advancing Object-Centric Process Mining with Multi-Dimensional Data Operations,['Databases'],"['Shahrzad Khayatbashi', 'Najmeh Miri', 'Amin Jalali']","Analyzing process data at varying levels of granularity is important to derive actionable insights and support informed decision-making. Object-Centric Event Data (OCED) enhances process mining by capturing interactions among multiple objects within events, leading to the discovery of more detailed and realistic yet complex process models. The lack of methods to adjust the granularity of the analysis limits users to leverage the full potential of Object-Centric Process Mining (OCPM). To address this gap, we propose four operations: drill-down, roll-up, unfold, and fold, which enable changing the granularity of analysis when working with Object-Centric Event Logs (OCEL). These operations allow analysts to seamlessly transition between detailed and aggregated process models, facilitating the discovery of insights that require varying levels of abstraction. We formally define these operations and implement them in an open-source Python library. To validate their utility, we applied the approach to real-world OCEL data extracted from a learning management system that covered a four-year period and approximately 400 students. Our evaluation demonstrates significant improvements in precision and fitness metrics for models discovered before and after applying these operations. This approach can empower analysts to perform more flexible and comprehensive process exploration, unlocking actionable insights through adaptable granularity adjustments.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00393
Collective quantum phases in frustrated arrays of Josephson junctions,['Strongly Correlated Electrons'],"['M. V. Fistul', 'O. Neyenhuys', 'B. Pernack', 'I. M. Eremin', 'Sergej Flach', 'Alexei Andreanov']","We study collective quantum phases and quantum phase transitions occurring in frustrated sawtooth arrays of small quantum Josephson junctions. Frustration is introduced through the periodic arrangement of $0$- and $π$- Josephson junctions with the Josephson coupling energies $αE_\mathrm{J}$ of different signs, $-1\leq α\leq 1$. The complexity of the potential landscape of the system is controlled by the frustration parameter $f=(1-α)/2$. The potential energy has a single global minimum in the non-frustrated regime ($f<f_\mathrm{cr}=0.75$) and a macroscopic number of equal minima in the frustrated regime ($f>f_\mathrm{cr}=0.75$). We address the coherent quantum regime and identify several collective quantum phases: disordered (insulating) and ordered (superconducting) phases in the non-frustrated regime, as well as highly entangled patterns of vortices and anti-vortices in the frustrated regime. These collective quantum phases are controlled by several physical parameters: the frustration $f$, the Josephson coupling, and the charging energies of junctions and islands. We map the control parameter phase diagram by characterizing the quantum dynamics of frustrated Josephson junction arrays by spatially and temporally resolved quantum-mechanical correlation function of the local magnetization.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00391
Sums related to Euler's totient function,['Number Theory'],['Artyom Radomskii'],"We obtain an upper bound for the sum $\sum_{n\leq N} (a_{n}/\varphi (a_{n}))^{s}$, where $\varphi$ is Euler's totient function, $s\in \mathbb{N}$, and $a_{1},\ldots, a_{N}$ are positive integers (not necessarily distinct) with some restrictions. As applications, for any $t>0$, we obtain an upper bound for the number of $n\in [1,N]$ such that $a_{n}/ \varphi (a_{n})> t$.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00389
On periodic approximate solutions of ordinary differential equations,['Classical Analysis and ODEs'],"['Wang Shiwei', 'Alexander Zorin', 'Marina Konyaeva', 'Mikhail Malykh', 'Leonid Sevastianov']","The issue of inheriting periodicity of an exact solution of a dynamic system by a difference scheme is considered. It is shown that some difference schemes (midpoint scheme, Kahan scheme) in some special cases provide approximate solutions of differential equations, which are periodic sequences. Such solutions are called periodic. A purely algebraic method for finding such solutions is developed. It is shown that midpoint scheme inherits periodicity not only in case of linear oscillator, but also in case of nonlinear oscillator, integrable into elliptic functions.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00388
A generalization of Burmester-Desmedt GKE based on a non-abelian finite group action,['Cryptography and Security'],"['Daniel Camazón Portela', 'Álvaro Otero Sánchez', 'Juan Antonio López Ramos']","The advent of large-scale quantum computers implies that our existing public-key cryptography infrastructure has become insecure. That means that the privacy of many mobile applications involving dynamic peer groups, such as multicast messaging or pay-per-view, could be compromised. In this work we propose a generalization of the well known group key exchange protocol proposed by Burmester and Desmedt to the non-abelian case by the use of finite group actions and we prove that the presented protocol is secure in Katz and Yung's model.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00387
Quaternion-Kähler manifolds with non-negative quaternionic sectional curvature,['Differential Geometry'],"['Andrei Moroianu', 'Uwe Semmelmann', 'Gregor Weingart']","Compact Hermitian symmetric spaces are Kähler manifolds with constant scalar curvature and non-negative sectional curvature. A famous result by A. Gray states that, conversely, a compact simply connected Kähler manifold with constant scalar curvature and non-negative sectional curvature is a Hermitian symmetric space. The aim of the present article is to transpose Gray's result to the quaternion-Kähler setting. In order to achieve this, we introduce the quaternionic sectional curvature of quaternion-Kähler manifolds, we show that every Wolf space has non-negative quaternionic sectional curvature, and we prove that, conversely, every quaternion-Kähler manifold with non-negative quaternionic sectional curvature is a Wolf space. The proof makes crucial use of the nearly Kähler twistor spaces of positive quaternion-Kähler manifolds.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00385
Unified Parameter-Efficient Unlearning for LLMs,['Artificial Intelligence'],"['Chenlu Ding', 'Jiancan Wu', 'Yancheng Yuan', 'Jinda Lu', 'Kai Zhang', 'Alex Su', 'Xiang Wang', 'Xiangnan He']","The advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like LoRA, has become a prevalent practice due to its efficiency. However, this raises significant privacy and security concerns, as models may inadvertently retain and disseminate sensitive or undesirable information. To address these issues, we introduce a novel instance-wise unlearning framework, LLMEraser, which systematically categorizes unlearning tasks and applies precise parameter adjustments using influence functions. Unlike traditional unlearning techniques that are often limited in scope and require extensive retraining, LLMEraser is designed to handle a broad spectrum of unlearning tasks without compromising model performance. Extensive experiments on benchmark datasets demonstrate that LLMEraser excels in efficiently managing various unlearning scenarios while maintaining the overall integrity and efficacy of the models.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00383
2-Factor Retrieval for Improved Human-AI Decision Making in Radiology,['Human-Computer Interaction'],"['Jim Solomon', 'Laleh Jalilian', 'Alexander Vilesov', 'Meryl Mathew', 'Tristan Grogan', 'Arash Bedayat', 'Achuta Kadambi']","Human-machine teaming in medical AI requires us to understand to what degree a trained clinician should weigh AI predictions. While previous work has shown the potential of AI assistance at improving clinical predictions, existing clinical decision support systems either provide no explainability of their predictions or use techniques like saliency and Shapley values, which do not allow for physician-based verification. To address this gap, this study compares previously used explainable AI techniques with a newly proposed technique termed '2-factor retrieval (2FR)', which is a combination of interface design and search retrieval that returns similarly labeled data without processing this data. This results in a 2-factor security blanket where: (a) correct images need to be retrieved by the AI; and (b) humans should associate the retrieved images with the current pathology under test. We find that when tested on chest X-ray diagnoses, 2FR leads to increases in clinician accuracy, with particular improvements when clinicians are radiologists and have low confidence in their decision. Our results highlight the importance of understanding how different modes of human-AI decision making may impact clinician accuracy in clinical decision support systems.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00372
Random Cycle Coding: Lossless Compression of Cluster Assignments via Bits-Back Coding,['Machine Learning'],"['Daniel Severo', 'Ashish Khisti', 'Alireza Makhzani']","We present an optimal method for encoding cluster assignments of arbitrary data sets. Our method, Random Cycle Coding (RCC), encodes data sequentially and sends assignment information as cycles of the permutation defined by the order of encoded elements. RCC does not require any training and its worst-case complexity scales quasi-linearly with the size of the largest cluster. We characterize the achievable bit rates as a function of cluster sizes and number of elements, showing RCC consistently outperforms previous methods while requiring less compute and memory resources. Experiments show RCC can save up to 2 bytes per element when applied to vector databases, and removes the need for assigning integer ids to identify vectors, translating to savings of up to 70% in vector database systems for similarity search applications.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00369
Basis set incompleteness errors in fixed-node diffusion Monte Carlo calculations on non-covalent interactions,['Chemical Physics'],"['Kousuke Nakano', 'Benjamin X. Shi', 'Dario Alfè', 'Andrea Zen']","Basis set incompleteness error (BSIE) is a common source of error in quantum chemistry (QC) calculations, but it has not been comprehensively studied in fixed-node Diffusion Monte Carlo (FN-DMC) calculations. FN-DMC, being a projection method, is often considered minimally affected by basis set biases. Here, we show that this assumption is not always valid. While the relative error introduced by a small basis set in the total FN-DMC energy is minor, it can become significant in binding energy ($E_{\rm b}$) evaluations of weakly interacting systems. We systematically investigated BSIEs in FN-DMC-based binding energy ($E_{\rm b}$) evaluations using the A24 dataset, a well-known benchmark set of 24 non-covalently bound dimers. Contrary to common expectations, we found that BSIEs in FN-DMC evaluations of $E_{\rm b}$ are indeed significant when small localized basis sets, such as cc-pVDZ, are employed. We observed that BSIEs are larger in dimers with hydrogen-bonding interactions and smaller in dispersion-dominated interactions. We also found that augmenting the basis sets with diffuse orbitals, using counterpoise (CP) correction, or both, effectively mitigates BSIEs.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00368
Probabilistic Prediction of Ship Maneuvering Motion using Ensemble Learning with Feedforward Neural Networks,['Systems and Control'],"['Kouki Wakita', 'Youhei Akimoto', 'Atsuo Maki']","In the field of Maritime Autonomous Surface Ships (MASS), the accurate modeling of ship maneuvering motion for harbor maneuvers is a crucial technology. Non-parametric system identification (SI) methods, which do not require prior knowledge of the target ship, have the potential to produce accurate maneuvering models using observed data. However, the modeling accuracy significantly depends on the distribution of the available data. To address these issues, we propose a probabilistic prediction method of maneuvering motion that incorporates ensemble learning into a non-parametric SI using feedforward neural networks. This approach captures the epistemic uncertainty caused by insufficient or unevenly distributed data. In this paper, we show the prediction accuracy and uncertainty prediction results for various unknown scenarios, including port navigation, zigzag, turning, and random control maneuvers, assuming that only port navigation data is available. Furthermore, this paper demonstrates the utility of the proposed method as a maneuvering simulator for assessing heading-keeping PD control. As a result, it was confirmed that the proposed method can achieve high accuracy if training data with similar state distributions is provided, and that it can also predict high uncertainty for states that deviate from the training data distribution. In the performance evaluation of PD control, it was confirmed that considering worst-case scenarios reduces the possibility of overestimating performance compared to the true system. Finally, we show the results of applying the proposed method to full-scale ship data, demonstrating its applicability to full-scale ships.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00363
Investigating the relation between environment and internal structure of massive elliptical galaxies using strong lensing,['Astrophysics of Galaxies'],"['S M Rafee Adnan', 'Muhammad Jobair Hasan', 'Ahmad Al - Imtiaz', 'Sulyman H. Robin', 'Fahim R. Shwadhin', 'Anowar J. Shajib', 'Mamun Hossain Nahid', 'Mehedi Hasan Tanver', 'Tanjela Akter', 'Nusrath Jahan', 'Zareef Jafar', 'Mamunur Rashid', 'Anik Biswas', 'Akbar Ahmed Chowdhury', 'Jannatul Feardous', 'Ajmi Rahaman', 'Masuk Ridwan', 'Rahul D. Sharma', 'Zannat Chowdhury', 'Mir Sazzat Hossain']","Strong lensing directly probes the internal structure of the lensing galaxies. In this paper, we investigate the relation between the internal structure of massive elliptical galaxies and their environment using a sample of 15 strong lensing systems. We performed lens modeling for them using Lenstronomy and constrained the mass and light distributions of the deflector galaxies. We adopt the local galaxy density as a metric for the environment and test our results against several alternative definitions of it. We robustly find that the centroid offset between the mass and light is not correlated with the local galaxy density. This result supports using centroid offsets as a probe of dark matter theories since the environment's impact on it can be treated as negligible. Although we find a strong correlation between the position angle offset and the standard definition of the local galaxy density, consistent with previous studies, the correlation becomes weaker for alternative definitions of the local galaxy density. This result weakens the support for interpreting the position angle misalignment as having originated from interaction with the environment. Furthermore, we find the 'residual shear' magnitude in the lens model to be uncorrelated with the local galaxy density, supporting the interpretation of the residual shear originating, in part, from the inadequacy in modeling the angular structure of the lensing galaxy and not solely from the structures present in the environment or along the line of sight.△ Less","30 November, 2024;",https://arxiv.org/pdf/2412.00361
Simultaneously Satisfying MXS and EFL,['Computer Science and Game Theory'],"['Arash Ashuri', 'Vasilis Gkatzelis']","The two standard fairness notions in the resource allocation literature are proportionality and envy-freeness. If there are n agents competing for the available resources, then proportionality requires that each agent receives at least a 1/n fraction of their total value for the set of resources. On the other hand, envy-freeness requires that each agent weakly prefers the resources allocated to them over those allocated to any other agent. Each of these notions has its own benefits, but it is well known that neither one of the two is always achievable when the resources being allocated are indivisible. As a result, a lot of work has focused on satisfying fairness notions that relax either proportionality or envy-freeness. In this paper, we focus on MXS (a relaxation of proportionality) and EFL (a relaxation of envy-freeness). Each of these notions was previously shown to be achievable on its own [Barman et al.,2018, Caragiannis et al., 2023], and our main result is an algorithm that computes allocations that simultaneously satisfy both, combining the benefits of approximate proportionality and approximate envy-freeness. In fact, we prove this for any instance involving agents with valuation functions that are restricted MMS-feasible, which are more general than additive valuations. Also, since every EFL allocation directly satisfies other well-studied fairness notions like EF1, 1/2-EFX, 1/2-GMMS, and 2/3-PMMS, and every MXS allocation satisfies 4/7-MMS, the allocations returned by our algorithm simultaneously satisfy a wide variety of fairness notions and are, therefore, universally fair [Amanatidis et al., 2020].△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00358
On the Role of Noise in Factorizers for Disentangling Distributed Representations,['Machine Learning'],"['Geethan Karunaratne', 'Michael Hersche', 'Abu Sebastian', 'Abbas Rahimi']","To efficiently factorize high-dimensional distributed representations to the constituent atomic vectors, one can exploit the compute-in-superposition capabilities of vector-symbolic architectures (VSA). Such factorizers however suffer from the phenomenon of limit cycles.
  Applying noise during the iterative decoding is one mechanism to address this issue. In this paper, we explore ways to further relax the noise requirement by applying noise only at the time of VSA's reconstruction codebook initialization. While the need for noise during iterations proves analog in-memory computing systems to be a natural choice as an implementation media, the adequacy of initialization noise allows digital hardware to remain equally indispensable. This broadens the implementation possibilities of factorizers. Our study finds that while the best performance shifts from initialization noise to iterative noise as the number of factors increases from 2 to 4, both extend the operational capacity by at least 50 times compared to the baseline factorizer resonator networks. Our code is available at: https://github.com/IBM/in-memory-factorizer△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00354
Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection,['Computation and Language'],"['Shanu Kumar', 'Saish Mendke', 'Karody Lubna Abdul Rahman', 'Santosh Kurasa', 'Parag Agrawal', 'Sandipan Dandapat']","Chain-of-thought (CoT) prompting has significantly enhanced the capability of large language models (LLMs) by structuring their reasoning processes. However, existing methods face critical limitations: handcrafted demonstrations require extensive human expertise, while trigger phrases are prone to inaccuracies. In this paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method, a novel approach that improves CoT prompting by utilizing uncertainty estimates to select effective demonstrations without needing access to model parameters. Unlike traditional methods, ZEUS offers high sensitivity in distinguishing between helpful and ineffective questions, ensuring more precise and reliable selection. Our extensive evaluation shows that ZEUS consistently outperforms existing CoT strategies across four challenging reasoning benchmarks, demonstrating its robustness and scalability.△ Less",v1,https://arxiv.org/pdf/2412.00353
Analyzing lump-type solutions in scalar field models through configurational information measure,['Pattern Formation and Solitons'],"['Marcelo A. Feitosa', 'Wesley B. Cardoso', 'Dionisio Bazeia', 'Ardiley T. Avelar']","In this paper we employ a configurational information measure, specifically the differential configurational complexity (DCC), to quantify the information content of lump-type solutions in various scalar field models, including two modified inverted $φ^{4}$ models, the modified $φ^{3}$ model, as well as two additional families of lump models. Our objective is to complement previous studies by providing an informational perspective that distinguishes different solutions based on their energy configurations. We explore how the DCC measure relates to energy and its applicability in analyzing degenerate states. Our findings indicate that DCC effectively correlates with the energy parameters of the solutions, offering significant insights into their informational properties. This study underscores the value of using informational metrics like DCC to deepen our understanding of the structural and dynamic characteristics of complex systems in theoretical physics.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00352
The impact of LHC precision measurements of inclusive jet and dijet production on the CTEQ-TEA global PDF fit,['High Energy Physics - Phenomenology'],"['Alim Ablat', 'Sayipjamal Dulat', 'Tie-Jiun Hou', 'Joey Huston', 'Pavel Nadolsky', 'Ibrahim Sitiwaldi', 'Keping Xie', 'C. -P. Yuan']","In this study, we investigate the impact of new LHC inclusive jet and dijet measurements on parton distribution functions (PDFs) that describe the proton structure, with a particular focus on the gluon distribution at large momentum fraction, $x$, and the corresponding partonic luminosities. We assess constraints from these datasets using next-to-next-to-leading-order (NNLO) theoretical predictions, accounting for a range of uncertainties from scale dependence and numerical integration. From the scale choices available for the calculations, our analysis shows that the central predictions for inclusive jet production show a smaller scale dependence than dijet production. We examine the relative constraints on the gluon distribution provided by the inclusive jet and dijet distributions and also explore the phenomenological implications for inclusive $H$, $t\bar{t}$, and $t\bar{t}H$ production at the LHC at 14 TeV.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00350
About the accuracy of the relxill/relxill_nk models in view of the next generation of X-ray missions,['High Energy Astrophysical Phenomena'],"['Honghui Liu', 'Askar B. Abdikamalov', 'Temurbek Mirzaev', 'Cosimo Bambi', 'Thomas Dauser', 'Javier A. Garcia', 'Zuobin Zhang']","X-ray reflection spectroscopy is a powerful tool to study the strong gravity region of black holes. The next generation of astrophysical X-ray missions promises to provide unprecedented high-quality data, which could permit us to get very precise measurements of the properties of the accretion flow and of the spacetime geometry in the strong gravity region around these objects. In this work, we test the accuracy of the relativistic calculations of the reflection model relxill and of its extension to non-Kerr spacetimes relxill_nk in view of the next generation of X-ray missions. We simulate simultaneous observations with Athena/X-IFU and LAD of bright Galactic black holes with a precise and accurate ray-tracing code and we fit the simulated data with the latest versions of relline and relline_nk. While we always recover the correct input parameters, we find residuals in the fits when the emission from the inner part of the accretion disk is higher. Such residuals disappear if we increase the number of interpolation points on the disk in the integral of the transfer function. We also simulate full reflection spectra and find that the emission angle from the accretion disk should be treated properly in this case.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00349
Spectra of $Λ$ and $Σ$ Baryons under Screened Potential,['High Energy Physics - Phenomenology'],"['Chandni Menapara', 'Ajay Kumar Rai']","The light, strange baryons have been studied through various approaches and attempted to be looked for rigorously in experiments. The screened potential has been applied to heavy baryon sector as well as meson systems in earlier works. Here, this article attempts to compare the results for linear and screened potential for light strange baryons. Also, the Regge trajectories depict the linear nature.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00344
Nonlinearity and Uncertainty Informed Moment-Matching Gaussian Mixture Splitting,['Machine Learning'],"['Jackson Kulik', 'Keith A. LeGrand']","Many problems in navigation and tracking require increasingly accurate characterizations of the evolution of uncertainty in nonlinear systems. Nonlinear uncertainty propagation approaches based on Gaussian mixture density approximations offer distinct advantages over sampling based methods in their computational cost and continuous representation. State-of-the-art Gaussian mixture approaches are adaptive in that individual Gaussian mixands are selectively split into mixtures to yield better approximations of the true propagated distribution. Despite the importance of the splitting process to accuracy and computational efficiency, relatively little work has been devoted to mixand selection and splitting direction optimization. The first part of this work presents splitting methods that preserve the mean and covariance of the original distribution. Then, we present and compare a number of novel heuristics for selecting the splitting direction. The choice of splitting direction is informed by the initial uncertainty distribution, properties of the nonlinear function through which the original distribution is propagated, and a whitening based natural scaling method to avoid dependence of the splitting direction on the scaling of coordinates. We compare these novel heuristics to existing techniques in three distinct examples involving Cartesian to polar coordinate transformation, Keplerian orbital element propagation, and uncertainty propagation in the circular restricted three-body problem.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00343
Fusing Physics-Driven Strategies and Cross-Modal Adversarial Learning: Toward Multi-Domain Applications,['Computer Vision and Pattern Recognition'],"['Hana Satou', 'Alan Mitkiy']","The convergence of cross-modal adversarial learning and physics-driven methods represents a cutting-edge direction for tackling challenges in complex multi-modal tasks and scientific computing. This review focuses on systematically analyzing how these two approaches can be synergistically integrated to enhance performance and robustness across diverse application domains. By addressing key obstacles such as modality discrepancies, limited data availability, and insufficient model robustness, this paper highlights the role of physics-based optimization frameworks in facilitating efficient and interpretable adversarial perturbation generation. The review also explores significant advancements in cross-modal adversarial learning, including applications in tasks such as image cross-modal retrieval (e.g., infrared and RGB matching), scientific computing (e.g., solving partial differential equations), and optimization under physical consistency constraints in vision systems. By examining theoretical foundations and experimental outcomes, this study demonstrates the potential of combining these approaches to handle complex scenarios and improve the security of multi-modal systems. Finally, we outline future directions, proposing a novel framework that unifies physical principles with adversarial optimization, providing a pathway for researchers to develop robust and adaptable cross-modal learning methods with both theoretical and practical significance.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00341
Pattern avoidance in nonnesting permutations,['Combinatorics'],"['Sergi Elizalde', 'Amya Luo']","Nonnesting permutations are permutations of the multiset $\{1,1,2,2,\dots,n,n\}$ that avoid subsequences of the form $abba$ for any $a\neq b$. These permutations have recently been studied in connection to noncrossing (also called quasi-Stirling) permutations, which are those that avoid subsequences of the form $abab$, and in turn generalize the well-known Stirling permutations. Inspired by the work by Archer et al. on pattern avoidance in noncrossing permutations, we consider the analogous problem in the nonnesting case. We enumerate nonnesting permutations that avoid each set of two or more patterns of length 3, as well as those that avoid some sets of patterns of length 4. We obtain closed formulas and generating functions, some of which involve unexpected appearances of the Catalan and Fibonacci numbers. Our proofs rely on decompositions, recurrences, and bijections.△ Less",v1,https://arxiv.org/pdf/2412.00336
Differentiable High-Order Markov Models for Spectrum Prediction,['Signal Processing'],"['Vincent Corlay', 'Tatsuya Nakazato', 'Kanako Yamaguchi', 'Akinori Nakajima']","The advent of deep learning and recurrent neural networks revolutionized the field of time-series processing. Therefore, recent research on spectrum prediction has focused on the use of these tools. However, spectrum prediction, which involves forecasting wireless spectrum availability, is an older field where many ""classical"" tools were considered around the 2010s, such as Markov models. This work revisits high-order Markov models for spectrum prediction in dynamic wireless environments. We introduce a framework to address mismatches between sensing length and model order as well as state-space complexity arising with large order. Furthermore, we extend this Markov framework by enabling fine-tuning of the probability transition matrix through gradient-based supervised learning, offering a hybrid approach that bridges probabilistic modeling and modern machine learning. Simulations on real-world Wi-Fi traffic demonstrate the competitive performance of high-order Markov models compared to deep learning methods, particularly in scenarios with constrained datasets containing outliers.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00328
MusicGen-Chord: Advancing Music Generation through Chord Progressions and Interactive Web-UI,['Sound'],"['Jongmin Jung', 'Andreas Jansson', 'Dasaem Jeong']","MusicGen is a music generation language model (LM) that can be conditioned on textual descriptions and melodic features. We introduce MusicGen-Chord, which extends this capability by incorporating chord progression features. This model modifies one-hot encoded melody chroma vectors into multi-hot encoded chord chroma vectors, enabling the generation of music that reflects both chord progressions and textual descriptions. Furthermore, we developed MusicGen-Remixer, an application utilizing MusicGen-Chord to generate remixes of input music conditioned on textual descriptions. Both models are integrated into Replicate's web-UI using cog, facilitating broad accessibility and user-friendly controllable interaction for creating and experiencing AI-generated music.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00325
Methodology for constraining ultralight vector bosons with gravitational wave searches targeting merger remnant black holes,['General Relativity and Quantum Cosmology'],"['Dana Jones', 'Nils Siemonsen', 'Ling Sun', 'William E. East', 'Andrew L. Miller', 'Karl Wette', 'Ornella J. Piccinni']","Ultralight bosons are a hypothetical class of particles predicted under various extensions of Standard Model physics. As a result of the superradiance mechanism, we expect ultralight bosons, should they exist in certain mass ranges, to form macroscopic clouds around rotating black holes, so that we can probe their existence by looking for the long-transient gravitational wave emission produced by such clouds. In this paper, we propose a statistically robust framework for constraining the existence of ultralight vector bosons in the absence of detecting such a signal from searches targeting merger remnant black holes, effectively marginalizing over the uncertainties present in the properties of the target black holes. We also determine the impact of weak kinetic mixing with the ordinary photon and vector mass generation through a hidden Higgs mechanism on the constraining power of these searches. We find that individual follow-up searches, particularly with the next-generation gravitational wave detectors, can probe regions of parameter space for such models where robust constraints are still lacking.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00320
Improving speaker verification robustness with synthetic emotional utterances,['Sound'],"['Nikhil Kumar Koditala', 'Chelsea Jui-Ting Ju', 'Ruirui Li', 'Minho Jin', 'Aman Chadha', 'Andreas Stolcke']","A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states.
  To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00319
Joint Coverage and Electromagnetic Field Exposure Analysis in Downlink and Uplink for RIS-assisted Networks,['Signal Processing'],"['Lin Chen', 'Ahmed Elzanaty', 'Mustafa A Kishk', 'Ying-Jun Angela Zhang']","Reconfigurable intelligent surfaces (RISs) have shown the potential to improve signal-to-interference-plus-noise ratio (SINR) related coverage, especially at high-frequency communications. However, assessing electromagnetic filed exposure (EMFE) and establishing EMFE regulations in RIS-assisted large-scale networks are still open issues. This paper proposes a framework to characterize SINR and EMFE in such networks for downlink and uplink scenarios. Particularly, we carefully consider the association rule with the presence of RISs, accurate antenna pattern at base stations (BSs), fading model, and power control mechanism at mobile devices in the system model. Under the proposed framework, we derive the marginal and joint distributions of SINR and EMFE in downlink and uplink, respectively. The first moment of EMFE is also provided. Additionally, we design the compliance distance (CD) between a BS/RIS and a user to comply with the EMFE regulations. To facilitate efficient identification, we further provide approximate closed-form expressions for CDs. From numerical results of the marginal distributions, we find that in the downlink scenario, deploying RISs may not always be beneficial, as the improved SINR comes at the cost of increased EMFE. However, in the uplink scenario, RIS deployment is promising to enhance coverage while still maintaining EMFE compliance. By simultaneously evaluating coverage and compliance metrics through joint distributions, we demonstrate the feasibility of RISs in improving uplink and downlink performance. Insights from this framework can contribute to establishing EMFE guidelines and achieving a balance between coverage and compliance when deploying RISs.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00317
BOTS: Batch Bayesian Optimization of Extended Thompson Sampling for Severely Episode-Limited RL Settings,['Machine Learning'],"['Karine Karine', 'Susan A. Murphy', 'Benjamin M. Marlin']","In settings where the application of reinforcement learning (RL) requires running real-world trials, including the optimization of adaptive health interventions, the number of episodes available for learning can be severely limited due to cost or time constraints. In this setting, the bias-variance trade-off of contextual bandit methods can be significantly better than that of more complex full RL methods. However, Thompson sampling bandits are limited to selecting actions based on distributions of immediate rewards. In this paper, we extend the linear Thompson sampling bandit to select actions based on a state-action utility function consisting of the Thompson sampler's estimate of the expected immediate reward combined with an action bias term. We use batch Bayesian optimization over episodes to learn the action bias terms with the goal of maximizing the expected return of the extended Thompson sampler. The proposed approach is able to learn optimal policies for a strictly broader class of Markov decision processes (MDPs) than standard Thompson sampling. Using an adaptive intervention simulation environment that captures key aspects of behavioral dynamics, we show that the proposed method can significantly out-perform standard Thompson sampling in terms of total return, while requiring significantly fewer episodes than standard value function and policy gradient methods.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00308
Spacetime-curvature induced uncertainty principle: linking the large-structure global effects to the local black hole physics,['General Relativity and Quantum Cosmology'],"['Reggie C. Pantig', 'Gaetano Lambiase', 'Ali Övgün', 'Nikko John Leo S. Lobos']","This paper links the advanced formulation of the Generalized Uncertainty Principle, termed the Asymptotic Generalized Extended Uncertainty Principle (AGEUP), to the corpuscular framework to derive the AGEUP-inspired black hole metric. The former incorporates spacetime curvature effects to explore black hole dynamics under quantum gravitational corrections, while the latter is a view that black holes are Bose-Einstein condensates of weakly interacting gravitons. In a particular case, the phenomenological union between the AGEUP with cosmological constant $Λ$ to the corpuscular framework enabled a black hole metric that has a scaled mass, which depends on $Λ$ and the Planck length $l_{\rm Pl}$. Interesting implications occur, such as the maximum limit for mass $M$ where $Λ$ ceases to influence the black hole. Another is the derived value of the modulation factor of the EUP term, $α$, if the large-scale fundamental length is defined solely as the cosmological horizon. Additional analysis were done through the shadow and deflection angle phenomena, deriving constraints on the quantum gravity modulation parameter $β$. Constraints from the Event Horizon Telescope (EHT) and Very Long Baseline Interferometry (VLBI) are discussed as avenues for verifying AGEUP-related deviations in black hole shadow radius and deflection angles, offering potential observational evidence of quantum gravitational effects at astrophysical scales. The findings suggest that AGEUP could be instrumental in providing hints on the quantum gravity nature of black holes, particularly in high-energy astrophysical contexts. By linking local black hole physics with large-scale curvature effects, AGEUP paves the way for further research at the intersection of quantum gravity and cosmology, with implications for observational astrophysics and the fundamental structure of spacetime.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00303
Improved calculation of the Young's modulus of rectangular prisms from their resonant frequency overtones by identifying appropriate shear constants,['Geophysics'],"['Paul A. Bosomworth', 'Rui Zhang', 'Lawrence M. Anovitz']","Young's modulus is an important parameter for characterizing the strength of, and wave propagation through, a given material. This study improves the estimation of Young's modulus using the impulse excitation (IE) technique based on an experimental analysis of 19 borosilicate glass bars. Analysis of the frequency equations relating Young's modulus to the out of plane and in plane flexural resonant frequencies of rectangular prisms has been conducted for both the fundamental frequency and its overtones at higher orders of vibration. The Young's modulus of three novaculite rocks with various porosities were then measured up to the seventh order of vibration to validate the optimum shear constant equation for estimating Young's modulus. Young's modulus was found to be nearly frequency independent for these rock samples.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00299
Adaptformer: Sequence models as adaptive iterative planners,['Robotics'],"['Akash Karthikeyan', 'Yash Vardhan Pant']","Despite recent advances in learning-based behavioral planning for autonomous systems, decision-making in multi-task missions remains a challenging problem. For instance, a mission might require a robot to explore an unknown environment, locate the goals, and navigate to them, even if there are obstacles along the way. Such problems are difficult to solve due to: a) sparse rewards, meaning a reward signal is available only once all the tasks in a mission have been satisfied, and b) the agent having to perform tasks at run-time that are not covered in the training data, e.g., demonstrations only from an environment where all doors were unlocked. Consequently, state-of-the-art decision-making methods in such settings are limited to missions where the required tasks are well-represented in the training demonstrations and can be solved within a short planning horizon. To overcome these limitations, we propose Adaptformer, a stochastic and adaptive planner that utilizes sequence models for sample-efficient exploration and exploitation. This framework relies on learning an energy-based heuristic, which needs to be minimized over a sequence of high-level decisions. To generate successful action sequences for long-horizon missions, Adaptformer aims to achieve shorter sub-goals, which are proposed through an intrinsic sub-goal curriculum. Through these two key components, Adaptformer allows for generalization to out-of-distribution tasks and environments, i.e., missions that were not a part of the training data. Empirical results in multiple simulation environments demonstrate the effectiveness of our method. Notably, Adaptformer not only outperforms the state-of-the-art method by up to 25% in multi-goal maze reachability tasks but also successfully adapts to multi-task missions that the state-of-the-art method could not complete, leveraging demonstrations from single-goal-reaching tasks.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00293
Adapting the re-ID challenge for static sensors,['Computer Vision and Pattern Recognition'],"['Avirath Sundaresan', 'Jason R. Parham', 'Jonathan Crall', 'Rosemary Warungu', 'Timothy Muthami', 'Margaret Mwangi', 'Jackson Miliko', 'Jason Holmberg', 'Tanya Y. Berger-Wolf', 'Daniel Rubenstein', 'Charles V. Stewart', 'Sara Beery']","In both 2016 and 2018, a census of the highly-endangered Grevy's zebra population was enabled by the Great Grevy's Rally (GGR), a citizen science event that produces population estimates via expert and algorithmic curation of volunteer-captured images. A complementary, scalable, and long-term Grevy's population monitoring approach involves deploying camera trap networks. However, in both scenarios, a substantial majority of zebra images are not usable for individual identification due to poor in-the-wild imaging conditions; camera trap images in particular present high rates of occlusion and high spatio-temporal similarity within image bursts. Our proposed filtering pipeline incorporates animal detection, species identification, viewpoint estimation, quality evaluation, and temporal subsampling to obtain individual crops suitable for re-ID, which are subsequently curated by the LCA decision management algorithm. Our method processed images taken during GGR-16 and GGR-18 in Meru County, Kenya, into 4,142 highly-comparable annotations, requiring only 120 contrastive human decisions to produce a population estimate within 4.6% of the ground-truth count. Our method also efficiently processed 8.9M unlabeled camera trap images from 70 cameras at the Mpala Research Centre in Laikipia County, Kenya over two years into 685 encounters of 173 individuals, requiring only 331 contrastive human decisions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00290
Optimizing Quantum Embedding using Genetic Algorithm for QML Applications,['Quantum Physics'],"['Koustubh Phalak', 'Archisman Ghosh', 'Swaroop Ghosh']","Quantum Embeddings (QE) are essential for loading classical data into quantum systems for Quantum Machine Learning (QML). The performance of QML algorithms depends on the type of QE and how features are mapped to qubits. Traditionally, the optimal embedding is found through optimization, but we propose framing it as a search problem instead. In this work, we use a Genetic Algorithm (GA) to search for the best feature-to-qubit mapping. Experiments on the MNIST and Tiny ImageNet datasets show that GA outperforms random feature-to-qubit mappings, achieving 0.33-3.33 (MNIST) and 0.5-3.36 (Tiny ImageNet) higher fitness scores, with up to 15% (MNIST) and 8.8% (Tiny ImageNet) reduced runtime. The GA approach is scalable with both dataset size and qubit count. Compared to existing methods like Quantum Embedding Kernel (QEK), QAOA-based embedding, and QRAC, GA shows improvements of 1.003X, 1.03X, and 1.06X, respectively.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00286
Optimal local storage policy based on stochastic intensities and its large scale behavior,['Probability'],"['Matias Carrasco', 'Andres Ferragut', 'Fernando Paganini']","In this paper, we analyze the optimal management of local memory systems, using the tools of stationary point processes. We provide a rigorous setting of the problem, building upon recent work, and characterize the optimal causal policy that maximizes the hit probability. We specialize the result for the case of renewal request processes and derive a suitable large scale limit as the catalog size N grows to infinity, when a fixed fraction c of items can be stored. We prove that in the limiting regime, the optimal policy amounts to comparing the stochastic intensity (observed hazard rate) of the process with a fixed threshold, defined by a quantile of an appropriate limit distribution, and derive asymptotic performance metrics, as well as sharp estimates for the pre-limit case. Moreover, we establish a connection with optimal timer based policies for the case of monotonic hazard rates. We also present detailed validation examples of our results, including some close form expressions for the miss probability that are compared to simulations. We also use these examples to exhibit the significant superiority of the optimal policy for the case of regular traffic patterns.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00279
Construction of observable and MDP convolutional codes with good decodable properties for erasure channels by I/S/O representations,['Information Theory'],"['Noemí DeCastro-García', 'Miguel V. Carriegos', 'Ángel Luis Muñoz Castañeda']",This paper addresses the construction of observable convolutional codes that exhibit good performance with the available decoding algorithms for erasure channels. Our construction is based on the use of input/state/output (I/S/O) representations and the invariance of certain properties of linear systems under various group actions.△ Less,"29 November, 2024;",https://arxiv.org/pdf/2412.00274
Friedel oscillations and chiral superconductivity in monolayer NbSe$_2$,['Superconductivity'],"['Julian Siegl', 'Anton Bleibaum', 'Wen Wan', 'Marcin Kurpas', 'John Schliemann', 'Miguel M. Ugeda', 'Magdalena Marganska', 'Milena Grifoni']","In 1965 Kohn and Luttinger proposed a genuine electronic mechanism for superconductivity. Despite the bare electrostatic interaction between two electrons being repulsive, in a metal electron-hole fluctuations can give rise to Friedel oscillations of the screened Coulomb potential. Cooper pairing among the electrons then emerges when taking advantage of the attractive regions. The nature of the leading pairing mechanism in some two-dimensional transition metal dichalcogenides is still debated. Focusing on NbSe$_2$, we show that superconductivity can be induced by the Coulomb interaction when accounting for screening effects on the trigonal lattice with multiple orbitals. Using ab initio-based tight-binding parametrizations for the relevant low-energy d-bands, we evaluate the screened interaction microscopically, in a scheme that includes Bloch overlaps and Umklapp processes. In the direct space, we find long-range Friedel oscillations which alternate in sign. The momentum-resolved gap equations predict two quasi-degenerate nematic solutions near the critical temperature $T_c$, signaling the unconventional nature of the pairing. Their complex linear combination, i.e., a chiral gap with p-like symmetry, provides the ground state of the system. Our prediction of a fully gapped chiral phase well below $T_c$ is in agreement with the spectral function extracted from tunneling spectroscopy measurements of single-layer NbSe$_2$.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00273
Micro-cavity length stabilization for fluorescence enhancement using schemes based on higher order spatial modes,['Optics'],"['A. Shehata Abdelatief', 'A. J. Renders', 'M. Alqedra', 'J. J. Hansen', 'D. Hunger', 'L. Rippe', 'A. Walther']","We report on experimental investigation of potential high-performance cavity length stabilization using odd-indexed higher-order spatial modes. Schemes based on higher-order modes are particularly useful for micro-cavities that are used for enhanced fluorescence detection of a few emitters, which need to minimize photons leaking from a stabilization beam. We describe the design and construction of an assembly for a microcavity setup with tunable high passive stability. In addition, different types of active stabilization techniques based on higher-order modes, are then implemented and characterized based on their performance. We achieved a stability of about 0.5 pm RMS, while the error photons leaking from the continuous locking beam to a fluorescence detector are suppressed by more than 100-fold. We expect these results to be important for quantum technology implementations of various emitter-cavity setups, where these techniques provide a useful tool to meet the highly challenging demands.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00271
High Magnitude Earthquake Identification Using an Anomaly Detection Approach on HR GNSS Data,['Geophysics'],"['Javier Quintero Arenas', 'Claudia Quinteros Cartaya', 'Andrea Padilla Lafarga', 'Carlos Moraila', 'Johannes Faber', 'Jonas Koehler', 'Nishtha Srivastava']","Earthquake early warning systems are crucial for protecting areas that are subject to these natural disasters. An essential part of these systems is the detection procedure. Traditionally these systems work with seismograph data, but high rate GNSS data has become a promising alternative for the usage in large earthquake early warning systems. Besides traditional methods, deep learning approaches have gained recent popularity in this field, as they are able to leverage the large amounts of real and synthetic seismic data. Nevertheless, the usage of deep learning on GNSS data remains a comparatively new topic. This work contributes to the field of early warning systems by proposing an autoencoder based deep learning pipeline that aims to be lightweight and customizable for the detection of anomalies viz. high magnitude earthquakes in GNSS data. This model, DetEQ, is trained using the noise data recordings from nine stations located in Chile. The detection pipeline encompasses: (i) the generation of an anomaly score using the ground truth and reconstructed output from the autoencoder, (ii) the detection of relevant seismic events through an appropriate threshold, and (iii) the filtering of local events, that would lead to false positives. Robustness of the model was tested on the HR GNSS real data of 2011 Mw 6.8 Concepcion earthquake recorded at six stations. The results highlight the potential of GNSS based deep learning models for effective earthquake detection.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00264
Recursive Formulas for MacMahon and Ramanujan $q$-series,['Number Theory'],"['Tewodros Amdeberhan', 'Rupam Barman', 'Ajit Singh']","In the present work, we extend current research in a nearly-forgotten but newly revived topic, initiated by P. A. MacMahon, on a generalized notion which relates the divisor sums to the theory of integer partitions and two infinite families of $q$-series by Ramanujan. Our main emphasis will be on explicit representations for a variety of $q$-series, studied primarily by MacMahon and Ramanujan, with an eye towards their modular properties and their proper place in the ring of quasimodular forms of level one and level two.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00262
Attribute-Enhanced Similarity Ranking for Sparse Link Prediction,['Machine Learning'],"['João Mattos', 'Zexi Huang', 'Mert Kosan', 'Ambuj Singh', 'Arlei Silva']","Link prediction is a fundamental problem in graph data. In its most realistic setting, the problem consists of predicting missing or future links between random pairs of nodes from the set of disconnected pairs. Graph Neural Networks (GNNs) have become the predominant framework for link prediction. GNN-based methods treat link prediction as a binary classification problem and handle the extreme class imbalance -- real graphs are very sparse -- by sampling (uniformly at random) a balanced number of disconnected pairs not only for training but also for evaluation. However, we show that the reported performance of GNNs for link prediction in the balanced setting does not translate to the more realistic imbalanced setting and that simpler topology-based approaches are often better at handling sparsity. These findings motivate Gelato, a similarity-based link-prediction method that applies (1) graph learning based on node attributes to enhance a topological heuristic, (2) a ranking loss for addressing class imbalance, and (3) a negative sampling scheme that efficiently selects hard training pairs via graph partitioning. Experiments show that Gelato outperforms existing GNN-based alternatives.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00261
Real-to-Sim via End-to-End Differentiable Simulation and Rendering,['Robotics'],"['Yifan Zhu', 'Tianyi Xiang', 'Aaron Dollar', 'Zherong Pan']","Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable simulators to identify world models are incapable of jointly optimizing the shape, appearance, and physical properties of the scene. In this work, we introduce a novel object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based object representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of benchmarking system identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only a few partial observations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00259
Predicting Non-Ideal Effects from the Diaphragm Opening Process in Shock Tubes,['Fluid Dynamics'],"['Janardhanraj Subburaj', 'Miguel Figueroa-Labastida', 'Aamir Farooq']","Shock tubes are instrumental in studying high-temperature kinetics and simulating high-speed flows. They swiftly elevate the thermodynamic conditions of test gases, making them ideal for examining rapid chemical reactions and generating high-enthalpy flows for aerodynamic research. However, non-ideal effects, stemming from factors like diaphragm opening processes and viscous effects, can significantly influence thermodynamic conditions behind the shock wave. This study investigates the impact of various diaphragm opening patterns on the shock parameters near the driven section end-wall. Experiments were conducted using helium and argon as driver and driven gases, respectively, at pressures ranging from 1.32 to 2.09 bar and temperatures from 1073 to 2126 K behind the reflected shock. High-speed imaging captured different diaphragm rupture profiles, classified into four distinct types based on their dynamics. Results indicate that the initial stages of diaphragm opening, including the rate and profile of opening, play crucial roles in resulting incident shock Mach number and test time. A sigmoid function was employed to fit the diaphragm opening profiles, allowing for accurate categorization and analysis. New correlations were developed to predict the incident shock attenuation rate and post-shock pressure rise, incorporating parameters such as diaphragm opening time, rupture profile constants, and normalized experimental Mach number. The results emphasize the importance of considering diaphragm rupture dynamics in shock tube experiments to achieve accurate predictions of shock parameters.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00257
Excretion Detection in Pigsties Using Convolutional and Transformerbased Deep Neural Networks,['Computer Vision and Pattern Recognition'],"['Simon Mielke', 'Anthony Stein']","Animal excretions in form of urine puddles and feces are a significant source of emissions in livestock farming. Automated detection of soiled floor in barns can contribute to improved management processes but also the derived information can be used to model emission dynamics. Previous research approaches to determine the puddle area require manual detection of the puddle in the barn. While humans can detect animal excretions on thermal images of a livestock barn, automated approaches using thresholds fail due to other objects of the same temperature, such as the animals themselves. In addition, various parameters such as the type of housing, animal species, age, sex, weather and unknown factors can influence the type and shape of excretions. Due to this heterogeneity, a method for automated detection of excretions must therefore be not only be accurate but also robust to varying conditions. These requirements can be met by using contemporary deep learning models from the field of artificial intelligence. This work is the first to investigate the suitability of different deep learning models for the detection of excretions in pigsties, thereby comparing established convolutional architectures with recent transformer-based approaches. The detection models Faster R-CNN, YOLOv8, DETR and DAB-DETR are compared and statistically assessed on two created training datasets representing two pig houses. We apply a method derived from nested cross-validation and report on the results in terms of eight common detection metrics. Our work demonstrates that all investigated deep learning models are generally suitable for reliably detecting excretions with an average precision of over 90%. The models also show robustness on out of distribution data that possesses differences from the conditions in the training data, however, with expected slight decreases in the overall detection performance.△ Less",v1,https://arxiv.org/pdf/2412.00256
The canonical lamination calibrated by a cohomology class,['Differential Geometry'],['Aidan Backus'],"Let $M$ be a closed oriented Riemannian manifold of dimension $d$, and let $ρ\in H^{d - 1}(M, \mathbb R)$ have unit norm. We construct a lamination $λ_ρ$ whose leaves are exactly the minimal hypersurfaces which are calibrated by every calibration in $ρ$. The geometry of $λ_ρ$ is closely related to the the geometry of the unit ball of the stable norm on $H_{d - 1}(M, \mathbb R)$, and so we deduce several results constraining the geometry of the stable norm ball in terms of the topology of $M$. These results establish a close analogy between the stable norm on $H_{d - 1}(M, \mathbb R)$ and the earthquake norm on the tangent space to Teichmüller space.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00255
EF2X Exists For Four Agents,['Computer Science and Game Theory'],"['Arash Ashuri', 'Vasilis Gkatzelis', 'Alkmini Sgouritsa']","We study the fair allocation of indivisible goods among a group of agents, aiming to limit the envy between any two agents. The central open problem in this literature, which has proven to be extremely challenging, is regarding the existence of an EFX allocation, i.e., an allocation such that any envy from some agent i toward another agent j would vanish if we were to remove any single good from the bundle allocated to j. When the agents' valuations are additive, which has been the main focus of prior works, Chaudhury et al. [2024] showed that an EFX allocation is guaranteed to exist for all instances involving up to three agents. Subsequently, Berger et al. [2022] extended this guarantee to nice-cancelable valuations and Akrami et al. [2023] to MMS-feasible valuations. However, the existence of EFX allocations for instances involving four agents remains open, even for additive valuations. We contribute to this literature by focusing on EF2X, a relaxation of EFX which requires that any envy toward some agent vanishes if any two of the goods allocated to that agent were to be removed. Our main result shows that EF2X allocations are guaranteed to exist for any instance with four agents, even for the class of cancelable valuations, which is more general than additive. Our proof is constructive, proposing an algorithm that computes such an allocation in pseudopolynomial time. Furthermore, for instances involving three agents we provide an algorithm that computes an EF2X allocation in polynomial time, in contrast to EFX, for which the fastest known algorithm for three agents is only pseudopolynomial.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00254
Surface chemistry-based continuous separation of polystyrene particles in a microchannel via diffusiophoresis and diffusioosmosis,['Soft Condensed Matter'],"['Adnan Chakra', 'Christina Puijk', 'Goran T. Vladisavljević', 'Cécile Cottin-Bizonne', 'Christophe Pirat', 'Guido Bolognesi']","The separation of colloidal particles is of great importance in many fields, such as chemistry, medicine, and chemical engineering. However, separating particles based on their surface physico-chemical properties remains challenging. Diffusiophoresis and diffusioosmosis have been used for the continuous separation of colloids based on particle size and zeta potential. Here, we demonstrate that diffusiophoresis and diffusioosmosis in electrolyte streams enable the separation of carboxylate polystyrene particles with similar sizes and zeta potentials but distinct surface concentrations of carboxyl groups. A double junction microfluidic device, fed with low and high electrolyte concentration streams, was used to create steady-state salt concentration gradients. As the streams mix along the channel, the particles are exposed to environments with varying salinity levels, and their dynamics are influenced by the sensitivity of their zeta potential to the local salt concentration. Typically, for carboxylate polystyrene particles, the magnitude of the electrophoretically measured zeta potential decreases with increasing salt concentrations. However, surface conductance effects, which depend on the surface concentration of the carboxyl groups, may lead to an opposite sensitivity of the zeta potential to salt. Therefore, under selected salt conditions, similarly sized polystyrene colloids with comparable zeta potentials in the low concentration stream, but different surface concentrations of carboxyl groups, can be separated in a continuous flow process when they exhibit opposite zeta potential sensitivities to salt. This mechanism has discipline-spanning potential for the continuous separation of colloids distinguished solely by surface physico-chemical properties that influence their zeta potential sensitivities, like roughness, permeability, heterogeneity, and chemical composition.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00246
Uni-SLAM: Uncertainty-Aware Neural Implicit SLAM for Real-Time Dense Indoor Scene Reconstruction,['Computer Vision and Pattern Recognition'],"['Shaoxiang Wang', 'Yaxu Xie', 'Chun-Peng Chang', 'Christen Millerdurai', 'Alain Pagani', 'Didier Stricker']","Neural implicit fields have recently emerged as a powerful representation method for multi-view surface reconstruction due to their simplicity and state-of-the-art performance. However, reconstructing thin structures of indoor scenes while ensuring real-time performance remains a challenge for dense visual SLAM systems. Previous methods do not consider varying quality of input RGB-D data and employ fixed-frequency mapping process to reconstruct the scene, which could result in the loss of valuable information in some frames. In this paper, we propose Uni-SLAM, a decoupled 3D spatial representation based on hash grids for indoor reconstruction. We introduce a novel defined predictive uncertainty to reweight the loss function, along with strategic local-to-global bundle adjustment. Experiments on synthetic and real-world datasets demonstrate that our system achieves state-of-the-art tracking and mapping accuracy while maintaining real-time performance. It significantly improves over current methods with a 25% reduction in depth L1 error and a 66.86% completion rate within 1 cm on the Replica dataset, reflecting a more accurate reconstruction of thin structures. Project page: https://shaoxiang777.github.io/project/uni-slam/△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00242
Anisotropic Hardy type inequalities with weights and conformable fractional differential operators,['Analysis of PDEs'],"['Abimbola Abolarinwa', 'Yisa O Anthony']","By a systematic development of fundamental concepts of conformable calculus we establish conformable divergence theorem and Green's identities which we combine with some new anisotropic Picone type identities to derive a generalized anisotropic Hardy type inequality with weights and conformable fractional differential operators. As a consequence, several Hardy type inequalities and Heisenberg Pauli-Weyl uncertainty principles are obtained.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00240
Hybrid Spiking Neural Network -- Transformer Video Classification Model,['Computer Vision and Pattern Recognition'],['Aaron Bateni'],"In recent years, Spiking Neural Networks (SNNs) have gathered significant interest due to their temporal understanding capabilities. This work introduces, to the best of our knowledge, the first Cortical Column like hybrid architecture for the Time-Series Data Classification Task that leverages SNNs and is inspired by the brain structure, inspired from the previous hybrid models. We introduce several encoding methods to use with this model. Finally, we develop a procedure for training this network on the training dataset. As an effort to make using these models simpler, we make all the implementations available to the public.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00237
Spectral Efficiency of Low Earth Orbit Satellite Constellations,['Signal Processing'],"['Cuneyd Ozturk', 'Dongning Guo', 'Randall A. Berry', 'Michael L. Honig']","This paper investigates the maximum downlink spectral efficiency of low earth orbit (LEO) constellations. Spectral efficiency, in this context, refers to the sum rate of the entire network per unit spectrum per unit area on the earth's surface. For practicality all links employ single-user codebooks and treat interference as noise. To estimate the maximum achievable spectral efficiency, we propose and analyze a regular configuration, which deploys satellites and ground terminals in hexagonal lattices. Additionally, for wideband networks with arbitrary satellite configurations, we introduce a subband allocation algorithm aimed at maximizing the overall spectral efficiency. Simulation results indicate that the regular configuration is more efficient than random configurations. As the number of randomly placed satellites increases within an area, the subband allocation algorithm achieves a spectral efficiency that approaches the spectral efficiency achieved by the regular configuration. Further improvements are demonstrated by reconfiguring associations so that nearby transmitters avoid pointing to the same area.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00235
Impact of propagation effects on the spectro-temporal properties of Fast Radio Bursts,['High Energy Astrophysical Phenomena'],"['Aishwarya Kumar', 'Fereshteh Rajabi', 'Martin Houde']","We present a mathematical analysis of the spectro-temporal properties of Fast Radio Bursts (FRBs), focusing on the distortions introduced by propagation effects such as scattering and inaccurate de-dispersion. By examining the impact of different scattering timescales and residual dispersion measures (DMs), both independently and in combination, we identify systematic trends in the sub-burst slope law as defined within the framework of the Triggered Relativistic Dynamical Model (TRDM). These effects primarily alter the measurements of the sub-burst slope and duration, thereby also modifying their correlations with other properties, such as central frequency and bandwidth. Our results show that scatter-induced temporal broadening affects duration more than slope, with weak to moderate scattering subtly modifying the sub-burst slope law and strong scattering causing significant deviations. Residual dispersion preferentially modifies the slope, further changing the trends predicted by the sub-burst slope law. Ultra-short bursts (or ultra-FRBs) emerge as particularly susceptible to these effects even at relatively high frequencies, underscoring the need for precise treatment of scattering and accurate dedispersion before performing analyses. Our findings emphasize the necessity for higher frequency observations (especially for ultra-FRBs) to improve the DM estimates as well as the measurements of spectro-temporal properties.△ Less",v1,https://arxiv.org/pdf/2412.00232
MATTER: Multi-stage Adaptive Thermal Trojan for Efficiency & Resilience degradation,['Cryptography and Security'],"['Mehdi Elahi', 'Mohamed R. Elshamy', 'Abdel-Hameed Badawy', 'Mahdi Fazeli', 'Ahmad Patooghy']","As mobile systems become more advanced, the security of System-on-Chips (SoCs) is increasingly threatened by thermal attacks. This research introduces a new attack method called the Multi-stage Adaptive Thermal Trojan for Efficiency and Resilience Degradation (MATTER). MATTER takes advantage of weaknesses in Dynamic Thermal Management (DTM) systems by manipulating temperature sensor interfaces, which leads to incorrect thermal sensing and disrupts the SoC's ability to manage heat effectively. Our experiments show that this attack can degrade DTM performance by as much as 73%, highlighting serious vulnerabilities in modern mobile devices. By exploiting the trust placed in temperature sensors, MATTER causes DTM systems to make poor decisions i.e., failing to activate cooling when needed. This not only affects how well the system works but also threatens the lifespan of the hardware. This paper provides a thorough analysis of how MATTER works and emphasizes the need for stronger thermal management systems in SoCs.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00226
An AI-Driven Data Mesh Architecture Enhancing Decision-Making in Infrastructure Construction and Public Procurement,['Artificial Intelligence'],"['Saurabh Mishra', 'Mahendra Shinde', 'Aniket Yadav', 'Bilal Ayyub', 'Anand Rao']","Infrastructure construction, often dubbed an ""industry of industries,"" is closely linked with government spending and public procurement, offering significant opportunities for improved efficiency and productivity through better transparency and information access. By leveraging these opportunities, we can achieve notable gains in productivity, cost savings, and broader economic benefits. Our approach introduces an integrated software ecosystem utilizing Data Mesh and Service Mesh architectures. This system includes the largest training dataset for infrastructure and procurement, encompassing over 100 billion tokens, scientific publications, activities, and risk data, all structured by a systematic AI framework. Supported by a Knowledge Graph linked to domain-specific multi-agent tasks and Q&A capabilities, our platform standardizes and ingests diverse data sources, transforming them into structured knowledge. Leveraging large language models (LLMs) and automation, our system revolutionizes data structuring and knowledge creation, aiding decision-making in early-stage project planning, detailed research, market trend analysis, and qualitative assessments. Its web-scalable architecture delivers domain-curated information, enabling AI agents to facilitate reasoning and manage uncertainties, while preparing for future expansions with specialized agents targeting particular challenges. This integration of AI with domain expertise not only boosts efficiency and decision-making in construction and infrastructure but also establishes a framework for enhancing government efficiency and accelerating the transition of traditional industries to digital workflows. This work is poised to significantly influence AI-driven initiatives in this sector and guide best practices in AI Operations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00224
Algorithms for Parameterized String Matching with Mismatches,['Data Structures and Algorithms'],"['Apurba Saha', 'Iftekhar Hakim Kaowsar', 'Mahdi Hasnat Siyam', 'M. Sohel Rahman']","Two strings are considered to have parameterized matching when there exists a bijection of the parameterized alphabet onto itself such that it transforms one string to another. Parameterized matching has application in software duplication detection, image processing, and computational biology. We consider the problem for which a pattern $p$, a text $t$ and a mismatch tolerance limit $k$ is given and the goal is to find all positions in text $t$, for which pattern $p$, parameterized matches with $|p|$ length substrings of $t$ with at most $k$ mismatches. Our main result is an algorithm for this problem with $O(α^2 n\log n + n α^2 \sqrtα \log \left( n α\right))$ time complexity, where $n = |t|$ and $α= |Σ|$ which is improving for $k=\tildeΩ(|Σ|^{5/3})$ the algorithm by Hazay, Lewenstein and Sokol. We also present a hashing based probabilistic algorithm for this problem when $k = 1$ with $O \left( n \log n \right)$ time complexity, which we believe is algorithmically beautiful.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00222
The Bernstein-von Mises theorem for Semiparametric Mixtures,['Statistics Theory'],"['Stefan Franssen', 'Jeanne Nguyen', 'Aad van der Vaart']","Semiparametric mixture models are parametric models with latent variables. They are defined kernel, $p_θ(x | z)$, where z is the unknown latent variable, and $θ$ is the parameter of interest. We assume that the latent variables are an i.i.d. sample from some mixing distribution $F$. A Bayesian would put a prior on the pair $(θ, F)$. We prove consistency for these models in fair generality and then study efficiency. We first prove an abstract Semiparametric Bernstein-von Mises theorem, and then provide tools to verify the assumptions. We use these tools to study the efficiency for estimating $θ$ in the frailty model and the errors in variables model in the case were we put a generic prior on $θ$ and a species sampling process prior on $F$.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00219
Digital Twin in Industries: A Comprehensive Survey,['Artificial Intelligence'],"['Md Bokhtiar Al Zami', 'Shaba Shaon', 'Vu Khanh Quy', 'Dinh C. Nguyen']","Industrial networks are undergoing rapid transformation driven by the convergence of emerging technologies that are revolutionizing conventional workflows, enhancing operational efficiency, and fundamentally redefining the industrial landscape across diverse sectors. Amidst this revolution, Digital Twin (DT) emerges as a transformative innovation that seamlessly integrates real-world systems with their virtual counterparts, bridging the physical and digital realms. In this article, we present a comprehensive survey of the emerging DT-enabled services and applications across industries, beginning with an overview of DT fundamentals and its components to a discussion of key enabling technologies for DT. Different from literature works, we investigate and analyze the capabilities of DT across a wide range of industrial services, including data sharing, data offloading, integrated sensing and communication, content caching, resource allocation, wireless networking, and metaverse. In particular, we present an in-depth technical discussion of the roles of DT in industrial applications across various domains, including manufacturing, healthcare, transportation, energy, agriculture, space, oil and gas, as well as robotics. Throughout the technical analysis, we delve into real-time data communications between physical and virtual platforms to enable industrial DT networking. Subsequently, we extensively explore and analyze a wide range of major privacy and security issues in DT-based industry. Taxonomy tables and the key research findings from the survey are also given, emphasizing important insights into the significance of DT in industries. Finally, we point out future research directions to spur further research in this promising area.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00209
Supercurrent flow in inhomogeneous superconductors,['Superconductivity'],"['Mikhail A. Skvortsov', 'Oleg B. Zuev', 'Dina I. Fazlizhanova']","We study how the supercurrent flow pattern is altered by inhomogeneities in superconducting films. Working in the vicinity of the critical temperature and assuming a model of short-range disorder in the quadratic term of the Ginzburg-Landau functional, we develop a perturbation theory in the inhomogeneity strength. Absorbing the ultraviolet divergences into the renormalization of the critical temperature, we arrive at a well-defined theory governed by large-scale physics. In the presence of inhomogeneities, the correlation functions of the order parameter and supercurrent exhibit a long-range power-law behavior, which can be attributed to the mixing of the amplitude and phase modes. The fluctuation magnitude grows with increasing the average current, and the system becomes strongly inhomogeneous near the critical current.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00203
Scaling of Stochastic Normalizing Flows in $\mathrm{SU}(3)$ lattice gauge theory,['High Energy Physics - Lattice'],"['Andrea Bulgarelli', 'Elia Cellini', 'Alessandro Nada']","Non-equilibrium Markov Chain Monte Carlo (NE-MCMC) simulations provide a well-understood framework based on Jarzynski's equality to sample from a target probability distribution. By driving a base probability distribution out of equilibrium, observables are computed without the need to thermalize. If the base distribution is characterized by mild autocorrelations, this approach provides a way to mitigate critical slowing down. Out-of-equilibrium evolutions share the same framework of flow-based approaches and they can be naturally combined into a novel architecture called Stochastic Normalizing Flows (SNFs). In this work we present the first implementation of SNFs for $\mathrm{SU}(3)$ lattice gauge theory in 4 dimensions, defined by introducing gauge-equivariant layers between out-of-equilibrium Monte Carlo updates. The core of our analysis is focused on the promising scaling properties of this architecture with the degrees of freedom of the system, which are directly inherited from NE-MCMC. Finally, we discuss how systematic improvements of this approach can realistically lead to a general and yet efficient sampling strategy at fine lattice spacings for observables affected by long autocorrelation times.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00200
"Chemical Evolution of R-process Elements in Stars (CERES): IV. An observational run-up of the third r-process peak with Hf, Os, Ir, and Pt",['Solar and Stellar Astrophysics'],"['Arthur Alencastro Puls', 'Jan Kuske', 'Camilla Juul Hansen', 'Linda Lombardo', 'Giorgio Visentin', 'Almudena Arcones', 'Raphaela Fernandes de Melo', 'Moritz Reichert', 'Piercarlo Bonifacio', 'Elisabetta Caffau', 'Stephan Fritzsche']","The third r-process peak (Os, Ir, Pt) is poorly understood due to observational challenges, with spectral lines located in the blue or near-ultraviolet region of stellar spectra. These challenges need to be overcome for a better understanding of the r-process in a broader context. To understand how the abundances of the third r-process peak are synthesised and evolve in the Universe, a homogeneous chemical analysis of metal-poor stars using high quality data observed in the blue region of the electromagnetic spectrum (< 400 nm) is necessary. We provide a homogeneous set of abundances for the third r-process peak (Os, Ir, Pt) and Hf, increasing by up to one order of magnitude their availability in the literature. A classical 1D, local thermodynamic equilibrium (LTE) analysis of four elements (Hf, Os, Ir, Pt) is performed, using ATLAS model atmospheres to fit synthetic spectra in high resolution (> 40,000), high signal-to-noise ratio, of 52 red giants observed with UVES/VLT. Due to the heavy line blending involved, a careful determination of upper limits and uncertainties is done. The observational results are compared with state-of-the-art nucleosynthesis models. Our sample displays larger abundances of Ir (Z=77) in comparison to Os (Z=76), which have been measured in a few stars in the past. The results also suggest decoupling between abundances of third r-process peak elements with respect to Eu (rare earth element) in Eu-poor stars. This seems to contradict a co-production scenario of Eu and the third r-process peak elements Os, Ir, and Pt in the progenitors of these objects. Our results are challenging to explain from the nucleosynthetic point of view: the observationally derived abundances indicate the need for an additional early, primary formation channel (or a non-robust r-process).△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00195
Spacetime Markov length: a diagnostic for fault tolerance via mixed-state phases,['Quantum Physics'],"['Amir-Reza Negari', 'Tyler D. Ellison', 'Timothy H. Hsieh']","We establish a correspondence between the fault-tolerance of local stabilizer codes experiencing measurement and physical errors and the mixed-state phases of decohered resource states in one higher dimension. Drawing from recent developments in mixed-state phases of matter, this motivates a diagnostic of fault-tolerance, which we refer to as the spacetime Markov length. This is a length scale determined by the decay of the (classical) conditional mutual information of repeated syndrome measurement outcomes in spacetime. The diagnostic is independent of the decoder, and its divergence signals the intrinsic breakdown of fault tolerance. As a byproduct, we find that decoherence may be useful for exposing transitions from higher-form symmetry-protected topological phases driven by both incoherent and coherent perturbations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00193
Renyi entropy of single-character CFTs on the torus,['High Energy Physics - Theory'],"['Luis Alberto Leon Andonayre', 'Rahul Poddar']","We introduce a non-perturbative approach to calculate the Rényi entropy of a single interval on the torus for single-character (meromorphic) conformal field theories. Our prescription uses the Wrońskian method of Mathur, Mukhi and Sen, in which we construct differential equations for torus conformal blocks of the twist two-point function. As an illustrative example, we provide a detailed calculation of the second Rényi entropy for the $\rm E_{8,1}$ WZW model. We find that the $\mathbb Z_2$ cyclic orbifold of a meromorphic CFT results in a four-character CFT which realizes the toric code modular tensor category. We show that the $\mathbb Z_2$ cyclic orbifold of the $\rm E_{8,1}$ WZW model yields a three-character CFT since two of the characters coincide. We find that the second Rényi entropy for the $\rm E_{8,1}$ WZW model has the universal logarithmic divergent behaviour in the decompactification limit of the torus as expected. Furthermore, we see that the $q$-expansion is UV finite, apart from the leading universal logarithmic divergence.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00192
Euclid preparation: TBD. The impact of line-of-sight projections on the covariance between galaxy cluster multi-wavelength observable properties -- insights from hydrodynamic simulations,['Cosmology and Nongalactic Astrophysics'],"['Euclid Collaboration', 'A. Ragagnin', 'A. Saro', 'S. Andreon', 'A. Biviano', 'K. Dolag', 'S. Ettori', 'C. Giocoli', 'A. M. C. Le Brun', 'G. A. Mamon', 'B. J. Maughan', 'M. Meneghetti', 'L. Moscardini', 'F. Pacaud', 'G. W. Pratt', 'M. Sereno', 'S. Borgani', 'F. Calura', 'G. Castignani', 'M. De Petris', 'D. Eckert', 'G. F. Lesci', 'J. Macias-Perez', 'M. Maturi', 'A. Amara']","Cluster cosmology can benefit from combining multi-wavelength studies, which can benefit from characterising the correlation coefficients between different mass-observable relations. In this work, we aim to provide information on the scatter, the skewness, and the covariance of various mass-observable relations in galaxy clusters in cosmological hydrodynamic simulations. This information will help future analyses to better tackle accretion histories and projection effects and model mass observable relations for cosmology studies.We identify galaxy clusters in Magneticum Box2b simulations with mass $M_{\rm 200c}>10^{14} {\rm M}_\odot$ at redshift $z=0.24$ and $z=0.90$. Our analysis includes \Euclid-derived properties such as richness, stellar mass, lensing mass, and concentration. Additionally, we investigate complementary multi-wavelength data, including X-ray luminosity, integrated Compton-$y$ parameter, gas mass, and temperature. The impact of projection effects on mass-observable residuals and correlations is then examined. At intermediate redshift ($z=0.24$), projection effects impact lensing concentration, richness, and gas mass the most in terms of scatter and skewness of log-residuals of scaling relations. The contribution of projection effects can be significant enough to boost a spurious hot- vs. cold-baryons correlation and consequently hide underlying correlations due to halo accretion histories.
  At high redshift ($z=0.9$), the richness has a much lower scatter (of log-residuals), and the quantity that is most impacted by projection effects is the lensing mass.
  Lensing concentration reconstruction, in particular, is affected by deviations of the reduced-shear profile shape from the one derived by an NFW profile rather than interlopers in the line of sight.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00191
Star formation rate surface density main sequence evolution. Insights from a semi-analytic simulation since $z=12$,['Astrophysics of Galaxies'],"['Jakub Nadolny', 'Michał J. Michałowski', 'Massimiliano Parente', 'Martín Solar', 'Przemysław Nowaczyk', 'Oleh Ryzhov', 'Aleksandra Leśniewska']","Recent high redshift ($z>4$) spatially resolved observations with JWST have shown the evolution of star formation rate (SFR) surface density ($Σ_{\rm SFR}$) and its main sequence in the $Σ_{\rm SFR}$-$M_*$ diagram ($Σ_{\rm SFR}$MS) which is already observed at the cosmic morning ($z\sim7.5$). The use of $Σ_{\rm SFR}$ is physically motivated due to its normalization by the area where the star formation (SF) occurs, thus indirectly considering the gas density. The $Σ_{\rm SFR}$-$M_*$ diagram has been shown to complement widely used (specific)SFR-$M_*$, particularly to select passive galaxies. In this work we aim to establish the $Σ_{\rm SFR}$ evolution since $z=12$ in the framework of LGalaxies2020 semi-analytical model (SAM) and to give an interpretation to recent observations. We estimate $Σ_{\rm SFR}$(-$M_*$) and cosmic star formation rate density (CSFRD) for the simulated galaxy population and the sub-samples divided in stellar mass bins in the given redshift. The simulated $Σ_{\rm SFR}$ decreases by $\sim3.5$ dex from $z=12$ to $z=0$. We show that galaxies with different stellar masses have different paths of $Σ_{\rm SFR}$ evolution. %, driven by SFR and effective radius. We find $Σ_{\rm SFR}$MS is already observed at $z\sim11$. The simulated $Σ_{\rm SFR}$MS agrees with the observed one at $z=0, 1, 2, 5$ and $7.5$ and with individual galaxies at $z>10$. We show that the highest $Σ_{\rm SFR}$MS slope of $0.709\pm0.005$ is at $z\sim3$, decreasing to $\sim0.085\pm0.003$ at $z=0$. This is mostly driven by a rapid decrease in SFR with additional size increase of the most massive galaxies in this redshift range. This coincides with the dominance of the most massive galaxies in the CSFRD from the SAM. Observations show the same picture in which $Σ_{\rm SFR}$ evolutionary path depends on the stellar mass.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00188
Bootstrapping leading hadronic muon anomaly,['High Energy Physics - Theory'],['Ahmadullah Zahed'],"We bootstrap the leading order hadronic contribution to muon anomalous magnetic moment. The leading hadronic contribution comes from the hadronic vacuum polarization function (HVP). We explore the bootstrap constraints, namely unitarity, analyticity, crossing symmetry and finite energy sum rules (FESR) from quantum chromodynamics (QCD). The unitarity appears as a positive semi-definite condition among the pion partial waves, form factor and spectral density function of HVP, which establishes a lower bound on leading order hadronic contribution to muon anomalous magnetic moment. We also impose chiral symmetry breaking to improve the bound slightly. By combining the lower bound with the remaining extensively calculated contributions, we achieve a bound on anomalous magnetic moment $a_μ^\text{bootstrap-min}=11659176.3^{+3}_{-3}\times 10^{-10}$ and standard model prediction saturates this bound within the error bars. We also present a possible improvement that is saturated by both lattice computation and measured value within the error bars.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00187
Interactive Multimodal Integral Field Spectroscopy,['Instrumentation and Methods for Astrophysics'],"['Adrián García Riber', 'Rubén García-Benito', 'Francisco Serradilla']","Using sonification on scientific data analysis provides additional dimensions to visualization, potentially increasing researchers' analytical capabilities and fostering inclusion and accessibility. This research explores the potential of multimodal Integral Field Spectroscopy (IFS) applied to galaxy analysis through the development and evaluation of a tool that complements the visualization of datacubes with sound. The proposed application, \textit{ViewCube}, provides interactive visualizations and sonifications of spectral information across a two-dimensional field-of-view, and its architecture is designed to incorporate future sonification approaches. The first sonification implementation described in this article uses a deep learning module to generate binaural unsupervised auditory representations. The work includes a qualitative and quantitative user study based on an online questionnaire, aimed at both specialized and non-specialized participants, focusing on the case study of datacubes of galaxies from the Calar Alto Integral Field Spectroscopy Area (CALIFA) survey. Out of 67 participants who completed the questionnaire, 42 had the opportunity to test the application in person prior to filling out the online survey. 81\% of these 42 participants expressed the good interactive response of the tool, 79.1\% of the complete sample found the application ""Useful"", and 58.2\% rated its aesthetics as ""Good"". The quantitative results suggest that all participants were able to retrieve information from the sonifications, pointing to previous experience in the analysis of sound events as more helpful than previous knowledge of the data for the proposed tasks, and highlighting the importance of training and attention to detail for the understanding of complex auditory information.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00185
Extra dimensional axion patterns,['High Energy Physics - Phenomenology'],"['Arturo de Giorgi', 'Maria Ramos']","We study for the first time the $\textit{complete}$ parameter space of a bulk extra-dimensional axion. We find novel regimes where no single KK mode is produced along the canonical QCD axion line, and instead, it is maximally deviated along with several other axions that constitute a multiple solution to the strong CP problem. In the most common extra-dimensional models, namely for flat and curved Randall-Sundrum scenarios, and assuming that all Peccei-Quinn breaking comes from QCD, we find that these solutions are however subject to tight phenomenological constraints. In light of these results, only one -- canonical -- pattern can be expected from a bulk axion in one or more extra spacetime dimensions. As a byproduct, we generalize the axions eigenvalue and eigenvector equations for an arbitrary number of spacetime dimensions and compactifications.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00179
LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting,['Computer Vision and Pattern Recognition'],"['Xiaoyan Xing', 'Konrad Groh', 'Sezer Karaoglu', 'Theo Gevers', 'Anand Bhattad']","We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.
  Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.△ Less",v1,https://arxiv.org/pdf/2412.00177
Art-Free Generative Models: Art Creation Without Graphic Art Knowledge,['Computer Vision and Pattern Recognition'],"['Hui Ren', 'Joanna Materzynska', 'Rohit Gandikota', 'David Bau', 'Antonio Torralba']","We explore the question: ""How much prior art knowledge is needed to create art?"" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00176
Ehrenfest dynamics with localized atomic-orbital basis sets within the projector augmented-wave method,['Materials Science'],"['Vladimír Zobač', 'Mikael Kuisma', 'Ask Hjorth Larsen', 'Tuomas Rossi', 'Toma Susi']","Density functional theory with linear combination of atomic orbitals (LCAO) basis sets is useful for studying large atomic systems, especially when it comes to computationally highly demanding time-dependent dynamics. We have implemented the Ehrenfest molecular dynamics (ED) method with the approximate approach of Tomfohr and Sankey within the projector augmented-wave code GPAW. We apply this method to small molecules as well as larger periodic systems, and elucidate its limits, advantages, and disadvantages in comparison to the existing implementation of Ehrenfest dynamics with a real-space grid representation. For modest atomic velocities, LCAO-ED shows satisfactory accuracy at a much reduced computational cost. This method will be particularly useful for modeling ion irradiation processes that require large amounts of vacuum in the simulation cell.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00168
To Ensemble or Not: Assessing Majority Voting Strategies for Phishing Detection with Large Language Models,['Computation and Language'],"['Fouad Trad', 'Ali Chehab']","The effectiveness of Large Language Models (LLMs) significantly relies on the quality of the prompts they receive. However, even when processing identical prompts, LLMs can yield varying outcomes due to differences in their training processes. To leverage the collective intelligence of multiple LLMs and enhance their performance, this study investigates three majority voting strategies for text classification, focusing on phishing URL detection. The strategies are: (1) a prompt-based ensemble, which utilizes majority voting across the responses generated by a single LLM to various prompts; (2) a model-based ensemble, which entails aggregating responses from multiple LLMs to a single prompt; and (3) a hybrid ensemble, which combines the two methods by sending different prompts to multiple LLMs and then aggregating their responses. Our analysis shows that ensemble strategies are most suited in cases where individual components exhibit equivalent performance levels. However, when there is a significant discrepancy in individual performance, the effectiveness of the ensemble method may not exceed that of the highest-performing single LLM or prompt. In such instances, opting for ensemble techniques is not recommended.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00166
How reproducible are data-driven subtypes of Alzheimer's disease atrophy?,['Quantitative Methods'],"['Emma Prevot', 'Cameron Shand', 'Neil Oxtoby', ""for Alzheimer's Disease Neuroimaging Initiative""]","Alzheimer's disease (AD) exhibits substantial clinical and biological heterogeneity, complicating efforts in treatment and intervention development. While new computational methods offer insights into AD progression, the reproducibility of these subtypes across datasets remains understudied, particularly concerning the robustness of subtype definitions when validated on diverse databases. This study evaluates the consistency of AD progression subtypes identified by the Subtype and Stage Inference (SuStaIn) algorithm using T1-weighted MRI data across 5,444 subjects from ANMerge, OASIS, and ADNI datasets, forming four independent cohorts. Each cohort was analyzed under two conditions: one using the full cohort, including cognitively normal controls, and another excluding controls to test subtype robustness. Results confirm the three primary atrophy subtypes identified in earlier studies: Typical, Cortical, and Subcortical, as well as the emergence of rare and atypical AD variants such as posterior cortical atrophy (PCA). Notably, each subtype displayed varying robustness to the inclusion of controls, with certain subtypes, like Subcortical, more influenced by cohort composition. This investigation underscores SuStaIn's reliability for defining stable AD subtypes and suggests its utility in clinical stratification for trials and diagnosis. However, our findings also highlight the need for improved dataset diversity, particularly in terms of ethnic representation, to enhance generalizability and support broader clinical application.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00160
Four years of wide-field search for nanosecond optical transients with the TAIGA-HiSCORE Cherenkov array,['Instrumentation and Methods for Astrophysics'],"['A. D. Panov', 'I. I. Astapov', 'G. M. Beskin', 'P. A. Bezyazykov', 'A. V. Blinov', 'E. A. Bonvech', 'A. N. Borodin', 'N. M. Budnev', 'A. V. Bulan', 'P. Busygina D. V. Chernov', 'A. Chiavassa', 'A. N. Dyachok', 'A. R. Gafarov', 'A. Yu. Garmash', 'V. M. Grebenyuk', 'E. O. Gress', 'O. A. Gress', 'T. I. Gress', 'A. A. Grinyuk', 'O. G. Grishin', 'A. L. Ivanova', 'A. D. Ivanova', 'M. A. Ilyushin', 'N. N. Kalmykov', 'V. V. Kindin']","It has been previously demonstrated [Panov et al. Physics of Atomic Nuclei 84(2021)1037] that the TAIGA-HiSCORE Cherenkov array, originally built for cosmic ray physics and ultrahigh-energy gamma-ray astronomy studies using the extensive air shower method, can be used in conventional optical astronomy for wide-field searches for rare nanosecond optical transients of astrophysical origin. The FOV of the facility is on the scale of 1~ster, and it is capable of detecting very rare transients in the visible light range with fluxes greater than approximately 3000~quanta/m$^2$/10~ns (10~ns is the apparatus integration time) and pulse durations of 10\,ns. Among the potential sources of distant nanosecond optical transients are the evaporation of primary black holes, magnetic reconnection in the accretion disks of black holes, and signals from distant lasers of extraterrestrial civilizations. The paper describes the methods and results of the search for optical transients using the TAIGA-HiSCORE Cherenkov array from 2018 to 2022 (four winter seasons of data collection). No reliable astrophysical candidates for optical transients were found. We set an upper bound on the flux of the searched events as $\sim 1\times10^{-3}$\,events/ster/h.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00159
T-3DGS: Removing Transient Objects for 3D Scene Reconstruction,['Computer Vision and Pattern Recognition'],"['Vadim Pryadilshchikov', 'Alexander Markin', 'Artem Komarichev', 'Ruslan Rakhimov', 'Peter Wonka', 'Evgeny Burnaev']","We propose a novel framework to remove transient objects from input videos for 3D scene reconstruction using Gaussian Splatting. Our framework consists of the following steps. In the first step, we propose an unsupervised training strategy for a classification network to distinguish between transient objects and static scene parts based on their different training behavior inside the 3D Gaussian Splatting reconstruction. In the second step, we improve the boundary quality and stability of the detected transients by combining our results from the first step with an off-the-shelf segmentation method. We also propose a simple and effective strategy to track objects in the input video forward and backward in time. Our results show an improvement over the current state of the art in existing sparsely captured datasets and significant improvements in a newly proposed densely captured (video) dataset. More results and code are available at https://transient-3dgs.github.io.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00155
DLaVA: Document Language and Vision Assistant for Answer Localization with Enhanced Interpretability and Trustworthiness,['Computer Vision and Pattern Recognition'],"['Ahmad Mohammadshirazi', 'Pinaki Prasad Guha Neogi', 'Ser-Nam Lim', 'Rajiv Ramnath']","Document Visual Question Answering (VQA) requires models to interpret textual information within complex visual layouts and comprehend spatial relationships to answer questions based on document images. Existing approaches often lack interpretability and fail to precisely localize answers within the document, hindering users' ability to verify responses and understand the reasoning process. Moreover, standard metrics like Average Normalized Levenshtein Similarity (ANLS) focus on text accuracy but overlook spatial correctness. We introduce DLaVA, a novel method that enhances Multimodal Large Language Models (MLLMs) with answer localization capabilities for Document VQA. Our approach integrates image annotation directly into the MLLM pipeline, improving interpretability by enabling users to trace the model's reasoning. We present both OCR-dependent and OCR-free architectures, with the OCR-free approach eliminating the need for separate text recognition components, thus reducing complexity. To the best of our knowledge, DLaVA is the first approach to introduce answer localization within multimodal QA, marking a significant step forward in enhancing user trust and reducing the risk of AI hallucinations. Our contributions include enhancing interpretability and reliability by grounding responses in spatially annotated visual content, introducing answer localization in MLLMs, proposing a streamlined pipeline that combines an MLLM with a text detection module, and conducting comprehensive evaluations using both textual and spatial accuracy metrics, including Intersection over Union (IoU). Experimental results on standard datasets demonstrate that DLaVA achieves SOTA performance, significantly enhancing model transparency and reliability. Our approach sets a new benchmark for Document VQA, highlighting the critical importance of precise answer localization and model interpretability.△ Less","29 November, 2024;",https://arxiv.org/pdf/2412.00151
LPCVD Grown Si-Doped $β$-Ga$_2$O$_3$ Films with Promising Electron Mobilities,['Materials Science'],"['Saleh Ahmed Khan', 'Ahmed Ibreljic', 'Stephen Margiotta', 'A F M Anhar Uddin Bhuiyan']","We systematically investigated the growth of Si-doped $β$-Ga$_2$O$_3$ films using LPCVD system, achieving high electron mobilities of 162 cm$^2$/V.s and 149 cm$^2$/V.s at carrier concentrations of $1.51 \times 10^{17}$ cm$^{-3}$ and $1.15 \times 10^{17}$ cm$^{-3}$, respectively, for homoepitaxial (010) $β$-Ga$_2$O$_3$ films grown on $β$-Ga$_2$O$_3$ substrates and heteroepitaxial (-201) $β$-Ga$_2$O$_3$ films grown on off-axis c-sapphire substrates with 6° miscut, representing the highest mobilities reported for LPCVD-grown $β$-Ga$_2$O$_3$ materials. Carrier concentrations were precisely tuned by varying SiCl$_4$ flow rates at a growth temperature of 1000°C, resulting in concentrations ranging from $1.15 \times 10^{17}$ to $1.19 \times 10^{19}$ cm$^{-3}$, as confirmed by both Hall and C-V measurements. The films exhibited high crystalline quality, confirmed by high-resolution XRD and Raman spectroscopy, indicating phase purity and structural integrity. Surface morphologies characterized by FESEM and AFM imaging showed a strong correlation between carrier concentrations and surface smoothness, with lower concentrations resulting in reduced RMS roughness. SIMS analysis revealed uniform Si incorporation, with low carbon, hydrogen, and chlorine impurities below detection limits, indicating high purity of the films. A high low-temperature peak mobility exceeding 843 cm$^2$/V$\cdot$s was achieved for (-201) $β$-Ga$_2$O$_3$ films at 80 K, highlighting the high purity and low compensation of these films. These findings emphasize the potential of LPCVD growth system for producing high-purity $β$-Ga$_2$O$_3$ films with thickness ranging between ~2.3-11.7 $μ$m and faster growth rates (~4.7-17 $μ$m/hr), promising transport properties, controllable doping, and scalability for developing high power vertical devices.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.00149
Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers,['Computer Vision and Pattern Recognition'],"['Chancharik Mitra', 'Brandon Huang', 'Tianning Chai', 'Zhiqiu Lin', 'Assaf Arbelle', 'Rogerio Feris', 'Leonid Karlinsky', 'Trevor Darrell', 'Deva Ramanan', 'Roei Herzig']","Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for foundational discriminative vision-language tasks (i.e., tasks requiring discrete label predictions) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for discriminative tasks is the extraction of useful features from generative models. To overcome this issue, we propose an approach for finding features in the model's latent space to more effectively leverage LMMs for discriminative tasks. Toward this end, we present Sparse Attention Vectors (SAVs) -- a finetuning-free method that leverages sparse attention head activations (fewer than 1\% of the heads) in LMMs as strong features for VL tasks. With only few-shot examples, SAVs demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of discriminative tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.00142
Exposure of Track Detectors in Xenon Ion Beams in NICA Accelerator Complex,['Instrumentation and Detectors'],"['A. A. Zaitsev', 'P. I. Zarubin', 'S. D. Murashko', 'N. Marimuthu', 'A. A. Slivin', 'G. A. Filatov']",The results of the analysis of solid-state track detectors CR39 and nuclear photoemulsion plates irradiated in beams of accelerated xenon ions with energies of 3.2 MeV/n and 3.8 GeV/n at the NICA accelerator complex are presented.△ Less,"28 November, 2024;",https://arxiv.org/pdf/2412.00141
Generators of Local Lorentz Transformation in ADM-Vielbein Formalism of Gravitational Relativity,['General Relativity and Quantum Cosmology'],"['Alireza Faraji', 'Zahra Molaee', 'Ahmad Shirzad']","General relativity contains 16 variables in the framework of ADM-Vielbein formalism which are 6 more than metric formalism. These variables emerge due to additional symmetry of Local Lorentz Transformations. In the framework of the Hamiltonian approach, it is expected to find first class constraints which generate this gauge symmetry. We introduce the complete form of such constraints and show that they exactly obey the algebra of the Lorentz group.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.00128
Electricity Price Prediction Using Multi-Kernel Gaussian Process Regression combined with Kernel-Based Support Vector Regression,['Machine Learning'],"['Abhinav Das', 'Stephan Schlüter', 'Lorenz Schneider']","This paper presents a new hybrid model for predicting German electricity prices. The algorithm is based on combining Gaussian Process Regression (GPR) and Support Vector Regression (SVR). While GPR is a competent model for learning the stochastic pattern within the data and interpolation, its performance for out-of-sample data is not very promising. By choosing a suitable data-dependent covariance function, we can enhance the performance of GPR for the tested German hourly power prices. However, since the out-of-sample prediction depends on the training data, the prediction is vulnerable to noise and outliers. To overcome this issue, a separate prediction is made using SVR, which applies margin-based optimization, having an advantage in dealing with non-linear processes and outliers, since only certain necessary points (support vectors) in the training data are responsible for regression. Both individual predictions are later combined using the performance-based weight assignment method. A test on historic German power prices shows that this approach outperforms its chosen benchmarks such as the autoregressive exogenous model, the naive approach, as well as the long short-term memory approach of prediction.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.00123
Boundary Control Behaviors of Multiple Low-cost AUVs Using Acoustic Communication,['Robotics'],"['Mohammed Tarnini', 'Saverio Iacoponi', 'Andrea Infanti', 'Cesare Stefanini', 'Giulia De Masi', 'Federico Renda']","This study presents acoustic-based methods for the control of multiple autonomous underwater vehicles (AUV). This study proposes two different models for implementing boundary and path control on low-cost AUVs using acoustic communication and a single central acoustic beacon. Two methods are presented: the Range Variation-Based (RVB) model completely relies on range data obtained by acoustic modems, whereas the Heading Estimation-Based (HEB) model uses ranges and range rates to estimate the position of the central boundary beacon and perform assigned behaviors. The models are tested on two boundary control behaviors: Fencing and Milling. Fencing behavior ensures AUVs return within predefined boundaries, while Milling enables the AUVs to move cyclically on a predefined path around the beacon. Models are validated by successfully performing the boundary control behaviors in simulations, pool tests, including artificial underwater currents, and field tests conducted in the ocean. All tests were performed with fully autonomous platforms, and no external input or sensor was provided to the AUVs during validation. Quantitative and qualitative analyses are presented in the study, focusing on the effect and application of a multi-robot system.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.00118
"$q$-Whittaker polynomials: bases, branching and direct limits",['Combinatorics'],"['Aritra Bhattacharya', 'T V Ratheesh', 'Sankaran Viswanath']","We study $q$-Whittaker polynomials and their monomial expansions given by the fermionic formula, the inv statistic of Haglund-Haiman-Loehr and the quinv statistic of Ayyer-Mandelshtam-Martin. The combinatorial models underlying these expansions are partition overlaid patterns and column strict fillings. The former model is closely tied to representations of the affine Lie algebra $\widehat{\mathfrak{sl}_n}$ and admits projections, branching maps and direct limits that mirror these structures in the Chari-Loktev basis of local Weyl modules. We formulate novel versions of these notions in the column strict fillings model and establish their main properties. We construct weight-preserving bijections between the models which are compatible with projection, branching and direct limits. We also establish connections to the coloured lattice paths formalism for $q$-Whittaker polynomials due to Wheeler and collaborators.△ Less","28 November, 2024;",https://arxiv.org/pdf/2412.00116
Demographic Predictability in 3D CT Foundation Embeddings,['Computer Vision and Pattern Recognition'],"['Guangyao Zheng', 'Michael A. Jacobs', 'Vishwa S. Parekh']","Self-supervised foundation models have recently been successfully extended to encode three-dimensional (3D) computed tomography (CT) images, with excellent performance across several downstream tasks, such as intracranial hemorrhage detection and lung cancer risk forecasting. However, as self-supervised models learn from complex data distributions, questions arise concerning whether these embeddings capture demographic information, such as age, sex, or race. Using the National Lung Screening Trial (NLST) dataset, which contains 3D CT images and demographic data, we evaluated a range of classifiers: softmax regression, linear regression, linear support vector machine, random forest, and decision tree, to predict sex, race, and age of the patients in the images. Our results indicate that the embeddings effectively encoded age and sex information, with a linear regression model achieving a root mean square error (RMSE) of 3.8 years for age prediction and a softmax regression model attaining an AUC of 0.998 for sex classification. Race prediction was less effective, with an AUC of 0.878. These findings suggest a detailed exploration into the information encoded in self-supervised learning frameworks is needed to help ensure fair, responsible, and patient privacy-protected healthcare AI.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.00110
"Predicting Extubation Failure in Intensive Care: The Development of a Novel, End-to-End Actionable and Interpretable Prediction System",['Machine Learning'],['Akram Yoosoofsah'],"Predicting extubation failure in intensive care is challenging due to complex data and the severe consequences of inaccurate predictions. Machine learning shows promise in improving clinical decision-making but often fails to account for temporal patient trajectories and model interpretability, highlighting the need for innovative solutions.
  This study aimed to develop an actionable, interpretable prediction system for extubation failure using temporal modelling approaches such as Long Short-Term Memory (LSTM) and Temporal Convolutional Networks (TCN). A retrospective cohort study of 4,701 mechanically ventilated patients from the MIMIC-IV database was conducted. Data from the 6 hours before extubation, including static and dynamic features, were processed through novel techniques addressing data inconsistency and synthetic data challenges. Feature selection was guided by clinical relevance and literature benchmarks.
  Iterative experimentation involved training LSTM, TCN, and LightGBM models. Initial results showed a strong bias toward predicting extubation success, despite advanced hyperparameter tuning and static data inclusion. Data was stratified by sampling frequency to reduce synthetic data impacts, leading to a fused decision system with improved performance. However, all architectures yielded modest predictive power (AUC-ROC ~0.6; F1 <0.5) with no clear advantage in incorporating static data or additional features. Ablation analysis indicated minimal impact of individual features on model performance.
  This thesis highlights the challenges of synthetic data in extubation failure prediction and introduces strategies to mitigate bias, including clinician-informed preprocessing and novel feature subsetting. While performance was limited, the study provides a foundation for future work, emphasising the need for reliable, interpretable models to optimise ICU outcomes.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.00105
Differential learning kinetics govern the transition from memorization to generalization during in-context learning,['Machine Learning'],"['Alex Nguyen', 'Gautam Reddy']","Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative rates at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.00104
MLLM-Search: A Zero-Shot Approach to Finding People using Multimodal Large Language Models,['Robotics'],"['Angus Fung', 'Aaron Hao Tan', 'Haitong Wang', 'Beno Benhabib', 'Goldie Nejat']","Robotic search of people in human-centered environments, including healthcare settings, is challenging as autonomous robots need to locate people without complete or any prior knowledge of their schedules, plans or locations. Furthermore, robots need to be able to adapt to real-time events that can influence a person's plan in an environment. In this paper, we present MLLM-Search, a novel zero-shot person search architecture that leverages multimodal large language models (MLLM) to address the mobile robot problem of searching for a person under event-driven scenarios with varying user schedules. Our approach introduces a novel visual prompting method to provide robots with spatial understanding of the environment by generating a spatially grounded waypoint map, representing navigable waypoints by a topological graph and regions by semantic labels. This is incorporated into a MLLM with a region planner that selects the next search region based on the semantic relevance to the search scenario, and a waypoint planner which generates a search path by considering the semantically relevant objects and the local spatial context through our unique spatial chain-of-thought prompting approach. Extensive 3D photorealistic experiments were conducted to validate the performance of MLLM-Search in searching for a person with a changing schedule in different environments. An ablation study was also conducted to validate the main design choices of MLLM-Search. Furthermore, a comparison study with state-of-the art search methods demonstrated that MLLM-Search outperforms existing methods with respect to search efficiency. Real-world experiments with a mobile robot in a multi-room floor of a building showed that MLLM-Search was able to generalize to finding a person in a new unseen environment.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.00103
Multi-Label Contrastive Learning : A Comprehensive Study,['Machine Learning'],"['Alexandre Audibert', 'Aurélien Gauffre', 'Massih-Reza Amini']","Multi-label classification, which involves assigning multiple labels to a single input, has emerged as a key area in both research and industry due to its wide-ranging applications. Designing effective loss functions is crucial for optimizing deep neural networks for this task, as they significantly influence model performance and efficiency. Traditional loss functions, which often maximize likelihood under the assumption of label independence, may struggle to capture complex label relationships. Recent research has turned to supervised contrastive learning, a method that aims to create a structured representation space by bringing similar instances closer together and pushing dissimilar ones apart. Although contrastive learning offers a promising approach, applying it to multi-label classification presents unique challenges, particularly in managing label interactions and data structure.
  In this paper, we conduct an in-depth study of contrastive learning loss for multi-label classification across diverse settings. These include datasets with both small and large numbers of labels, datasets with varying amounts of training data, and applications in both computer vision and natural language processing.
  Our empirical results indicate that the promising outcomes of contrastive learning are attributable not only to the consideration of label interactions but also to the robust optimization scheme of the contrastive loss. Furthermore, while the supervised contrastive loss function faces challenges with datasets containing a small number of labels and ranking-based metrics, it demonstrates excellent performance, particularly in terms of Macro-F1, on datasets with a large number of labels.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.00101
Mixture of Cache-Conditional Experts for Efficient Mobile Device Inference,['Machine Learning'],"['Andrii Skliar', 'Ties van Rozendaal', 'Romain Lepert', 'Todor Boinovski', 'Mart van Baalen', 'Markus Nagel', 'Paul Whatmough', 'Babak Ehteshami Bejnordi']","Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or ""experts"" for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$\times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.00099
Regularity of deficiency modules through spectral sequences,['Commutative Algebra'],"['Alberto F. Boix', 'Santiago Zarzuela']","The main goal of this paper is to obtain upper bounds for the regularity of graded deficiency modules in the spirit of the one obtained by Kumini--Murai in the monomial case building upon the spectral sequence formalism developed by Àlvarez Montaner, Boix and Zarzuela. This spectral sequence formalism allows us not only to recover Kumini--Murai's upper bound for monomial ideals, but also to extend it for other types of rings, which include toric face rings and some binomial edge rings, producing to the best of our knowledge new upper bounds for the regularity of graded deficiency modules of this type of rings.△ Less","27 November, 2024;",https://arxiv.org/pdf/2412.00092
Task Singular Vectors: Reducing Task Interference in Model Merging,['Machine Learning'],"['Antonio Andrea Gargiulo', 'Donato Crisostomi', 'Maria Sofia Bucarelli', 'Simone Scardapane', 'Fabrizio Silvestri', 'Emanuele Rodolà']","Task Arithmetic has emerged as a simple yet effective method to merge models without additional training. However, by treating entire networks as flat parameter vectors, it overlooks key structural information and is susceptible to task interference. In this paper, we study task vectors at the layer level, focusing on task layer matrices and their singular value decomposition. In particular, we concentrate on the resulting singular vectors, which we refer to as Task Singular Vectors (TSV). Recognizing that layer task matrices are often low-rank, we propose TSV-Compress (TSV-C), a simple procedure that compresses them to 10% of their original size while retaining 99% of accuracy. We further leverage this low-rank space to define a new measure of task interference based on the interaction of singular vectors from different tasks. Building on these findings, we introduce TSV-Merge (TSV-M), a novel model merging approach that combines compression with interference reduction, significantly outperforming existing methods.△ Less","26 November, 2024;",https://arxiv.org/pdf/2412.00081
"Black Hole Orbit Classification: The Synergistic Effects of Cloud Strings, Gauss-Bonnet Terms, and Non-Commutative Parameters in Identifying WGC candidate Models: WGC as WCCC protector",['General Physics'],"['Mohammad Ali S. Afshar', 'Jafar Sadeghi']","The integration of non-commutative geometry and Gauss-Bonnet corrections in an action and the study of their black hole responses can provide highly intriguing insights. Our primary motivation for this study is to understand the interplay of these two parameters on the geodesics of spacetime, including photon spheres and time-like orbits. In this study, we found that this integration, in its initial form, can limit the value of the Gauss-Bonnet parameter ($α$), creating a critical threshold beyond which changes in the non-commutative parameter ($Ξ$) become ineffective, and the structure can only manifest as a naked singularity. Furthermore, we found that using a more complex model, which includes additional factors such as a cloud of strings and linear charge, as a sample for studying spacetime geodesics, yield different and varied results. In this scenario, negative $α$ values can also play a role, notably preserving the black hole form even with a super-extremal charge ($q > m$). For $α> 0.1$, the black hole mass parameter becomes significantly influential, with a critical mass below which the impact of other parameter changes is nullified. Interestingly, considering a more massive black hole, this high-mass state also maintains its black hole form within the super-extremal charge range. The existence of these two models led us to our main goal. By examining the temperature for these two cases, we find that both situations are suitable for studying the WGC. Finally, based on the behavior of these two models, we will explain how the WGC acts as a logical solution and a protector for the WCCC.△ Less","26 November, 2024;",https://arxiv.org/pdf/2412.00079
Learning the physics of open quantum systems from experiments,['Quantum Physics'],['Alexandra Ramôa'],"This thesis explores adaptive inference as a tool to characterize quantum systems using experimental data, with applications in sensing, calibration, control, and metrology. I propose and test algorithms for learning Hamiltonian and Kraus operators using Bayesian experimental design and advanced Monte Carlo techniques, including Sequential and Hamiltonian Monte Carlo. Application to the characterization of quantum devices from IBMQ shows a robust performance, surpassing the built-in characterization functions of Qiskit for the same number of measurements. Introductions to Bayesian statistics, experimental design, and numerical integration are provided, as well as an overview of existing literature.△ Less","26 November, 2024;",https://arxiv.org/pdf/2412.00078
Selfish Evolution: Making Discoveries in Extreme Label Noise with the Help of Overfitting Dynamics,['Computer Vision and Pattern Recognition'],"['Nima Sedaghat', 'Tanawan Chatchadanoraset', 'Colin Orion Chandler', 'Ashish Mahabal', 'Maryam Eslami']","Motivated by the scarcity of proper labels in an astrophysical application, we have developed a novel technique, called Selfish Evolution, which allows for the detection and correction of corrupted labels in a weakly supervised fashion. Unlike methods based on early stopping, we let the model train on the noisy dataset. Only then do we intervene and allow the model to overfit to individual samples. The ``evolution'' of the model during this process reveals patterns with enough information about the noisiness of the label, as well as its correct version. We train a secondary network on these spatiotemporal ``evolution cubes'' to correct potentially corrupted labels. We incorporate the technique in a closed-loop fashion, allowing for automatic convergence towards a mostly clean dataset, without presumptions about the state of the network in which we intervene. We evaluate on the main task of the Supernova-hunting dataset but also demonstrate efficiency on the more standard MNIST dataset.△ Less","26 November, 2024;",https://arxiv.org/pdf/2412.00077
Safe to Serve: Aligning Instruction-Tuned Models for Safety and Helpfulness,['Computation and Language'],"['Avinash Amballa', 'Durga Sandeep Saluru', 'Gayathri Akkinapalli', 'Abhishek Sureddy', 'Akshay Kumar Sureddy']","Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning and text generation. However, these models can inadvertently generate unsafe or biased responses when prompted with problematic inputs, raising significant ethical and practical concerns for real-world deployment. This research addresses the critical challenge of developing language models that generate both helpful and harmless content, navigating the delicate balance between model performance and safety. We demonstrate that incorporating safety-related instructions during the instruction-tuning of pre-trained models significantly reduces toxic responses to unsafe prompts without compromising performance on helpfulness datasets. We found Direct Preference Optimization (DPO) to be particularly effective, outperforming both SIT and RAFT by leveraging both chosen and rejected responses for learning. Our approach increased safe responses from 40$\%$ to over 90$\%$ across various harmfulness benchmarks. In addition, we discuss a rigorous evaluation framework encompassing specialized metrics and diverse datasets for safety and helpfulness tasks ensuring a comprehensive assessment of the model's capabilities.△ Less","26 November, 2024;",https://arxiv.org/pdf/2412.00074
Enhanced Lung Cancer Survival Prediction using Semi-Supervised Pseudo-Labeling and Learning from Diverse PET/CT Datasets,['Computer Vision and Pattern Recognition'],"['Mohammad R. Salmanpour', 'Arman Gorji', 'Amin Mousavi', 'Ali Fathi Jouzdani', 'Nima Sanati', 'Mehdi Maghsudi', 'Bonnie Leung', 'Cheryl Ho', 'Ren Yuan', 'Arman Rahmim']","Objective: This study explores a semi-supervised learning (SSL), pseudo-labeled strategy using diverse datasets to enhance lung cancer (LCa) survival predictions, analyzing Handcrafted and Deep Radiomic Features (HRF/DRF) from PET/CT scans with Hybrid Machine Learning Systems (HMLS). Methods: We collected 199 LCa patients with both PET & CT images, obtained from The Cancer Imaging Archive (TCIA) and our local database, alongside 408 head&neck cancer (HNCa) PET/CT images from TCIA. We extracted 215 HRFs and 1024 DRFs by PySERA and a 3D-Autoencoder, respectively, within the ViSERA software, from segmented primary tumors. The supervised strategy (SL) employed a HMLSs: PCA connected with 4 classifiers on both HRF and DRFs. SSL strategy expanded the datasets by adding 408 pseudo-labeled HNCa cases (labeled by Random Forest algorithm) to 199 LCa cases, using the same HMLSs techniques. Furthermore, Principal Component Analysis (PCA) linked with 4 survival prediction algorithms were utilized in survival hazard ratio analysis. Results: SSL strategy outperformed SL method (p-value<0.05), achieving an average accuracy of 0.85 with DRFs from PET and PCA+ Multi-Layer Perceptron (MLP), compared to 0.65 for SL strategy using DRFs from CT and PCA+ K-Nearest Neighbor (KNN). Additionally, PCA linked with Component-wise Gradient Boosting Survival Analysis on both HRFs and DRFs, as extracted from CT, had an average c-index of 0.80 with a Log Rank p-value<<0.001, confirmed by external testing. Conclusions: Shifting from HRFs and SL to DRFs and SSL strategies, particularly in contexts with limited data points, enabling CT or PET alone to significantly achieve high predictive performance.△ Less","25 November, 2024;",https://arxiv.org/pdf/2412.00068
DiffGuard: Text-Based Safety Checker for Diffusion Models,['Computer Vision and Pattern Recognition'],"['Massine El Khader', 'Elias Al Bouzidi', 'Abdellah Oumida', 'Mohammed Sbaihi', 'Eliott Binard', 'Jean-Philippe Poli', 'Wassila Ouerdane', 'Boussad Addad', 'Katarzyna Kapusta']","Recent advances in Diffusion Models have enabled the generation of images from text, with powerful closed-source models like DALL-E and Midjourney leading the way. However, open-source alternatives, such as StabilityAI's Stable Diffusion, offer comparable capabilities. These open-source models, hosted on Hugging Face, come equipped with ethical filter protections designed to prevent the generation of explicit images. This paper reveals first their limitations and then presents a novel text-based safety filter that outperforms existing solutions. Our research is driven by the critical need to address the misuse of AI-generated content, especially in the context of information warfare. DiffGuard enhances filtering efficacy, achieving a performance that surpasses the best existing filters by over 14%.△ Less","25 November, 2024;",https://arxiv.org/pdf/2412.00064
High-precision medical speech recognition through synthetic data and semantic correction: UNITED-MEDASR,['Audio and Speech Processing'],"['Sourav Banerjee', 'Ayushi Agarwal', 'Promila Ghosh']","Automatic Speech Recognition (ASR) systems in the clinical domain face significant challenges, notably the need to recognise specialised medical vocabulary accurately and meet stringent precision requirements. We introduce United-MedASR, a novel architecture that addresses these challenges by integrating synthetic data generation, precision ASR fine-tuning, and advanced semantic enhancement techniques. United-MedASR constructs a specialised medical vocabulary by synthesising data from authoritative sources such as ICD-10 (International Classification of Diseases, 10th Revision), MIMS (Monthly Index of Medical Specialties), and FDA databases. This enriched vocabulary helps finetune the Whisper ASR model to better cater to clinical needs. To enhance processing speed, we incorporate Faster Whisper, ensuring streamlined and high-speed ASR performance. Additionally, we employ a customised BART-based semantic enhancer to handle intricate medical terminology, thereby increasing accuracy efficiently. Our layered approach establishes new benchmarks in ASR performance, achieving a Word Error Rate (WER) of 0.985% on LibriSpeech test-clean, 0.26% on Europarl-ASR EN Guest-test, and demonstrating robust performance on Tedlium (0.29% WER) and FLEURS (0.336% WER). Furthermore, we present an adaptable architecture that can be replicated across different domains, making it a versatile solution for domain-specific ASR systems.△ Less","24 November, 2024;",https://arxiv.org/pdf/2412.00055
Brick Kiln Dataset for Pakistan's IGP Region Using AI,['Computer Vision and Pattern Recognition'],"['Muhammad Suleman Ali Hamdani', 'Khizer Zakir', 'Neetu Kushwaha', 'Syeda Eman Fatima', 'Hassan Aftab Sheikh']","Brick kilns are a major source of air pollution in Pakistan, with many operating without regulation. A key challenge in Pakistan and across the Indo-Gangetic Plain is the limited air quality monitoring and lack of transparent data on pollution sources. To address this, we present a two-fold AI approach that combines low-resolution Sentinel-2 and high-resolution imagery to map brick kiln locations. Our process begins with a low-resolution analysis, followed by a post-processing step to reduce false positives, minimizing the need for extensive high-resolution imagery. This analysis initially identified 20,000 potential brick kilns, with high-resolution validation confirming around 11,000 kilns. The dataset also distinguishes between Fixed Chimney and Zigzag kilns, enabling more accurate pollution estimates for each type. Our approach demonstrates how combining satellite imagery with AI can effectively detect specific polluting sources. This dataset provides regulators with insights into brick kiln pollution, supporting interventions for unregistered kilns and actions during high pollution episodes.△ Less","24 November, 2024;",https://arxiv.org/pdf/2412.00052
Operations with f correlated fuzzy numbers,['General Mathematics'],"['Diogo Sampaio da Silva', 'Roberto Antonio Cordeiro Prata']","We present a brief introduction to a class of interactive fuzzy numbers, called f correlated fuzzy numbers, which consist of pairs of fuzzy numbers where one is dependent on the other by a continuous montone injector function. We present some results of operations with these numbers, obtaining equations that can directly calculate the results of these operations using only basic operations with real numbers, intervals on the real line and the function that relates the fuzzy numbers being considered.△ Less","23 November, 2024;",https://arxiv.org/pdf/2412.00046
Properties of f correlated fuzzy numbers,['General Mathematics'],"['Diogo Sampaio da Silva', 'Roberto Antonio Cordeiro Prata']","This paper presents some concepts of the theory of interactive fuzzy numbers, and mainly, a class of interactive fuzzy numbers, called $f$-correlated fuzzy numbers. We start from the foundations of general fuzzy mathematics and go through operations and the notion of interactivity for fuzzy numbers. The main result is that $f$-correlation preserve the shape of certains fuzzy numbers. More specificaly, if two fuzzy numbers are $f$ correlated, and one is a LR-type fuzzy number, the other is also a LR-type fuzzy number. This paper also presents some operations with the $f$-correlated fuzzy numbers wich are interesting to applications like biomathematics.△ Less","23 November, 2024;",https://arxiv.org/pdf/2412.00045
Classification of monads and a new moduli component of stable rank 2 bundles on $\mathbb{P}^3$ with even determinant and $c_2=9$,['Algebraic Geometry'],['Aislan Leal Fontes'],"The goal of this paper is to classify all minimal monads whose cohomology is a stable rank 2 bundle on $\mathbb{P}^3$ with Chern classes $c_1=0$ and $c_2=9$, with possible exception of two non-negative minimal monads, and thus we extend the classification of the minimal monads made by Hartshorne and Rao in \cite[Section 5.3]{HR91} when $c_2\leq8$. We also prove the existence of a new component of the moduli space $\mathcal{B}(9)$ which is distinct from the Hartshorne and Ein components.△ Less","22 November, 2024;",https://arxiv.org/pdf/2412.00043
"Central extensions of Lie algebras, dynamical systems, and symplectic nilmanifolds",['Differential Geometry'],['I. A. Taimanov'],"The connections between Euler's equations on central extensions of Lie algebras and Euler's equations on the original, extended algebras are described. A special infinite sequence of central extensions of nilpotent Lie algebras constructed from the Lie algebra of formal vector fields on the line is considered, and the orbits of coadjoint representations for these algebras are described. By using the compact nilmanifolds constructed from these algebras by I.K. Babenko and the author, it is shown that covering Lie groups for symplectic nilmanifolds can have any rank as solvable Lie groups.△ Less","21 November, 2024;",https://arxiv.org/pdf/2412.00037
Beyond Monte Carlo: Harnessing Diffusion Models to Simulate Financial Market Dynamics,['Computational Finance'],"['Andrew Lesniewski', 'Giulio Trigila']","We propose a highly efficient and accurate methodology for generating synthetic financial market data using a diffusion model approach. The synthetic data produced by our methodology align closely with observed market data in several key aspects: (i) they pass the two-sample Cramer - von Mises test for portfolios of assets, and (ii) Q - Q plots demonstrate consistency across quantiles, including in the tails, between observed and generated market data. Moreover, the covariance matrices derived from a large set of synthetic market data exhibit significantly lower condition numbers compared to the estimated covariance matrices of the observed data. This property makes them suitable for use as regularized versions of the latter. For model training, we develop an efficient and fast algorithm based on numerical integration rather than Monte Carlo simulations. The methodology is tested on a large set of equity data.△ Less","21 November, 2024;",https://arxiv.org/pdf/2412.00036
A Fractional Model of Abalone Growth using Adomian Decomposition Method,['General Mathematics'],"['Marliadi Susanto', 'Nadihah Wahi', 'Adem Kilicman']","This study is a modification of the McKendrick equation into a growth model with fractional order to predict the abalone length growth. We have shown that the model is a special case of Taylor's series after it was analysed using Adomian decomposition method and Caputo fractional derivative. By simulating the series with some fractional orders, the results indicate that the greater the fractional order of the model, the series values generated are greater as well. Moreover, the series that is close to the real data is the one with a fractional order of $0.5$. Therefore, the growth model with a fractional order provides more accuracy than a classical integer order.△ Less","21 November, 2024;",https://arxiv.org/pdf/2412.00035
On polynomial equations over split octonions,['Rings and Algebras'],"['Artem Lopatin', 'Alexander N. Rybalov']","Working over the split octonions over an algebraically closed field, we solve a polynomial equation, where all coefficients are scalar with the possible exception for the free term. As a consequence, we calculated the n-th radicals of an octonion.△ Less","20 November, 2024;",https://arxiv.org/pdf/2412.00032
"Evaluating Large Language Models on Business Process Modeling: Framework, Benchmark, and Self-Improvement Analysis",['Databases'],"['Humam Kourani', 'Alessandro Berti', 'Daniel Schuster', 'Wil M. P. van der Aalst']","Large Language Models (LLMs) are rapidly transforming various fields, and their potential in Business Process Management (BPM) is substantial. This paper assesses the capabilities of LLMs on business process modeling using a framework for automating this task, a comprehensive benchmark, and an analysis of LLM self-improvement strategies. We present a comprehensive evaluation of 16 state-of-the-art LLMs from major AI vendors using a custom-designed benchmark of 20 diverse business processes. Our analysis highlights significant performance variations across LLMs and reveals a positive correlation between efficient error handling and the quality of generated models. It also shows consistent performance trends within similar LLM groups. Furthermore, we investigate LLM self-improvement techniques, encompassing self-evaluation, input optimization, and output optimization. Our findings indicate that output optimization, in particular, offers promising potential for enhancing quality, especially in models with initially lower performance. Our contributions provide insights for leveraging LLMs in BPM, paving the way for more advanced and automated process modeling techniques.△ Less","17 November, 2024;",https://arxiv.org/pdf/2412.00023
"Norm upper-semicontinuity of functions supported on open abelian isotropy in étale groupoids (a corrigendum to ""Reconstruction of groupoids and C*-rigidity of dynamical systems,"" Adv. Math 390 (2021), 107923)",['Operator Algebras'],"['Toke Meier Carlsen', 'Anna Duwenig', 'Efren Ruiz', 'Aidan Sims']","We consider étale Hausdorff groupoids in which the interior of the isotropy is abelian. We prove that the norms of the images under regular representations, of elements of the reduced groupoid $C^*$-algebra whose supports are contained in the interior of the isotropy vary upper semicontinuously. This corrects an error in [T.M. Carlsen, E. Ruiz, A. Sims and M. Tomforde, ""Reconstruction of groupoids and C*-rigidity of dynamical systems,"" Adv. Math 390 (2021), 107923].△ Less","15 November, 2024;",https://arxiv.org/pdf/2412.00018
The use of knowledge in open-ended systems,['Neural and Evolutionary Computing'],"['Abigail Devereaux', 'Roger Koppl']","Economists model knowledge use and acquisition as a cause-and-effect calculus associating observations made by a decision-maker about their world with possible underlying causes. Knowledge models are well-established for static contexts, but not for contexts of innovative and unbounded change. We develop a representation of knowledge use and acquisition in open-ended evolutionary systems and demonstrate its primary results, including that observers embedded in open-ended evolutionary systems can agree to disagree and that their ability to theorize about their systems is fundamentally local and constrained to their frame of reference what we call frame relativity. The results of our framework formalize local knowledge use, the many-selves interpretation of reasoning through time, and motivate the emergence of nonlogical modes of reasoning like institutional and aesthetic codes.△ Less","13 November, 2024;",https://arxiv.org/pdf/2412.00011
A Mathematical Problem-Solving Pipeline (MPSP) to strengthen scaffolding in higher education STEM courses,['History and Overview'],"['Rochelle E. Tractenberg', 'Andrew C. Lee', 'Rachelle DeCoste']","We outline a new tool that can promote coherence within and across higher education mathematics courses by focusing on problem-solving: the Mathematical Problem-Solving Pipeline or MPSP. The MPSP can be used for teaching mathematics and mathematical reasoning authentically. It can be used across courses to develop problem solving skills that can be generalized beyond the course and topic. It has a specific focus on documentation and communication that lets students leverage skills they grow, and use, in other courses or fields. The MPSP can be used in singleton (service) courses, throughout a curriculum, and/or within a capstone experience.△ Less",v1,https://arxiv.org/pdf/2412.00009
Efficient short-wave infrared upconversion by self-sensitized holmium-doped nanoparticles,['Optics'],"['Rakesh Arul', 'Zhao Jiang', 'Xinjuan Li', 'Fiona M. Bell', 'Alasdair Tew', 'Caterina Ducati', 'Akshay Rao', 'Zhongzheng Yu']","Photon upconversion, combining several low-energy photons to generate one high-energy photon is of wide interest for biomedical, catalytic and photonic applications. Lanthanide-doped nanoparticles (LnNP) are a unique type of upconversion nanoconverter, which can realize ultralarge anti-Stokes shift (>1000 nm) and high photostability, without photo-bleaching and photo-blinking. The excitation wavelength of LnNPs has been limited to the second near-infrared window (1000-1700 nm), mainly sensitized by erbium ions with absorption centered around 1.5 $μ$m. Here, we demonstrate novel self-sensitized holmium (Ho)-doped nanoconverters to further expand the sensitization range to the short-wave infrared at 2 $μ$m and achieve efficient upconversion to 640 nm. We show that this upconversion is a 4-photon conversion process with an underlying energy transfer upconversion mechanism. Via careful control of dopant concentration and shelling we achieve a relative upconversion-to-downconversion efficiency up to 15.2%, more than half the theoretical maximum. The placement of the Ho doped LnNPs into a plasmonic nanocavity device enables large gains in emission intensity (up to 32-fold), due to the dramatic shortening of the emission lifetime of Ho from 29 $μ$s to <1 ns, indicating a high Purcell-enhancement factor of 3x10$^4$. These results open new possibilities at the frontier of short-wave infrared upconversion and the nanoplasmonic enhancement of LnNP emission, with potential applications in detection, theranostics, photonics and optoelectronics.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19949
DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation,['Computer Vision and Pattern Recognition'],"['Zhiqiang Shen', 'Ammar Sherif', 'Zeyuan Yin', 'Shitong Shao']","Recent advances in dataset distillation have led to solutions in two main directions. The conventional batch-to-batch matching mechanism is ideal for small-scale datasets and includes bi-level optimization methods on models and syntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like distribution matching, gradient matching, and weight trajectory matching. Conversely, batch-to-global matching typifies decoupled methods, which are particularly advantageous for large-scale datasets. This approach has garnered substantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD, and CDA. A primary challenge with the second approach is the lack of diversity among syntheses within each class since samples are optimized independently and the same global supervision signals are reused across different synthetic images. In this study, we propose a new Diversity-driven EarlyLate Training (DELT) scheme to enhance the diversity of images in batch-to-global matching with less computation. Our approach is conceptually simple yet effective, it partitions predefined IPC samples into smaller subtasks and employs local optimizations to distill each subset into distributions from distinct phases, reducing the uniformity induced by the unified optimization process. These distilled images from the subtasks demonstrate effective generalization when applied to the entire task. We conduct extensive experiments on CIFAR, Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the previous state-of-the-art by 2$\sim$5% on average across different datasets and IPCs (images per class), increasing diversity per class by more than 5% while reducing synthesis time by up to 39.3% for enhancing the training efficiency. Code is available at: https://github.com/VILA-Lab/DELT.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19946
The hot circumgalactic medium in the eROSITA All-Sky Survey III. Star-forming and quiescent galaxies,['Astrophysics of Galaxies'],"['Yi Zhang', 'Johan Comparat', 'Gabriele Ponti', 'Andrea Merloni', 'Kirpal Nandra', 'Frank Haberl', 'Nhut Truong', 'Annalisa Pillepich', 'Paola Popesso', 'Nicola Locatelli', 'Xiaoyuan Zhang', 'Jeremy Sanders', 'Xueying Zheng', 'Ang Liu', 'Teng Liu', 'Peter Predehl', 'Mara Salvato', 'Marcus Bruggen', 'Soumya Shreeram', 'Michael C. H. Yeung']","The circumgalactic medium (CGM), as the gas repository for star formation, might contain the answer to the mysterious galaxy quenching and bimodal galaxy population origin. We measured the X-ray emission of the hot CGM around star-forming and quiescent galaxies. We detect extended X-ray emission from the hot CGM around star-forming galaxies with $\log(M_*/M_\odot)>11.0$ and quiescent galaxies with $\log(M_*/M_\odot)>10.5$, extending out to $R_{\rm 500c}$. $L_{\rm X, CGM}$ of star-forming galaxies with median stellar masses $\log(M_{\rm *,med}/M_\odot) = 10.7, 11.1, 11.3$ are approximately $0.8\,, 2.3\,, 4.0 \times 10^{40}\,\rm erg/s$, while for quiescent galaxies with $\log(M_{\rm *,med}/M_\odot) = 10.8, 11.1, 11.4$, they are $1.1\,, 6.2\,, 30 \times 10^{40}\,\rm erg/s$. Notably, quiescent galaxies with $\log(M_{\rm *,med}/M_\odot) > 11.0$ exhibit brighter hot CGM than their star-forming counterparts. In halo mass bins, we detect similar X-ray emission around star-forming and quiescent galaxies with $\log(M_{\rm 200m}/M_\odot) > 12.5$, suggesting that galaxies in the same mass dark matter halos host equally bright hot CGM. We emphasize the observed $L_{\rm X, CGM} - M_{\rm 500c}$ relations of star-forming and quiescent galaxies are sensitive to the stellar-to-halo mass relation (SHMR). A comparison with cosmological hydrodynamical simulations (EAGLE, TNG100, and SIMBA) reveals varying degrees of agreement, contingent on the simulation and the specific stellar or halo mass ranges considered. Either selected in stellar mass or halo mass, the star-forming galaxies do not host brighter stacked X-ray emission from the hot CGM than their quiescent counterparts at the same mass range. The result provides useful constraints on the extent of feedback's impacts as a mechanism for quenching star formation as implemented in current cosmological simulations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19945
Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark,['Computer Vision and Pattern Recognition'],"['Joseph Heyward', 'João Carreira', 'Dima Damen', 'Andrew Zisserman', 'Viorica Pătrăucean']","Following the successful 2023 edition, we organised the Second Perception Test challenge as a half-day workshop alongside the IEEE/CVF European Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking state-of-the-art video models and measuring the progress since last year using the Perception Test benchmark. This year, the challenge had seven tracks (up from six last year) and covered low-level and high-level tasks, with language and non-language interfaces, across video, audio, and text modalities; the additional track covered hour-long video understanding and introduced a novel video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks were: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, grounded video question-answering, and hour-long video question-answering. We summarise in this report the challenge tasks and results, and introduce in detail the novel hour-long video QA benchmark 1h-walk VQA.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19941
Enhancement of the superconducting transition temperature due to multiband effect in the topological nodal-line semimetal Pb$_{1-x}$Sn$_{x}$TaSe$_{2}$,['Superconductivity'],"['K. Kumarasinghe', 'A. Rahman', 'M. Tomlinson', 'Y. Nakajima']","We report a systematic study of the normal-state and superconducting properties of single crystal Pb$_{1-x}$Sn$_{x}$TaSe$_{2}$ $(0\leq x \leq 0.23)$. Sn doping enhances the superconducting temperature $T_{c}$ up to 5.1 K, while also significantly increasing impurity scattering in the crystals. For $x=0$, the specific heat jump at $T_{c}$ exceeds the Bardeen-Cooper-Schrieffer (BCS) weak-coupling value of 1.43, indicating the realization of strong-coupling superconductivity in PbTaSe$_{2}$. In contrast, substituting Pb with Sn lowers the specific heat jump at $T_{c}$ below the BSC value of 1.43, which cannot be explained by a single-gap model. Rather, the observed specific heat of Sn-doped PbTaSe$_{2}$ is reproduced by a two-gap model. Our observations suggest that additional Fermi pockets appear due to a reduction of the spin-orbit gap with Sn doping, and the multiband effect arising from these emergent Fermi pockets enhances the effective electron-phonon coupling strength, leading to the increase in $T_{c}$.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19932
Linearization (in)stabilities and crossed products,['High Energy Physics - Theory'],"['Julian De Vuyst', 'Stefan Eccles', 'Philipp A. Hoehn', 'Josh Kirklin']","Modular crossed product algebras have recently assumed an important role in perturbative quantum gravity as they lead to an intrinsic regularization of entanglement entropies by introducing quantum reference frames (QRFs) in place of explicit regulators. This is achieved by imposing certain boost constraints on gravitons, QRFs and other fields. Here, we revisit the question of how these constraints should be understood through the lens of perturbation theory and particularly the study of linearization (in)stabilities, exploring when linearized solutions can be integrated to exact ones. Our aim is to provide some clarity about the status of justification, under various conditions, for imposing such constraints on the linearized theory in the $G_N\to0$ limit as they turn out to be of second-order. While for spatially compact spacetimes there is an essentially unambiguous justification, in the presence of boundaries or the absence of isometries this depends on whether one is also interested in second-order observables. Linearization (in)stabilities occur in any gauge-covariant field theory with non-linear equations and to address this in a unified framework, we translate the subject from the usual canonical formulation into a systematic covariant phase space language. This overcomes theory-specific arguments, exhibiting the universal structure behind (in)stabilities, and permits us to cover arbitrary generally covariant theories. We comment on the relation to modular flow and illustrate our findings in several gravity and gauge theory examples.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19931
Traction force microscopy for linear and nonlinear elastic materials as a parameter identification inverse problem,['Numerical Analysis'],"['Gesa Sarnighausen', 'Tram Thi Ngoc Nguyen', 'Thorsten Hohage', 'Mangalika Sinha', 'Sarah Koester', 'Timo Betz', 'Ulrich Sebastian Schwarz', 'Anne Wald']","Traction force microscopy is a method widely used in biophysics and cell biology to determine forces that biological cells apply to their environment. In the experiment, the cells adhere to a soft elastic substrate, which is then deformed in response to cellular traction forces. The inverse problem consists in computing the traction stress applied by the cell from microscopy measurements of the substrate deformations. In this work, we consider a linear model, in which 3D forces are applied at a 2D interface, called 2.5D traction force microscopy, and a nonlinear pure 2D model, from which we directly obtain a linear pure 2D model. All models lead to a linear resp. nonlinear parameter identification problem for a boundary value problem of elasticity. We analyze the respective forward operators and conclude with some numerical experiments for simulated and experimental data.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19917
A gravitational wave detectable candidate Type Ia supernova progenitor,['Solar and Stellar Astrophysics'],"['Emma T. Chickles', 'Kevin B. Burdge', 'Joheen Chakraborty', 'Vik S. Dhillon', 'Paul Draghis', 'Scott A. Hughes', 'James Munday', 'Saul A. Rappaport', 'John Tonry', 'Evan Bauer', 'Alex Brown', 'Noel Castro', 'Deepto Chakrabarty', 'Martin Dyer', 'Kareem El-Badry', 'Anna Frebel', 'Gabor Furesz', 'James Garbutt', 'Matthew J. Green', 'Aaron Householder', 'Daniel Jarvis', 'Erin Kara', 'Mark R. Kennedy', 'Paul Kerry', 'Stuart P Littlefair']","Type Ia supernovae, critical for studying cosmic expansion, arise from thermonuclear explosions of white dwarfs, but their precise progenitor pathways remain unclear. Growing evidence supports the ``double-degenerate'' scenario, where two white dwarfs interact. The absence of other companion types capable of explaining the observed Ia rate, along with observations of hyper-velocity white dwarfs interpreted as surviving companions of such systems provide compelling evidence in favor of this scenario. Upcoming millihertz gravitational wave observatories like the Laser Interferometer Space Antenna (LISA) are expected to detect thousands of double-degenerate systems, though the most compact known candidate Ia progenitors produce only marginally detectable gravitational wave signals. Here, we report observations of ATLAS J1138-5139, a binary white dwarf system with an orbital period of 28 minutes. Our analysis reveals a 1 solar mass carbon-oxygen white dwarf accreting from a helium-core white dwarf. Given its mass, the accreting carbon-oxygen white dwarf is poised to trigger a typical-luminosity Type Ia supernova within a few million years, or to evolve into a stably mass-transferring AM CVn system. ATLAS J1138-5139 provides a rare opportunity to calibrate binary evolution models by directly comparing observed orbital parameters and mass transfer rates closer to merger than any previously identified candidate Type Ia progenitor. Its compact orbit ensures detectability by LISA, demonstrating the potential of millihertz gravitational wave observatories to reveal a population of Type Ia progenitors on a Galactic scale, paving the way for multi-messenger studies offering insights into the origins of these cosmologically significant explosions.△ Less",v1,https://arxiv.org/pdf/2411.19916
Sparse Partitions of Graphs with Bounded Clique Number,['Combinatorics'],"['António Girão', 'Toby Insley']","We prove that for each integer $r\geq 2$, there exists a constant $C_r>0$ with the following property: for any $0<\varepsilon \leq 1/2$ and any graph $G$ with clique number at most $r,$ there is a partition of $V(G)$ into at most $(1/\varepsilon)^{C_r}$ sets $S_1, \dots, S_t,$ such that $G[S_i]$ has maximum degree at most $\varepsilon |S_i|$ for each $1 \leq i \leq t.$ This answers a question of Fox, Nguyen, Scott and Seymour, who proved a similar result for graphs with no induced $P_4.$△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19915
Learning Feedback Mechanisms for Measurement-Based Variational Quantum State Preparation,['Quantum Physics'],"['Daniel Alcalde Puente', 'Matteo Rizzi']","This work introduces a self-learning protocol that incorporates measurement and feedback into variational quantum circuits for efficient quantum state preparation. By combining projective measurements with conditional feedback, the protocol learns state preparation strategies that extend beyond unitary-only methods, leveraging measurement-based shortcuts to reduce circuit depth. Using the spin-1 Affleck-Kennedy-Lieb-Tasaki state as a benchmark, the protocol learns high-fidelity state preparation by overcoming a family of measurement induced local minima through adjustments of parameter update frequencies and ancilla regularization. Despite these efforts, optimization remains challenging due to the highly non-convex landscapes inherent to variational circuits. The approach is extended to larger systems using translationally invariant ansätze and recurrent neural networks for feedback, demonstrating scalability. Additionally, the successful preparation of a specific AKLT state with desired edge modes highlights the potential to discover new state preparation protocols where none currently exist. These results indicate that integrating measurement and feedback into variational quantum algorithms provides a promising framework for quantum state preparation.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19914
Quantifying the synthetic and real domain gap in aerial scene understanding,['Computer Vision and Pattern Recognition'],['Alina Marcu'],"Quantifying the gap between synthetic and real-world imagery is essential for improving both transformer-based models - that rely on large volumes of data - and datasets, especially in underexplored domains like aerial scene understanding where the potential impact is significant. This paper introduces a novel methodology for scene complexity assessment using Multi-Model Consensus Metric (MMCM) and depth-based structural metrics, enabling a robust evaluation of perceptual and structural disparities between domains. Our experimental analysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes) datasets, demonstrates that real-world scenes generally exhibit higher consensus among state-of-the-art vision transformers, while synthetic scenes show greater variability and challenge model adaptability. The results underline the inherent complexities and domain gaps, emphasizing the need for enhanced simulation fidelity and model generalization. This work provides critical insights into the interplay between domain characteristics and model performance, offering a pathway for improved domain adaptation strategies in aerial scene understanding.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19913
Interacting Dark Sector (ETHOS $n=0$): Cosmological Constraints from SPT Cluster Abundance with DES and HST Weak Lensing Data,['Cosmology and Nongalactic Astrophysics'],"['Asmaa Mazoun', 'Sebastian Bocquet', 'Joseph J. Mohr', 'Mathias Garny', 'Henrique Rubira', 'Matthias Klein', 'Lindsey Bleem', 'Sebastian Grandis', 'Tim Schrabback']","We use galaxy cluster abundance measurements from the South Pole Telescope (SPT) enhanced by Multi-Component Matched Filter (MCMF) confirmation and complemented with mass information obtained using weak-lensing data from Dark Energy Survey Year~3 (DES Y3) and targeted Hubble Space Telescope (HST) observations for probing deviations from the cold dark matter paradigm. Concretely, we consider a class of dark sector models featuring interactions between dark matter (DM) and a dark radiation (DR) component within the framework of the Effective Theory of Structure Formation (ETHOS). We focus on scenarios that lead to power suppression over a wide range of scales, and thus can be tested with data sensitive to large scales, as realized for example for DM$-$DR interactions following from an unbroken non-Abelian $SU(N)$ gauge theory (interaction rate with power-law index $n=0$ within the ETHOS parameterization). Cluster abundance measurements are mostly sensitive to the amount of DR interacting with DM, parameterized by the ratio of DR temperature to the cosmic microwave background (CMB) temperature, $ξ_{\rm DR}=T_{\rm DR}/T_{\rm CMB}$. We find an upper limit $ξ_{\rm DR}<17\%$ at $95\%$ credibility. When the cluster data are combined with Planck 2018 CMB data along with baryon acoustic oscillation (BAO) measurements we find $ξ_{\rm DR}<10\%$, corresponding to a limit on the abundance of interacting DR that is around three times tighter than that from CMB+BAO data alone. We also discuss the complementarity of weak lensing informed cluster abundance studies with probes sensitive to smaller scales, explore the impact on our analysis of massive neutrinos, and comment on a slight preference for the presence of a non-zero interacting DR abundance, which enables a physical solution to the $S_8$ tension.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19911
Gravitational form factors of the deuteron,['Nuclear Theory'],"['J. Yu. Panteleeva', 'E. Epelbaum', 'A. M. Gasparyan', 'J. Gegelia']","The gravitational form factors of the deuteron are calculated in the framework of non-relativistic chiral effective field theory. Non-relativistic reduction of the matrix element of the energy-momentum tensor operator for spin-one systems is worked out, and the gravitational form factors of the deuteron are extracted from the three-point function of the energy-momentum tensor using the LSZ reduction formula. The obtained form factors are compared to results of model calculations available in the literature.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19909
Classical and Quantum Algorithms for the Deterministic L-system Inductive Inference Problem,['Quantum Physics'],"['Ali Lotfi', 'Ian McQuillan', 'Steven Rayan']","L-systems can be made to model and create simulations of many biological processes, such as plant development. Finding an L-system for a given process is typically solved by hand, by experts, in a hugely time-consuming process. It would be significant if this could be done automatically from data, such as from sequences of images. In this paper, we are interested in inferring a particular type of L-system, deterministic context-free L-system (D0L-system) from a sequence of strings. We introduce the characteristic graph of a sequence of strings, which we then utilize to translate our problem (inferring D0L-system) in polynomial time into the maximum independent set problem (MIS) and the SAT problem. After that, we offer a classical exact algorithm and an approximate quantum algorithm for the problem.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19906
$C^{3}$-NeRF: Modeling Multiple Scenes via Conditional-cum-Continual Neural Radiance Fields,['Computer Vision and Pattern Recognition'],"['Prajwal Singh', 'Ashish Tiwari', 'Gautam Vashishtha', 'Shanmuganathan Raman']","Neural radiance fields (NeRF) have exhibited highly photorealistic rendering of novel views through per-scene optimization over a single 3D scene. With the growing popularity of NeRF and its variants, they have become ubiquitous and have been identified as efficient 3D resources. However, they are still far from being scalable since a separate model needs to be stored for each scene, and the training time increases linearly with every newly added scene. Surprisingly, the idea of encoding multiple 3D scenes into a single NeRF model is heavily under-explored. In this work, we propose a novel conditional-cum-continual framework, called $C^{3}$-NeRF, to accommodate multiple scenes into the parameters of a single neural radiance field. Unlike conventional approaches that leverage feature extractors and pre-trained priors for scene conditioning, we use simple pseudo-scene labels to model multiple scenes in NeRF. Interestingly, we observe the framework is also inherently continual (via generative replay) with minimal, if not no, forgetting of the previously learned scenes. Consequently, the proposed framework adapts to multiple new scenes without necessarily accessing the old data. Through extensive qualitative and quantitative evaluation using synthetic and real datasets, we demonstrate the inherent capacity of the NeRF model to accommodate multiple scenes with high-quality novel-view renderings without adding additional parameters. We provide implementation details and dynamic visualizations of our results in the supplementary file.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19903
Noncommutative Model Selection for Data Clustering and Dimension Reduction Using Relative von Neumann Entropy,['Machine Learning'],"['Araceli Guzmán-Tristán', 'Antonio Rieser']","We propose a pair of completely data-driven algorithms for unsupervised classification and dimension reduction, and we empirically study their performance on a number of data sets, both simulated data in three-dimensions and images from the COIL-20 data set. The algorithms take as input a set of points sampled from a uniform distribution supported on a metric space, the latter embedded in an ambient metric space, and they output a clustering or reduction of dimension of the data. They work by constructing a natural family of graphs from the data and selecting the graph which maximizes the relative von Neumann entropy of certain normalized heat operators constructed from the graphs. Once the appropriate graph is selected, the eigenvectors of the graph Laplacian may be used to reduce the dimension of the data, and clusters in the data may be identified with the kernel of the associated graph Laplacian. Notably, these algorithms do not require information about the size of a neighborhood or the desired number of clusters as input, in contrast to popular algorithms such as $k$-means, and even more modern spectral methods such as Laplacian eigenmaps, among others.
  In our computational experiments, our clustering algorithm outperforms $k$-means clustering on data sets with non-trivial geometry and topology, in particular data whose clusters are not concentrated around a specific point, and our dimension reduction algorithm is shown to work well in several simple examples.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19902
Anti-topological crystal and non-Abelian liquid in twisted semiconductor bilayers,['Mesoscale and Nanoscale Physics'],"['Aidan P. Reddy', 'D. N. Sheng', 'Ahmed Abouelkomsan', 'Emil J. Bergholtz', 'Liang Fu']","We show that electron crystals compete closely with non-Abelian fractional Chern insulators in the half-full second moiré band of twisted bilayer MoTe$_2$. Depending on the twist angle and microscopic model, these crystals can have non-zero or zero Chern numbers. The latter relies on cancellation between contributions from the full first miniband (+1) and the half-full second miniband (-1). For this reason, we call it an anti-topological crystal. Surprisingly, it occurs despite the lowest two non-interacting bands in a given valley having the same Chern number of +1. The anti-topological crystal is a novel type of electron crystal that may appear in systems with multiple Chern bands at filling factors $n > 1$.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19898
Input-Output Optics as a Causal Time Series Mapping: A Generative Machine Learning Solution,['Quantum Physics'],"['Abhijit Sen', 'Bikram Keshari Parida', 'Kurt Jacobs', 'Denys I. Bondar']","The response of many-body quantum systems to an optical pulse can be extremely challenging to model. Here we explore the use of neural networks, both traditional and generative, to learn and thus simulate the response of such a system from data. The quantum system can be viewed as performing a complex mapping from an input time-series (the optical pulse) to an output time-series (the systems response) which is often also an optical pulse. Using both the transverse and non-integrable Ising models as examples, we show that not only can temporal convolutional networks capture the input/output mapping generated by the system but can also be used to characterize the complexity of the mapping. This measure of complexity is provided by the size of the smallest latent space that is able to accurately model the mapping. We further find that a generative model, in particular a variational auto-encoder, significantly outperforms traditional auto-encoders at learning the complex response of many-body quantum systems. For the example that generated the most complex mapping, the variational auto-encoder produces outputs that have less than 10% error for more than 90% of inputs across our test data.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19897
Efficient quantum-enhanced classical simulation for patches of quantum landscapes,['Quantum Physics'],"['Sacha Lerch', 'Ricard Puig', 'Manuel S. Rudolph', 'Armando Angrisani', 'Tyson Jones', 'M. Cerezo', 'Supanut Thanasilp', 'Zoë Holmes']","Understanding the capabilities of classical simulation methods is key to identifying where quantum computers are advantageous. Not only does this ensure that quantum computers are used only where necessary, but also one can potentially identify subroutines that can be offloaded onto a classical device. In this work, we show that it is always possible to generate a classical surrogate of a sub-region (dubbed a ""patch"") of an expectation landscape produced by a parameterized quantum circuit. That is, we provide a quantum-enhanced classical algorithm which, after simple measurements on a quantum device, allows one to classically simulate approximate expectation values of a subregion of a landscape. We provide time and sample complexity guarantees for a range of families of circuits of interest, and further numerically demonstrate our simulation algorithms on an exactly verifiable simulation of a Hamiltonian variational ansatz and long-time dynamics simulation on a 127-qubit heavy-hex topology.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19896
Noncommutative Model Selection and the Data-Driven Estimation of Real Cohomology Groups,['Computational Geometry'],"['Araceli Guzmán-Tristán', 'Antonio Rieser', 'Eduardo Velázquez-Richards']","We propose three completely data-driven methods for estimating the real cohomology groups $H^k (X ; \mathbb{R})$ of a compact metric-measure space $(X, d_X, μ_X)$ embedded in a metric-measure space $(Y,d_Y,μ_Y)$, given a finite set of points $S$ sampled from a uniform distrbution $μ_X$ on $X$, possibly corrupted with noise from $Y$. We present the results of several computational experiments in the case that $X$ is embedded in $\mathbb{R}^n$, where two of the three algorithms performed well.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19894
PDDLFuse: A Tool for Generating Diverse Planning Domains,['Artificial Intelligence'],"['Vedant Khandelwal', 'Amit Sheth', 'Forest Agostinelli']","Various real-world challenges require planning algorithms that can adapt to a broad range of domains. Traditionally, the creation of planning domains has relied heavily on human implementation, which limits the scale and diversity of available domains. While recent advancements have leveraged generative AI technologies such as large language models (LLMs) for domain creation, these efforts have predominantly focused on translating existing domains from natural language descriptions rather than generating novel ones. In contrast, the concept of domain randomization, which has been highly effective in reinforcement learning, enhances performance and generalizability by training on a diverse array of randomized new domains. Inspired by this success, our tool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language (PDDL). PDDLFuse is designed to generate new, diverse planning domains that can be used to validate new planners or test foundational planning models. We have developed methods to adjust the domain generators parameters to modulate the difficulty of the domains it generates. This adaptability is crucial as existing domain-independent planners often struggle with more complex problems. Initial tests indicate that PDDLFuse efficiently creates intricate and varied domains, representing a significant advancement over traditional domain generation methods and making a contribution towards planning research.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19886
Statistical inference of a ranked community in a directed graph,['Statistics Theory'],"['Dmitriy Kunisky', 'Daniel A. Spielman', 'Alexander S. Wein', 'Xifan Yu']","We study the problem of detecting or recovering a planted ranked subgraph from a directed graph, an analog for directed graphs of the well-studied planted dense subgraph model. We suppose that, among a set of $n$ items, there is a subset $S$ of $k$ items having a latent ranking in the form of a permutation $π$ of $S$, and that we observe a fraction $p$ of pairwise orderings between elements of $\{1, \dots, n\}$ which agree with $π$ with probability $\frac{1}{2} + q$ between elements of $S$ and otherwise are uniformly random. Unlike in the planted dense subgraph and planted clique problems where the community $S$ is distinguished by its unusual density of edges, here the community is only distinguished by the unusual consistency of its pairwise orderings. We establish computational and statistical thresholds for both detecting and recovering such a ranked community. In the log-density setting where $k$, $p$, and $q$ all scale as powers of $n$, we establish the exact thresholds in the associated exponents at which detection and recovery become statistically and computationally feasible. These regimes include a rich variety of behaviors, exhibiting both statistical-computational and detection-recovery gaps. We also give finer-grained results for two extreme cases: (1) $p = 1$, $k = n$, and $q$ small, where a full tournament is observed that is weakly correlated with a global ranking, and (2) $p = 1$, $q = \frac{1}{2}$, and $k$ small, where a small ""ordered clique"" (totally ordered directed subgraph) is planted in a random tournament.△ Less","18 November, 2024;",https://arxiv.org/pdf/2411.19885
Open source Differentiable ODE Solving Infrastructure,['Machine Learning'],"['Rakshit Kr. Singh', 'Aaron Rock Menezes', 'Rida Irfan', 'Bharath Ramsundar']","Ordinary Differential Equations (ODEs) are widely used in physics, chemistry, and biology to model dynamic systems, including reaction kinetics, population dynamics, and biological processes. In this work, we integrate GPU-accelerated ODE solvers into the open-source DeepChem framework, making these tools easily accessible. These solvers support multiple numerical methods and are fully differentiable, enabling easy integration into more complex differentiable programs. We demonstrate the capabilities of our implementation through experiments on Lotka-Volterra predator-prey dynamics, pharmacokinetic compartment models, neural ODEs, and solving PDEs using reaction-diffusion equations. Our solvers achieved high accuracy with mean squared errors ranging from $10^{-4}$ to $10^{-6}$ and showed scalability in solving large systems with up to 100 compartments.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19882
Enhanced anomaly detection in well log data through the application of ensemble GANs,['Geophysics'],"['Abdulrahman Al-Fakih', 'A. Koeshidayatullah', 'Tapan Mukerji', 'SanLinn I. Kaka']","Although generative adversarial networks (GANs) have shown significant success in modeling data distributions for image datasets, their application to structured or tabular data, such as well logs, remains relatively underexplored. This study extends the ensemble GANs (EGANs) framework to capture the distribution of well log data and detect anomalies that fall outside of these distributions. The proposed approach compares the performance of traditional methods, such as Gaussian mixture models (GMMs), with EGANs in detecting anomalies outside the expected data distributions. For the gamma ray (GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76, outperforming GMM's precision of 0.38 and F1 score of 0.54. Similarly, for travel time (DT), EGANs achieved a precision of 0.70 and F1 score of 0.79, surpassing GMM 0.56 and 0.71. In the neutron porosity (NPHI) dataset, EGANs recorded a precision of 0.53 and F1 score of 0.68, outshining GMM 0.47 and 0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52 and an F1 score of 0.67, slightly outperforming GMM, which yielded a precision of 0.50 and an F1 score of 0.65. This work's novelty lies in applying EGANs for well log data analysis, showcasing their ability to learn data patterns and identify anomalies that deviate from them. This approach offers more reliable anomaly detection compared to traditional methods like GMM. The findings highlight the potential of EGANs in enhancing anomaly detection for well log data, delivering significant implications for optimizing drilling strategies and reservoir management through more accurate, data-driven insights into subsurface characterization.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19875
Quantum Key Distribution with Basis-Dependent Detection Probability,['Quantum Physics'],"['Federico Grasselli', 'Giovanni Chesi', 'Nathan Walk', 'Hermann Kampermann', 'Adam Widomski', 'Maciej Ogrodnik', 'Michał Karpiński', 'Chiara Macchiavello', 'Dagmar Bruß', 'Nikolai Wyderka']","Quantum Key Distribution (QKD) is a promising technology for secure communication. Nevertheless, QKD is still treated with caution in certain contexts due to potential gaps between theoretical models and actual QKD implementations. A common assumption in security proofs is that the detection probability at the receiver, for a given input state, is independent of the measurement basis, which might not always be verified and could lead to security loopholes. This paper presents a security proof for QKD protocols that does not rely on the above assumption and is thus applicable in scenarios with detection probability mismatches, even when induced by the adversary. We demonstrate, through simulations, that our proof can extract positive key rates for setups vulnerable to large detection probability mismatches. This is achieved by monitoring whether an adversary is actively exploiting such vulnerabilities, instead of considering the worst-case scenario as in previous proofs. Our work highlights the importance of accounting for basis-dependent detection probabilities and provides a concrete solution for improving the security of practical QKD systems.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19874
To the Problem of Cosmic Expansion in Massive Gravity,['General Relativity and Quantum Cosmology'],"['Lavinia Heisenberg', 'Alessandro Longo', 'Giovanni Tambalo', 'Miguel Zumalacarregui']","We consider evolving, spatially flat isotropic and homogeneous (FLRW) cosmologies in ghost-free (dRGT) massive gravity. In this theory, no dynamical flat FLRW background exists if the reference metric is chosen to be Minkowski and the Stueckelberg fields are homogeneous. Relaxing the assumptions on the Stueckelberg profiles gives access to dynamical backgrounds. We propose a classification of the viable flat FLRW cosmological solutions of dRGT massive gravity. Instead of specifying an initial ansatz for the Stueckelberg fields $φ^a$ and the reference metric $f_{ab}$, we show that imposing homogeneity and isotropy on the square root tensor $X^μ_ν=\left(\sqrt{g^{-1}\partialφ^a \partialφ^bf_{ab}}\right)^μ_ν$ leads to dynamical cosmological solutions, and we characterize their properties. These solutions become dynamical only when the Stueckelberg fields acquire a sufficiently inhomogeneous and/or anisotropic profile. We explore the consequences for the minimal model and the complete dRGT theory, and show that perturbations are strongly coupled, at the quadratic level, on these backgrounds.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19873
AIDetx: a compression-based method for identification of machine-learning generated text,['Computation and Language'],"['Leonardo Almeida', 'Pedro Rodrigues', 'Diogo Magalhães', 'Armando J. Pinho', 'Diogo Pratas']","This paper introduces AIDetx, a novel method for detecting machine-generated text using data compression techniques. Traditional approaches, such as deep learning classifiers, often suffer from high computational costs and limited interpretability. To address these limitations, we propose a compression-based classification framework that leverages finite-context models (FCMs). AIDetx constructs distinct compression models for human-written and AI-generated text, classifying new inputs based on which model achieves a higher compression ratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores exceeding 97% and 99%, respectively, highlighting its high accuracy. Compared to current methods, such as large language models (LLMs), AIDetx offers a more interpretable and computationally efficient solution, significantly reducing both training time and hardware requirements (e.g., no GPUs needed). The full implementation is publicly available at https://github.com/AIDetx/AIDetx.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19869
The étendue of a combinatorial space and its dimension,['Category Theory'],['Matí as Menni'],"To each simplicial set $X$ we naturally assign an étendue ${É X}$ whose internal logic captures information about the geometry of $X$. In particular, we show that, for 'non-singular' objects $X$ and $Y$, the étendues ${É X}$ and ${É Y}$ are equivalent if, and only if, $X$ and $Y$ have the same dimension. Many of the results apply to presheaf toposes over 'well-founded' sites.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19863
Cross-Domain Recommendation Meets Large Language Models,['Information Retrieval'],"['Ajay Krishna Vajjala', 'Dipak Meher', 'Ziwei Zhu', 'David S. Rosenblum']","Cross-domain recommendation (CDR) has emerged as a promising solution to the cold-start problem, faced by single-domain recommender systems. However, existing CDR models rely on complex neural architectures, large datasets, and significant computational resources, making them less effective in data-scarce scenarios or when simplicity is crucial. In this work, we leverage the reasoning capabilities of large language models (LLMs) and explore their performance in the CDR domain across multiple domain pairs. We introduce two novel prompt designs tailored for CDR and demonstrate that LLMs, when prompted effectively, outperform state-of-the-art CDR baselines across various metrics and domain combinations in the rating prediction and ranking tasks. This work bridges the gap between LLMs and recommendation systems, showcasing their potential as effective cross-domain recommenders.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19862
Gravity's role in taming the Tayler instability in red giant cores,['Solar and Stellar Astrophysics'],"['Domenico G. Meduri', 'Rainer Arlt', 'Alfio Bonanno', 'Giovanni Licciardello']","The stability of toroidal magnetic fields within the interior of stars remains a significant unresolved issue in contemporary astrophysics. In this study, we combine a nonlocal linear analysis with 3D direct numerical simulations to examine the instability of toroidal fields within nonrotating, stably stratified stellar interiors in spherical geometry. Both analyses start from an equilibrium solution derived from balancing the Lorentz force with an anisotropic component of the fluid pressure, which is unstable to the (nonaxisymmetric) Tayler instability, and account for the combined effects of gravity and thermal diffusion. The numerical simulations incorporate finite magnetic resistivity and fluid viscosity while reaching a regime of highly stable stratification that has never been explored before. The linear analysis, which is global in the radial direction, shows that gravity significantly reduces the growth rate of the instability and uncovers the importance of unstable modes with low radial wavenumbers operating at low latitudes. The simulations trace the entire evolution of the instability from the linear to the nonlinear phase and strongly corroborate the findings of the linear analysis. Our results reveal that in highly stratified stellar interiors, the newly configured magnetic fields remain unstable only on the thermal diffusion timescale. Combining the linear analysis results with stellar evolution models of low-mass stars, we find that the limiting toroidal field strength for Tayler instability in red giant cores decreases with the stellar evolution. The predicted field strengths align with the ones expected from recent asteroseismic observations, suggesting that the observed fields may be remnants of a Tayler instability during the transition from the main sequence to the giant phase.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19849
Scaling Transformers for Low-Bitrate High-Quality Speech Coding,['Audio and Speech Processing'],"['Julian D Parker', 'Anton Smirnov', 'Jordi Pons', 'CJ Carr', 'Zack Zukowski', 'Zach Evans', 'Xubo Liu']","The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19842
Parallel Stacked Aggregated Network for Voice Authentication in IoT-Enabled Smart Devices,['Sound'],"['Awais Khan', 'Ijaz Ul Haq', 'Khalid Mahmood Malik']","Voice authentication on IoT-enabled smart devices has gained prominence in recent years due to increasing concerns over user privacy and security. The current authentication systems are vulnerable to different voice-spoofing attacks (e.g., replay, voice cloning, and audio deepfakes) that mimic legitimate voices to deceive authentication systems and enable fraudulent activities (e.g., impersonation, unauthorized access, financial fraud, etc.). Existing solutions are often designed to tackle a single type of attack, leading to compromised performance against unseen attacks. On the other hand, existing unified voice anti-spoofing solutions, not designed specifically for IoT, possess complex architectures and thus cannot be deployed on IoT-enabled smart devices. Additionally, most of these unified solutions exhibit significant performance issues, including higher equal error rates or lower accuracy for specific attacks. To overcome these issues, we present the parallel stacked aggregation network (PSA-Net), a lightweight framework designed as an anti-spoofing defense system for voice-controlled smart IoT devices. The PSA-Net processes raw audios directly and eliminates the need for dataset-dependent handcrafted features or pre-computed spectrograms. Furthermore, PSA-Net employs a split-transform-aggregate approach, which involves the segmentation of utterances, the extraction of intrinsic differentiable embeddings through convolutions, and the aggregation of them to distinguish legitimate from spoofed audios. In contrast to existing deep Resnet-oriented solutions, we incorporate cardinality as an additional dimension in our network, which enhances the PSA-Net ability to generalize across diverse attacks. The results show that the PSA-Net achieves more consistent performance for different attacks that exist in current anti-spoofing solutions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19841
Neuroplasticity and Psychedelics: a comprehensive examination of classic and non-classic compounds in pre and clinical models,['Neurons and Cognition'],"['Claudio Agnorelli', 'Meg Spriggs', 'Kate Godfrey', 'Gabriela Sawicka', 'Bettina Bohl', 'Hannah Douglass', 'Andrea Fagiolini', 'Hashemi Parastoo', 'Robin Carhart-Harris', 'David Nutt', 'David Erritzoe']","Neuroplasticity, the ability of the nervous system to adapt throughout an organism's lifespan, offers potential as both a biomarker and treatment target for neuropsychiatric conditions. Psychedelics, a burgeoning category of drugs, are increasingly prominent in psychiatric research, prompting inquiries into their mechanisms of action. Distinguishing themselves from traditional medications, psychedelics demonstrate rapid and enduring therapeutic effects after a single or few administrations, believed to stem from their neuroplasticity-enhancing properties. This review examines how classic psychedelics (e.g., LSD, psilocybin, N,N-DMT) and non-classic psychedelics (e.g., ketamine, MDMA) influence neuroplasticity. Drawing from preclinical and clinical studies, we explore the molecular, structural, and functional changes triggered by these agents. Animal studies suggest psychedelics induce heightened sensitivity of the nervous system to environmental stimuli (meta-plasticity), re-opening developmental windows for long-term structural changes (hyper-plasticity), with implications for mood and behavior. Translating these findings to humans faces challenges due to limitations in current imaging techniques. Nonetheless, promising new directions for human research are emerging, including the employment of novel positron-emission tomography (PET) radioligands, non-invasive brain stimulation methods, and multimodal approaches. By elucidating the interplay between psychedelics and neuroplasticity, this review informs the development of targeted interventions for neuropsychiatric disorders and advances understanding of psychedelics' therapeutic potential.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19840
Searching for MeV-mass neutrinophilic Dark Matter with Large Scale Dark Matter Detectors,['High Energy Physics - Phenomenology'],"['Anna M. Suliga', 'George M. Fuller']","The indirect detection of dark matter (DM) through its annihilation products is one of the primary strategies for DM detection. One of the least constrained classes of models is neutrinophilic DM, because the annihilation products, weakly interacting neutrinos, are challenging to observe. Here, we consider a scenario where MeV-mass DM exclusively annihilates to the third neutrino mass eigenstate, which is predominantly of tau and muon flavor. In such a scenario, the potential detection rate of the neutrinos originating from the DM annihilation in our galaxy in the conventional detectors would be suppressed by up to approximately two orders of magnitude. This is because the best sensitivity of such detectors for neutrinos with energies below approximately 100~MeV is for electron neutrino flavor. In this work, we highlight the potential of large-scale DM detectors in uncovering such signals in the tens of MeV range of DM masses. In addition, we discuss how coincident signals in direct detection DM experiments and upcoming neutrino detectors such as DUNE, Hyper-Kamiokande, and JUNO could provide new perspectives on the DM problem.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19836
Continuous Eddy Simulation (CES): Conceptual approach and applications,['Fluid Dynamics'],"['Stefan Heinz', 'Adeyemi Fagbade']","The simulation of high Reynolds number (Re) separated turbulent flows faces significant problems for decades: large eddy simulation (LES) is computationally too expensive, and Reynolds-averaged Navier-Stokes (RANS) methods and hybrid RANS-LES methods often provide unreliable results. This has serious consequences, we are currently unable to reliably predict very high Re regimes, which hampers applications and our understanding of turbulence structures. The paper reports the advantages of a strict mathematical approach, continuous eddy simulation (CES), to derive partially resolving turbulence models. In contrast to popular hybrid RANS-LES, this minimal error approach includes a dynamic modification of the turbulence model in response to the actual flow resolution: the model can increase (decrease) its contribution to the simulation in dependence of a low (high) flow resolution. This property is the essential requirement to seamlessly cover RANS and LES regimes. The CES modeling approach offers essential advantages regarding its functionality: basically, it is independent of a variety of simulation settings applied in popular hybrid RANS-LES to improve the model performance. In addition, the CES computational cost can be below the cost of other hybrid RANS-LES and LES by orders of magnitude. Essential simulation performance advantages of CES simulations are described here with respect to three complex flow applications: periodic hill flows at high Reynolds number, the NASA wall-mounted hump flow, and the Bachalo & Johnson axisymmetric transonic bump flow.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19834
"Identification of a monotone Boolean function with $k$ ""reasons"" as a combinatorial search problem",['Combinatorics'],"['Dániel Gerbner', 'András Imolay', 'Gyula O. H. Katona', 'Dániel T. Nagy', 'Kartal Nagy', 'Balázs Patkós', 'Domonkos Stadler', 'Kristóf Zólomy']","We study the number of queries needed to identify a monotone Boolean function $f:\{0,1\}^n \rightarrow \{0,1\}$. A query consists of a 0-1-sequence, and the answer is the value of $f$ on that sequence. It is well-known that the number of queries needed is $\binom{n}{\lfloor n/2\rfloor}+\binom{n}{\lfloor n/2\rfloor+1}$ in general. Here we study a variant where $f$ has $k$ ``reasons'' to be 1, i.e., its disjunctive normal form has $k$ conjunctions if the redundant conjunctions are deleted. This problem is equivalent to identifying an upfamily in $2^{[n]}$ that has exactly $k$ minimal members. We find the asymptotics on the number of queries needed for fixed $k$. We also study the non-adaptive version of the problem, where the queries are asked at the same time, and determine the exact number of queries for most values of $k$ and $n$.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19833
On Visser's inequality concerning coefficient estimates for a polynomial,['Complex Variables'],"['Suhail Gulzar', 'N. A. Rather', 'M. S Wani']","If $P(z)=\sum_{j=0}^{n}a_jz^j$ is a polynomial of degree $n$ having no zero in $|z|<1,$ then it was recently proved that for every $p\in[0,+\infty]$ and $s=0,1,\ldots,n-1,$ \begin{align*} \left\|a_nz+\frac{a_s}{\binom{n}{s}}\right\|_{p}\leq \frac{\left\|z+δ_{0s}\right\|_p}{\left\|1+z\right\|_p}\left\|P\right\|_{p}, \end{align*} where $δ_{0s}$ is the Kronecker delta. In this paper, we consider the class of polynomials having no zero in $|z|<ρ,$ $ρ\geq 1$ and obtain some generalizations of above inequality.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19831
A Tidy Data Structure and Visualisations for Multiple Variable Correlations and Other Pairwise Scores,['Computation'],"['Amit Chinwan', 'Catherine B. Hurley']","We provide a pipeline for calculating, managing and visualising correlations and other pairwise scores for numerical and categorical data. We present a uniform interface for calculating a plethora of pairwise scores and a new tidy data structure for managing the results. We also provide new visualisations which simultaneously show multiple and/or grouped pairwise scores. The visualisations are far richer than a traditional heatmap of correlation scores, as they help identify relationships with categorical variables, numeric variable pairs with non-linear associations or those which exhibit Simpson's paradox. These methods are available in our R package bullseye.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19830
Collective decision-making with heterogeneous biases: Role of network topology and susceptibility,['Physics and Society'],"['Yunus Sevinchan', 'Petro Sarkanych', 'Abi Tenenbaum', 'Yurij Holovatch', 'Pawel Romanczuk']","The ability of groups to make accurate collective decisions depends on a complex interplay of various factors, such as prior information, biases, social influence, and the structure of the interaction network. Here, we investigate a spin model that accounts for heterogeneous preferences and enables control over the non-linearity of social interactions. Building on previous results for complete graphs and regular 2D lattices, we investigate how the modification of network topology towards (sparse) random graphs can affect collective decision-making. We use two different measures of susceptibility to assess the responsiveness of the system to internal and external perturbations. In particular, we investigate how the maximum of susceptibility depends on network connectivity. Based on our findings, we discuss how collective systems might adapt to changes in environmental fluctuations by adjusting their network structure or the nature of their social interactions in order to remain in the region of maximal susceptibility.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19829
Classical transport in a maximally chaotic chain,['Chaotic Dynamics'],"['William Alderson', 'Rémy Dubertrand', 'Akira Shudo']","A model for a lattice of coupled cat maps has been recently introduced. This new and specific choice of the coupling makes the description especially easy and nontrivial quantities as Lyapunov exponents determined exactly. We studied the ergodic property of the dynamics along such a chain for a local perturbation. While the perturbation spreads across a front growing ballistically, the position and momentum profiles show large fluctuations due to chaos leading to diffusive transport in the phase space. It provides an example where the diffusion can be directly inferred from the microscopic chaos.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19828
Conducting High Frequency Radio SETI using ALMA,['Instrumentation and Methods for Astrophysics'],"['Louisa A Mason', 'Michael A Garrett', 'Kelvin Wandia', 'Andrew P V Siemion']","The Atacama Millimeter/Submillimeter Array (ALMA) remains unparalleled in sensitivity at radio frequencies above 35 GHz. In this paper, we explore ALMA's potential for narrowband technosignature detection, considering factors such as the interferometer's undistorted field of view, signal dilution due to significant drift rates at high frequencies and the possibility of spectral confusion. We present the first technosignature survey using archival ALMA data in Band 3, focusing on two spectral windows centred on 90.642 GHz and 93.151 GHz. Our survey places new limits at these frequencies on the prevalence of extraterrestrial transmitters for 28 galactic stars, selected from the Gaia DR3 catalogue. We employ a stellar 'bycatch' method to sample these objects within the undistorted field of view of four ALMA calibrators. For the closest star in our sample, we find no evidence of transmitters with EIRP_min > 7 x 10^17 W. To the best of our knowledge, this represents the first technosignature search conducted using ALMA data.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19827
On operators preserving inequalities between polynomials,['Functional Analysis'],"['S. Gulzar', 'Ravinder Kumar', 'Mudassir A Bhat']","In this review paper, we explore operator aspects in extremal properties of Bernstein-type polynomial inequalities. We shall also see that a linear operator which send polynomials to polynomials and have zero-preserving property naturally preserve Bernstein's inequality.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19821
Open and trapping channels in complex resonant media,['Optics'],"['Romain Rescanieres', 'Romain Pierrat', 'Arthur Goetschy']","We present a statistical study of the transmission and dwell-time matrices in disordered media composed of resonators, focusing on how frequency detuning influences their eigenvalue distributions. Our analysis reveals that the distribution of transmission eigenvalues undergoes a transition from a monomodal to a bimodal profile, and back to monomodal, as the frequency approaches the resonant frequency of the particles. Moreover, the distribution of dwell-time eigenvalues broadens significantly near resonance, with the longest lifetimes exceeding the median by several orders of magnitude. These results are explained by examining how frequency $ω$ affects the transport mean free path of light, $\ell(ω)$, and the energy transport velocity, $v_E(ω)$, which in turn shape the observed distributions. We demonstrate the strong potential of wavefront shaping to enhance both transmission and energy storage in resonant disordered media. In the diffusive regime, where the system thickness $L$ exceeds the mean free path, both transmission and dwell time can be enhanced by a factor $\varpropto L/\ell(ω) \gg 1$ when using wavefronts associated with the largest eigenvalues instead of plane waves. In the localized regime, the enhancements become $\varpropto Ne^{2L/ξ}$ for transmission and $\varpropto Nξ/L$ for dwell time, where $ξ$ is the localization length and $N$ is the number of controlled scattering channels. Finally, we show that employing high-$Q$ resonators instead of low-$Q$ ones increases energy storage within the medium by a factor of $\varpropto Q/k\ell(ω)$, in both the diffusive and localized regimes.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19818
Gaussian multi-target filtering with target dynamics driven by a stochastic differential equation,['Computer Vision and Pattern Recognition'],"['Ángel F. García-Fernández', 'Simo Särkkä']","This paper proposes multi-target filtering algorithms in which target dynamics are given in continuous time and measurements are obtained at discrete time instants. In particular, targets appear according to a Poisson point process (PPP) in time with a given Gaussian spatial distribution, targets move according to a general time-invariant linear stochastic differential equation, and the life span of each target is modelled with an exponential distribution. For this multi-target dynamic model, we derive the distribution of the set of new born targets and calculate closed-form expressions for the best fitting mean and covariance of each target at its time of birth by minimising the Kullback-Leibler divergence via moment matching. This yields a novel Gaussian continuous-discrete Poisson multi-Bernoulli mixture (PMBM) filter, and its approximations based on Poisson multi-Bernoulli and probability hypothesis density filtering. These continuous-discrete multi-target filters are also extended to target dynamics driven by nonlinear stochastic differential equations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19814
Certain Bernstein-type $L_p$ inequalities for polynomials,['Complex Variables'],"['N. A. Rather', 'Aijaz Bhat', 'Suhail Guzlar']","Let $P(z)$ be a polynomial of degree $n,$ then it is known that for $α\in\mathbb{C}$ with $|α|\leq \frac{n}{2},$ \begin{align*} \underset{|z|=1}{\max}|\left|zP^{\prime}(z)-αP(z)\right|\leq \left|n-α\right|\underset{|z|=1}{\max}|P(z)|. \end{align*} This inequality includes Bernstein's inequality, concerning the estimate for $|P^\prime(z)|$ over $|z|\leq 1,$ as a special case. In this paper, we extend this inequality to $L_p$ norm which among other things shows that the condition on $α$ can be relaxed. We also prove similar inequalities for polynomials with restricted zeros.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19811
Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures,['Sound'],"['Alain Riou', 'Antonin Gagneré', 'Gaëtan Hadjeres', 'Stefan Lattner', 'Geoffroy Peeters']","In this paper, we tackle the task of musical stem retrieval. Given a musical mix, it consists in retrieving a stem that would fit with it, i.e., that would sound pleasant if played together. To do so, we introduce a new method based on Joint-Embedding Predictive Architectures, where an encoder and a predictor are jointly trained to produce latent representations of a context and predict latent representations of a target. In particular, we design our predictor to be conditioned on arbitrary instruments, enabling our model to perform zero-shot stem retrieval. In addition, we discover that pretraining the encoder using contrastive learning drastically improves the model's performance.
  We validate the retrieval performances of our model using the MUSDB18 and MoisesDB datasets. We show that it significantly outperforms previous baselines on both datasets, showcasing its ability to support more or less precise (and possibly unseen) conditioning. We also evaluate the learned embeddings on a beat tracking task, demonstrating that they retain temporal structure and local information.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19806
On Monitoring Edge-Geodetic Sets of Dynamic Graph,['Combinatorics'],"['Zin Mar Myint', 'Ashish Saxena']","The concept of a monitoring edge-geodetic set (MEG-set) in a graph $G$, denoted $MEG(G)$, refers to a subset of vertices $MEG(G)\subseteq V(G)$ such that every edge $e$ in $G$ is monitored by some pair of vertices $ u, v \in MEG(G)$, where $e$ lies on all shortest paths between $u$ and $v$. The minimum number of vertices required to form such a set is called the monitoring edge-geodetic number, denoted $meg(G)$. The primary motivation for studying $MEG$-sets in previous works arises from scenarios in which certain edges are removed from $G$. In these cases, the vertices of the $MEG$-set are responsible for detecting these deletions. Such detection is crucial for identifying which edges have been removed from $G$ and need to be repaired. In real life, repairing these edges may be costly, or sometimes it is impossible to repair edges. In this case, the original $MEG$-set may no longer be effective in monitoring the modified graph. This highlights the importance of reassessing and adapting the $MEG$-set after edge deletions. This work investigates the monitoring edge-geodetic properties of graphs, focusing on how the removal of $k$ edges affects the structure of a graph and influences its monitoring capabilities. Specifically, we explore how the monitoring edge-geodetic number $meg(G)$ changes when $k$ edges are removed. The study aims to compare the monitoring properties of the original graph with those of the modified graph and to understand the impact of edge deletions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19800
INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge,['Computation and Language'],"['Angelika Romanou', 'Negar Foroutan', 'Anna Sotnikova', 'Zeming Chen', 'Sree Harsha Nelaturu', 'Shivalika Singh', 'Rishabh Maheshwary', 'Micol Altomare', 'Mohamed A. Haggag', 'Snegha A', 'Alfonso Amayuelas', 'Azril Hafizi Amirudin', 'Viraat Aryabumi', 'Danylo Boiko', 'Michael Chang', 'Jenny Chim', 'Gal Cohen', 'Aditya Kumar Dalmia', 'Abraham Diress', 'Sharad Duwal', 'Daniil Dzenhaliou', 'Daniel Fernando Erazo Florez', 'Fabian Farestam', 'Joseph Marvin Imperial', 'Shayekh Bin Islam']","The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19799
Linear methods for non-linear inverse problems,['Statistics Theory'],"['Geerten Koers', 'Botond Szabo', 'Aad van der Vaart']","We consider the recovery of an unknown function $f$ from a noisy observation of the solution $u_f$ to a partial differential equation that can be written in the form $\mathcal{L} u_f=c(f,u_f)$, for a differential operator $\mathcal{L}$ that is rich enough to recover $f$ from $\mathcal{L} u_f$. Examples include the time-independent Schrödinger equation $Δu_f = 2u_ff$, the heat equation with absorption term $(\partial_t -Δ_x/2) u_f=fu_f$, and the Darcy problem $\nabla\cdot (f \nabla u_f) = h$. We transform this problem into the linear inverse problem of recovering $\mathcal{L} u_f$ under the Dirichlet boundary condition, and show that Bayesian methods with priors placed either on $u_f$ or $\mathcal{L} u_f$ for this problem yield optimal recovery rates not only for $u_f$, but also for $f$. We also derive frequentist coverage guarantees for the corresponding Bayesian credible sets. Adaptive priors are shown to yield adaptive contraction rates for $f$, thus eliminating the need to know the smoothness of this function. The results are illustrated by numerical experiments on synthetic data sets.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19797
Voice Communication Analysis in Esports,['Sound'],"['Aymeric Vinot', 'Nicolas Perez']","In most team-based esports, voice communications are prominent in the team efficiency and synergy. In fact it has been observed that not only the skill aspect of the team but also the team effective voice communication comes into play when trying to have good performance in official matches. With the recent emergence of LLM (Large Language Models) tools regarding NLP (Natural Language Processing) (Vaswani et. al.), we decided to try applying them in order to have a better understanding on how to improve the effectiveness of the voice communications. In this paper the study has been made through the prism of League of Legends esport. However the main concepts and ideas can be easily applicable in any other team related esports.△ Less","21 November, 2024;",https://arxiv.org/pdf/2411.19793
Tractable Agreement Protocols,['Machine Learning'],"['Natalie Collina', 'Surbhi Goel', 'Varun Gupta', 'Aaron Roth']","We present an efficient reduction that converts any machine learning algorithm into an interactive protocol, enabling collaboration with another party (e.g., a human) to achieve consensus on predictions and improve accuracy. This approach imposes calibration conditions on each party, which are computationally and statistically tractable relaxations of Bayesian rationality. These conditions are sensible even in prior-free settings, representing a significant generalization of Aumann's classic ""agreement theorem.""
  In our protocol, the model first provides a prediction. The human then responds by either agreeing or offering feedback. The model updates its state and revises its prediction, while the human may adjust their beliefs. This iterative process continues until the two parties reach agreement. Initially, we study a setting that extends Aumann's Agreement Theorem, where parties aim to agree on a one-dimensional expectation by iteratively sharing their current estimates. Here, we recover the convergence theorem of Aaronson'05 under weaker assumptions. We then address the case where parties hold beliefs over distributions with d outcomes, exploring two feedback mechanisms. The first involves vector-valued estimates of predictions, while the second adopts a decision-theoretic approach: the human, needing to take an action from a finite set based on utility, communicates their utility-maximizing action at each round. In this setup, the number of rounds until agreement remains independent of d. Finally, we generalize to scenarios with more than two parties, where computational complexity scales linearly with the number of participants. Our protocols rely on simple, efficient conditions and produce predictions that surpass the accuracy of any individual party's alone.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19791
Polaron formation within quantum acoustics,['Mesoscale and Nanoscale Physics'],"['Alhun Aydin', 'Joonas Keski-Rahkonen', 'Anton M. Graf', 'Shaobing Yuan', 'Xiao-Yu Ouyang', 'Özgür E. Müstecaplıoğlu', 'Eric J. Heller']","The quantum acoustic framework has recently emerged as a non-perturbative, coherent approach to electron-lattice interactions, uncovering rich physics often obscured by perturbative methods with incoherent scattering events. Here, we model the coupled dynamics of electrons and acoustic lattice vibrations within this framework, representing lattice vibrations as coherent states and electrons as quantum wavepackets. We derive and numerically implement electron backaction on the lattice, providing both visual and quantitative insights into electron wavepacket evolution, self-trapping, and the formation of small acoustic polarons. We investigate polaron binding energies across varying material parameters and compute key observables-including mean square displacement, kinetic energy, potential energy, and vibrational energy-over time. Our findings reveal the conditions that favor polaron formation, which is enhanced by low temperatures, high deformation potential constants, slow sound velocities, high effective masses, and small Fermi surfaces. Additionally, we explore the impact of external electric and magnetic fields, showing that while polaron formation remains robust under moderate fields, it is weakly suppressed at higher field strengths. These results deepen our understanding of polaron dynamics and pave the way for future studies into non-trivial transport behavior in quantum materials.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19788
CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives,['Machine Learning'],"['Armin Saghafian', 'Amirmohammad Izadi', 'Negin Hashemi Dijujin', 'Mahdieh Soleymani Baghshah']","Grounding the instruction in the environment is a key step in solving language-guided goal-reaching reinforcement learning problems. In automated reinforcement learning, a key concern is to enhance the model's ability to generalize across various tasks and environments. In goal-reaching scenarios, the agent must comprehend the different parts of the instructions within the environmental context in order to complete the overall task successfully. In this work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a new framework to solve this problem using auxiliary loss functions inspired by video-text retrieval literature and a novel method called instruction tracking, which automatically keeps track of progress in an environment. The results of our experiments suggest superior sample efficiency and systematic generalization for this framework in multi-modal reinforcement learning problems. Our code base is available here.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19787
The 2024 Motile Active Matter Roadmap,['Soft Condensed Matter'],"['Gerhard Gompper', 'Howard A. Stone', 'Christina Kurzthaler', 'David Saintillan', 'Fernado Peruani', 'Dmitry A. Fedosov', 'Thorsten Auth', 'Cecile Cottin-Bizonne', 'Christophe Ybert', 'Eric Clement', 'Thierry Darnige', 'Anke Lindner', 'Raymond E. Goldstein', 'Benno Liebchen', 'Jack Binysh', 'Anton Souslov', 'Lucio Isa', 'Roberto di Leonardo', 'Giacomo Frangipane', 'Hongri Gu', 'Bradley J. Nelson', 'Fridtjof Brauns', 'M. Cristina Marchetti', 'Frank Cichos', 'Veit-Lorenz Heuthe']","Activity and autonomous motion are fundamental aspects of many living and engineering systems. Here, the scale of biological agents covers a wide range, from nanomotors, cytoskeleton, and cells, to insects, fish, birds, and people. Inspired by biological active systems, various types of autonomous synthetic nano- and micromachines have been designed, which provide the basis for multifunctional, highly responsive, intelligent active materials. A major challenge for understanding and designing active matter is their inherent non-equilibrium nature due to persistent energy consumption, which invalidates equilibrium concepts such as free energy, detailed balance, and time-reversal symmetry. Furthermore, interactions in ensembles of active agents are often non-additive and non-reciprocal. An important aspect of biological agents is their ability to sense the environment, process this information, and adjust their motion accordingly. It is an important goal for the engineering of micro-robotic systems to achieve similar functionality. With many fundamental properties of motile active matter now reasonably well understood and under control, the ground is prepared for the study of physical aspects and mechanisms of motion in complex environments, of the behavior of systems with new physical features like chirality, of the development of novel micromachines and microbots, of the emergent collective behavior and swarming of intelligent self-propelled particles, and of particular features of microbial systems. The vast complexity of phenomena and mechanisms involved in the self-organization and dynamics of motile active matter poses major challenges, which can only be addressed by a truly interdisciplinary effort involving scientists from biology, chemistry, ecology, engineering, mathematics, and physics.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19783
Observation of the open-charm tetraquark state $T_{cs 0}^{*}(2870)^0$ in the $B^- \rightarrow D^- D^0 K_\mathrm{S}^0$ decay,['High Energy Physics - Experiment'],"['LHCb collaboration', 'R. Aaij', 'A. S. W. Abdelmotteleb', 'C. Abellan Beteta', 'F. Abudinén', 'T. Ackernley', 'A. A. Adefisoye', 'B. Adeva', 'M. Adinolfi', 'P. Adlarson', 'C. Agapopoulou', 'C. A. Aidala', 'Z. Ajaltouni', 'S. Akar', 'K. Akiba', 'P. Albicocco', 'J. Albrecht', 'F. Alessio', 'M. Alexander', 'Z. Aliouche', 'P. Alvarez Cartelle', 'R. Amalric', 'S. Amato', 'J. L. Amey', 'Y. Amhis']","An amplitude analysis of $B^-\rightarrow D^- D^0 K_\mathrm{S}^0$ decays is performed using proton-proton collision data, corresponding to an integrated luminosity of $9\,\text{fb}^{-1}$, collected with the LHCb detector at center-of-mass energies of 7, 8, and 13$\mathrm{\,Te\kern -0.1em V}$. A resonant structure of spin-parity $0^+$ is observed in the $D^0 K_\mathrm{S}^0$ invariant-mass spectrum with a significance of $5.3\,σ$. The mass and width of the state, modeled with a Breit$-$Wigner lineshape, are determined to be $2883\pm11\pm6\mathrm{\,Me\kern -0.1em V\!/}c^2$ and $87_{-47}^{+22}\pm6\mathrm{\,Me\kern -0.1em V}$ respectively, where the first uncertainties are statistical and the second systematic. These properties and the quark content are consistent with those of the open-charm tetraquark state $T_{cs 0}^{*}(2870)^0$ observed previously in the $D^+ K^-$ final state of the $B^-\rightarrow D^- D^+ K^-$ decay. This result confirms the existence of the $T_{cs 0}^{*}(2870)^0$ state in a new decay mode. The $T_{cs1}^{*}(2900)^0$ state, reported in the $B^-\rightarrow D^- D^+ K^-$ decay, is also searched for in the $D^0 K_\mathrm{S}^0$ invariant-mass spectrum of the $B^- \rightarrow D^- D^0 K_\mathrm{S}^0$ decay, without finding evidence for it.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19781
Machine learning force-field model for kinetic Monte Carlo simulations of itinerant Ising magnets,['Statistical Mechanics'],"['Alexa Tyberg', 'Yunhao Fan', 'Gia-Wei Chern']","We present a scalable machine learning (ML) framework for large-scale kinetic Monte Carlo (kMC) simulations of itinerant electron Ising systems. As the effective interactions between Ising spins in such itinerant magnets are mediated by conducting electrons, the calculation of energy change due to a local spin update requires solving an electronic structure problem. Such repeated electronic structure calculations could be overwhelmingly prohibitive for large systems. Assuming the locality principle, a convolutional neural network (CNN) model is developed to directly predict the effective local field and the corresponding energy change associated with a given spin update based on Ising configuration in a finite neighborhood. As the kernel size of the CNN is fixed at a constant, the model can be directly scalable to kMC simulations of large lattices. Our approach is reminiscent of the ML force-field models widely used in first-principles molecular dynamics simulations. Applying our ML framework to a square-lattice double-exchange Ising model, we uncover unusual coarsening of ferromagnetic domains at low temperatures. Our work highlights the potential of ML methods for large-scale modeling of similar itinerant systems with discrete dynamical variables.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19780
Complete tripartite subgraphs of balanced tripartite graphs with large minimum degree,['Combinatorics'],"['Yihan Chen', 'Jialin He', 'Allan Lo', 'Cong Luo', 'Jie Ma', 'Yi Zhao']","In 1975 Bollobás, Erdős, and Szemerédi asked what minimum degree guarantees an octahedral subgraph $K_3(2)$ in any tripartite graph $G$ with $n$ vertices in each vertex class. We show that $δ(G)\geq n+2n^{\frac{5}{6}}$ suffices thus improving the bound $n+(1+o(1))n^{\frac{11}{12}}$ of Bhalkikar and Zhao obtained by following their approach. Bollobás, Erdős, and Szemerédi conjectured that $n+cn^{\frac{1}{2}}$ suffices and there are many $K_3(2)$-free tripartite graphs $G$ with $δ(G)\geq n+cn^{\frac{1}{2}}$. We confirm this conjecture under the additional assumption that every vertex in $G$ is adjacent to at least $(1/5+\varepsilon)n$ vertices in any other vertex class.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19773
Secure Filtering against Spatio-Temporal False Data under Asynchronous Sampling,['Systems and Control'],"['Zishuo Li', 'Anh Tung Nguyen', 'André M. H. Teixeira', 'Yilin Mo', 'Karl H. Johansson']","This paper addresses the state estimation problem in continuous LTI systems under attacks with non-periodic and asynchronous sampled measurements. The non-periodic and asynchronous sampling requires sensors to transmit not only the measurement values but also the sampling time-stamps to the fusion center via unprotected communication channels. This communication scheme leaves the system vulnerable to a variety of malicious activities such as (i) manipulating measurement values, (ii) manipulating time-stamps, (iii) hybrid manipulations such as generating fake measurements or eliminating the measurement. To deal with such more powerful attacks, we propose a decentralized local estimation algorithm where each sensor maintains its local state estimate based on its measurements in an asynchronous fashion. The local states are synchronized by time-prediction and fused in an event-triggered manner. In the absence of attacks, local estimates are proved to recover the optimal Kalman estimation by our carefully designed weighted least square problem, given that the sample time is non-pathological. In the presence of attacks, an $\ell_1$ regularized least square problem is proposed to generate secure estimates with uniformly bounded error as long as the observability redundancy is satisfied. The effectiveness of the proposed algorithm is demonstrated through a benchmark example of the IEEE 14-bus system.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19765
Pair Correlation of zeros of Dirichlet $L$-Functions: A path towards the Montgomery and Elliott-Halberstam Conjectures,['Number Theory'],"['Neelam Kandhil', 'Alessandro Languasco', 'Pieter Moree']","Assuming the Generalized Riemann Hypothesis and a pair correlation conjecture for the zeros of Dirichlet $L$-functions, we establish the truth of a conjecture of Montgomery (in its corrected form stated by Friedlander and Granville) on the magnitude of the error term in the prime number theorem in arithmetic progressions. As a consequence, we obtain that, under the same assumptions, the Elliott-Halberstam conjecture holds true.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19762
DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering,['Computer Vision and Pattern Recognition'],"['Yihao Wang', 'Marcus Klasson', 'Matias Turkulainen', 'Shuzhe Wang', 'Juho Kannala', 'Arno Solin']","Gaussian splatting enables fast novel view synthesis in static 3D environments. However, reconstructing real-world environments remains challenging as distractors or occluders break the multi-view consistency assumption required for accurate 3D reconstruction. Most existing methods rely on external semantic information from pre-trained models, introducing additional computational overhead as pre-processing steps or during optimization. In this work, we propose a novel method, DeSplat, that directly separates distractors and static scene elements purely based on volume rendering of Gaussian primitives. We initialize Gaussians within each camera view for reconstructing the view-specific distractors to separately model the static 3D scene and distractors in the alpha compositing stages. DeSplat yields an explicit scene separation of static elements and distractors, achieving comparable results to prior distractor-free approaches without sacrificing rendering speed. We demonstrate DeSplat's effectiveness on three benchmark data sets for distractor-free novel view synthesis. See the project website at https://aaltoml.github.io/desplat/.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19756
Quantum Dot Behaviour under the Influence of Non-Resonant Structured Laser Beams,['Mesoscale and Nanoscale Physics'],"['T. A. Sargsian', 'P. A. Mantashyan', 'D. B. Hayrapetyan']","The study investigates and compares the impact of intense, non-diffractive, non-resonant structured laser beams with various intensity profiles on the properties of InAs/GaAs cylindrical quantum dot. The comparative study demon-strates that the different structured beams, having unique symmetries and hence properties, have significant effects on the confinement potentials and electron probability densities of the quantum dot. It is shown that the system is most sensitive to the intensity and peak position changes of the zeroth order Bessel beam. Drastic changes in the dressed confinement potentials and electron probability densities under the impact of the above-mentioned beams were demonstrated, which can lead to the usage of the following method in such applications, where the precise manipulation of the charge carrier location is required. This study provides new insights into the role of structured laser fields in quantum dot systems, offering possibilities for their use in advanced nanophotonics and quantum information technologies.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19752
Templicial nerve of an A-infinity category,['Category Theory'],"['Violeta Borges Marques', 'Arne Mertens']","The framework of templicial vector spaces was put forth in arXiv:2302.02484v2 as a suitable generalization of simplicial sets in order to develop a theory of enriched quasi-categories, called quasi-categories in vector spaces. We construct a lift of Faonte's $A_{\infty}$-nerve arXiv:1312.2127v2 which lands in templicial vector spaces. Further, we show that when restricted to dg-categories, this nerve recovers the templicial dg-nerve of arXiv:2005.04778v4, and that the nerve of any $A_{\infty}$-category is a quasi-category in vector spaces.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19751
Totally elliptic surface group representations,['Representation Theory'],['Arnaud Maret'],We characterize all totally elliptic surface group representations into $\mathrm{PSL}_2\mathbb{R}$ by showing that they are either representations into a compact subgroup or Deroin--Tholozan representations.△ Less,"29 November, 2024;",https://arxiv.org/pdf/2411.19748
"A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining Off-Road, Diversity, and Directional Consistency Losses",['Computer Vision and Pattern Recognition'],"['Ahmad Rahimi', 'Alexandre Alahi']","Trajectory prediction is essential for the safety and efficiency of planning in autonomous vehicles. However, current models often fail to fully capture complex traffic rules and the complete range of potential vehicle movements. Addressing these limitations, this study introduces three novel loss functions: Offroad Loss, Direction Consistency Error, and Diversity Loss. These functions are designed to keep predicted paths within driving area boundaries, aligned with traffic directions, and cover a wider variety of plausible driving scenarios. As all prediction modes should adhere to road rules and conditions, this work overcomes the shortcomings of traditional ""winner takes all"" training methods by applying the loss functions to all prediction modes. These loss functions not only improve model training but can also serve as metrics for evaluating the realism and diversity of trajectory predictions. Extensive validation on the nuScenes and Argoverse 2 datasets with leading baseline models demonstrates that our approach not only maintains accuracy but significantly improves safety and robustness, reducing offroad errors on average by 47% on original and by 37% on attacked scenes. This work sets a new benchmark for trajectory prediction in autonomous driving, offering substantial improvements in navigating complex environments. Our code is available at https://github.com/vita-epfl/stay-on-track .△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19747
HVAC-DPT: A Decision Pretrained Transformer for HVAC Control,['Machine Learning'],['Anaïs Berkes'],"Building operations consume approximately 40% of global energy, with Heating, Ventilation, and Air Conditioning (HVAC) systems responsible for up to 50% of this consumption. As HVAC energy demands are expected to rise, optimising system efficiency is crucial for reducing future energy use and mitigating climate change. Existing control strategies lack generalisation and require extensive training and data, limiting their rapid deployment across diverse buildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformer using in-context Reinforcement Learning (RL) for multi-zone HVAC control. HVAC-DPT frames HVAC control as a sequential prediction task, training a causal transformer on interaction histories generated by diverse RL agents. This approach enables HVAC-DPT to refine its policy in-context, without modifying network parameters, allowing for deployment across different buildings without the need for additional training or data collection. HVAC-DPT reduces energy consumption in unseen buildings by 45% compared to the baseline controller, offering a scalable and effective approach to mitigating the increasing environmental impact of HVAC systems.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19746
On Multi-Split Continuity and Split Homeomorphisms,['General Topology'],"['Finn Michler', 'Argha Ghosh']","We introduce multi-split continuous functions between topological spaces, a weaker form of continuity that generalizes split continuity while being stable under compositions. We will define the associated star multifunction and pre-multi-split multifunctions. Moreover, we will prove that multi-split continuity naturally emerges as the continuity property of selections of finite usco maps, relating their study to set-valued analysis. Finally, we introduce split homeomorphisms and split homeomorphic spaces, showing that for compact, regular Hausdorff spaces, split homeomorphisms characterize deformations with cuts and subsequent re-glues.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19745
Amplifying human performance in combinatorial competitive programming,['Machine Learning'],"['Petar Veličković', 'Alex Vitvitskyi', 'Larisa Markeeva', 'Borja Ibarz', 'Lars Buesing', 'Matej Balog', 'Alexander Novikov']","Recent years have seen a significant surge in complex AI systems for competitive programming, capable of performing at admirable levels against human competitors. While steady progress has been made, the highest percentiles still remain out of reach for these methods on standard competition platforms such as Codeforces. Here we instead focus on combinatorial competitive programming, where the target is to find as-good-as-possible solutions to otherwise computationally intractable problems, over specific given inputs. We hypothesise that this scenario offers a unique testbed for human-AI synergy, as human programmers can write a backbone of a heuristic solution, after which AI can be used to optimise the scoring function used by the heuristic. We deploy our approach on previous iterations of Hash Code, a global team programming competition inspired by NP-hard software engineering problems at Google, and we leverage FunSearch to evolve our scoring functions. Our evolved solutions significantly improve the attained scores from their baseline, successfully breaking into the top percentile on all previous Hash Code online qualification rounds, and outperforming the top human teams on several. Our method is also performant on an optimisation problem that featured in a recent held-out AtCoder contest.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19744
The role of ammonia in the distribution of volatiles in the primordial hydrosphere of Europa,['Earth and Planetary Astrophysics'],"['Alizée Amsler Moulanier', 'Olivier Mousis', 'Alexis Bouquet', 'Christopher R. Glein']","The presence of a hydrosphere on Europa raises questions about its habitability, and studies of its volatile inventory can provide insight into its formation process. Different scenarios suggest that Europa's volatiles could be derived from cometary ices or devolatilized building blocks. The study of post-accretion processes, in particular the ""open ocean"" phase that likely occurred before the formation of the icy crust, is crucial to distinguish these origins, as this phase is likely to have influenced the volatile inventory. The abundance of ammonia in Europa's building blocks is also crucial for understanding the composition of its ocean and primordial atmosphere. We aim to investigate ocean-atmosphere equilibrium during the post-accretion period by varying the ammonia fraction in the atmosphere. Our model evaluates the vapor-liquid equilibrium of water and volatiles, as well as the chemical equilibrium within the ocean, to study Europa's early hydrosphere. We explore two initial conditions: one in which Europa's hydrosphere originates from comet-like building blocks, and another in which it forms in equilibrium with a thick, CO$_2$-rich atmosphere. In both scenarios, the initial ratio of accreted CO$_2$ to NH$_3$ determines the magnitude of their partial pressures in Europa's early atmosphere. If this ratio exceeds a certain threshold (set to $10^{-4}$ in this study), the atmosphere will be CO$_2$-rich; otherwise, it will be CO$_2$-depleted by multiple orders of magnitude. Overall, our work provides a initial assessment of the distribution of primordial volatiles in Europa's primitive hydrosphere, and provides a baseline for interpreting data from the upcoming Europa Clipper mission.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19743
Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph,['Machine Learning'],"['Heloisa Oss Boll', 'Ali Amirahmadi', 'Amira Soliman', 'Stefan Byttner', 'Mariana Recamonde-Mendoza']","Objective: In modern healthcare, accurately predicting diseases is a crucial matter. This study introduces a novel approach using graph neural networks (GNNs) and a Graph Transformer (GT) to predict the incidence of heart failure (HF) on a patient similarity graph at the next hospital visit. Materials and Methods: We used electronic health records (EHR) from the MIMIC-III dataset and applied the K-Nearest Neighbors (KNN) algorithm to create a patient similarity graph using embeddings from diagnoses, procedures, and medications. Three models - GraphSAGE, Graph Attention Network (GAT), and Graph Transformer (GT) - were implemented to predict HF incidence. Model performance was evaluated using F1 score, AUROC, and AUPRC metrics, and results were compared against baseline algorithms. An interpretability analysis was performed to understand the model's decision-making process. Results: The GT model demonstrated the best performance (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168). Although the Random Forest (RF) baseline achieved a similar AUPRC value, the GT model offered enhanced interpretability due to the use of patient relationships in the graph structure. A joint analysis of attention weights, graph connectivity, and clinical features provided insight into model predictions across different classification groups. Discussion and Conclusion: Graph-based approaches such as GNNs provide an effective framework for predicting HF. By leveraging a patient similarity graph, GNNs can capture complex relationships in EHR data, potentially improving prediction accuracy and clinical interpretability.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19742
A New Look at Disk Winds and External Photoevaporation in the $σ$-Orionis Cluster,['Solar and Stellar Astrophysics'],"['K. Maucó', 'C. F. Manara', 'A. Bayo', 'J. Hernández', 'J. Campbell-White', 'N. Calvet', 'G. Ballabio', 'M. L. Aru', 'J. M. Alcalá', 'M. Ansdell', 'C. Briceño', 'S. Facchini', 'T. J. Haworth', 'M. McClure', 'J. P. Williams']","Disk winds play a crucial role in the evolution of protoplanetary disks. Typical conditions for star and planet formation are in regions with intermediate or strong UV radiation fields produced by massive stars. The $σ$-Orionis cluster is the ideal site to study disk winds under these conditions; its outer parts can be used to study disk evolution, while its innermost regions to study the effect of external irradiation. For this, we analyze the $\rm [OI]\,λ$6300, $\rm [NII]\,λ$6583, and $\rm [SII]\,λ$6731,$λ$6716 lines using high-resolution MIKE spectra of 27 classical T Tauri stars and complemented by intermediate-resolution X-shooter data. We decompose the line profiles into multiple Gaussian components. We calculated luminosities, line ratios, and kinematic properties of these components. We found that the $\rm [OI]\,λ$6300 line luminosity and kinematic properties are similar to those found in low-mass star-forming regions (SFRs). The frequency of single-component $\rm [OI]\,λ$6300 line profiles reflects the expected evolutionary stage given the intermediate age of $σ$-Orionis. This points to internal processes contributing to the line emission. However, the highly irradiated disks do not follow the accretion - [OI] luminosity relation found in low-mass SFRs, and all exhibit single-component line profiles. Line ratios of highly ionized species of [NII] and [SII] show higher ratios than typical values found in low-mass SFRs. The innermost regions of $σ$-Orionis are clearly affected by external irradiation, evidenced by the lack of correlation in the accretion - [OI] luminosity relation. The broad line widths of close-in sources, however, indicate a contribution from internal processes, such as magnetohydrodynamical winds and/or internal photoevaporation. This suggests a coevolution of internal and external winds in $σ$-Orionis.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19741
Extensive analysis of reconstruction algorithms for DESI 2024 baryon acoustic oscillations,['Cosmology and Nongalactic Astrophysics'],"['X. Chen', 'Z. Ding', 'E. Paillas', 'S. Nadathur', 'H. Seo', 'S. Chen', 'N. Padmanabhan', 'M. White', 'A. de Mattia', 'P. McDonald', 'A. J. Ross', 'A. Variu', 'A. Carnero Rosell', 'B. Hadzhiyska', 'M. M. S Hanif', 'D. Forero-Sánchez', 'S. Ahlen', 'O. Alves', 'U. Andrade', 'S. BenZvi', 'D. Bianchi', 'D. Brooks', 'E. Chaussidon', 'T. Claybaugh', 'A. de la Macorra']","Reconstruction of the baryon acoustic oscillation (BAO) signal has been a standard procedure in BAO analyses over the past decade and has helped to improve the BAO parameter precision by a factor of ~2 on average. The Dark Energy Spectroscopic Instrument (DESI) BAO analysis for the first year (DR1) data uses the ``standard'' reconstruction framework, in which the displacement field is estimated from the observed density field by solving the linearized continuity equation in redshift space, and galaxy and random positions are shifted in order to partially remove nonlinearities. There are several approaches to solving for the displacement field in real survey data, including the multigrid (MG), iterative Fast Fourier Transform (iFFT), and iterative Fast Fourier Transform particle (iFFTP) algorithms. In this work, we analyze these algorithms and compare them with various metrics including two-point statistics and the displacement itself using realistic DESI mocks. We focus on three representative DESI samples, the emission line galaxies (ELG), quasars (QSO), and the bright galaxy sample (BGS), which cover the extreme redshifts and number densities, and potential wide-angle effects. We conclude that the MG and iFFT algorithms agree within 0.4% in post-reconstruction power spectrum on BAO scales with the RecSym convention, which does not remove large-scale redshift space distortions (RSDs), in all three tracers. The RecSym convention appears to be less sensitive to displacement errors than the RecIso convention, which attempts to remove large-scale RSDs. However, iFFTP deviates from the first two; thus, we recommend against using iFFTP without further development. In addition, we provide the optimal settings for reconstruction for five years of DESI observation. The analyses presented in this work pave the way for DESI DR1 analysis as well as future BAO analyses.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19738
PKS 1424-418: A persistent candidate source of the mm$-γ$-ray connection?,['High Energy Astrophysical Phenomena'],"['Dae-Won Kim', 'Eduardo Ros', 'Matthias Kadler', 'Thomas P. Krichbaum', 'Guang-Yao Zhao', 'Florian Rösch', 'Andrei P. Lobanov', 'J. Anton Zensus']","We present a long-term strong correlation between millimeter (mm) radio and $γ$-ray emission in the flat-spectrum radio quasar (FSRQ) PKS 1424-418. The mm$-γ$-ray connection in blazars is generally thought to originate from the relativistic jet close to the central engine. We confirm a unique long-lasting mm$-γ$-ray correlation of PKS 1424-418 by using detailed correlation analyses and statistical tests, and we find its physical meaning in the source. We employed ~8.5 yr of (sub)mm and $γ$-ray light curves observed by ALMA and Fermi-LAT, respectively. From linear and cross-correlation analyses between the light curves, we found a significant, strong mm$-γ$-ray correlation over the whole period. We did not find any notable time delay within the uncertainties for the mm$-γ$-ray correlation, which means zero lag. The mm wave spectral index values (S$_ν$ $\propto$ $ν_α$) between the band 3 and 7 flux densities indicate a time-variable opacity of the source at (sub)mm wavelengths. Interestingly, the mm wave spectral index becomes temporarily flatter (i.e., $α$ > $-$0.5) when the source flares in the $γ$-rays. We relate our results with the jet of PKS 1424-418, and we discuss the origin of the $γ$-rays and opacity of the inner (sub)parsec-scale jet regions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19737
A Note on Small Percolating Sets on Hypercubes via Generative AI,['Machine Learning'],"['Gergely Bérczi', 'Adam Zsolt Wagner']","We apply a generative AI pattern-recognition technique called PatternBoost to study bootstrap percolation on hypercubes. With this, we slightly improve the best existing upper bound for the size of percolating subsets of the hypercube.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19734
A Deep Learning Approach to Language-independent Gender Prediction on Twitter,['Computation and Language'],"['Reyhaneh Hashempour', 'Barbara Plank', 'Aline Villavicencio', 'Renato Cordeiro de Amorim']","This work presents a set of experiments conducted to predict the gender of Twitter users based on language-independent features extracted from the text of the users' tweets. The experiments were performed on a version of TwiSty dataset including tweets written by the users of six different languages: Portuguese, French, Dutch, English, German, and Italian. Logistic regression (LR), and feed-forward neural networks (FFNN) with back-propagation were used to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual (CL). In the IL setting, the training and testing were performed on the same language whereas in the CL, Italian and German datasets were set aside and only used as test sets and the rest were combined to compose training and development sets. In the IL, the highest accuracy score belongs to LR whereas in the CL, FFNN with three hidden layers yields the highest score. The results show that neural network based models underperform traditional models when the size of the training set is small; however, they beat traditional models by a non-trivial margin, when they are fed with large enough data. Finally, the feature analysis confirms that men and women have different writing styles independent of their language.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19733
Risk-Averse Certification of Bayesian Neural Networks,['Machine Learning'],"['Xiyue Zhang', 'Zifan Wang', 'Yulong Gao', 'Licio Romao', 'Alessandro Abate', 'Marta Kwiatkowska']","In light of the inherently complex and dynamic nature of real-world environments, incorporating risk measures is crucial for the robustness evaluation of deep learning models. In this work, we propose a Risk-Averse Certification framework for Bayesian neural networks called RAC-BNN. Our method leverages sampling and optimisation to compute a sound approximation of the output set of a BNN, represented using a set of template polytopes. To enhance robustness evaluation, we integrate a coherent distortion risk measure--Conditional Value at Risk (CVaR)--into the certification framework, providing probabilistic guarantees based on empirical distributions obtained through sampling. We validate RAC-BNN on a range of regression and classification benchmarks and compare its performance with a state-of-the-art method. The results show that RAC-BNN effectively quantifies robustness under worst-performing risky scenarios, and achieves tighter certified bounds and higher efficiency in complex tasks.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19729
Towards Santali Linguistic Inclusion: Building the First Santali-to-English Translation Model using mT5 Transformer and Data Augmentation,['Computation and Language'],"['Syed Mohammed Mostaque Billah', 'Ateya Ahmed Subarna', 'Sudipta Nandi Sarna', 'Ahmad Shawkat Wasit', 'Anika Fariha', 'Asif Sushmit', 'Arig Yousuf Sadeque']","Around seven million individuals in India, Bangladesh, Bhutan, and Nepal speak Santali, positioning it as nearly the third most commonly used Austroasiatic language. Despite its prominence among the Austroasiatic language family's Munda subfamily, Santali lacks global recognition. Currently, no translation models exist for the Santali language. Our paper aims to include Santali to the NPL spectrum. We aim to examine the feasibility of building Santali translation models based on available Santali corpora. The paper successfully addressed the low-resource problem and, with promising results, examined the possibility of creating a functional Santali machine translation model in a low-resource setup. Our study shows that Santali-English parallel corpus performs better when in transformers like mt5 as opposed to untrained transformers, proving that transfer learning can be a viable technique that works with Santali language. Besides the mT5 transformer, Santali-English performs better than Santali-Bangla parallel corpus as the mT5 has been trained in way more English data than Bangla data. Lastly, our study shows that with data augmentation, our model performs better.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19726
A Statistical Study of Lopsided Galaxies using Random Forest,['Astrophysics of Galaxies'],"['Valentina Fontirroig', 'Facundo A. Gomez', 'Marcelo Jaque Arancibia', 'Arianna Dolfi', 'Nicolas Monsalves']","Lopsided galaxies are late-type galaxies with a non-axisymmetric disc due to an uneven distribution of their stellar mass. Despite being a relatively common perturbation, several questions regarding its origin and the information that can be extracted from them about the evolutionary history of late-type galaxies. The advent of several large multi-band photometric surveys will allow us to statistically analyze this perturbation, with information that was not previously available. Given the strong correlation between lopsidedness and the structural properties of the galaxies, this paper aims to develop a method to automatically classify late-type galaxies between lopsided and symmetric. We seek to explore if an accurate classification can be obtain by only considering their internal properties, without additional information about the environment. We select 8000 late-type galaxies from TNG50. A Fourier decomposition of their stellar mass surface density is used to label galaxies as lopsided and symmetric. We trained a Random Forest classifier to rapidly and automatically identify this type of perturbations, exclusively using galaxies internal properties. We test different algorithms to deal with the imbalance of our data and select the most suitable approach based on the considered metrics. We show that our trained algorithm can provide a very accurate and rapid classification of lopsided galaxies. The excellent results obtained by our classifier strongly supports the hypothesis that lopsidedness is mainly a tracer of galaxies internal structures. We show that similar results can be obtained using observable quantities, readily obtainable from multi-bad photometric surveys. Our results show it allows a rapid and accurate classification of lopsided galaxies, allowing us to explore whether lopsidedness in present-day disc galaxies is connected to galaxies specific evolutionary histories.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19723
JetFormer: An Autoregressive Generative Model of Raw Images and Text,['Machine Learning'],"['Michael Tschannen', 'André Susano Pinto', 'Alexander Kolesnikov']","Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we further streamline joint generative modeling of images and text. We propose an autoregressive decoder-only transformer - JetFormer - which is trained to directly maximize the likelihood of raw data, without relying on any separately pretrained components, and can understand and generate both text and images. Specifically, we leverage a normalizing flow model to obtain a soft-token image representation that is jointly trained with an autoregressive multimodal transformer. The normalizing flow model serves as both an image encoder for perception tasks and an image decoder for image generation tasks during inference. JetFormer achieves text-to-image generation quality competitive with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained image autoencoders, which are trained with a complex mixture of losses, including perceptual ones. At the same time, JetFormer demonstrates robust image understanding capabilities. To the best of our knowledge, JetFormer is the first model that is capable of generating high-fidelity images and producing strong log-likelihood bounds.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19722
Lattice dynamics of the frustrated kagome compound Y-kapellasite,['Strongly Correlated Electrons'],"['P. Doležal', 'T. Biesner', 'Y. Li', 'R. Mathew Roy', 'S. Roh', 'R. Valentí', 'M. Dressel', 'P. Puphal', 'A. Pustogow']","Studying the magnetic ground states of frustrated antiferromagnets provides unique insight into the stability of quantum spin liquids, even if the anticipated state is not realized towards T = 0. Particularly relevant are structural modifications setting in at temperatures where the magnetic correlations come into play. Here we explore the lattice dynamics of Y-kapellasite (Y3Cu9(OH)19Cl8) single crystals by infrared spectroscopy in combination with ab initio calculations. We observe significant changes in the phonon spectra at Ts = 32 K, that gradually evolve down to low temperatures. The increase in the number of phonon modes provides evidence for a lowering of symmetry and we discuss several possibilities of crystal structure modifications. Our analysis also reveals that the structural variation involves exclusively H and O atoms, while the other atoms remain rather unaffected. An 8% red shift of the lowest-lying phonon mode upon cooling indicates strong magneto-elastic effects upon decoupling Cu-6f hexagons through the lattice vibrations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19720
Forensics Adapter: Adapting CLIP for Generalizable Face Forgery Detection,['Computer Vision and Pattern Recognition'],"['Xinjie Cui', 'Yuezun Li', 'Ao Luo', 'Jiaran Zhou', 'Junyu Dong']","We describe the Forensics Adapter, an adapter network designed to transform CLIP into an effective and generalizable face forgery detector. Although CLIP is highly versatile, adapting it for face forgery detection is non-trivial as forgery-related knowledge is entangled with a wide range of unrelated knowledge. Existing methods treat CLIP merely as a feature extractor, lacking task-specific adaptation, which limits their effectiveness. To address this, we introduce an adapter to learn face forgery traces -- the blending boundaries unique to forged faces, guided by task-specific objectives. Then we enhance the CLIP visual tokens with a dedicated interaction strategy that communicates knowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its versatility is highly retained, naturally ensuring strong generalizability in face forgery detection. With only $\bm{5.7M}$ trainable parameters, our method achieves a significant performance boost, improving by approximately $\bm{7\%}$ on average across five standard datasets. We believe the proposed method can serve as a baseline for future CLIP-based face forgery detection methods.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19715
CantorNet: A Sandbox for Testing Geometrical and Topological Complexity Measures,['Neural and Evolutionary Computing'],"['Michal Lewandowski', 'Hamid Eghbalzadeh', 'Bernhard A. Moser']","Many natural phenomena are characterized by self-similarity, for example the symmetry of human faces, or a repetitive motif of a song. Studying of such symmetries will allow us to gain deeper insights into the underlying mechanisms of complex systems. Recognizing the importance of understanding these patterns, we propose a geometrically inspired framework to study such phenomena in artificial neural networks. To this end, we introduce \emph{CantorNet}, inspired by the triadic construction of the Cantor set, which was introduced by Georg Cantor in the $19^\text{th}$ century. In mathematics, the Cantor set is a set of points lying on a single line that is self-similar and has a counter intuitive property of being an uncountably infinite null set. Similarly, we introduce CantorNet as a sandbox for studying self-similarity by means of novel topological and geometrical complexity measures. CantorNet constitutes a family of ReLU neural networks that spans the whole spectrum of possible Kolmogorov complexities, including the two opposite descriptions (linear and exponential as measured by the description length). CantorNet's decision boundaries can be arbitrarily ragged, yet are analytically known. Besides serving as a testing ground for complexity measures, our work may serve to illustrate potential pitfalls in geometry-ignorant data augmentation techniques and adversarial attacks.△ Less",v1,https://arxiv.org/pdf/2411.19713
Predictive orientational phase behavior in convex polyhedral entropic crystals,['Soft Condensed Matter'],"['Sumitava Kundu', 'Kaustav Chakraborty', 'Avisek Das']","Hard convex polyhedra, idealized models for anisotropic colloids and nanoparticles, are known to form variety of orientational phases despite the regular arrangement of particles in the crystalline assemblies. Based on the orientational behavior of the constituents particles, such phases could be categorized into freely rotating plastic crystals (PC), discrete plastic crystals (DPC) and orientationally ordered crystals (OC). In this article, we report an extensive Monte Carlo computer simulation study of sixty hard convex polyhedral shape indicating a direct predictive relationship between the nature of orientational phases in the crystalline assemblies and single-particle shape attributes. The influence of three attributes namely; (i) Isoperimetric Quotient (IQ) i.e., the extent of asphericity; (ii) isotropy of the moment of inertia tensor in the principal frame and (iii) number of symmetry operations in the point group of the particle and self-assembled crystal structure, were observed to control the orientational phase behavior of the entire solid region in many-body system. The translational order in the crystal appeared to play significant role only in the DPC phase, where as, other two phases were completely governed by the combination of two attributes. In this study, the role of shape attributes were characterized by sequential appearance of one or two of the aforementioned rotational phases across the phase diagram in a pressure dependent manner which could be regarded as an important stepping stone towards fully predictive self-assembly behavior of hard particle systems.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19707
Challenges and Opportunities for Global Cellular Connectivity,['Networking and Internet Architecture'],"['Viktoria Vomhoff', 'Hyunseok Daniel Jang', 'Matteo Varvello', 'Stefan Geißler', 'Yasir Zaki', 'Tobias Hoßfeld', 'Andra Lutu']","Traditional cellular service was designed for global connectivity, but business and logistical constraints led to its fragmentation, with deployments limited to individual countries and regions. Initiatives like Mobile Virtual Network Operators (MVNOs), Mobile Network Aggregators (MNAs), and regulations like ''roam-like-at-home'' have partially restored global service potential, though often at high costs in terms of user bills, application performance, and traffic efficiency. This paper makes two key contributions: first, it surveys the global cellular ecosystem, analyzing the strengths and weaknesses of major players using data from prior research, proprietary datasets, and public sources. Second, it argues that the technology for seamless global service exists in Local Breakout (LBO), a roaming architecture which allows user traffic to be routed directly to the Internet through the visited network, bypassing the home network and/or third-party infrastructures. However, LBO adoption is hindered by issues such as policy enforcement, billing, and Quality of Service (QoS) guarantees, rooted in a lack of trust between operators. The paper concludes by exploring technological advances that could enable LBO, and pave the way for truly global cellular connectivity.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19706
On zeros of discrete paraorthogonal polynomials on the unit circle,['Classical Analysis and ODEs'],"['G. Gordillo-Núñez', 'A. Suzuki']","In this note we investigate, as a natural continuation of [K. Castillo, Constr. Approx., 55 (2022) 605-627], the behaviour of the zeros of discrete paraorthogonal polynomials on the unit circle with respect to a real parameter.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19705
Fast Mutual Information Computation for Large Binary Datasets,['Machine Learning'],['Andre O. Falcao'],"Mutual Information (MI) is a powerful statistical measure that quantifies shared information between random variables, particularly valuable in high-dimensional data analysis across fields like genomics, natural language processing, and network science. However, computing MI becomes computationally prohibitive for large datasets where it is typically required a pairwise computational approach where each column is compared to others. This work introduces a matrix-based algorithm that accelerates MI computation by leveraging vectorized operations and optimized matrix calculations. By transforming traditional pairwise computational approaches into bulk matrix operations, the proposed method enables efficient MI calculation across all variable pairs. Experimental results demonstrate significant performance improvements, with computation times reduced up to 50,000 times in the largest dataset using optimized implementations, particularly when utilizing hardware optimized frameworks. The approach promises to expand MI's applicability in data-driven research by overcoming previous computational limitations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19702
Precision benchmarks for solids: G0W0 calculations with different basis sets,['Materials Science'],"['Maryam Azizi', 'Francisco A. Delesma', 'Matteo Giantomassi', 'Davis Zavickis', 'Mikael Kuisma', 'Kristian Thyghesen', 'Dorothea Golze', 'Alexander Buccheri', 'Min-Ye Zhang', 'Patrick Rinke', 'Claudia Draxl', 'Andris Gulans', 'Xavier Gonze']","The GW approximation within many-body perturbation theory is the state of the art for computing quasiparticle energies in solids. Typically, Kohn-Sham (KS) eigenvalues and eigenfunctions, obtained from a Density Functional Theory (DFT) calculation are used as a starting point to build the Green's function G and the screened Coulomb interaction W, yielding the one-shot G0W0 selfenergy if no further update of these quantities are made. Multiple implementations exist for both the DFT and the subsequent G0W0 calculation, leading to possible differences in quasiparticle energies. In the present work, the G0W0 quasiparticle energies for states close to the band gap are calculated for six crystalline solids, using four different codes: Abinit, exciting, FHI-aims, and GPAW. This comparison helps to assess the impact of basis-set types (planewaves versus localized orbitals) and the treatment of core and valence electrons (all-electron full potentials versus pseudopotentials). The impact of unoccupied states as well as the algorithms for solving the quasiparticle equation are also briefly discussed. For the KS-DFT band gaps, we observe good agreement between all codes, with differences not exceeding 0.1 eV, while the G0W0 results deviate on the order of 0.1-0.3 eV. Between all-electron codes (FHI-aims and exciting), the agreement is better than 15 meV for KS-DFT and, with one exception, about 0.1 eV for G0W0 band gaps.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19701
Explaining the Impact of Training on Vision Models via Activation Clustering,['Computer Vision and Pattern Recognition'],"['Ahcène Boubekki', 'Samuel G. Fadel', 'Sebastian Mair']","Recent developments in the field of explainable artificial intelligence (XAI) for vision models investigate the information extracted by their feature encoder. We contribute to this effort and propose Neuro-Activated Vision Explanations (NAVE), which extracts the information captured by the encoder by clustering the feature activations of the frozen network to be explained. The method does not aim to explain the model's prediction but to answer questions such as which parts of the image are processed similarly or which information is kept in deeper layers. Experimentally, we leverage NAVE to show that the training dataset and the level of supervision affect which concepts are captured. In addition, our method reveals the impact of registers on vision transformers (ViT) and the information saturation caused by the watermark Clever Hans effect in the training set.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19700
Observation of a non-reciprocal skyrmion Hall effect of hybrid chiral skyrmion tubes in synthetic antiferromagnetic multilayers,['Mesoscale and Nanoscale Physics'],"['Takaaki Dohi', 'Mona Bhukta', 'Fabian Kammerbauer', 'Venkata Krishna Bharadwaj', 'Ricardo Zarzuela', 'Aakanksha Sud', 'Maria-Andromachi Syskaki', 'Duc Minh Tran', 'Sebastian Wintz', 'Markus Weigand', 'Simone Finizio', 'Jörg Raabe', 'Robert Frömter', 'Jairo Sinova', 'Mathias Kläui']","Topological spin textures in magnetic materials beyond two-dimensional skyrmions have attracted attention for electronics beyond CMOS technologies. In particular, three-dimensional (3D) topological spin textures are promising due to the expected complex non-linear dynamics as well as high static and dynamic thermal stability. In multilayer heterostructures, a hybrid chiral skyrmion tube is a well-known example of a 3D topological spin texture, exhibiting an intriguing chirality transition along the thickness direction. This transition progresses from left-handed to right-handed Néel-type chirality, passing through a Bloch-type intermediate state. Such an exotic spin configuration potentially exhibits distinctly different dynamics from that of the common skyrmion tube that exhibits a homogeneous chirality; yet these dynamics have not been ascertained so far. Here we reveal the distinct features of current-induced dynamics that result from the hybrid chiral skyrmion tube structure in synthetic antiferromagnetic (SyAFM) multilayers. Strikingly, the SyAFM hybrid chiral skyrmion tubes exhibit a non-reciprocal skyrmion Hall effect in the flow regime. The non-reciprocity can even be tuned by the degree of magnetic compensation in the SyAFM systems. Our theoretical modeling qualitatively corroborates that the non-reciprocity stems from the dynamic oscillation of skyrmion helicity during its current-induced motion. The findings highlight the critical role of the internal degrees of freedom of these complex skyrmion tubes for their current-induced dynamics.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19698
MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating Multi-Insight Multi-Document Extraction Tasks,['Computation and Language'],"['John Francis', 'Saba Esnaashari', 'Anton Poletaev', 'Sukankana Chakraborty', 'Youmna Hashem', 'Jonathan Bright']","Large language models (LLMs) have demonstrated remarkable capabilities in text analysis tasks, yet their evaluation on complex, real-world applications remains challenging. We define a set of tasks, Multi-Insight Multi-Document Extraction (MIMDE) tasks, which involves extracting an optimal set of insights from a document corpus and mapping these insights back to their source documents. This task is fundamental to many practical applications, from analyzing survey responses to processing medical records, where identifying and tracing key insights across documents is crucial. We develop an evaluation framework for MIMDE and introduce a novel set of complementary human and synthetic datasets to examine the potential of synthetic data for LLM evaluation. After establishing optimal metrics for comparing extracted insights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis reveals a strong correlation (0.71) between the ability of LLMs to extracts insights on our two datasets but synthetic data fails to capture the complexity of document-level analysis. These findings offer crucial guidance for the use of synthetic data in evaluating text analysis systems, highlighting both its potential and limitations.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19689
MIRI Deep Imaging Survey (MIDIS) of the Hubble Ultra Deep Field,['Astrophysics of Galaxies'],"['Göran Östlin', 'Pablo G. Pérez-González', 'Jens Melinder', 'Steven Gillman', 'Edoardo Iani', 'Luca Costantin', 'Leindert A. Boogaard', 'Pierluigi Rinaldi', 'Luis Colina', 'Hans Ulrik Nørgaard-Nielsen', 'Daniel Dicken', 'Thomas R. Greve', 'Gillian Wright', 'Almudena Alonso-Herrero', 'Javier Alvarez-Marquez', 'Marianna Annunziatella', 'Arjan Bik', 'Sarah E. I. Bosman', 'Karina I. Caputi', 'Alejandro Crespo Gomez', 'Andreas Eckart', 'Macarena Garcia-Marin', 'Jens Hjorth', 'Olivier Ilbert', 'Iris Jermann']","The recently launched James Webb Space Telescope (JWST) is opening new observing windows on the distant universe. Among JWST's instruments, the Mid Infrared Instrument (MIRI) offers the unique capability of imaging observations at wavelengths $λ> 5μ$m. This enables unique access to the rest frame near infra-red (NIR, $λ\ge 1$\mum) emission from galaxies at redshifts $z>4$ and the visual ($λ\gtrsim 5000$Å) rest frame for $z>9$. We here report on the guaranteed time observations (GTO) from the MIRI European Consortium, of the Hubble Ultra Deep Field (HUDF), forming the MIRI Deep Imaging Survey (MIDIS), consisting of an on source integration time of $\sim41$ hours in the MIRI/F560W (5.6 $μ$m) filter. To our knowledge, this constitutes the longest single filter exposure obtained with JWST of an extragalactic field as yet.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19686
Periodicity shadows II. Computational aspects,['Representation Theory'],"['Jerzy Białkowski', 'Adam Skowyrski']","This article provides the second part of the research initiated in arXiv:2411.17381, where we introduced and investigated so called periodicity shadows, which are special skew-symmetric matrices related to symmetric algebras with periodic simple modules. In arXiv:2411.17381 we focused on theoretical aspects, whereas here we present complementary cosiderations concerning computational issues. Namely, we discuss an algorithm, which computes all tame periodicity shadows of given size (see Section 2), and then present lists of all tame periodicity shadows of small sizes, that is at most 6.△ Less",v1,https://arxiv.org/pdf/2411.19682
Heavy-light mesons from a flavour-dependent interaction,['High Energy Physics - Phenomenology'],"['Fei Gao', 'Angel S. Miramontes', 'Joannis Papavassiliou', 'Jan M. Pawlowski']","We introduce a new symmetry-preserving framework for the physics of heavy-light mesons, whose key element is the effective incorporation of flavour-dependent contributions into the corresponding bound-state and quark gap equations. These terms originate from the fully-dressed quark-gluon vertices appearing in the kernels of these equations, and provide a natural distinction between ``light"" and ``heavy"" quarks. In this approach, only the classical form factor of the quark-gluon vertex is retained, and is evaluated in the so-called ``symmetric"" configuration. The standard Slavnov-Taylor identity links this form factor to the quark wave-function, allowing for the continuous transition from light to heavy quarks through the mere variation of the current quark mass in the gap equation. The method is used to compute the masses and decay constants of specific pseudoscalars and vector heavy-light systems, showing good overall agreement with both experimental data and lattice simulations.△ Less",v1,https://arxiv.org/pdf/2411.19680
Delegated verification of quantum randomness with linear optics,['Quantum Physics'],"['Rodrigo Piera', 'Jaideep Singh', 'Yury Kurochkin', 'James A. Grieve']","Randomness is a critical resource of modern cryptosystems. Quantum mechanics offers the best properties of an entropy source in terms of unpredictability. However, these sources are often fragile and can fail silently. Therefore, statistical tests on their outputs should be performed continuously. Testing a sequence for randomness can be very resource-intensive, especially for longer sequences, and transferring this to other systems can put the secrecy at risk. In this paper, we present a method that allows a third party to publicly perform statistical testing without compromising the confidentiality of the random bits by connecting the quality of a public sequence to the private sequence generated using a quantum process. We implemented our protocol over two different optical systems and compared them.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19677
Vibrational excitations in magnetic triangular nanographenes,['Mesoscale and Nanoscale Physics'],"['Nils Krane', 'Elia Turco', 'Annika Bernhardt', 'Michal Juríček', 'Roman Fasel', 'Pascal Ruffieux']","Inelastic electron tunneling spectroscopy (IETS) is a powerful measurement technique often used in scanning tunneling spectroscopy to probe excited states of various nanostructures, e.g., the magnetic properties of complex spin systems. The observed excited states can be of magnetic and vibrational origin and it is therefore necessary to differentiate between these two excitation mechanisms. Here, we investigate the spin S = 1/2 phenalenyl radical on Au(111). IETS measurements feature inelastic excitations, whereas the spatial distribution of their intensity excludes any spin excitations. Comparison to theoretical simulations proves the vibrational origin of those excitations and allows us to assign the observed features to distinct vibrational modes.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19670
Multimodal Whole Slide Foundation Model for Pathology,['Image and Video Processing'],"['Tong Ding', 'Sophia J. Wagner', 'Andrew H. Song', 'Richard J. Chen', 'Ming Y. Lu', 'Andrew Zhang', 'Anurag J. Vaidya', 'Guillaume Jaume', 'Muhammad Shaban', 'Ahrong Kim', 'Drew F. K. Williamson', 'Bowen Chen', 'Cristina Almagro-Perez', 'Paul Doucet', 'Sharifa Sahai', 'Chengkuan Chen', 'Daisuke Komura', 'Akihiro Kawabe', 'Shumpei Ishikawa', 'Georg Gerber', 'Tingying Peng', 'Long Phi Le', 'Faisal Mahmood']","The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL). However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose TITAN, a multimodal whole slide foundation model pretrained using 335,645 WSIs via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any finetuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that TITAN outperforms both ROI and slide foundation models across machine learning settings such as linear probing, few-shot and zero-shot classification, rare cancer retrieval and cross-modal retrieval, and pathology report generation.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19666
Retrieving wind properties from the ultra-hot dayside of WASP-189b with CRIRES$^+$,['Earth and Planetary Astrophysics'],"['F. Lesjak', 'L. Nortmann', 'D. Cont', 'F. Yan', 'A. Reiners', 'N. Piskunov', 'A. Hatzes', 'L. Boldt-Christmas', 'S. Czesla', 'A. Lavail', 'E. Nagel', 'A. D. Rains', 'M. Rengel', 'U. Seemann', 'D. Shulyak']","The extreme temperature gradients from day- to nightside in the atmospheres of hot Jupiters generate fast winds in the form of equatorial jets or day-to-night flows. Observations of blue-shifted and red-shifted signals in the transmission and dayside spectra of WASP-189b have sparked discussions about the nature of winds on this planet. To investigate the structure of winds in the atmosphere of the ultra-hot Jupiter WASP-189b, we studied its dayside emission spectrum with CRIRES$^+$ in the spectral K band. We used the cross-correlation method to detect emission signals of CO and Fe, and employed a Bayesian framework to retrieve the atmospheric parameters relating to the temperature-pressure structure and chemistry. The retrieval incorporated a numerical model of the line profile influenced by various dynamic effects to determine the wind structure. The cross-correlation signals of CO and Fe showed a velocity offset of ~6km/s, which could be caused by a fast day-to-night wind in the atmosphere of WASP-189b. The atmospheric retrieval showed that the line profile of the observed spectra is best fitted by the presence of a day-to-night wind of 4.4km/s, while the retrieved equatorial jet velocity of 1.0km/s is consistent with the absence of such a jet. Such a wind pattern is consistent with the observed line broadening and can explain the majority of the velocity offset, while uncertainties in the ephemerides and the effects of a hot spot could also contribute to this offset. We further retrieved an inverted temperature-pressure profile and determined the C/O ratio and metallicity. We showed that red-shifts of a few km/s in the dayside spectra could be explained by day-to-night winds. Further studies combining transmission and dayside observations could advance our understanding of WASP-189b's atmospheric circulation by improving the uncertainties in the velocity offset and wind parameters.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19662
Real-time surface plasmon polariton propagation in silver nanowires,['Optics'],"['Wenhua Zhao', 'Álvaro Rodríguez Echarri', 'Alberto Eljarrat', 'Hannah C. Nerl', 'Thomas Kiel', 'Benedikt Haas', 'Henry Halim', 'Yan Lu', 'Kurt Busch', 'Christoph T. Koch']","Electron microscopy techniques such as electron energy-loss spectroscopy (EELS) facilitate the spatio-spectral characterization of plasmonic nanostructures. In this work, a time-dependent perspective is presented, which significantly enhances the utility of EELS. Specifically, silver nanowires offer the material and geometric features for various high-quality plasmonic excitations. This provides an ideal illustrative system for combined experimental-theoretical analyses of the different plasmonic excitations and their real-time dynamics. It is demonstrated how the plasmonic excitations propagating inside the wire repeatedly interact with the swift electrons in an EELS configuration. In addition, the role of azimuthal modes, often overlooked for very thin wires, is observed and analyzed in both the energy-loss spectrum and the dynamical perspective. Such a complete understanding of the interaction of electrons and plasmonic excitation is key for the design of efficient plasmonic sensors, the study of hot electron dynamics in metals, and applications in the context of electron quantum optics, where full control of the spatial and temporal characteristics of the fields at the nanometer and femtosecond scales is highly desirable.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19661
Large Nernst effect in Te-based van der Waals materials,['Materials Science'],"['M. Behnami', 'M. Gillig', 'A. G. Moghaddam', 'D. V. Efremov', 'G. Shipunov', 'B. R. Piening', 'I. V. Morozov', 'S. Aswartham', 'J. Dufouleur', 'K. Ochkan', 'J. Zemen', 'V. Kocsis', 'C. Hess', 'M. Putti', 'B. Büchner', 'F. Caglieris', 'H. Reichlova']","Layered van der Waals tellurides reveal topologically non-trivial properties that give rise to unconventional magneto-transport phenomena. Additionally, their semimetallic character with high mobility makes them promising candidates for large magneto-thermoelectric effects. Remarkable studies on the very large and unconventional Nernst effect in WTe$_2$ have been reported, raising questions about whether this property is shared across the entire family of van der Waals tellurides.
  In this study, systematic measurements of the Nernst effect in telluride van der Waals Weyl semimetals are presented. Large linear Nernst coefficients in WTe$_2$ and MoTe$_2$ are identified, and moderate Nernst coefficients with non-linear behavior in magnetic fields are observed in W$_{0.65}$Mo$_{0.35}$Te$_2$, TaIrTe$_4$, and TaRhTe$_4$. Within this sample set, a correlation between the dominant linear-in-magnetic-field component of the Nernst coefficient and mobility is established, aligning with the established Nernst scaling framework, though with a different scaling factor compared to existing literature. This enhancement might be caused by the shared favorable electronic band structure of this family of materials. Conversely, the non-linear component of the Nernst effect in a magnetic field could not be correlated with mobility. This non-linear term is almost absent in the binary compounds, suggesting a multiband origin and strong compensation between electron-like and hole-like carriers. This comprehensive study highlights the potential of van der Waals tellurides for thermoelectric conversion.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19660
Design of structured La$_{2-x}$Sr$_{x}$CuO$_{4}$ films as superconducting transition-edge sensors at 4.2K,['Superconductivity'],"['M. M. Botana', 'A. S. Viz', 'M. V. Ramallo']","We calculate the effects of carrier-density structuration and patterning on thin films of the cuprate superconductor La$_{2-x}$Sr$_{x}$CuO$_{4}$, in order to optimize its functional characteristics as sensing material for resistive transition-edge bolometers at liquid-He temperature. We perform finite-element computations considering two major contributions to structuration: The intrinsic random nanoscale disorder associated to carrier density nonstoichiometry, plus the imposition of regular arrangements of zones with different nominal carrier densities. Using ad-hoc seek algorithms, we obtain various structuration designs that markedly improve the bolometric performance, mainly the saturation power and dynamic range. Bolometric operation becomes favorable even in the easier-to-implement constant current mode of measurement.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19656
Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis,['Computation and Language'],"['Alessandro Scirè', 'Andrei Stefan Bejgu', 'Simone Tedeschi', 'Karim Ghonim', 'Federico Martelli', 'Roberto Navigli']","After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent.
  Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification.
  To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems.
  Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field.△ Less",v1,https://arxiv.org/pdf/2411.19655
Nonparametric Instrumental Regression via Kernel Methods is Minimax Optimal,['Machine Learning'],"['Dimitri Meunier', 'Zhu Li', 'Tim Christensen', 'Arthur Gretton']","We study the kernel instrumental variable algorithm of \citet{singh2019kernel}, a nonparametric two-stage least squares (2SLS) procedure which has demonstrated strong empirical performance. We provide a convergence analysis that covers both the identified and unidentified settings: when the structural function cannot be identified, we show that the kernel NPIV estimator converges to the IV solution with minimum norm. Crucially, our convergence is with respect to the strong $L_2$-norm, rather than a pseudo-norm. Additionally, we characterize the smoothness of the target function without relying on the instrument, instead leveraging a new description of the projected subspace size (this being closely related to the link condition in inverse learning literature). With the subspace size description and under standard kernel learning assumptions, we derive, for the first time, the minimax optimal learning rate for kernel NPIV in the strong $L_2$-norm. Our result demonstrates that the strength of the instrument is essential to achieve efficient learning. We also improve the original kernel NPIV algorithm by adopting a general spectral regularization in stage 1 regression. The modified regularization can overcome the saturation effect of Tikhonov regularization.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19653
Ice inventory towards the protostar Ced 110 IRS4 observed with the James Webb Space Telescope. Results from the ERS Ice Age program,['Solar and Stellar Astrophysics'],"['W. R. M. Rocha', 'M. K. McClure', 'J. A. Sturm', 'T. L. Beck', 'Z. L. Smith', 'H. Dickinson', 'F. Sun', 'E. Egami', 'A. C. A. Boogert', 'H. J. Fraser', 'E. Dartois', 'I. Jimenez-Serra', 'J. A. Noble', 'J. Bergner', 'P. Caselli', 'S. B. Charnley', 'J. Chiar', 'L. Chu', 'I. Cooke', 'N. Crouzet', 'E. F. van Dishoeck', 'M. N. Drozdovskaya', 'R. Garrod', 'D. Harsono', 'S. Ioppolo']","This work focuses on the ice features toward the binary protostellar system Ced 110 IRS 4A and 4B, and observed with JWST as part of the Early Release Science Ice Age collaboration. We aim to explore the JWST observations of the binary protostellar system Ced~110~IRS4A and IRS4B to unveil and quantify the ice inventories toward these sources. We compare the ice abundances with those found for the same molecular cloud. The analysis is performed by fitting or comparing laboratory infrared spectra of ices to the observations. Spectral fits are carried out with the ENIIGMA fitting tool that searches for the best fit. For Ced~110~IRS4B, we detected the major ice species H$_2$O, CO, CO$_2$ and NH$_3$. All species are found in a mixture except for CO and CO$_2$, which have both mixed and pure ice components. In the case of Ced~110~IRS4A, we detected the same major species as in Ced~110~IRS4B, as well as the following minor species CH$_4$, SO$_2$, CH$_3$OH, OCN$^-$, NH$_4^+$ and HCOOH. Tentative detection of N$_2$O ice (7.75~$μ$m), forsterite dust (11.2~$μ$m) and CH$_3^+$ gas emission (7.18~$μ$m) in the primary source are also presented. Compared with the two lines of sight toward background stars in the Chameleon I molecular cloud, the protostar has similar ice abundances, except in the case of the ions that are higher in IRS4A. The clearest differences are the absence of the 7.2 and 7.4~$μ$m absorption features due to HCOO$^-$ and icy complex organic molecules in IRS4A and evidence of thermal processing in both IRS4A and IRS4B as probed by the CO$_2$ ice features. We conclude that the binary protostellar system Ced~110~IRS4A and IRS4B has a large inventory of icy species. The similar ice abundances in comparison to the starless regions in the same molecular cloud suggest that the chemical conditions of the protostar were set at earlier stages in the molecular cloud.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19651
Enhancing Security in Third-Party Library Reuse -- Comprehensive Detection of 1-day Vulnerability through Code Patch Analysis,['Software Engineering'],"['Shangzhi Xu', 'Jialiang Dong', 'Weiting Cai', 'Juanru Li', 'Arash Shaghaghi', 'Nan Sun', 'Siqi Ma']","Nowadays, software development progresses rapidly to incorporate new features. To facilitate such growth and provide convenience for developers when creating and updating software, reusing open-source software (i.e., thirdparty library reuses) has become one of the most effective and efficient methods. Unfortunately, the practice of reusing third-party libraries (TPLs) can also introduce vulnerabilities (known as 1-day vulnerabilities) because of the low maintenance of TPLs, resulting in many vulnerable versions remaining in use. If the software incorporating these TPLs fails to detect the introduced vulnerabilities and leads to delayed updates, it will exacerbate the security risks. However, the complicated code dependencies and flexibility of TPL reuses make the detection of 1-day vulnerability a challenging task. To support developers in securely reusing TPLs during software development, we design and implement VULTURE, an effective and efficient detection tool, aiming at identifying 1-day vulnerabilities that arise from the reuse of vulnerable TPLs. It first executes a database creation method, TPLFILTER, which leverages the Large Language Model (LLM) to automatically build a unique database for the targeted platform. Instead of relying on code-level similarity comparison, VULTURE employs hashing-based comparison to explore the dependencies among the collected TPLs and identify the similarities between the TPLs and the target projects. Recognizing that developers have the flexibility to reuse TPLs exactly or in a custom manner, VULTURE separately conducts version-based comparison and chunk-based analysis to capture fine-grained semantic features at the function levels. We applied VULTURE to 10 real-world projects to assess its effectiveness and efficiency in detecting 1-day vulnerabilities. VULTURE successfully identified 175 vulnerabilities from 178 reused TPLs.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19648
CAdam: Confidence-Based Optimization for Online Learning,['Machine Learning'],"['Shaowen Wang', 'Anan Liu', 'Jian Xiao', 'Huan Liu', 'Yuekui Yang', 'Cong Xu', 'Qianqian Pu', 'Suncong Zheng', 'Wei Zhang', 'Jian Li']","Modern recommendation systems frequently employ online learning to dynamically update their models with freshly collected data. The most commonly used optimizer for updating neural networks in these contexts is the Adam optimizer, which integrates momentum ($m_t$) and adaptive learning rate ($v_t$). However, the volatile nature of online learning data, characterized by its frequent distribution shifts and presence of noises, poses significant challenges to Adam's standard optimization process: (1) Adam may use outdated momentum and the average of squared gradients, resulting in slower adaptation to distribution changes, and (2) Adam's performance is adversely affected by data noise. To mitigate these issues, we introduce CAdam, a confidence-based optimization strategy that assesses the consistence between the momentum and the gradient for each parameter dimension before deciding on updates. If momentum and gradient are in sync, CAdam proceeds with parameter updates according to Adam's original formulation; if not, it temporarily withholds updates and monitors potential shifts in data distribution in subsequent iterations. This method allows CAdam to distinguish between the true distributional shifts and mere noise, and adapt more quickly to new data distributions. Our experiments with both synthetic and real-world datasets demonstrate that CAdam surpasses other well-known optimizers, including the original Adam, in efficiency and noise robustness. Furthermore, in large-scale A/B testing within a live recommendation system, CAdam significantly enhances model performance compared to Adam, leading to substantial increases in the system's gross merchandise volume (GMV).△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19647
A frequency tunable low-noise YIG-GGG based oscillator with strong magneto-elastic coupling,['Applied Physics'],"['Paolo Sgarro', 'Roman Ovcharov', 'Roman Khymyn', 'Sambit Ghosh', 'Ahmad A. Awad', 'Johan Åkerman', 'Artem Litvinenko']","We present a frequency tunable magneto-acoustic oscillator (MAO) operating in low-phase-noise and complex dynamical regimes based on a single composite YIG-GGG resonator. The magneto-acoustic resonator (MAR) is based on a YIG (yttrium iron garnet) layer epitaxially grown on a GGG (gadolinium gallium garnet) substrate. By optimizing the YIG thickness, we obtain a high magneto-elastic coupling of around 1 MHz between the ferromagnetic resonance (FMR) in YIG and high overtone acoustic resonances (HBARs) in the YIG-GGG structure in the 1-2 GHz frequency range. It allows to eliminate the need for pre-selectors and bulky circulators, thus simplifying the MAO design while maintaining the possibility to lock to HBAR YIG-GGG modes. With an adjustment in the loop over-amplification parameter, the MAO can be locked either only to high-Q magneto-acoustic HBARs or to both types of resonance including HBARs and the FMR mode of the YIG film. In a low-phase-noise regime, MAO generates only at certain values of the applied field and exhibits discrete frequency tunability with a 3.281 MHz step corresponding to the frequency separation between the adjacent HBAR modes in a YIG-GGG structure. In a complex regime where oscillation conditions expand to include both HBAR and FMR modes, MAO demonstrates continuous generation as the function of the applied field with variable phase noise parameters. Moreover, in low-phase-noise regime, MAO phase noise plot improves by 30 dB compared to the operational regime locked to the pure FMR in YIG which is in agreement with the measured FMR and HBAR Q-factors.△ Less",v1,https://arxiv.org/pdf/2411.19646
Gradient projection method for constrained quantum control,['Quantum Physics'],"['Oleg Morzhin', 'Alexander Pechen']","In this work, we adopt the Gradient Projection Method (GPM) to problems of quantum control. For general $N$-level closed and open quantum systems, we derive the corresponding adjoint systems and gradients of the objective functionals, and provide the projection versions of the Pontryagin maximum principle and the GPM, all directly in terms of quantum objects such as evolution operator, Hamiltonians, density matrices, etc. Various forms of the GPM, including one- and two-step, are provided and compared. We formulate the GPM both for closed and open quantum systems, latter for the general case with simultaneous coherent and incoherent controls. The GPM is designed to perform local gradient based optimization in the case when bounds are imposed on the controls. The main advantage of the method is that it allows to exactly satisfy the bounds, in difference to other approaches such as adding constraints as weight to objective. We apply the GPM to several examples including generation of one- and two-qubit gates and two-qubit Bell and Werner states for models of superconducting qubits under the constraint when controls are zero at the initial and final times, and steering an open quantum system state to a target density matrix for simulating action of the Werner-Holevo channel, etc.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19644
Measurement of the Inclusive Cross Sections of Prompt $J/ψ$ and $ψ(3686)$ Production in $e^{+}e^{-}$ Annihilation from $\sqrt{s}=3.808$ to $4.951$ GeV,['High Energy Physics - Experiment'],"['BESIII Collaboration', 'M. Ablikim', 'M. N. Achasov', 'P. Adlarson', 'X. C. Ai', 'R. Aliberti', 'A. Amoroso', 'M. R. An', 'Q. An', 'Y. Bai', 'O. Bakina', 'I. Balossino', 'Y. Ban', 'V. Batozskaya', 'K. Begzsuren', 'N. Berger', 'M. Berlowski', 'M. Bertani', 'D. Bettoni', 'F. Bianchi', 'E. Bianco', 'A. Bortone', 'I. Boyko', 'R. A. Briere', 'A. Brueggemann']","The inclusive cross sections of prompt $J/ψ$ and $ψ(3686)$ production are measured at center-of-mass energies from 3.808 to 4.951 GeV. The dataset used is 22 fb$^{-1}$ of $e^{+}e^{-}$ annihilation data collected with the BESIII detector operating at the BEPCII storage ring. The results obtained are in agreement with the previous BESIII measurements of exclusive $J/ψ$ and $ψ(3686)$ production. The average values obtained for the cross sections measured in the center-of-mass energy ranges from 4.527 to 4.951 GeV for $J/ψ$ and from 4.843 to 4.951 GeV for $ψ(3686)$, where the impact of known resonances is negligible, are $14.0\pm1.7\pm3.1$ pb and $15.3\pm3.0$ pb, respectively. For $J/ψ$, the first and the second uncertainties are statistical and systematic, respectively. For $ψ(3686)$, the uncertainty is total. These values are useful for testing charmonium production models.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19642
Isotropy testing in spatial point patterns: nonparametric versus parametric replication under misspecification,['Methodology'],"['Jakub J. Pypkowski', 'Adam M. Sykulski', 'James S. Martin']","Several hypothesis testing methods have been proposed to validate the assumption of isotropy in spatial point patterns. A majority of these methods are characterised by an unknown distribution of the test statistic under the null hypothesis of isotropy. Parametric approaches to approximating the distribution involve simulation of patterns from a user-specified isotropic model. Alternatively, nonparametric replicates of the test statistic under isotropy can be used to waive the need for specifying a model. In this paper, we first develop a general framework which allows for the integration of a selected nonparametric replication method into isotropy testing. We then conduct a large simulation study comprising application-like scenarios to assess the performance of tests with different parametric and nonparametric replication methods. In particular, we explore distortions in test size and power caused by model misspecification, and demonstrate the advantages of nonparametric replication in such scenarios.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19633
PACMANN: Point Adaptive Collocation Method for Artificial Neural Networks,['Numerical Analysis'],"['Coen Visser', 'Alexander Heinlein', 'Bianca Giovanardi']","Physics-Informed Neural Networks (PINNs) are an emerging tool for approximating the solution of Partial Differential Equations (PDEs) in both forward and inverse problems. PINNs minimize a loss function which includes the PDE residual determined for a set of collocation points. Previous work has shown that the number and distribution of these collocation points have a significant influence on the accuracy of the PINN solution. Therefore, the effective placement of these collocation points is an active area of research. Specifically, adaptive collocation point sampling methods have been proposed, which have been reported to scale poorly to higher dimensions. In this work, we address this issue and present the Point Adaptive Collocation Method for Artificial Neural Networks (PACMANN). Inspired by classic optimization problems, this approach incrementally moves collocation points toward regions of higher residuals using gradient-based optimization algorithms guided by the gradient of the squared residual. We apply PACMANN for forward and inverse problems, and demonstrate that this method matches the performance of state-of-the-art methods in terms of the accuracy/efficiency tradeoff for the low-dimensional problems, while outperforming available approaches for high-dimensional problems; the best performance is observed for the Adam optimizer. Key features of the method include its low computational cost and simplicity of integration in existing physics-informed neural network pipelines.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19632
Understanding the anisotropic growth of VS grown PbSnTe nanowires,['Materials Science'],"['Mathijs G. C. Mientjes', 'Xin Guan', 'Marcel A. Verheijen', 'Erik P. A. M. Bakkers']","PbSnTe is a topological crystalline insulator (TCI), which holds promise for scattering-free transport channels and fault-tolerant quantum computing. As the topologically non-trivial states live on the surface, the nanowire geometry, with a high surface-to-volume ratio, is ideal for probing these states. The controlled growth of PbSnTe nanowires using molecular beam epitaxy has been shown before, but an understanding of the anisotropic growth and the resulting morphology is lacking. Here, based on experimental observations, we develop a model that describes the evolution of NW morphology as a function of growth time. It is found that the anisotropic morphology can be described by a combination of direct impingement, mask diffusion and facet diffusion which results in a transition from a Te-limited growth regime to a group IV-limited growth regime. This growth model allows us to design more targeted experiments which could lead to a higher flexibility in device design.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19627
A nonconservative macroscopic traffic flow model in a two-dimensional urban-porous city,['Numerical Analysis'],"['N. Garcia-Chan', 'L. J. Alvarez-Vazquez', 'A. Martinez', 'M. E. Vazquez-Mendez']","In this paper we propose a novel traffic flow model based on understanding the city as a porous media, this is, streets and building-blocks characterizing the urban landscape are seen now as the fluid-phase and the solid-phase of a porous media, respectively. Moreover, based in the interchange of mass in the porous media models, we can model the interchange of cars between streets and off-street parking-spaces. Therefore, our model is not a standard conservation law, being formulated as the coupling of a non-stationary convection-diffusion-reaction PDE with a Darcy-Brinkman-Forchheimer PDE system. To solve this model, the classical Galerkin P1 finite element method combined with an explicit time marching scheme of strong stability-preserving type was enough to stabilize our numerical solutions. Numerical experiences on an urban-porous domain inspired by the city of Guadalajara (Mexico) allow us to simulate the influence of the porosity terms on the traffic speed, the traffic flow at rush-valley hours, and the streets congestions due to the lack of parking spaces.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19625
Global restrictions under local state discrimination,['Quantum Physics'],"['Carles Roch i Carceller', 'Alexander Bernal']","We investigate how local distinguishability can restrict global properties of bi-partite states. We begin exploring how non-locality becomes limited by optimal local state discrimination and observe a non-trivial trade-off between CHSH violation and success probability of local discrimination. We extend our findings to bounding the maximally entangled sate fidelity and global observables such as the energy. Our results show that optimal local state discrimination can become a powerful tool to limit global behaviours, e.g. from entangled adversaries in quantum cryptography.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19619
Higher-order Ising model on hypergraphs,['Statistical Mechanics'],"['Thomas Robiglio', 'Leonardo Di Gaetano', 'Ada Altieri', 'Giovanni Petri', 'Federico Battiston']","Non-dyadic higher-order interactions affect collective behavior in various networked dynamical systems. Here we discuss the properties of a novel Ising model with higher-order interactions and characterize its phase transitions between the ordered and the disordered phase. By a mean-field treatment, we show that the transition is continuous when only three-body interactions are considered but becomes abrupt when interactions of higher orders are introduced. Using a Georges-Yedidia expansion to go beyond a naïve mean-field approximation, we reveal a quantitative shift in the critical point of the phase transition, which does not affect the universality class of the model. Finally, we compare our results with traditional $p$-spin models with many-body interactions. Our work unveils new collective phenomena on complex interacting systems, revealing the importance of investigating higher-order systems beyond three-body interactions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19618
Materials Learning Algorithms (MALA): Scalable Machine Learning for Electronic Structure Calculations in Large-Scale Atomistic Simulations,['Materials Science'],"['Attila Cangi', 'Lenz Fiedler', 'Bartosz Brzoza', 'Karan Shah', 'Timothy J. Callow', 'Daniel Kotik', 'Steve Schmerler', 'Matthew C. Barry', 'James M. Goff', 'Andrew Rohskopf', 'Dayton J. Vogel', 'Normand Modine', 'Aidan P. Thompson', 'Sivasankaran Rajamanickam']","We present the Materials Learning Algorithms (MALA) package, a scalable machine learning framework designed to accelerate density functional theory (DFT) calculations suitable for large-scale atomistic simulations. Using local descriptors of the atomic environment, MALA models efficiently predict key electronic observables, including local density of states, electronic density, density of states, and total energy. The package integrates data sampling, model training and scalable inference into a unified library, while ensuring compatibility with standard DFT and molecular dynamics codes. We demonstrate MALA's capabilities with examples including boron clusters, aluminum across its solid-liquid phase boundary, and predicting the electronic structure of a stacking fault in a large beryllium slab. Scaling analyses reveal MALA's computational efficiency and identify bottlenecks for future optimization. With its ability to model electronic structures at scales far beyond standard DFT, MALA is well suited for modeling complex material systems, making it a versatile tool for advanced materials research.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19617
Laterally Extended States of Interlayer Excitons in Reconstructed MoSe$_2$/WSe$_2$ Heterostructures,['Mesoscale and Nanoscale Physics'],"['Johannes Figueiredo', 'Marten Richter', 'Mirco Troue', 'Jonas Kiemle', 'Hendrik Lambers', 'Torsten Stiehm', 'Takashi Taniguchi', 'Kenji Watanabe', 'Ursula Wurstbauer', 'Andreas Knorr', 'Alexander W. Holleitner']","Heterostructures made from 2D transition-metal dichalcogenides are known as ideal platforms to explore excitonic phenomena ranging from correlated moiré excitons to degenerate interlayer exciton ensembles. So far, it is assumed that the atomic reconstruction appearing in some of the heterostructures gives rise to a dominating localization of the exciton states. We demonstrate that excitonic states in reconstructed MoSe$_2$/WSe$_2$ heterostructures can extend well beyond the moiré periodicity of the investigated heterostructures. The results are based on real-space calculations yielding a lateral potential map for interlayer excitons within the strain-relaxed heterostructures and corresponding real-space excitonic wavefunctions. We combine the theoretical results with cryogenic photoluminescence experiments, which support the computed level structure and relaxation characteristics of the interlayer excitons.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19616
Optimal management of open-channel raceway ponds for cultivation of algal biomass intended for bioenergy production,['Optimization and Control'],"['L. J. Alvarez-Vazquez', 'A. Martinez', 'M. E. Vazquez-Mendez']","In this work we present a novel methodology to deal with the optimal performance of raceways (open-channel ponds where the circulating wastewater, during its purification process, is used to grow algae that will be used as a source for the production of bioenergy). The maximization of algal productivity is addressed here within an optimal control framework for partial differential equations. Thus, after introducing a rigorously detailed mathematical formulation of the real-world control problem, we prove the existence of optimal solutions, we propose a numerical algorithm for its computational resolution and, finally, we show some results for the numerical optimization of a realistic case.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19615
Memristive Nanowire Network for Energy Efficient Audio Classification: Pre-Processing-Free Reservoir Computing with Reduced Latency,['Sound'],"['Akshaya Rajesh', 'Pavithra Ananthasubramanian', 'Nagarajan Raghavan', 'Ankush Kumar']","Speech recognition is a key challenge in natural language processing, requiring low latency, efficient computation, and strong generalization for real-time applications. While software-based artificial neural networks (ANNs) excel at this task, they are computationally intensive and depend heavily on data pre-processing. Neuromorphic computing, with its low-latency and energy-efficient advantages, holds promise for audio classification. Memristive nanowire networks, combined with pre-processing techniques like Mel-Frequency Cepstrum Coefficient extraction, have been widely used for associative learning, but such pre-processing can be power-intensive, undermining latency benefits. This study pioneers the use of memristive and spatio-temporal properties of nanowire networks for audio signal classification without pre-processing. A nanowire network simulation is paired with three linear classifiers for 10-class MNIST audio classification and binary speaker generalization tests. The hybrid system achieves significant benefits: excellent data compression with only 3% of nanowire output utilized, a 10-fold reduction in computational latency, and up to 28.5% improved classification accuracy (using a logistic regression classifier). Precision and recall improve by 10% and 17% for multispeaker datasets, and by 24% and 17% for individual speaker datasets, compared to raw data classifiers. This work provides a foundational proof of concept for utilizing memristive nanowire networks (NWN) in edge-computing devices, showcasing their potential for efficient, real-time audio signal processing with reduced computational overhead and power consumption, and enabling the development of advanced neuromorphic computing solutions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19611
"A Ramanujan's hypergeometric transformation formula, its validity range and implications",['Classical Analysis and ODEs'],['M. A. Shpot'],"We extend the validity range of a Ramanujan's hypergeometric transformation formula proved by Berndt, Bhargava and Garvan, Trans. Amer. Math. Soc. 347, 4163 (1995) and study its implications. Relations to special values of complete elliptic integrals of the first kind in the singular value theory are established. Consequently, we derive several closed-form evaluations of hypergeometric functions $_2F_1$ with different sets of parameters and arguments. Connections with other hypergeometric transformations and some recent results are discussed.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19608
Continuous Approximation of the Fully Connected Ising Hamiltonian: Exact Ground State Solutions for a Novel Class of Ising Models with Applications to Fidelity Assessment in Ising Machines,['Computational Physics'],"['Amirhossein Rezaei', 'Mahmood Hasani', 'Alireza Rezaei', 'Seyed M. Hassan Halataei']","In this study, we present a novel analytical approach to solving large-scale Ising problems by reformulating the discrete Ising Hamiltonian into a continuous framework. This transformation enables us to derive exact solutions for a non-trivial class of fully connected Ising models. To validate our method, we conducted numerical experiments comparing our analytical solutions with those obtained from a quantum-inspired Ising algorithm and a quantum Ising machine. The results demonstrate that the quantum-inspired algorithm and brute-force method successfully align with our solutions, while the quantum Ising machine exhibits notable deviations. Our method offers promising avenues for analytically solving diverse Ising problem instances, while the class of Ising problems addressed here provides a robust framework for assessing the fidelity of Ising machines.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19604
Adaptive dynamics of Ising spins in one dimension leveraging Reinforcement Learning,['Soft Condensed Matter'],"['Anish Kumar', 'Pawan Kumar Mishra', 'Riya Singh', 'Shradha Mishra', 'Debaprasad Giri']","A one-dimensional flocking model using active Ising spins is studied, where the system evolves through the reinforcement learning approach \textit{via} defining state, action, and cost function for each spin. The orientation of spin with respect to its neighbouring spins defines its state. The state of spin is updated by altering its spin orientation in accordance with the $\varepsilon$-greedy algorithm (action) and selecting a finite step from a uniform distribution to update position. The $\varepsilon$ parameter is analogous to the thermal noise in the system. The cost function addresses cohesion among the spins. By exploring the system in the plane of the self-propulsion speed and $\varepsilon$ parameter, four distinct phases are found: disorder, flocking, flipping, and oscillatory. In the flipping phase, a condensed flock reverses its direction of motion stochastically. The mean reversal time $\langle T \rangle $ exponentially decays with $\varepsilon$. A new phase, an oscillatory phase, is also found, which is a chaotic phase with a positive Lyapunov exponent.
  The findings obtained from the reinforcement learning approach for the active Ising model system exhibit similarities with the outcomes of other conventional techniques, even without defining any explicit interaction among the spins.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19602
Unleashing the Transformative Power of Deliberation With Contextual Citizens,['Physics and Society'],"['Ariane Lambert-Mogiliansky', 'Irénée Frérot']","In this paper, we investigate deliberation procedures that invite citizens with contextual opinions to explore alternative thinking frames. Contextuality is captured in a simple quantum cognitive model. We show how disagreeing citizens endowed with contextual opinions, can reach consensus in a binary collective decision problem with no improvement in their information. A necessary condition is that they are willing to (mentally) experience their fellow citizens' way of thinking. The diversity of thinking frames is what makes it possible to overcome initial disagreement. Consensus does not emerge spontaneously from deliberations: it requires facilitation.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19596
Self-Supervised Denoiser Framework,['Image and Video Processing'],"['Emilien Valat', 'Andreas Hauptmann', 'Ozan Öktem']","Reconstructing images using Computed Tomography (CT) in an industrial context leads to specific challenges that differ from those encountered in other areas, such as clinical CT. Indeed, non-destructive testing with industrial CT will often involve scanning multiple similar objects while maintaining high throughput, requiring short scanning times, which is not a relevant concern in clinical CT. Under-sampling the tomographic data (sinograms) is a natural way to reduce the scanning time at the cost of image quality since the latter depends on the number of measurements. In such a scenario, post-processing techniques are required to compensate for the image artifacts induced by the sinogram sparsity. We introduce the Self-supervised Denoiser Framework (SDF), a self-supervised training method that leverages pre-training on highly sampled sinogram data to enhance the quality of images reconstructed from undersampled sinogram data. The main contribution of SDF is that it proposes to train an image denoiser in the sinogram space by setting the learning task as the prediction of one sinogram subset from another. As such, it does not require ground-truth image data, leverages the abundant data modality in CT, the sinogram, and can drastically enhance the quality of images reconstructed from a fraction of the measurements. We demonstrate that SDF produces better image quality, in terms of peak signal-to-noise ratio, than other analytical and self-supervised frameworks in both 2D fan-beam or 3D cone-beam CT settings. Moreover, we show that the enhancement provided by SDF carries over when fine-tuning the image denoiser on a few examples, making it a suitable pre-training technique in a context where there is little high-quality image data. Our results are established on experimental datasets, making SDF a strong candidate for being the building block of foundational image-enhancement models in CT.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19593
Axial anomaly in the presence of arbitrary spinor interactions,['High Energy Physics - Theory'],"['Jan Dereziński', 'Adam Latosiński']","We consider N Dirac fermions on a 4-dimensional Euclidean space with a quadratic interaction given by arbitrary external Clifford-valued fields. The divergence of the axial current satisfies on the classical level a relation that is violated after quantization. Using the Pauli-Villars method to regularize the fields, we find the conditions that guarantee the finiteness of the anomaly. We also find this anomaly. Our result generalizes the well-known computation of axial anomaly of Dirac fermions interacting with an external Yang-Mills field.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19592
Long-term stability and dynamical spacing of compact planetary systems,['Earth and Planetary Astrophysics'],['Antoine C. Petit'],"Exoplanet detection surveys revealed the existence of numerous multi-planetary systems packed close to their stability limit. In this proceeding, we review the mechanism driving the instability of compact systems, originally published in Petit et al. (2020). Compact systems dynamics are dominated by the interactions between resonances involving triplets of planets. The complex network of three-planet mean motion resonances drives a slow chaotic semi-major axes diffusion, leading to a fast and destructive scattering phase. This model reproduces quantitatively the instability timescale found numerically. We can observe signpost of this process on exoplanet systems architecture. The critical spacing ensuring stability scales as the planet-to star mass ratio to the power 1/4. It explains why the Hill radius is not an adapted measure of dynamical compactness of exoplanet systems, particularly for terrestrial planets. We also provide some insight on the theoretical tools developed in the original work and how they can be of interest in other problems.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19590
Can Large Language Models Reason about the Region Connection Calculus?,['Computation and Language'],"['Anthony G Cohn', 'Robert E Blackwell']","Qualitative Spatial Reasoning is a well explored area of Knowledge Representation and Reasoning and has multiple applications ranging from Geographical Information Systems to Robotics and Computer Vision. Recently, many claims have been made for the reasoning capabilities of Large Language Models (LLMs). Here, we investigate the extent to which a set of representative LLMs can perform classical qualitative spatial reasoning tasks on the mereotopological Region Connection Calculus, RCC-8. We conduct three pairs of experiments (reconstruction of composition tables, alignment to human composition preferences, conceptual neighbourhood reconstruction) using state-of-the-art LLMs; in each pair one experiment uses eponymous relations and one, anonymous relations (to test the extent to which the LLM relies on knowledge about the relation names obtained during training). All instances are repeated 30 times to measure the stochasticity of the LLMs.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19589
Through the Telco Lens: A Countrywide Empirical Study of Cellular Handovers,['Networking and Internet Architecture'],"['Michail Kalntis', 'José Suárez-Varela', 'Jesús Omaña Iglesias', 'Anup Kiran Bhattacharjee', 'George Iosifidis', 'Fernando A. Kuipers', 'Andra Lutu']","Cellular networks rely on handovers (HOs) as a fundamental element to enable seamless connectivity for mobile users. A comprehensive analysis of HOs can be achieved through data from Mobile Network Operators (MNOs); however, the vast majority of studies employ data from measurement campaigns within confined areas and with limited end-user devices, thereby providing only a partial view of HOs. This paper presents the first countrywide analysis of HO performance, from the perspective of a top-tier MNO in a European country. We collect traffic from approximately 40M users for 4 weeks and study the impact of the radio access technologies (RATs), device types, and manufacturers on HOs across the country. We characterize the geo-temporal dynamics of horizontal (intra-RAT) and vertical (inter-RATs) HOs, at the district level and at millisecond granularity, and leverage open datasets from the country's official census office to associate our findings with the population. We further delve into the frequency, duration, and causes of HO failures, and model them using statistical tools. Our study offers unique insights into mobility management, highlighting the heterogeneity of the network and devices, and their effect on HOs.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19586
Early Versus Late Traffic Management For Autonomous Agents,['Systems and Control'],"['Salman Ghori', 'Ania Adil', 'Eric Feron']","Intersections pose critical challenges in traffic management, where maintaining operational constraints and ensuring safety are essential for efficient flow. This paper investigates the effect of intervention timing in management strategies on maintaining operational constraints at intersections while ensuring safe separation distance, avoiding collisions, and minimizing delay. We introduce control regions, represented as circles around the intersection, which refers to the timing of interventions by a centralized control system when agents approach the intersection. We use a mixed-integer linear programming (MILP) approach to optimize the system's performance. To analyze the effectiveness of early and late control measures, a simulation study is conducted, focusing on the safe, efficient, and robust management of agent movement within the control regions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19582
The ATTUNE model for Artificial Trust Towards Human Operators,['Robotics'],"['Giannis Petousakis', 'Angelo Cangelosi', 'Rustam Stolkin', 'Manolis Chiou']","This paper presents a novel method to quantify Trust in HRI. It proposes an HRI framework for estimating the Robot Trust towards the Human in the context of a narrow and specified task. The framework produces a real-time estimation of an AI agent's Artificial Trust towards a Human partner interacting with a mobile teleoperation robot. The approach for the framework is based on principles drawn from Theory of Mind, including information about the human state, action, and intent. The framework creates the ATTUNE model for Artificial Trust Towards Human Operators. The model uses metrics on the operator's state of attention, navigational intent, actions, and performance to quantify the Trust towards them. The model is tested on a pre-existing dataset that includes recordings (ROSbags) of a human trial in a simulated disaster response scenario. The performance of ATTUNE is evaluated through a qualitative and quantitative analysis. The results of the analyses provide insight into the next stages of the research and help refine the proposed approach.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19580
Supersymmetric Klein-Gordon and Dirac oscillators,['High Energy Physics - Theory'],['Alexander D. Popov'],"We have recently shown that the space of initial data (covariant phase space) of the relativistic oscillator in Minkowski space $\mathbb{R}^{3,1}$ is a homogeneous Kähler-Einstein manifold $Z_6$=AdS$_7$/U(1)=U(3,1)/U(3)$\times$U(1). It was also shown that the energy eigenstates of the quantum relativistic oscillator form a direct sum of two weighted Bergman spaces of holomorphic (particles) and antiholomorphic (antiparticles) square-integrable functions on the covariant phase space $Z_6$ of the classical oscillator. Here we show that the covariant phase space of the supersymmetric version of the relativistic oscillator (oscillating spinning particle) is the odd tangent bundle of the space $Z_6$. Quantizing this model yields a Dirac oscillator equation on the phase space whose solution space is a direct sum of two spinor spaces parametrized by holomorphic and antiholomorphic functions on the odd tangent bundle of $Z_6$. After expanding the general solution in Grassmann variables, we obtain components of the spinor field that are holomorphic and antiholomorphic functions from Bergman spaces on $Z_6$ with different weight functions. Thus, the supersymmetric model under consideration is exactly solvable, Lorentz covariant and unitary.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19578
A Review of LLM-based Explanations in Recommender Systems,['Information Retrieval'],['Alan Said'],"The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has opened new opportunities for enhancing recommender systems through improved explainability. This paper provides a systematic literature review focused on leveraging LLMs to generate explanations for recommendations -- a critical aspect for fostering transparency and user trust. We conducted a comprehensive search within the ACM Guide to Computing Literature, covering publications from the launch of ChatGPT (November 2022) to the present (November 2024). Our search yielded 232 articles, but after applying inclusion criteria, only six were identified as directly addressing the use of LLMs in explaining recommendations. This scarcity highlights that, despite the rise of LLMs, their application in explainable recommender systems is still in an early stage. We analyze these select studies to understand current methodologies, identify challenges, and suggest directions for future research. Our findings underscore the potential of LLMs improving explanations of recommender systems and encourage the development of more transparent and user-centric recommendation explanation solutions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19576
Fabrication and characterization of InAs nanowire-based quantum dot structures utilizing buried bottom gates,['Mesoscale and Nanoscale Physics'],"['Anton Faustmann', 'Patrick Liebisch', 'Benjamin Bennemann', 'Pujitha Perla', 'Mihail Ion Lepsa', 'Alexander Pawlis', 'Detlev Grützmacher', 'Joachim Knoch', 'Thomas Schäpers']","Semiconductor nanowires can be utilized to create quantum dot qubits. The formation of quantum dots is typically achieved by means of bottom gates created by a lift-off process. As an alternative, we fabricated flat buried bottom gate structures by filling etched trenches in a Si substrate with sputtered TiN, followed by mechanical polishing. This method achieved gate line pitches as small as 60 nm. The gate fingers have low gate leakage. As a proof of principle, we fabricated quantum dot devices using InAs nanowires placed on the gate fingers. These devices exhibit single electron tunneling and Coulomb blockade.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19575
Are Stellar Embryos in Perseus Radio-Synchrotron Emitters? Statistical data analysis with Herschel and LOFAR paving the way for the SKA,['Astrophysics of Galaxies'],"['Andrea Bracco', 'Marco Padovani', 'Daniele Galli', 'Stefania Pezzuto', 'Alexandre Cipriani', 'Alexander Drabent']","Cosmic rays (CRs) are fundamental to the chemistry and physics of star-forming regions, influencing molecular gas ionization, mediating interactions with interstellar magnetic fields, and regulating star formation from the diffuse interstellar medium to the creation of stellar cores. The electronic GeV component of CRs is expected to produce non-thermal synchrotron radiation detectable at radio frequencies, yet such emissions from Galactic star-forming regions remain elusive. This study reports the first statistical attempt to detect synchrotron emission at 144 MHz using the LOw Frequency ARray (LOFAR) in the nearby Perseus molecular cloud (300 pc). By median-stacking 353 prestellar and 132 protostellar cores from the Herschel Gould Belt Survey and using LOFAR Two-Meter Sky Survey (LoTSS) data (20"" resolution), 18 protostellar and 5 prestellar radio candidates were initially identified. However, these were likely extragalactic contaminants within the Herschel catalog. Stacked analyses did not reveal significant radio counterparts for prestellar and protostellar cores, with upper limits of $5\, μ$Jy beam$^{-1}$ and $8\, μ$Jy beam$^{-1}$, respectively. Non-detections suggest strong extinction mechanisms like free-free absorption and the Razin-Tsytovich effect for protostellar cores. For prestellar cores, analytical magnetostatic-isothermal models constrain the maximum ordered magnetic-field strength to 100 $μ$G. Future predictions suggest that Square Kilometre Array-Low (SKA-Low) arrays could detect this emission in 9 hours (AA*) or 4 hours (AA4), enabling more sensitive constraints on synchrotron radiation in star-forming cores.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19573
Fixed-relative-switch strategies for learning based event-triggered control of nonlinear multiagent systems,['Systems and Control'],"['Ziming Wang', 'Yun gao', 'Apostolos I. Rikos', 'Xin Wang', 'Yiding Ji']","This paper investigates event-triggered control for consensus tracking in nonlinear semi-strict-feedback multi-agent systems (MASs) with unknown states and subject to disturbances. We begin by employing radial basis function neural networks combined with the backstepping method to approximate the unknown nonlinear dynamics of the MASs, which facilitates the development of state and disturbance observers for estimation. We then propose three adaptive event-triggered control strategies: fixed-threshold, relative-threshold, and switch-threshold, whose controllers and triggering conditions are defined acocrdingly. By applying Lyapunov stability theory, we rigorously prove that all follower agents' outputs consistently track the reference of their leader, and all error signals remain uniformly bounded. Moreover, our control strategies effectively mitigate the occurrence of Zeno behaviors. To validate our control framework, an illustrative example is provided to demonstrate its desirable performance in consensus tracking and triggering efficiency.△ Less",v1,https://arxiv.org/pdf/2411.19571
A note on transformations of edge colorings of chordless graphs and triangle-free graphs,['Combinatorics'],['Armen Asratian'],Bonamy et al. (2023) proved that an optimal edge coloring of a simple triangle--free graph $G$ can be reached from any given proper edge coloring of $G$ through a series of Kempe changes. We show that a small modification of their proof gives a possibility to obtain a similar result for a larger class of simple graphs consisting of all triangle-free and all chordless graphs (a graph $G$ is chordless if in every cycle $C$ of $G$ any two nonconsecutive vertices of $C$ are not adjacent).△ Less,"29 November, 2024;",https://arxiv.org/pdf/2411.19569
Mixed-Integer Linear Programming Model for Collision Avoidance Planning in Commercial Aircraft Formations,['Systems and Control'],"['Songqiying Yang', 'Ania Adil', 'Eric Feron']","With advancements in technology, commercial aircraft formation flying is becoming increasingly feasible as an efficient and environmentally friendly flight method. However, gaps remain in practical implementation, particularly in collision avoidance for aircraft formations. Existing avoidance algorithms mainly focus on single aircraft or UAV swarms, lacking comprehensive studies on the complex interactions within commercial aircraft formations. To address this, this paper proposes an optimization model designed to generate safe and effective collision avoidance solutions for commercial aircraft formations. This model demonstrates avoidance paths for formations facing intruders and offers insights for developing formation flight strategies. This study explores response strategies for commercial aircraft formations encountering intruders, considering the difficulty of pilot maneuvers. The findings provide theoretical support for the practical implementation of commercial formation flying and may advance the adoption of this technology.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19568
Hot-carrier trapping preserves high quantum yields but limits optical gain in InP-based quantum dots,['Materials Science'],"['Sander J. W. Vonk', 'P. Tim Prins', 'Tong Wang', 'Jan Matthys', 'Luca Giordano', 'Pieter Schiettecatte', 'Navendu Mondal', 'Jessi E. S. van der Hoeven', 'Thomas R. Hopper', 'Zeger Hens', 'Pieter Geiregat', 'Artem A. Bakulin', 'Freddy T. Rabouw']","Indium phosphide is the leading material for commercial applications of colloidal quantum dots. To date, however, the community has failed to achieve successful operation under strong excitation conditions, contrasting sharply with other materials. Here, we report how the unusual photophysics of state-of-the-art InP-based quantum dots make them unattractive as a gain material. A combination of ensemble-based time-resolved spectroscopy over timescales from femtoseconds to microseconds and single-quantum-dot spectroscopy reveals ultrafast trapping of hot charge carriers. This process leads to charge-carrier losses, thereby reducing the achievable population inversion which limits amplification of light in a gain material. Interestingly, fluorescence is only delayed, not quenched, by hot charge-carrier trapping, explaining why InP-based quantum dots are successful as bright luminescent colour convertors for low-intensity applications. Comparison with other popular quantum-dot materials, such as CdSe, Pb-halide perovskites, and CuInS2, indicate that the hot-carrier dynamics observed are unique to InP.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19565
A Comprehensive Framework for Automated Segmentation of Perivascular Spaces in Brain MRI with the nnU-Net,['Image and Video Processing'],"['William Pham', 'Alexander Jarema', 'Donggyu Rim', 'Zhibin Chen', 'Mohamed S. H. Khlif', 'Vaughan G. Macefield', 'Luke A. Henderson', 'Amy Brodtmann']","Background: Enlargement of perivascular spaces (PVS) is common in neurodegenerative disorders including cerebral small vessel disease, Alzheimer's disease, and Parkinson's disease. PVS enlargement may indicate impaired clearance pathways and there is a need for reliable PVS detection methods which are currently lacking. Aim: To optimise a widely used deep learning model, the no-new-UNet (nnU-Net), for PVS segmentation. Methods: In 30 healthy participants (mean$\pm$SD age: 50$\pm$18.9 years; 13 females), T1-weighted MRI images were acquired using three different protocols on three MRI scanners (3T Siemens Tim Trio, 3T Philips Achieva, and 7T Siemens Magnetom). PVS were manually segmented across ten axial slices in each participant. Segmentations were completed using a sparse annotation strategy. In total, 11 models were compared using various strategies for image handling, preprocessing and semi-supervised learning with pseudo-labels. Model performance was evaluated using 5-fold cross validation (5FCV). The main performance metric was the Dice Similarity Coefficient (DSC). Results: The voxel-spacing agnostic model (mean$\pm$SD DSC=64.3$\pm$3.3%) outperformed models which resampled images to a common resolution (DSC=40.5-55%). Model performance improved substantially following iterative label cleaning (DSC=85.7$\pm$1.2%). Semi-supervised learning with pseudo-labels (n=12,740) from 18 additional datasets improved the agreement between raw and predicted PVS cluster counts (Lin's concordance correlation coefficient=0.89, 95%CI=0.82-0.94). We extended the model to enable PVS segmentation in the midbrain (DSC=64.3$\pm$6.5%) and hippocampus (DSC=67.8$\pm$5%). Conclusions: Our deep learning models provide a robust and holistic framework for the automated quantification of PVS in brain MRI.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19564
Updating Katz centrality by counting walks,['Numerical Analysis'],"['Francesca Arrigo', 'Daniele Bertaccini', 'Alessandro Filippo']","We develop efficient and effective strategies for the update of Katz centralities after node and edge removal in simple graphs. We provide explicit formulas for the ``loss of walks"" a network suffers when nodes/edges are removed, and use these to inform our algorithms. The theory builds on the newly introduced concept of $\cF$-avoiding first-passage walks. Further, bounds on the change of total network communicability are also derived. Extensive numerical experiments on synthetic and real-world networks complement our theoretical results.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19560
Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning,['Computation and Language'],"['Kaustubh Ponkshe', 'Raghav Singhal', 'Eduard Gorbunov', 'Alexey Tumanov', 'Samuel Horvath', 'Praneeth Vepakomma']","Low-rank adapters have become a standard approach for efficiently fine-tuning large language models (LLMs), but they often fall short of achieving the performance of full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that approximates full fine-tuning within low-rank subspaces using a carefully designed initialization strategy. We theoretically demonstrate that the architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B and A while keeping other matrices fixed, provides the precise conditions needed for this approximation. We leverage its constrained update space to achieve optimal scaling for high-rank gradient updates while removing the need for hyperparameter tuning. We prove that our initialization offers an optimal low-rank approximation of the initial gradient and preserves update directions throughout training. Extensive experiments across mathematical reasoning, commonsense reasoning, and language understanding tasks demonstrate that our approach exceeds the performance of standard LoRA while using 27-90x fewer parameters, and comprehensively outperforms LoRA-XS. Our findings establish that it is possible to simulate full fine-tuning in low-rank subspaces, and achieve significant efficiency gains without sacrificing performance. Our code is publicly available at https://github.com/RaghavSinghal10/lora-sb.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19557
Geometric invariants for $p$-groups of class 2 and exponent $p$,['Group Theory'],"[""E. A. O'Brien"", 'Mima Stanojkovski']",We introduce geometric invariants for $p$-groups of class $2$ and exponent $p$. We report on their effectiveness in distinguishing among 5-generator $p$-groups of this type.△ Less,"29 November, 2024;",https://arxiv.org/pdf/2411.19555
Contextual Checkerboard Denoise -- A Novel Neural Network-Based Approach for Classification-Aware OCT Image Denoising,['Image and Video Processing'],"['Md. Touhidul Islam', 'Md. Abtahi M. Chowdhury', 'Sumaiya Salekin', 'Aye T. Maung', 'Akil A. Taki', 'Hafiz Imtiaz']","In contrast to non-medical image denoising, where enhancing image clarity is the primary goal, medical image denoising warrants preservation of crucial features without introduction of new artifacts. However, many denoising methods that improve the clarity of the image, inadvertently alter critical information of the denoised images, potentially compromising classification performance and diagnostic quality. Additionally, supervised denoising methods are not very practical in medical image domain, since a \emph{ground truth} denoised version of a noisy medical image is often extremely challenging to obtain. In this paper, we tackle both of these problems by introducing a novel neural network based method -- \emph{Contextual Checkerboard Denoising}, that can learn denoising from only a dataset of noisy images, while preserving crucial anatomical details necessary for image classification/analysis. We perform our experimentation on real Optical Coherence Tomography (OCT) images, and empirically demonstrate that our proposed method significantly improves image quality, providing clearer and more detailed OCT images, while enhancing diagnostic accuracy.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19549
Feller property and convergence for semigroups of time-changed processes,['Probability'],"['Ali BenAmor', 'Kazuhiro Kuwae']","We give a substitute to Feller property for semigroups of time-changed processes; under some conditions this leads to establish sufficient (new) conditions for the semigroups to be Feller. Moreover, given a standard process and a sequence of measures converging vaguely to a final measure, under some assumptions, we establish convergence of the sequence of the semigroups and the resolvents of the corresponding time changed-processes. Some applications are given: convergence of solutions of evolution equations and convergence of finite time distributions.△ Less",v1,https://arxiv.org/pdf/2411.19543
Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook,['Computer Vision and Pattern Recognition'],"['Florinel-Alin Croitoru', 'Andrei-Iulian Hiji', 'Vlad Hondru', 'Nicolae Catalin Ristea', 'Paul Irofti', 'Marius Popescu', 'Cristian Rusu', 'Radu Tudor Ionescu', 'Fahad Shahbaz Khan', 'Mubarak Shah']","With the recent advancements in generative modeling, the realism of deepfake content has been increasing at a steady pace, even reaching the point where people often fail to detect manipulated media content online, thus being deceived into various kinds of scams. In this paper, we survey deepfake generation and detection techniques, including the most recent developments in the field, such as diffusion models and Neural Radiance Fields. Our literature review covers all deepfake media types, comprising image, video, audio and multimodal (audio-visual) content. We identify various kinds of deepfakes, according to the procedure used to alter or generate the fake content. We further construct a taxonomy of deepfake generation and detection methods, illustrating the important groups of methods and the domains where these methods are applied. Next, we gather datasets used for deepfake detection and provide updated rankings of the best performing deepfake detectors on the most popular datasets. In addition, we develop a novel multimodal benchmark to evaluate deepfake detectors on out-of-distribution content. The results indicate that state-of-the-art detectors fail to generalize to deepfake content generated by unseen deepfake generators. Finally, we propose future directions to obtain robust and powerful deepfake detectors. Our project page and new benchmark are available at https://github.com/CroitoruAlin/biodeep.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19537
Heterogeneity of tumor biophysical properties and their potential role as prognostic markers,['Biological Physics'],"['Anja Madleine Markl', 'Daniel Nieder', 'Diana Isabel Sandoval-Bojorquez', 'Anna Taubenberger', 'Jean-François Berret', 'Artur Yakimovich', 'Eduardo Sergio Oliveros- Mata', 'Larysa Baraban', 'Anna Dubrovska']","Progress in our knowledge of tumor mechanisms and complexity led to the understanding of the physical parameters of cancer cells and their microenvironment, including the mechanical, thermal, and electrical properties, solid stress, and liquid pressure, as critical regulators of tumor progression and potential prognostic traits associated with clinical outcomes. The biological hallmarks of cancer and physical abnormalities of tumors are mutually reinforced, promoting a vicious cycle of tumor progression. A comprehensive analysis of the biological and physical tumor parameters is critical for developing more robust prognostic and diagnostic markers and improving treatment efficiency. Like the biological tumor traits, physical tumor features are characterized by inter-and intratumoral heterogeneity. The dynamic changes of physical tumor traits during tumor progression and as a result of tumor treatment highlight the necessity of their spatial and temporal analysis in clinical settings. This review focuses on the biological basis of the tumor-specific physical traits, the state-of-the-art methods of their analyses, and the perspective of clinical translation. The importance of tumor physical parameters for disease progression and therapy resistance, as well as current treatment strategies to monitor and target tumor physical traits in clinics, is highlighted.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19532
Radio-loudness statistics of quasars from Quaia-VLASS,['Astrophysics of Galaxies'],"['Nestor Arsenov', 'Sandor Frey', 'András Kovács', 'Lyuba Slavcheva-Mihova']","Quasars are objects of high interest for applications in extragalactic astrophysics, cosmology, and astrometry. One of their useful qualities is owed to their relativistic jets: they can be radio-loud. However, the fraction of radio-loud vs. radio-quiet quasars is subject to ongoing investigations, where the statistical power is limited by the low number of known quasars with radio counterparts. In this analysis, we revisited the radio-loudness statistics of quasars by significantly expanding the pool of known sources. Our main goal was to create a new, value-added catalogue of quasars with information about their extinction-corrected magnitudes, radio flux density, possible contamination levels, and other data flags, besides their sky coordinates and photometric redshifts. We cross-matched the optical Quaia catalogue of about 1.3 million quasars (selected from the Gaia data set) with 1.9 million sources from the Very Large Array Sky Survey (VLASS) radio catalogue. We explored different thresholds for the matching radius, balancing the completeness and purity of the resulting Quaia-VLASS catalogue, and found that 1.5 arc seconds is a sufficient choice. Our main finding is that the radio-loud fraction of quasars is in good agreement with previous works (< 10%), and there is no significant large-scale pattern in radio-loudness across the sky. The exact estimate depends on the G-band magnitude limit, and we observed some weak trends with redshift and absolute optical magnitude, possibly indicating remnant systematic effects in our data sets. The cross-matched Quaia-VLASS catalogue with 43,650 sources is available to the public for future analyses. This latest census of QSOs with radio counterparts will facilitate further investigations of the dichotomy of radio-loud and radio-quiet quasars, and it may also support other lines of investigations using quasars in cosmology and astrophysics.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19531
Experimental tests of the calibration of high precision differential astrometry for exoplanets,['Instrumentation and Methods for Astrophysics'],"['Manon Lizzana', 'Fabien Malbet', 'Pierre Kern', 'Fabrice Pancher', 'Sébastien Soler', 'Thierry Lepine', 'Alain Leger']","High precision differential Astrometry is the branch of astronomy that evaluates the relative position, distance and motion of celestial objects with respect to the stars present in the field of view. A mission called Theia has been submitted in 2022 for ESA's M7 call for missions, using a diffraction-limited telescope about 1m in diameter and with a field of view of 0.5 degrees, capable of achieving sub-micro-arcsecond angular accuracy, corresponding to 1e-5 pixel on the detector. Such precision makes it possible to study the nature of dark matter in our galaxy and to reveal the architecture of exoplanetary systems close to the Sun, down to the mass of the Earth. The aim of the experimental tests presented in this poster is to improve the TRL of 2 specific aspects: the calibration of new CMOS detectors with very large number of pixels and the calibration of the telescope aberrations.First, a key element of such a space telescope is the focal plane, which must be calibrated spatially with an extreme precision down to the 1e-5 pixel level. Previous work has shown that this is possible with small detector matrices (80x80 px) [1]. The goal is now to check the performances and validate this method with the new very large detectors. Pyxalis, a company based near Grenoble, is developing very large detectors (8000x5000 px) that have a low noise level and high sensitivity. The aim is to characterize and validate this type of detectors in a laboratory demonstration (see poster Pancher et al.), to ensure that the performance achieved meets the required specifications. We present the results of these characterization in this contribution.The telescope stability is also a sensitive issue. Recent work [2] has shown that the reference stars in the field of the telescope can be used as actual metrology sources in order to compute the field distortion function. Our simulations allow to model the optical aberrations with bivariate polynoms. The effects on the calibration accuracy of the degrees of the polynoms, the number of reference stars and the tilt perturbation of the M2 mirror are investigated. This poster will present the latest results obtained on a test bed developed to experimentally study the performances of this new field calibration method.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19524
The omega invariant of a matroid,['Combinatorics'],"['Alex Fink', 'Kris Shaw', 'David E Speyer']","The third author introduced the $g$-polynomial $g_M(t)$ of a matroid, a covaluative matroid statistic which is unchanged under series and parallel extension. The $g$-polynomial of a rank $r$ matroid $M$ has the form $g_1 t + g_2 t^2 + \cdots + g_r t^r$. The coefficient $g_1$ is Crapo's classical $β$-invariant. In this paper, we study the coefficient $g_r$, which we term the $ω$-invariant of $M$. We show that, if $ω(N)$ is nonnegative for every minor $N$ of $M$, then all the coefficients of $g_M(t)$ are nonnegative. We give several simplified versions of Ferroni's formula for $ω(M)$, and compute $ω(M)$ when $r$ or $|E(M)|-2r$ is small.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19521
Enhancing AI microscopy for foodborne bacterial classification via adversarial domain adaptation across optical and biological variability,['Image and Video Processing'],"['Siddhartha Bhattacharya', 'Aarham Wasit', 'Mason Earles', 'Nitin Nitin', 'Luyao Ma', 'Jiyoon Yi']","Rapid detection of foodborne bacteria is critical for food safety and quality, yet traditional culture-based methods require extended incubation and specialized sample preparation. This study addresses these challenges by i) enhancing the generalizability of AI-enabled microscopy for bacterial classification using adversarial domain adaptation and ii) comparing the performance of single-target and multi-domain adaptation. Three Gram-positive (Bacillus coagulans, Bacillus subtilis, Listeria innocua) and three Gram-negative (E. coli, Salmonella Enteritidis, Salmonella Typhimurium) strains were classified. EfficientNetV2 served as the backbone architecture, leveraging fine-grained feature extraction for small targets. Few-shot learning enabled scalability, with domain-adversarial neural networks (DANNs) addressing single domains and multi-DANNs (MDANNs) generalizing across all target domains. The model was trained on source domain data collected under controlled conditions (phase contrast microscopy, 60x magnification, 3-h bacterial incubation) and evaluated on target domains with variations in microscopy modality (brightfield, BF), magnification (20x), and extended incubation to compensate for lower resolution (20x-5h). DANNs improved target domain classification accuracy by up to 54.45% (20x), 43.44% (20x-5h), and 31.67% (BF), with minimal source domain degradation (<4.44%). MDANNs achieved superior performance in the BF domain and substantial gains in the 20x domain. Grad-CAM and t-SNE visualizations validated the model's ability to learn domain-invariant features across diverse conditions. This study presents a scalable and adaptable framework for bacterial classification, reducing reliance on extensive sample preparation and enabling application in decentralized and resource-limited environments.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19514
ContextGNN: Beyond Two-Tower Recommendation Systems,['Information Retrieval'],"['Yiwen Yuan', 'Zecheng Zhang', 'Xinwei He', 'Akihiro Nitta', 'Weihua Hu', 'Dong Wang', 'Manan Shah', 'Shenyang Huang', 'Blaž Stojanovič', 'Alan Krumholz', 'Jan Eric Lenssen', 'Jure Leskovec', 'Matthias Fey']","Recommendation systems predominantly utilize two-tower architectures, which evaluate user-item rankings through the inner product of their respective embeddings. However, one key limitation of two-tower models is that they learn a pair-agnostic representation of users and items. In contrast, pair-wise representations either scale poorly due to their quadratic complexity or are too restrictive on the candidate pairs to rank. To address these issues, we introduce Context-based Graph Neural Networks (ContextGNNs), a novel deep learning architecture for link prediction in recommendation systems. The method employs a pair-wise representation technique for familiar items situated within a user's local subgraph, while leveraging two-tower representations to facilitate the recommendation of exploratory items. A final network then predicts how to fuse both pair-wise and two-tower recommendations into a single ranking of items. We demonstrate that ContextGNN is able to adapt to different data characteristics and outperforms existing methods, both traditional and GNN-based, on a diverse set of practical recommendation tasks, improving performance by 20% on average.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19513
Real-time Anomaly Detection at the L1 Trigger of CMS Experiment,['High Energy Physics - Experiment'],['Abhijith Gandrakota'],"We present the preparation, deployment, and testing of an autoencoder trained for unbiased detection of new physics signatures in the CMS experiment Global Trigger (GT) test crate FPGAs during LHC Run 3. The GT makes the final decision whether to readout or discard the data from each LHC collision, which occur at a rate of 40 MHz, within a 50 ns latency. The Neural Network makes a prediction for each event within these constraints, which can be used to select anomalous events for further analysis. The GT test crate is a copy of the main GT system, receiving the same input data, but whose output is not used to trigger the readout of CMS, providing a platform for thorough testing of new trigger algorithms on live data, but without interrupting data taking. We describe the methodology to achieve ultra low latency anomaly detection, and present the integration of the DNN into the GT test crate, as well as the monitoring, testing, and validation of the algorithm during proton collisions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19506
COLD: Causal reasOning in cLosed Daily activities,['Computation and Language'],"['Abhinav Joshi', 'Areeb Ahmad', 'Ashutosh Modi']","Large Language Models (LLMs) have shown state-of-the-art performance in a variety of tasks, including arithmetic and reasoning; however, to gauge the intellectual capabilities of LLMs, causal reasoning has become a reliable proxy for validating a general understanding of the mechanics and intricacies of the world similar to humans. Previous works in natural language processing (NLP) have either focused on open-ended causal reasoning via causal commonsense reasoning (CCR) or framed a symbolic representation-based question answering for theoretically backed-up analysis via a causal inference engine. The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding. In this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed Daily activities) framework, which is built upon human understanding of daily real-world activities to reason about the causal nature of events. We show that the proposed framework facilitates the creation of enormous causal queries (~ 9 million) and comes close to the mini-turing test, simulating causal reasoning to evaluate the understanding of a daily real-world task. We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans. We further explore (the causal reasoning abilities of LLMs) using the backdoor criterion to determine the causal strength between events.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19500
SANGO: Socially Aware Navigation through Grouped Obstacles,['Robotics'],"['Rahath Malladi', 'Amol Harsh', 'Arshia Sangwan', 'Sunita Chauhan', 'Sandeep Manjanna']","This paper introduces SANGO (Socially Aware Navigation through Grouped Obstacles), a novel method that ensures socially appropriate behavior by dynamically grouping obstacles and adhering to social norms. Using deep reinforcement learning, SANGO trains agents to navigate complex environments leveraging the DBSCAN algorithm for obstacle clustering and Proximal Policy Optimization (PPO) for path planning. The proposed approach improves safety and social compliance by maintaining appropriate distances and reducing collision rates. Extensive experiments conducted in custom simulation environments demonstrate SANGO's superior performance in significantly reducing discomfort (by up to 83.5%), reducing collision rates (by up to 29.4%) and achieving higher successful navigation in dynamic and crowded scenarios. These findings highlight the potential of SANGO for real-world applications, paving the way for advanced socially adept robotic navigation systems.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19497
Diorama: Unleashing Zero-shot Single-view 3D Scene Modeling,['Computer Vision and Pattern Recognition'],"['Qirui Wu', 'Denys Iliash', 'Daniel Ritchie', 'Manolis Savva', 'Angel X. Chang']","Reconstructing structured 3D scenes from RGB images using CAD objects unlocks efficient and compact scene representations that maintain compositionality and interactability. Existing works propose training-heavy methods relying on either expensive yet inaccurate real-world annotations or controllable yet monotonous synthetic data that do not generalize well to unseen objects or domains. We present Diorama, the first zero-shot open-world system that holistically models 3D scenes from single-view RGB observations without requiring end-to-end training or human annotations. We show the feasibility of our approach by decomposing the problem into subtasks and introduce robust, generalizable solutions to each: architecture reconstruction, 3D shape retrieval, object pose estimation, and scene layout optimization. We evaluate our system on both synthetic and real-world data to show we significantly outperform baselines from prior work. We also demonstrate generalization to internet images and the text-to-scene task.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19492
ATOMS: ALMA Three-millimeter Observations of Massive Star-forming regions -- XIX. The origin of SiO emission,['Astrophysics of Galaxies'],"['Rong Liu', 'Tie Liu', 'Izaskun Jiménez-Serra', 'Jin-Zeng Li', 'Jesús Martín-Pintado', 'Xunchuan Liu', 'Chang Won Lee', 'Patricio Sanhueza', 'James O. Chibueze', 'Víctor M. Rivilla', 'Mika Juvela', 'Laura Colzi', 'Leonardo Bronfman', 'Hong-Li Liu', 'Miguel Sanz-Novo', 'Álvaro López-Gallifa', 'Shanghuo Li', 'Andrés Megías', 'David San Andrés', 'Guido Garay', 'Jihye Hwang', 'Jianwen Zhou', 'Fengwei Xu', 'Antonio Martínez-Henares', 'Anindya Saha']","The production of silicon monoxide (SiO) can be considered as a fingerprint of shock interaction. In this work, we use high-sensitivity observations of the SiO (2-1) and H$^{13}$CO$^{+}$ (1-0) emission to investigate the broad and narrow SiO emission toward 146 massive star-forming regions in the ATOMS survey. We detected SiO emission in 136 regions and distinguished broad and narrow components across the extension of 118 sources (including 58 UC $H_{II}$ regions) with an average angular resolution of 2.5$^{\prime}$$^{\prime}$. The derived SiO luminosity ($L_{SiO}$) across the whole sample shows that the majority of $L_{SiO}$ (above 66$\%$) can be attributed to broad SiO, indicating its association with strong outflows. The comparison of the ALMA SiO images with the filamentary skeletons identified from H$^{13}$CO$^{+}$ and in the infrared data (at 4.5, 8, and 24 $mu$m), further confirms that most SiO emission originates from outflows. However, note that for nine sources in our sample, the observed SiO emission may be generated by expanding UC $H_{II}$ regions. There is a moderate positive correlation between the bolometric luminosity ($L_{bol}$) and $L_{SiO}$ for both components (narrow and broad). The UC $H_{II}$ sources show a weaker positive correlation between $L_{bol}$ and $L_{SiO}$ and higher $L_{SiO}$ compared to the sources without UC $H_{II}$ regions. These results imply that the SiO emission from UC $H_{II}$ sources might be affected by UV-photochemistry induced by UC $H_{II}$ regions.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19489
HE2C: A Holistic Approach for Allocating Latency-Sensitive AI Tasks across Edge-Cloud,"['Distributed, Parallel, and Cluster Computing']","['Minseo Kim', 'Wei Shu', 'Mohsen Amini Salehi']","The high computational, memory, and energy demands of Deep Learning (DL) applications often exceed the capabilities of battery-powered edge devices, creating difficulties in meeting task deadlines and accuracy requirements. Unlike previous solutions that optimize a single metric (e.g., accuracy or energy efficiency), HE2C framework is designed to holistically address the latency, memory, accuracy, throughput, and energy demands of DL applications across edge-cloud continuum, thereby, delivering a more comprehensive and effective user experience. HE2C comprises three key modules: (a) a ""feasibility-check module that evaluates the likelihood of meeting deadlines across both edge and cloud resources; (b) a ""resource allocation strategy"" that maximizes energy efficiency without sacrificing the inference accuracy; and (c) a ""rescue module"" that enhances throughput by leveraging approximate computing to trade accuracy for latency when necessary. Our primary objective is to maximize system prolong battery lifespan, throughput, and accuracy while adhering to strict latency constraints. Experimental evaluations in the context of wearable technologies for blind and visually impaired users demonstrate that HE2C significantly improves task throughput via completing a larger number of tasks within their specified deadlines, while preserving edge device battery and maintaining prediction accuracy with minimal latency impact. These results underscore HE2C's potential as a robust solution for resource management in latency-sensitive, energy-constrained edge-to-cloud environments.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19487
Action Engine: An LLM-based Framework for Automatic FaaS Workflow Generation,"['Distributed, Parallel, and Cluster Computing']","['Akiharu Esashi', 'Pawissanutt Lertpongrujikorn', 'Mohsen Amini Salehi']","Function as a Service (FaaS) is poised to become the foundation of the next generation of cloud systems due to its inherent advantages in scalability, cost-efficiency, and ease of use. However, challenges such as the need for specialized knowledge and difficulties in building function workflows persist for cloud-native application developers. To overcome these challenges and mitigate the burden of developing FaaS-based applications, in this paper, we propose a mechanism called Action Engine, that makes use of Tool-Augmented Large Language Models (LLMs) at its kernel to interpret human language queries and automates FaaS workflow generation, thereby, reducing the need for specialized expertise and manual design. Action Engine includes modules to identify relevant functions from the FaaS repository and seamlessly manage the data dependency between them, ensuring that the developer's query is processed and resolved. Beyond that, Action Engine can execute the generated workflow by feeding the user-provided parameters. Our evaluations show that Action Engine can generate workflows with up to 20\% higher correctness without developer involvement. We notice that Action Engine can unlock FaaS workflow generation for non-cloud-savvy developers and expedite the development cycles of cloud-native applications.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19485
Blurred LiDAR for Sharper 3D: Robust Handheld 3D Scanning with Diffuse LiDAR and RGB,['Image and Video Processing'],"['Nikhil Behari', 'Aaron Young', 'Siddharth Somasundaram', 'Tzofi Klinghoffer', 'Akshat Dave', 'Ramesh Raskar']","3D surface reconstruction is essential across applications of virtual reality, robotics, and mobile scanning. However, RGB-based reconstruction often fails in low-texture, low-light, and low-albedo scenes. Handheld LiDARs, now common on mobile devices, aim to address these challenges by capturing depth information from time-of-flight measurements of a coarse grid of projected dots. Yet, these sparse LiDARs struggle with scene coverage on limited input views, leaving large gaps in depth information. In this work, we propose using an alternative class of ""blurred"" LiDAR that emits a diffuse flash, greatly improving scene coverage but introducing spatial ambiguity from mixed time-of-flight measurements across a wide field of view. To handle these ambiguities, we propose leveraging the complementary strengths of diffuse LiDAR with RGB. We introduce a Gaussian surfel-based rendering framework with a scene-adaptive loss function that dynamically balances RGB and diffuse LiDAR signals. We demonstrate that, surprisingly, diffuse LiDAR can outperform traditional sparse LiDAR, enabling robust 3D scanning with accurate color and geometry estimation in challenging environments.△ Less","29 November, 2024;",https://arxiv.org/pdf/2411.19474
Simultaneous development of antiferromagnetism and local symmetry breaking in a kagome magnet (Co$_{0.45}$Fe$_{0.55}$)Sn,['Strongly Correlated Electrons'],"['Tsung-Han Yang', 'Shang Gao', 'Yuanpeng Zhang', 'Daniel Olds', 'William R. Meier', 'Matthew B. Stone', 'Brian C. Sales', 'Andrew D. Christianson', 'Qiang Zhang']","CoSn and FeSn, two kagome-lattice metals, have recently attracted significant attention as hosts of electronic flat bands and emergent physical properties. However, current understandings of their physical properties are limited to the knowledge of the average crystal structure. Here, we report the Fe-doping induced co-emergence of the antiferromagentic (AFM) order and local symmetry breaking in (Co0.45Fe0.55)Sn. Rietveld analysis on the neutron and synchrotron x-ray diffraction data indicates A-type antiferromagnetic order with the moment pointing perpendicular to the kagome layers, associated with the anomaly in the MSn(1)2Sn(2)4 (M = Co/Fe) octahedral distortion and the lattice constant c. Reverse Monte Carlo (RMC) modeling of the synchrotron x-ray total scattering results captured the subtle local orthorhombic distortion involving off-axis displacements of Sn2. Our results indicate that the stable hexagonal lattice above TN becomes unstable once the A-type AFM order is formed below TN. We argue that the local symmetry breaking has a magnetic origin and is driven by the out-of-plane magnetic exchange coupling. Our study provides comprehensive information on the crystal structure in both long-range scale and local scale, unveiling unique coupling between AFM order, octahedral distortion, and hidden local symmetry breaking.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19464
Chip games and multipartite graph paintability,['Combinatorics'],"['Peter Bradshaw', 'Tianyue Cao', 'Atlas Chen', 'Braden Dean', 'Siyu Gan', 'Ramon I. Garcia', 'Amit Krishnaiyer', 'Grace McCourt', 'Arvind Murty']","We study the paintability, an on-line version of choosability, of complete multipartite graphs. We do this by considering an equivalent chip game introduced by Duraj, Gutowski, and Kozik. We consider complete multipartite graphs with $ n $ parts of size at most 3. Using a computational approach, we establish upper bounds on the paintability of such graphs for small values of $ n. $
  The choosability of complete multipartite graphs is closely related to value $ p(n, m) $, the minimum number of edges in a $n$-uniform hypergraph with no panchromatic $m$-coloring. We consider an online variant of this parameter $ p_{OL}(n, m), $ introduced by Khuzieva et al. using a symmetric chip game. With this symmetric chip game, we find an improved upper bound for $ p_{OL}(n, m)$ when $m \geq 3$ and $n$ is large. Our method also implies a lower bound on the paintability of complete multipartite graphs with $m \geq 3$ parts of equal size.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19462
Fleximo: Towards Flexible Text-to-Human Motion Video Generation,['Computer Vision and Pattern Recognition'],"['Yuhang Zhang', 'Yuan Zhou', 'Zeyu Liu', 'Yuxuan Cai', 'Qiuyue Wang', 'Aidong Men', 'Huan Yang']","Current methods for generating human motion videos rely on extracting pose sequences from reference videos, which restricts flexibility and control. Additionally, due to the limitations of pose detection techniques, the extracted pose sequences can sometimes be inaccurate, leading to low-quality video outputs. We introduce a novel task aimed at generating human motion videos solely from reference images and natural language. This approach offers greater flexibility and ease of use, as text is more accessible than the desired guidance videos. However, training an end-to-end model for this task requires millions of high-quality text and human motion video pairs, which are challenging to obtain. To address this, we propose a new framework called Fleximo, which leverages large-scale pre-trained text-to-3D motion models. This approach is not straightforward, as the text-generated skeletons may not consistently match the scale of the reference image and may lack detailed information. To overcome these challenges, we introduce an anchor point based rescale method and design a skeleton adapter to fill in missing details and bridge the gap between text-to-motion and motion-to-video generation. We also propose a video refinement process to further enhance video quality. A large language model (LLM) is employed to decompose natural language into discrete motion sequences, enabling the generation of motion videos of any desired length. To assess the performance of Fleximo, we introduce a new benchmark called MotionBench, which includes 400 videos across 20 identities and 20 motions. We also propose a new metric, MotionScore, to evaluate the accuracy of motion following. Both qualitative and quantitative results demonstrate that our method outperforms existing text-conditioned image-to-video generation methods. All code and model weights will be made publicly available.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19459
Unsupervised Learning Approach to Anomaly Detection in Gravitational Wave Data,['General Relativity and Quantum Cosmology'],['Ammar Fayad'],"Gravitational waves (GW), predicted by Einstein's General Theory of Relativity, provide a powerful probe of astrophysical phenomena and fundamental physics. In this work, we propose an unsupervised anomaly detection method using variational autoencoders (VAEs) to analyze GW time-series data. By training on noise-only data, the VAE accurately reconstructs noise inputs while failing to reconstruct anomalies, such as GW signals, which results in measurable spikes in the reconstruction error. The method was applied to data from the LIGO H1 and L1 detectors. Evaluation on testing datasets containing both noise and GW events demonstrated reliable detection, achieving an area under the ROC curve (AUC) of 0.89. This study introduces VAEs as a robust, unsupervised approach for identifying anomalies in GW data, which offers a scalable framework for detecting known and potentially new phenomena in physics.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19450
Centre vortex evidence for a second finite-temperature QCD transition,['High Energy Physics - Lattice'],"['Jackson A. Mickley', 'Chris Allton', 'Ryan Bignell', 'Derek B. Leinweber']","Evidence for the existence of a second finite-temperature transition in Quantum Chromodynamics (QCD) is obtained through the study of centre vortex geometry and its evolution with temperature. The dynamical anisotropic ensembles of the FASTSUM collaboration are utilised to conduct a comprehensive analysis at eight temperatures beyond the established chiral transition. Visualisations of the centre vortex structure in temporal and spatial slices of the lattice reveal that vortex percolation persists through the chiral transition and ceases at a temperature that is approximately twice the chiral transition temperature $T_c$. This implies that confinement is retained through temperatures up to $T \approx 2\,T_c$, pointing towards a second transition corresponding to deconfinement. The loss of percolation is quantified by the vortex cluster extent, providing a clear signal for the deconfinement transition. Additional vortex statistics, including temporal correlations, vortex and branching point densities, the number of secondary clusters and vortex chain lengths between branching points, are scrutinised as a function of temperature. All ten measures investigated herein show the characteristics of two transitions in QCD, encompassing the chiral transition at $T_c$ and the deconfinement transition at $T \approx 2\,T_c$. Performing an inflection point analysis on the vortex and branching point densities produces an estimate of $T_c$ that agrees with the known FASTSUM value. By the same procedure, a precise estimate of the deconfinement point is extracted as $T_d = 321(6)\,$MeV.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19446
Capital Asset Pricing Model with Size Factor and Normalizing by Volatility Index,['Mathematical Finance'],"['Abraham Atsiwo', 'Andrey Sarantsev']","The Capital Asset Pricing Model (CAPM) relates a well-diversified stock portfolio to a benchmark portfolio. We insert size effect in CAPM, capturing the observation that small stocks have higher risk and return than large stocks, on average. Dividing stock index returns by the Volatility Index makes them independent and normal. In this article, we combine these ideas to create a new discrete-time model, which includes volatility, relative size, and CAPM. We fit this model using real-world data, prove the long-term stability, and connect this research to Stochastic Portfolio Theory.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19444
MCUCoder: Adaptive Bitrate Learned Video Compression for IoT Devices,['Image and Video Processing'],"['Ali Hojjat', 'Janek Haberer', 'Olaf Landsiedel']","The rapid growth of camera-based IoT devices demands the need for efficient video compression, particularly for edge applications where devices face hardware constraints, often with only 1 or 2 MB of RAM and unstable internet connections. Traditional and deep video compression methods are designed for high-end hardware, exceeding the capabilities of these constrained devices. Consequently, video compression in these scenarios is often limited to M-JPEG due to its high hardware efficiency and low complexity. This paper introduces , an open-source adaptive bitrate video compression model tailored for resource-limited IoT settings. MCUCoder features an ultra-lightweight encoder with only 10.5K parameters and a minimal 350KB memory footprint, making it well-suited for edge devices and MCUs. While MCUCoder uses a similar amount of energy as M-JPEG, it reduces bitrate by 55.65% on the MCL-JCV dataset and 55.59% on the UVG dataset, measured in MS-SSIM. Moreover, MCUCoder supports adaptive bitrate streaming by generating a latent representation that is sorted by importance, allowing transmission based on available bandwidth. This ensures smooth real-time video transmission even under fluctuating network conditions on low-resource devices. Source code available at https://github.com/ds-kiel/MCUCoder.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19442
Gradient Inversion Attack on Graph Neural Networks,['Machine Learning'],"['Divya Anand Sinha', 'Yezi Liu', 'Ruijie Du', 'Yanning Shen']","Graph federated learning is of essential importance for training over large graph datasets while protecting data privacy, where each client stores a subset of local graph data, while the server collects the local gradients and broadcasts only the aggregated gradients. Recent studies reveal that a malicious attacker can steal private image data from gradient exchanging of neural networks during federated learning. However, none of the existing works have studied the vulnerability of graph data and graph neural networks under such attack. To answer this question, the present paper studies the problem of whether private data can be recovered from leaked gradients in both node classification and graph classification tasks and { proposes a novel attack named Graph Leakage from Gradients (GLG)}. Two widely-used GNN frameworks are analyzed, namely GCN and GraphSAGE. The effects of different model settings on recovery are extensively discussed. Through theoretical analysis and empirical validation, it is shown that parts of the graph data can be leaked from the gradients.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19440
qlbm -- A Quantum Lattice Boltzmann Software Framework,['Quantum Physics'],"['Călin Andrei Georgescu', 'Merel Annelise Schalkers', 'Matthias Möller']","We present qlbm, a Python software package designed to facilitate the development, simulation, and analysis of Quantum Lattice Boltzmann Methods (QBMs). qlbm is a modular framework that introduces a quantum component abstraction hierarchy tailored to the implementation of novel QBMs. The framework interfaces with state-of-the-art quantum software infrastructure to enable efficient simulation and validation pipelines, and leverages novel execution and pre-processing techniques that significantly reduce the computational resources required to develop quantum circuits. We demonstrate the versatility of the software by showcasing multiple QBMs in 2D and 3D with complex boundary conditions, integrated within automated benchmarking utilities. Accompanying the source code are extensive test suites, thorough online documentation resources, analysis tools, visualization methods, and demos that aim to increase the accessibility of QBMs while encouraging reproducibility and collaboration. The source code of qlbm is publicly available under a permissive MPL 2.0 license at \url{https://github.com/QCFD-Lab/qlbm}.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19439
Actions and Objects Pathways for Domain Adaptation in Video Question Answering,['Computer Vision and Pattern Recognition'],"['Safaa Abdullahi Moallim Mohamud', 'Ho-Young Jung']","In this paper, we introduce the Actions and Objects Pathways (AOPath) for out-of-domain generalization in video question answering tasks. AOPath leverages features from a large pretrained model to enhance generalizability without the need for explicit training on the unseen domains. Inspired by human brain, AOPath dissociates the pretrained features into action and object features, and subsequently processes them through separate reasoning pathways. It utilizes a novel module which converts out-of-domain features into domain-agnostic features without introducing any trainable weights. We validate the proposed approach on the TVQA dataset, which is partitioned into multiple subsets based on genre to facilitate the assessment of generalizability. The proposed approach demonstrates 5% and 4% superior performance over conventional classifiers on out-of-domain and in-domain datasets, respectively. It also outperforms prior methods that involve training millions of parameters, whereas the proposed approach trains very few parameters.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19434
Singular mean-field backward stochastic Volterra integral equations in infinite dimensional spaces,['Probability'],"['Javad A. Asadzade', 'Nazim I. Mahmudov']","This paper investigates the well-posedness of singular mean-field backward stochastic Volterra integral equations (MF-BSVIEs) in infinite-dimensional spaces. We consider the equation:
  \[ X(t) = Ψ(t) + \int_t^b F\big(t, s, X(s), Z(t, s), Z(s, t), \mathbb{E}[X(s)], \mathbb{E}[Z(t, s)], \mathbb{E}[Z(s, t)]\big) ds - \int_t^b Z(t, s) dB_s, \]
  where the focus lies on establishing the existence and uniqueness of adapted M-solutions under appropriate conditions. A key contribution of this work is the development of essential lemmas that provide a rigorous foundation for analyzing the well-posedness of these equations. In addition, we extend our analysis to singular mean-field forward stochastic Volterra integral equations (MF-FSVIEs) in infinite-dimensional spaces, demonstrating their solvability and unique adapted solutions. Finally, we strengthen our theoretical results by applying them to derive stochastic maximum principles, showcasing the practical relevance of the proposed framework. These findings contribute to the growing body of research on mean-field stochastic equations and their applications in control theory and mathematical finance.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19433
Friedrich-Wintgen bound states in the continuum in dimerized dielectric metasurfaces,['Optics'],"['Xia Zhang', 'A. Louise Bradley']","Bound states in the continuum (BIC) are trapped eigenmodes with infinite $Q$ factors that are confined in the system. In this work, we propose a simple design for engineering a Friedrich-Wintgen BIC through the interference between a symmetry protected BIC and a surface lattice mode in a dimerized dielectric metasurface. The meta-atoms are comprised a symmetric double bar dimer. Using incident angle tuning an avoided crossing between the symmetry protected BIC and surface lattice mode is observed. At a specific detuning, the lower energy resonance vanishes, resulting in the formation of a Friedrich-Wintgen BIC. Investigations using coupled mode theory elucidate the role of the damping rate and coupling strength in the formation of the Friedrich-Wintgen BIC in the dimerized bar metasurface. By tuning the spacing between the two bars, Friedrich-Wintgen BIC can be engineered with controlled energy and damping rate. It is shown that the damping rate of the coupled modes can be considerably suppressed in this system. We envision that these results will not only enhance the understanding of strongly coupled interactions in metasurfaces but also indicate new paths for actively and passively controlling metasurface resonators for photonic applications.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19429
Cayley Incidence Graphs,['Combinatorics'],"['Arnbjörg Soffía Árnadóttir', 'Alexey Gordeev', 'Sabrina Lato', 'Tovohery Randrianarisoa', 'Joannes Vermant']","Evra, Feigon, Maurischat, and Parzanchevski (2023) introduced a biregular extension of Cayley graphs. In this paper, we reformulate their definition and provide some basic properties. We also show how these Cayley incidence graphs relate to various notions of Cayley hypergraphs. We further establish connections between Cayley incidence graphs and certain geometric and combinatorial structures, including coset geometries, difference sets and cages.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19428
Tracing the origins of galaxy lopsidedness across cosmic time,['Astrophysics of Galaxies'],"['Arianna Dolfi', 'Facundo A. Gomez', 'Antonela Monachesi', 'Patricia B. Tissera', 'Cristobal Sifon', 'Gaspar Galaz']","Current studies of large-scale asymmetries (i.e. lopsidedness) in the stellar density distribution of disk galaxies have mainly focused on the local Universe. Recent observations have found a significant fraction (over 60%) of lopsided galaxies at high-redshift ($1.5 < z < 3$), which is significantly larger than the fraction (~30%) observed in the nearby Universe. We aim to understand whether the more widespread lopsidedness at high- than low-redshift can be associated to environmental mechanisms being more effective in producing lopsided perturbations at high-redshift. At each redshift between $0 < z < 2$, we independently select a sample of disk-like galaxies from the IllustrisTNG simulations. We then characterize lopsidedness in the disks of galaxies at each redshift, study the relevant mechanisms generating lopsidedness, as well as the correlation between such perturbation, the local environment and the galaxy internal properties as a function of redshift. Consistent with previous and new observational results, we find that: 1) simulations predict a significant fraction (~60%) of lopsided galaxies at high-redshift ($1.5 < z < 2$), 2) the fraction of lopsided galaxies, as well as the lopsided amplitude, decreases from high- to low-redshift, and 3) there is not a significant dependence of lopsidedness on the local environment, but there is a strong correlation between the lopsided amplitude and basic galaxies' structural properties at all redshift between $0 < z < 2$. This means that, independent of the mechanisms on-setting lopsidedness, galaxies with low central stellar mass density and more extended disks are more susceptible of developing strong lopsidedness. We find that both recent interactions with mass-ratio >1:10 and gas accretion with subsequent star formation can produce lopsided perturbations at all redshift, but they are both significantly more effective at high-redshift.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19426
Bayesian Hierarchical Modeling for Predicting Spatially Correlated Curves in Irregular Domains: A Case Study on PM10 Pollution,['Methodology'],"['Alvaro Alexander Burbano Moreno', 'Ronaldo Dias']","This study presents a Bayesian hierarchical model for analyzing spatially correlated functional data and handling irregularly spaced observations. The model uses Bernstein polynomial (BP) bases combined with autoregressive random effects, allowing for nuanced modeling of spatial correlations between sites and dependencies of observations within curves. Moreover, the proposed procedure introduces a distinct structure for the random effect component compared to previous works. Simulation studies conducted under various challenging scenarios verify the model's robustness, demonstrating its capacity to accurately recover spatially dependent curves and predict observations at unmonitored locations. The model's performance is further supported by its application to real-world data, specifically PM$_{10}$ particulate matter measurements from a monitoring network in Mexico City. This application is of practical importance, as particles can penetrate the respiratory system and aggravate various health conditions. The model effectively predicts concentrations at unmonitored sites, with uncertainty estimates that reflect spatial variability across the domain. This new methodology provides a flexible framework for the FDA in spatial contexts and addresses challenges in analyzing irregular domains with potential applications in environmental monitoring.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19425
Nuclear and Particle Physics through Quantum Sensing Using Cold Atoms and Molecules,['Atomic Physics'],['Akio Kawasaki'],"As techniques for manipulating single atoms and molecules developed, quantum technologies witnessed drastic advancements. These technologies are now so mature that they can be applied to various fields. One of such fields is nuclear and particle physics, which were conventionally investigated with high energy accelerators. In general, quantum sensing enables high-precision measurements, opening ways to investigate high-energy phenomena that slightly modify energy levels of low-energy systems by higher-order perturbations. In other cases, such high-precision measurements are sensitive to faint low-energy interactions between the low-energy system and exotic fields. These measurements open new parameter spaces to explore in conventionally investigated topics, e.g., variations of fundamental constants, dark matter, fifth force, parity violation, CP-violation, supersymmetric theories, and other physics beyond the Standard Model. Additionally, such measurements can serve as confirmations for existing results from completely different systems, or surpass existing results using smaller-scale experiments. The field of atomic and molecular physics oriented to nuclear and particle physics motivations evolved significantly. This paper provides an overview of the field of nuclear and particle physics through high-precision quantum measurements using atoms and molecules.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19424
Wafer2Spike: Spiking Neural Network for Wafer Map Pattern Classification,['Neural and Evolutionary Computing'],"['Abhishek Mishra', 'Suman Kumar', 'Anush Lingamoorthy', 'Anup Das', 'Nagarajan Kandasamy']","In integrated circuit design, the analysis of wafer map patterns is critical to improve yield and detect manufacturing issues. We develop Wafer2Spike, an architecture for wafer map pattern classification using a spiking neural network (SNN), and demonstrate that a well-trained SNN achieves superior performance compared to deep neural network-based solutions. Wafer2Spike achieves an average classification accuracy of 98\% on the WM-811k wafer benchmark dataset. It is also superior to existing approaches for classifying defect patterns that are underrepresented in the original dataset. Wafer2Spike achieves this improved precision with great computational efficiency.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19422
Proto Successor Measure: Representing the Space of All Possible Solutions of Reinforcement Learning,['Machine Learning'],"['Siddhant Agarwal', 'Harshit Sikchi', 'Peter Stone', 'Amy Zhang']","Having explored an environment, intelligent agents should be able to transfer their knowledge to most downstream tasks within that environment. Referred to as ""zero-shot learning,"" this ability remains elusive for general-purpose reinforcement learning algorithms. While recent works have attempted to produce zero-shot RL agents, they make assumptions about the nature of the tasks or the structure of the MDP. We present \emph{Proto Successor Measure}: the basis set for all possible solutions of Reinforcement Learning in a dynamical system. We provably show that any possible policy can be represented using an affine combination of these policy independent basis functions. Given a reward function at test time, we simply need to find the right set of linear weights to combine these basis corresponding to the optimal policy. We derive a practical algorithm to learn these basis functions using only interaction data from the environment and show that our approach can produce the optimal policy at test time for any given reward function without additional environmental interactions. Project page: https://agarwalsiddhant10.github.io/projects/psm.html.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19418
$S_h$-sets and linear codes over $\mathbb{F}_q$,['Number Theory'],"['Viviana Carolina Guerrero Pantoja', 'John H. Castillo', 'Carlos Alberto Trujillo Solarte']","Let $(G,+)$ be an Abelian group. Given $h\in \mathbb{Z}^+$, a non-empty subset $A$ of $G$ is called an $S_h$-set if all the sums of $h$ distinct elements of $A$ are different. We extend the concept of $S_h$-set to a more general context in the context of finite vectorial spaces over finite fields. More precisely, a $\emptyset \neq A\subseteq \mathbb{F}_q^r$ is called an $S_h$-linear set if all the linear combinations of $h$ elements of $A$ are different. We establish a correspondence between $q$-ary linear codes and $S_h$-linear sets. This connection allow us to find lower bounds for the maximum size of $S_h$-sets in $\mathbb{F}_q^r$.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19413
Quantum-Optimal Frequency Estimation of Stochastic AC Fields,['Quantum Physics'],"['Anirban Dey', 'Sara Mouradian', 'Cosmo Lupo', 'Zixin Huang']","Classically, measurement bandwidth limits the achievable frequency resolution of stochastic time-dependent fields. We frame the problem of frequency measurement as the estimation of the noise parameter of a dephasing quantum channel. Using this framework, we find an exact upper bound for estimating frequency centroids and separations. In particular, given two closely separated frequencies with separation $ω_r$, the quantum Fisher information (QFI) upper bound is approximately $2/ω_r^2$, inversely proportional to the separation parameter. We show that this is achievable with a superposition of Dicke states, and show that GHZ states improve precision over unentangled states, achieving Heisenberg scaling in the low-bandwidth limit. This work established a robust framework for stochastic AC signal sensing and can be extended to an arbitrary number of frequencies.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19412
Refuting a recent proof of the Invariant Subspace Problem,['Functional Analysis'],['Ahmed Ghatasheh'],This article demonstrates that the recent proof of the invariant subspace problem presented by Khalil et al. is incorrect.△ Less,"28 November, 2024;",https://arxiv.org/pdf/2411.19409
Counting Problems for Orthogonal Sets and Sublattices in Function Fields,['Combinatorics'],"['Noy Soffer Aranov', 'Angelot Behajaina']","Let $\mathcal{K}=\mathbb{F}_q((x^{-1}))$. Analogous to orthogonality in the Euclidean space $\mathbb{R}^n$, there exists a well-studied notion of ultrametric orthogonality in $\mathcal{K}^n$. In this paper, we extend the work of \cite{AB24} about counting results related to orthogonality in $\mathcal{K}^n$. For example, we answer an open question from \cite{AB24} by bounding the size of the largest ``orthogonal sets'' in $\mathcal{K}^n$. Furthermore, we investigate analogues of Hadamard matrices over $\mathcal{K}$. Finally, we use orthogonality to compute the number of sublattices of $\mathbb{F}_q[x]^n$ with a certain geometric structure, as well as to determine the number of orthogonal bases for a sublattice in $\mathcal{K}^n$. The resulting formulas depend crucially on successive minima.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19406
Weighted norm inequalities of some singular integrals associated with Laguerre expansions,['Classical Analysis and ODEs'],['The Anh Bui'],"Let $ν=(ν_1,\ldots,ν_n)\in (-1,\infty)^n$, $n\ge 1$, and let $\mathcal{L}_ν$ be a self-adjoint extension of the differential operator \[ L_ν:= \sum_{i=1}^n \left[-\frac{\partial^2}{\partial x_i^2} + x_i^2 + \frac{1}{x_i^2}(ν_i^2 - \frac{1}{4})\right] \] on $C_c^\infty(\mathbb{R}_+^n)$ as the natural domain. In this paper, we investigate the weighted estimates of singular integrals in the Laguerre setting including the maximal function, the Riesz transform and the square functions associated to the Laguerre operator $\mathcal L_ν$. In the special case of the Riesz transform, the paper completes the description of the Riesz transform for the full range of $ν\in (-1,\infty)^n$ which significant improves the result in [J. Funct. Anal. 244 (2007), 399--443] for $ν_i\ge -1/2, ν_i\notin (-1/2,1/2)$ for $i=1,\ldots, n$.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19404
"Riesz transforms, Hardy spaces and Campanato spaces associated with Laguerre expansions",['Classical Analysis and ODEs'],['The Anh Bui'],"Let $ν\in [-1/2,\infty)^n$, $n\ge 1$, and let $\mathcal{L}_ν$ be a self-adjoint extension of the differential operator \[ L_ν:= \sum_{i=1}^n \left[-\frac{\partial^2}{\partial x_i^2} + x_i^2 + \frac{1}{x_i^2}(ν_i^2 - \frac{1}{4})\right] \] on $C_c^\infty(\mathbb{R}_+^n)$ as the natural domain. In this paper, we first prove that the Riesz transform associated with $\mathcal L_ν$ is a Calderón-Zygmund operator, answering the open problem in [JFA, 244 (2007), 399-443]. In addition, we develop the theory of Hardy spaces and Campanato spaces associated with $\mathcal{L}_ν$. As applications, we prove that the Riesz transform related to $\mathcal{L}_ν$ is bounded on these Hardy spaces and Campanato spaces, completing the description of the boundedness of the Riesz transform in the Laguerre expansion setting.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19403
"Hardy spaces, Besov spaces and Triebel--Lizorkin spaces associated with a discrete Laplacian and applications",['Classical Analysis and ODEs'],"['The Anh Bui', 'Xuan Thinh Duong']","Consider the discrete Laplacian $Δ_d$ defined on the set of integers $\mathbb Z$ by
  \[
  Δ_d f(n) = -f(n+1) + 2f(n) -f(n-1), \ \ \ \ n\in \mathbb Z,
  \]
  where $f$ is a function defined on $\mathbb Z$. In this paper, we define Hardy spaces, Besov spaces and Triebel--Lizorkin spaces associated with $Δ_d$ and then show that these function spaces coincide with the classical function spaces defined on $\mathbb Z$. As applications, we prove the boundedness of the spectral multipliers and the Riesz transforms associated with $Δ_d$ on these function spaces.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19399
SNS Junctions along the BCS-BEC Crossover,['Superconductivity'],"['Gautam Rai', 'Arman Babakhani', 'Ying Wang', 'Stephan Haas', 'Stefan Kettemann']","We present a theory of SNS junctions, a normal metal sandwiched between two superconductors, along the crossover from the BCS to the BEC regime. We calculate the Josephson current as a function of the chemical potential relative to the band edge in the superconducting region, $μ_S$, where the BEC phase is indicated by $μ_S <0$. The chemical potential relative to the band edge in the normal metal, $μ_N$, allows us to tune the junction between the SNS case ($μ_N>0$) and the SIS case, where the superconductors are separated by a tunneling barrier. We find that there are Andreev levels in the BEC regime, as long as there is sufficient density of states in the normal region, i.e. when $μ_N>Δ$, where $Δ$ is the amplitude of the superconducting order parameter. For 1D SNS junctions, we find the Josephson current $I_S$ carried by these Andreev levels to be a function of the ratio $Δ/Δ_d$, where $Δ_d$ is the Andreev level spacing. At zero temperature, the Josephson current has a maximum on the BCS side of the transition where $Δ$ is maximal. At finite temperature, however, we find that the maximum moves to the BEC side of the crossover. We identify the mechanism for this phenomenon to be the decrease in the number of Andreev levels at the BCS-BEC crossover, accompanied by an increase in excitation energy to the unoccupied levels, making it less likely that these states are thermally occupied. Thereby, at finite temperature, the Josephson current is more strongly reduced on the BCS side of the crossover, resulting in a maximal Josephson current at the BCS-BEC crossover.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19396
Hashing for Sampling-Based Estimation,['Data Structures and Algorithms'],"['Anders Aamand', 'Ioana O. Bercea', 'Jakob Bæk Tejs Houen', 'Jonas Klausen', 'Mikkel Thorup']","Hash-based sampling and estimation are common themes in computing. Using hashing for sampling gives us the coordination needed to compare samples from different sets. Hashing is also used when we want to count distinct elements. The quality of the estimator for, say, the Jaccard similarity between two sets, depends on the concentration of the number of sampled elements from their intersection. Often we want to compare one query set against many stored sets to find one of the most similar sets, so we need strong concentration and low error-probability. In this paper, we provide strong explicit concentration bounds for Tornado Tabulation hashing [Bercea, Beretta, Klausen, Houen, and Thorup, FOCS'23] which is a realistic constant time hashing scheme. Previous concentration bounds for fast hashing were off by orders of magnitude, in the sample size needed to guarantee the same concentration. The true power of our result appears when applied in the local uniformity framework by [Dahlgaard, Knudsen, Rotenberg, and Thorup, STOC'15].△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19394
Global Tensor Motion Planning,['Robotics'],"['An T. Le', 'Kay Hansel', 'João Carvalho', 'Joe Watson', 'Julen Urain', 'Armin Biess', 'Georgia Chalvatzaki', 'Jan Peters']","Batch planning is increasingly crucial for the scalability of robotics tasks and dataset generation diversity. This paper presents Global Tensor Motion Planning (GTMP) -- a sampling-based motion planning algorithm comprising only tensor operations. We introduce a novel discretization structure represented as a random multipartite graph, enabling efficient vectorized sampling, collision checking, and search. We provide an early theoretical investigation showing that GTMP exhibits probabilistic completeness while supporting modern GPU/TPU. Additionally, by incorporating smooth structures into the multipartite graph, GTMP directly plans smooth splines without requiring gradient-based optimization. Experiments on lidar-scanned occupancy maps and the MotionBenchMarker dataset demonstrate GTMP's computation efficiency in batch planning compared to baselines, underscoring GTMP's potential as a robust, scalable planner for diverse applications and large-scale robot learning tasks.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19393
A New LISA-Detectable Type Ia Supernova Progenitor in the Southern Sky: SMSS J1138-5139,['Solar and Stellar Astrophysics'],"['Alekzander Kosakowski', 'Matti Dorsch', 'Warren R. Brown', 'Thomas Kupfer', 'Fatma Ben Daya', 'Mukremin Kilic']","We present the discovery and analysis of a nearby eclipsing ultra-compact accreting binary at coordinates 11:38:10.91 $-$51:39:49.15 (SMSS J1138$-$5139), the first well-constrained LISA-detectable Type Ia supernova progenitor. Our time series optical spectroscopy identifies its orbital period through radial velocity monitoring at $P_{\rm orb,RV}=27.69\pm0.03~{\rm min}$; twice the photometric period seen in 2-minute cadence data from TESS Sector 37. We model our optical spectroscopy together with new simultaneous multi-band time series photometry from Gemini to place constraints on the binary parameters. Our light curve modeling finds that SMSS J1138$-$5139 contains an $M_2=0.24~{\rm M_\odot}$ pre-white dwarf donor with a massive $M_1=0.99~{\rm M_\odot}$ white dwarf accretor at orbital inclination $i=88.7~{\rm deg}$. Based on our photometrically derived system parameters, we expect that gravitational wave radiation will drive SMSS J1138$-$5139 to a merger within $τ=5.7\pm0.3~{\rm Myr}$ and result in a Type Ia supernova. Even without a direct merger event, the component masses of SMSS J1138$-$5139 and active hydrogen accretion suggest that eventual helium accretion will likely also trigger a Type Ia supernova explosion through the dynamically-driven double-degenerate double-detonation (D6) channel. We expect LISA to detect the gravitational wave emission from SMSS J1138$-$5139 with signal-to-noise $7-10$ after a 48-month mission.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19391
DreamBlend: Advancing Personalized Fine-tuning of Text-to-Image Diffusion Models,['Computer Vision and Pattern Recognition'],"['Shwetha Ram', 'Tal Neiman', 'Qianli Feng', 'Andrew Stuart', 'Son Tran', 'Trishul Chilimbi']","Given a small number of images of a subject, personalized image generation techniques can fine-tune large pre-trained text-to-image diffusion models to generate images of the subject in novel contexts, conditioned on text prompts. In doing so, a trade-off is made between prompt fidelity, subject fidelity and diversity. As the pre-trained model is fine-tuned, earlier checkpoints synthesize images with low subject fidelity but high prompt fidelity and diversity. In contrast, later checkpoints generate images with low prompt fidelity and diversity but high subject fidelity. This inherent trade-off limits the prompt fidelity, subject fidelity and diversity of generated images. In this work, we propose DreamBlend to combine the prompt fidelity from earlier checkpoints and the subject fidelity from later checkpoints during inference. We perform a cross attention guided image synthesis from a later checkpoint, guided by an image generated by an earlier checkpoint, for the same prompt. This enables generation of images with better subject fidelity, prompt fidelity and diversity on challenging prompts, outperforming state-of-the-art fine-tuning methods.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19390
Limitations of Quantum Approximate Optimization in Solving Generic Higher-Order Constraint-Satisfaction Problems,['Quantum Physics'],"['Thorge Müller', 'Ajainderpal Singh', 'Frank K. Wilhelm', 'Tim Bode']","The ability of the Quantum Approximate Optimization Algorithm (QAOA) to deliver a quantum advantage on combinatorial optimization problems is still unclear. Recently, a scaling advantage over a classical solver was postulated to exist for random 8-SAT at the satisfiability threshold. At the same time, the viability of quantum error mitigation for deep circuits on near-term devices has been put in doubt. Here, we analyze the QAOA's performance on random Max-$k$XOR as a function of $k$ and the clause-to-variable ratio. As a classical benchmark, we use the Mean-Field Approximate Optimization Algorithm (MF-AOA) and find that it performs better than or equal to the QAOA on average. Still, for large $k$ and numbers of layers $p$, there may remain a window of opportunity for the QAOA. However, by extrapolating our numerical results, we find that reaching high levels of satisfaction would require extremely large $p$, which must be considered rather difficult both in the variational context and on near-term devices.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19388
Random Effects Misspecification and its Consequences for Prediction in Generalized Linear Mixed Models,['Methodology'],"['Quan Vu', 'Francis K. C. Hui', 'Samuel Muller', 'A. H. Welsh']","When fitting generalized linear mixed models (GLMMs), one important decision to make relates to the choice of the random effects distribution. As the random effects are unobserved, misspecification of this distribution is a real possibility. In this article, we investigate the consequences of random effects misspecification for point prediction and prediction inference in GLMMs, a topic on which there is considerably less research compared to consequences for parameter estimation and inference. We use theory, simulation, and a real application to explore the effect of using the common normality assumption for the random effects distribution when the correct specification is a mixture of normal distributions, focusing on the impacts on point prediction, mean squared prediction errors (MSEPs), and prediction intervals. We found that the optimal shrinkage is different under the two random effect distributions, so is impacted by misspecification. The unconditional MSEPs for the random effects are almost always larger under the misspecified normal random effects distribution, especially when cluster sizes are small. Results for the MSEPs conditional on the random effects are more complicated, but they remain generally larger under the misspecified distribution when the true random effect is close to the mean of one of the component distributions in the true mixture distribution. Results for prediction intervals indicate that overall coverage probability is not greatly impacted by misspecification.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19384
Can the central compact object in HESS J1731--347 be indeed the lightest neutron star observed?,['High Energy Astrophysical Phenomena'],"['Shu-Rui Zhang', 'Jorge A. Rueda', 'Rodrigo Negreiros']","The exceptionally low mass of $0.77_{-0.17}^{+0.2} M_{\odot}$ for the central compact object (CCO) XMMU J173203.3 -- 344518 (XMMU J1732) in the supernova remnant (SNR) HESS J1731 -- 347 challenges standard neutron star (NS) formation models. The nearby post-AGB star IRAS 17287 -- 3443 ($\approx 0.6 M_\odot$), also within the SNR, enriches the scenario. To address this puzzle, we advance the possibility that the gravitational collapse of a rotating pre-SN iron core ($\approx 1.2 M_\odot$) could result in a low-mass NS. We show that angular momentum conservation during the collapse of an iron core rotating at $\approx 45\%$ of the Keplerian limit results in a mass loss of $\approx 0.3 M_\odot$, producing a stable newborn NS of $\approx 0.9 M_\odot$. Considering the possible spin-down, this indicates that the NS is now slowly rotating, thus fulfilling the observed mass-radius relation. Additionally, the NS's surface temperature ($\approx 2 \times 10^6$ K) aligns with canonical thermal evolution for its $\approx 4.5$ kyr age. We propose the pre -- SN star, likely an ultra-stripped core of $\approx 4.2 M_\odot$, formed a tidally locked binary with IRAS 17287 -- 3443, having a 1.43-day orbital period. The supernova led to a $\approx 3 M_\odot$ mass loss, imparting a kick velocity $\lesssim 670$ km s$^{-1}$, which disrupted the binary. This scenario explains the observed 0.3 pc offset between XMMU J1732 and IRAS 17287 -- 3443 and supports the possibility of CCOs forming in binaries, with rotation playing a key role in core-collapse, and the CCO XMMU J1732 being the lightest NS ever observed.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19382
Dynamic matching games: stationary equilibria under varying commitments,['Theoretical Economics'],"['Nadia Guiñazú', 'Pablo Neme', 'Jorge Oviedo']","This paper examines equilibria in dynamic two-sided matching games, extending Gale and Shapley's foundational model to a non-cooperative, decentralized, and dynamic framework. We focus on markets where agents have utility functions and commitments vary. Specifically, we analyze a dynamic matching game in which firms make offers to workers in each period, considering three types of commitment: (i) no commitment from either side, (ii) firms' commitment, and (iii) workers' commitment. Our results demonstrate that stable matchings can be supported as stationary equilibria under different commitment scenarios, depending on the strategies adopted by firms and workers. Furthermore, we identify key conditions, such as discount factors, that influence agents' decisions to switch partners, thereby shaping equilibrium outcomes.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19372
Parameter-Efficient Transfer Learning for Music Foundation Models,['Sound'],"['Yiwei Ding', 'Alexander Lerch']","More music foundation models are recently being released, promising a general, mostly task independent encoding of musical information. Common ways of adapting music foundation models to downstream tasks are probing and fine-tuning. These common transfer learning approaches, however, face challenges. Probing might lead to suboptimal performance because the pre-trained weights are frozen, while fine-tuning is computationally expensive and is prone to overfitting. Our work investigates the use of parameter-efficient transfer learning (PETL) for music foundation models which integrates the advantage of probing and fine-tuning. We introduce three types of PETL methods: adapter-based methods, prompt-based methods, and reparameterization-based methods. These methods train only a small number of parameters, and therefore do not require significant computational resources. Results show that PETL methods outperform both probing and fine-tuning on music auto-tagging. On key detection and tempo estimation, they achieve similar results as fine-tuning with significantly less training cost. However, the usefulness of the current generation of foundation model on key and tempo tasks is questioned by the similar results achieved by training a small model from scratch. Code available at https://github.com/suncerock/peft-music/△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19371
Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control,['Artificial Intelligence'],"['Dickness Kakitahi Kwesiga', 'Suyash Chandra Vishnoi', 'Angshuman Guin', 'Michael Hunter']","This study integrates Transit Signal Priority (TSP) into multi-agent reinforcement learning (MARL) based traffic signal control. The first part of the study develops adaptive signal control based on MARL for a pair of coordinated intersections in a microscopic simulation environment. The two agents, one for each intersection, are centrally trained using a value decomposition network (VDN) architecture. The trained agents show slightly better performance compared to coordinated actuated signal control based on overall intersection delay at v/c of 0.95. In the second part of the study the trained signal control agents are used as background signal controllers while developing event-based TSP agents. In one variation, independent TSP agents are formulated and trained under a decentralized training and decentralized execution (DTDE) framework to implement TSP at each intersection. In the second variation, the two TSP agents are centrally trained under a centralized training and decentralized execution (CTDE) framework and VDN architecture to select and implement coordinated TSP strategies across the two intersections. In both cases the agents converge to the same bus delay value, but independent agents show high instability throughout the training process. For the test runs, the two independent agents reduce bus delay across the two intersections by 22% compared to the no TSP case while the coordinated TSP agents achieve 27% delay reduction. In both cases, there is only a slight increase in delay for a majority of the side street movements.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19359
Characterizing JavaScript Security Code Smells,['Cryptography and Security'],"['Vikas Kambhampati', 'Nehaz Hussain Mohammed', 'Amin Milani Fard']","JavaScript has been consistently among the most popular programming languages in the past decade. However, its dynamic, weakly-typed, and asynchronous nature can make it challenging to write maintainable code for developers without in-depth knowledge of the language. Consequently, many JavaScript applications tend to contain code smells that adversely influence program comprehension, maintenance, and debugging. Due to the widespread usage of JavaScript, code security is an important matter. While JavaScript code smells and detection techniques have been studied in the past, current work on security smells for JavaScript is scarce. Security code smells are coding patterns indicative of potential vulnerabilities or security weaknesses. Identifying security code smells can help developers to focus on areas where additional security measures may be needed. We present a set of 24 JavaScript security code smells, map them to a possible security awareness defined by Common Weakness Enumeration (CWE), explain possible refactoring, and explain our detection mechanism. We implement our security code smell detection on top of an existing open source tool that was proposed to detect general code smells in JavaScript.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19358
Fine-Tuning Magnetism in CrI$_3$ Monolayers and Bilayers: A DFT+U Approach,['Mesoscale and Nanoscale Physics'],"['Diego Lauer', 'Jhon W. González', 'Eric Suárez Morell', 'Andrés Ayuela']","The magnetic properties of CrI$_3$ monolayers, which were recently measured, have been investigated considering electronic repulsion and localization effects in Cr 3d orbitals. In this study, we propose a DFT approach using Hubbard U corrections to improve accuracy. We compare the valence density-of-states using the HSE06 hybrid functional and the DFT+U approach, which includes U parameters for both Cr 3d and I 5p orbitals. The results of our study indicate that the optimal values for U(Cr$_{3d}$) and U(I$_{5p}$) are 3.5 eV and 2.0 eV, respectively. This approach is further applied to improve calculations of electronic and magnetic properties in CrI$_3$ monolayers and, more importantly, in bilayers combined with van der Waals functionals. These refinements hold promise for further studies of complex CrI$_3$ nanostructures, and may also be of interest for other trihalide few-layer systems.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19357
"On the matching arrangement of a graph,improper weight function problem and its application",['Combinatorics'],"['A. I. Bolotnikov', 'A. A. Irmatov']","This article presents examples of an application of the finite field method for the computation of the characteristic polynomial of the matching arrangement of a graph. Weight functions on edges of a graph with weights from a finite field are divided into proper and improper functions in connection with proper colorings of vertices of the matching polytope of a graph. An improper weight function problem is introduced, a proof of its NP-completeness is presented, and a knapsack-like public key cryptosystem is constructed based on the improper weight function problem.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19351
Quadrupole gauge theory: anti-Higgs mechanism and elastic dual,['Strongly Correlated Electrons'],"['Aleksander Głódkowski', 'Paweł Matus', 'Francisco Peña-Benítez', 'Lazaros Tsaloukidis']","Motivated by the duality between elasticity and fracton gauge theory, we study an extension of the gauge group that includes the quadrupole moment. Remarkably, we find that spontaneous breaking of the quadrupole symmetry increases the number of massless excitations. This result appears to challenge the well-established paradigm, according to which gauge fields acquire mass through the Higgs mechanism, and the would-be Goldstone bosons are rendered massive. We refer to this phenomenon as the anti-Higgs mechanism. Furthermore, in the standard treatment of the fracton-elasticity duality, the conservation of quadrupole moment is incorporated ad hoc in order to restrict the motion of dislocations to glide along their Burgers vectors. Our study clarifies the origin of the quadrupole symmetry in the dual theory of elasticity, demonstrating that quadrupole gauge theory is not a priori dual to elastic solids but rather to a distinct class of systems, which we dub incompressible crystals. We argue, however, that the phenomenology of ordinary elasticity is recovered after the quadrupole symmetry is Higgsed and an incompressible crystal undergoes a phase transition to a compressible solid state.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19350
Exact solutions and reductions of nonlinear Schrödinger equations with delay,['Exactly Solvable and Integrable Systems'],"['Andrei D. Polyanin', 'Nikolay A. Kudryashov']","For the first time, Schrödinger equations with cubic and more complex nonlinearities containing the unknown function with constant delay are analyzed. The physical considerations that can lead to the appearance of a delay in such nonlinear equations and mathematical models are expressed. One-dimensional non-symmetry reductions are described, which lead the studied partial differential equations with delay to simpler ordinary differential equations and ordinary differential equations with delay. New exact solutions of the nonlinear Schrödinger equation of the general form with delay, which are expressed in quadratures, are found. To construct exact solutions, a combination of methods of generalized separation of variables and the method of functional constraints are used. Special attention is paid to three equations with cubic nonlinearity, which allow simple solutions in elementary functions, as well as more complex exact solutions with generalized separation of variables. Solutions representing a nonlinear superposition of two traveling waves, the amplitude of which varies periodically in time and space, are constructed. Some more complex nonlinear Schrödinger equations of a general form with variable delay are also studied. The results of this work can be useful for the development and improvement of mathematical models described by nonlinear Schrödinger equations with delay and related functional PDEs, and the obtained exact solutions can be used as test problems intended to assess the accuracy of numerical methods for integrating nonlinear equations of mathematical physics with delay.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19349
CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections,['Computer Vision and Pattern Recognition'],"['Mohamed Fazli Imam', 'Rufael Fedaku Marew', 'Jameel Hassan', 'Mustansar Fiaz', 'Alham Fikri Aji', 'Hisham Cholakkal']","In the era of foundation models, CLIP has emerged as a powerful tool for aligning text and visual modalities into a common embedding space. However, the alignment objective used to train CLIP often results in subpar visual features for fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at extracting rich visual features due to their specialized training paradigm. Yet, these SSL models require an additional supervised linear probing step, which relies on fully labeled data which is often expensive and difficult to obtain at scale. In this paper, we propose a label-free prompt-tuning method that leverages the rich visual features of self-supervised learning models (DINO) and the broad textual knowledge of large language models (LLMs) to largely enhance CLIP-based image classification performance using unlabeled images. Our approach unfolds in three key steps: (1) We generate robust textual feature embeddings that more accurately represent object classes by leveraging class-specific descriptions from LLMs, enabling more effective zero-shot classification compared to CLIP's default name-specific prompts. (2) These textual embeddings are then used to produce pseudo-labels to train an alignment module that integrates the complementary strengths of LLM description-based textual embeddings and DINO's visual features. (3) Finally, we prompt-tune CLIP's vision encoder through DINO-assisted supervision using the trained alignment module. This three-step process allows us to harness the best of visual and textual foundation models, resulting in a powerful and efficient approach that surpasses state-of-the-art label-free classification methods. Notably, our framework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6% over the state-of-the-art LaFter across 11 diverse image classification datasets.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19346
3D Wasserstein generative adversarial network with dense U-Net based discriminator for preclinical fMRI denoising,['Image and Video Processing'],"['Sima Soltanpour', 'Arnold Chang', 'Dan Madularu', 'Praveen Kulkarni', 'Craig Ferris', 'Chris Joslin']","Functional magnetic resonance imaging (fMRI) is extensively used in clinical and preclinical settings to study brain function, however, fMRI data is inherently noisy due to physiological processes, hardware, and external noise. Denoising is one of the main preprocessing steps in any fMRI analysis pipeline. This process is challenging in preclinical data in comparison to clinical data due to variations in brain geometry, image resolution, and low signal-to-noise ratios. In this paper, we propose a structure-preserved algorithm based on a 3D Wasserstein generative adversarial network with a 3D dense U-net based discriminator called, 3D U-WGAN. We apply a 4D data configuration to effectively denoise temporal and spatial information in analyzing preclinical fMRI data. GAN-based denoising methods often utilize a discriminator to identify significant differences between denoised and noise-free images, focusing on global or local features. To refine the fMRI denoising model, our method employs a 3D dense U-Net discriminator to learn both global and local distinctions. To tackle potential over-smoothing, we introduce an adversarial loss and enhance perceptual similarity by measuring feature space distances. Experiments illustrate that 3D U-WGAN significantly improves image quality in resting-state and task preclinical fMRI data, enhancing signal-to-noise ratio without introducing excessive structural changes in existing methods. The proposed method outperforms state-of-the-art methods when applied to simulated and real data in a fMRI analysis pipeline.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19345
Stoch-IMC: A Bit-Parallel Stochastic In-Memory Computing Architecture Based on STT-MRAM,['Hardware Architecture'],"['Amir M. Hajisadeghi', 'Hamid R. Zarandi', 'Mahmoud Momtazpour']","In-memory computing (IMC) offloads parts of the computations to memory to fulfill the performance and energy demands of applications such as neuromorphic computing, machine learning, and image processing. Fortunately, the main features that stochastic computing (SC) and IMC share, which are low computation complexity and high bit-parallel computation capability, promise great potential for integrating SC and IMC. In this paper, we exploit this potential by using stochastic computation as an approximation method to present effective in-memory computations with a good trade-off among design parameters. To this end, first, commonly used stochastic arithmetic operations of applications are effectively implemented using the primitive logic gates of the IMC method. Next, the in-memory scheduling and mapping of applications are obtained efficiently by a proposed algorithm. This algorithm reduces the computation latency by enabling intra-subarray parallelism while considering the IMC method constraints. Subsequently, a bit-parallel stochastic IMC architecture, Stoch-IMC, is presented that enables bit parallelization of stochastic computations over memory subarrays/banks. To evaluate Stoch-IMC's effectiveness, various analyses were conducted. Results show average performance improvements of 135.7X and 124.2X across applications compared to binary IMC and related in-memory SC methods, respectively. The results also demonstrate an average energy reduction of 1.5X compared to binary IMC, with limited energy overhead relative to the in-memory SC method. Furthermore, the results reveal average lifetime improvements of 4.9X and 216.3X over binary IMC and in-memory SC methods, respectively, along with high bitflip tolerance.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19344
Understanding the Growth and Properties of Sputter-Deposited Phase-Change Superlattice Films,['Materials Science'],"['Simone Prili', 'Valeria Bragaglia', 'Vara Prasad Jonnalagadda', 'Jesse Luchtenveld', 'Fabrizio Arciprete', 'Bart J. Kooi', 'Abu Sebastian', 'Ghazi Sarwat Syed']","Highly textured chalcogenide films have recently gained significant interest for phase-change memory applications. Several reports have highlighted that programming efficiency improves in devices featuring superlattice stacks, such as Ge2Sb2Te5/Sb2Te3. However, to be technologically relevant, these films must be deposited on foundry-scale wafers using processes compatible with back end of the line (BEOL) integration and complementary metal-oxide-semiconductor (CMOS) technology, such as, for example, sputter deposition. In this work, we present our observations on the influence of temperature, pressure, and seeding layer parameters on the sputter growth processes of superlattice films. By measuring various material properties, we construct a pseudo-phase diagram to illustrate the growth of both individual and superlattice films with different periodicities on technologically relevant substrates, namely SiO2 and carbon. These results provide important insights into the structure, intermixing and electro-optical properties of superlattice films,△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19343
CME propagation in the dynamically coupled space weather tool: COCONUT + EUHFORIA,['Solar and Stellar Astrophysics'],"['L. Linan', 'T. Baratashvili', 'A. Lani', 'B. Schmieder', 'M. Brchnelova', 'J. H. Guo', 'S. Poedts']","This paper aims to present the time-dependent coupling between the coronal model COolfluid COroNal UnsTructured (COCONUT) and the heliospheric forecasting tool EUHFORIA.
  We perform six COCONUT simulations where a flux rope is implemented at the solar surface using either the Titov-Démoulin CME model or the Regularized Biot-Savart Laws (RBSL) CME model. At regular intervals, the magnetic field, velocity, temperature, and density of the 2D surface $R_{b}=21.5~\;R_{\odot}$ are saved in boundary files. This series of coupling files is read in a modified version of EUHFORIA to update progressively its inner boundary. After presenting the early stage of the propagation in COCONUT, we examine how the disturbance of the solar corona created by the propagation of flux ropes is transmitted into EUHFORIA. In particular, we consider the thermodynamic and magnetic profiles at L1 and compare them with those obtained at the interface between the two models.
  We demonstrate that the properties of the heliospheric solar wind in EUHFORIA are consistent with those in COCONUT, acting as a direct extension of the coronal domain. Moreover, the disturbances initially created from the propagation of flux ropes in COCONUT continue evolving from the corona in the heliosphere to Earth with a smooth transition at the interface between the two simulations. Looking at the profile of magnetic field components at Earth and different distances from the Sun, we also find that the transient magnetic structures have a self-similar expansion in COCONUT and EUHFORIA. However, the amplitude of the profiles depends on the flux rope model used and its properties, thus emphasizing the important role of the initial properties in solar source regions for accurately predicting the impact of CMEs.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19340
Diode effect in Fraunhofer patterns of disordered multi-terminal Josephson junctions,['Superconductivity'],"['Luca Chirolli', 'Angelo Greco', 'Alessandro Crippa', 'Elia Strambini', 'Mario Cuoco', 'Luigi Amico', 'Francesco Giazotto']","We study the role of different spatial inhomogeneities in generating the conditions for the appearance of a superconducting diode effect in the Fraunhofer pattern of wide Josephson junctions. Through the scattering matrix approach, we highlight the role of mirror symmetry of the junction in forbidding the diode effect in both the two-terminal and the multi-terminal case. As sources of mirror symmetry breaking, we study spatial potentials of long and short wavelength with respect to the size of the system, mimicking the effect of side gates and atomic scale disorder, respectively, as well as the geometry of the junction, and assess their impact on the diode effect. As a common trend, we observe qualitatively similar rectification patterns magnified at the nodal points of the Fraunhofer pattern by destructing interference. In multi-terminal mirror-symmetric setups, we single out the phase at additional terminals as a controllable knob to tune the diode effect at the finite field. The work presents a comprehensive treatment of the role of pure spatial inhomogeneity in the emergence of a diode effect in wide junctions.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19338
Continuity aspects for traces of Dirichlet forms with respect to monotone weak convergence of G-Kato measures,['Functional Analysis'],['Ali BenAmor'],"We investigate some analytic properties of traces of Dirichlet forms with respect to measures satisfying Hardy-type inequality. Among other results we prove convergence of spectra, ordered eigenvalues, eigenfunctions as well as convergence of resolvents on appropriate spaces, for traces of Dirichlet forms when the speed measure is the monotone weak limit of G-Kato measures. Some quantitative estimates are also given. As applications we show continuity of stationary solutions of some elliptic operators with measure-valued coefficients with respect to the coefficients and give an approximation procedure for the eigenvalues of one-dimensional graph Laplacian as well as for the Laplacian on annular thin sets with mixed Neumann- Wentzell boundary conditions.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19336
Deviations from the universal Initial Mass Function in binary star clusters,['Astrophysics of Galaxies'],"['Sunder S. K. Singh-Bal', 'George A. Blaylock-Squibbs', 'Richard J. Parker', 'Simon P. Goodwin']","The stellar mass distribution in star-forming regions, stellar clusters and associations, the Initial Mass Function (IMF), appears to be invariant across different star-forming environments, and is consistent with the IMF observed in the Galactic field. Deviations from the field, or standard, IMF, if genuine, would be considered strong evidence for a different set of physics at play during the formation of stars in the birth region in question. We analyse N-body simulations of the evolution of spatially and kinematically substructured star-forming regions to identify the formation of binary star clusters, where two (sub)clusters which form from the same Giant Molecular Cloud orbit a common centre of mass. We then compare the mass distributions of stars in each of the subclusters and compare them to the standard IMF, which we use to draw the stellar masses in the star-forming region from which the binary cluster(s) form. In each binary cluster that forms, the mass distributions of stars in one subcluster deviates from the standard IMF, and drastically so when we apply similar mass resolution limits as for the observed binary clusters. Therefore, if a binary subcluster is observed to have an unusual IMF, this may simply be the result of dynamical evolution, rather than different physical conditions for star formation in these systems.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19333
Signatures of black hole seeding in the local Universe: Predictions from the BRAHMA cosmological simulations,['Astrophysics of Galaxies'],"['Aklant K Bhowmick', 'Laura Blecha', 'Paul Torrey', 'Rachel S Somerville', 'Luke Zoltan Kelley', 'Rainer Weinberger', 'Mark Vogelsberger', 'Lars Hernquist', 'Priyamvada Natarajan', 'Jonathan Kho', 'Tiziana Di Matteo']","The first ""seeds"" of supermassive black holes (BHs) continue to be an outstanding puzzle, and it is currently unclear whether the imprints of early seed formation survive today. Here we examine the signatures of seeding in the local Universe using five $[18~\mathrm{Mpc}]^3$ BRAHMA simulation boxes run to $z=0$. They initialize $1.5\times10^5~M_{\odot}$ BHs using different seeding models. The first four boxes initialize BHs as heavy seeds using criteria that depend on dense & metal-poor gas, Lyman-Werner radiation, gas spin, and environmental richness. The fifth box initializes BHs as descendants of lower mass seeds ($\sim10^3~M_{\odot}$) using a new stochastic seed model built in our previous work. We find that strong signatures of seeding survive in $\sim10^5-10^6~M_{\odot}$ local BHs hosted in $M_*\lesssim10^{9}~M_{\odot}$ dwarf galaxies. The signatures survive due to two reasons: 1) there is a substantial population of local $\sim10^5~M_{\odot}$ BHs that are ungrown relics of early seeds from $z\sim5-10$; 2) BH growth up to $\sim10^6~M_{\odot}$ is dominated by mergers all the way down to $z\sim0$. As the contribution from gas accretion increases, the signatures of seeding start to weaken in more massive $\gtrsim10^6~M_{\odot}$ BHs, and they eventually disappear for $\gtrsim10^7~M_{\odot}$ BHs. This is in contrast to high-z ($z\gtrsim5$) BH populations wherein the BH growth is fully merger dominated, which causes the seeding signatures to persist at least up to $\sim10^8~M_{\odot}$. The different seed models predict abundances of local $\sim10^6~M_{\odot}$ BHs ranging from $\sim0.01-0.05~\mathrm{Mpc}^{-3}$ with occupation fractions of $\sim20-100\%$ in $M_*\sim10^{9}~M_{\odot}$ galaxies. Our results highlight the potential for local $\sim10^5-10^6~M_{\odot}$ BH populations in dwarf galaxies to serve as a promising probe for BH seeding models.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19332
The dramatic transition of the extreme Red Supergiant WOH G64 to a Yellow Hypergiant,['Solar and Stellar Astrophysics'],"['G. Munoz-Sanchez', 'M. Kalitsounaki', 'S. de Wit', 'K. Antoniadis', 'A. Z. Bonanos', 'E. Zapartas', 'K. Boutsia', 'E. Christodoulou', 'G. Maravelias', 'I. Soszynski', 'A. Udalski']","Red Supergiants (RSGs) are cool, evolved massive stars in their final evolutionary stage before exploding as a supernova. However, the evolution and fate of the most luminous RSGs is uncertain. Observational evidence for luminous warm, post-RSG objects and the lack of luminous RSGs as supernova progenitors suggest a blueward evolution. In this work, we analyze WOH G64, considered since the 1980s as the most extreme RSG in the Large Magellanic Cloud in terms of its size, luminosity, and mass-loss rate. Time-series photometry over the last 30 years reveals a sudden, yet smooth change from semi-regular to irregular variability in 2014. Multi-epoch optical spectroscopy confirms the transition, as WOH~G64 now exhibits properties of a B[e] star in the optical, and warm-star features in the near-infrared. We discovered that WOH G64 is a rare, massive symbiotic system formed by a RSG, which transitioned to a Yellow Hypergiant, and a B-star companion. The dramatic transition can be explained by: a) binary interactions partially stripping the envelope, b) the return of WOH G64 to a quiescent state after an outstanding eruption exceeding 30 years, and c) the expulsion of its outer layers due to a pre-SN superwind phase, indicating its imminent explosion. WOH~G64 offers a unique opportunity to witness stellar evolution in real-time, providing crucial clues for the final phases of massive stars and their resulting supernovae.△ Less",v1,https://arxiv.org/pdf/2411.19329
Tayler-Spruit dynamo in binary neutron star merger remnants,['High Energy Astrophysical Phenomena'],"['Alexis Reboul-Salze', 'Paul Barrère', 'Kenta Kiuchi', 'Jérôme Guilet', 'Raphaël Raynaud', 'Sho Fujibayashi', 'Masaru Shibata']","In binary neutron star mergers, the remnant can be stabilized by differential rotation before it collapses into a black hole. Therefore, the angular momentum transport mechanisms are crucial for predicting the lifetime of the hypermassive neutron star. One such mechanism is the Tayler-Spruit dynamo, and recent simulations have shown that it could grow in proto-neutron stars formed during supernova explosions. We aim to investigate whether hypermassive neutron stars with high neutrino viscosity could be unstable to the Tayler-Spruit dynamo and study how magnetic fields would evolve in this context. Using a one-zone model based on the result of a 3D GRMHD simulation, we investigate the time evolution of the magnetic fields generated by the Tayler-Spruit dynamo. In addition, we analyze the dynamics of the 3D GRMHD simulation to determine whether the dynamo is present. Our one-zone model predicts that the Tayler-Spruit dynamo can increase the toroidal magnetic field to $ \ge 10^{17}$ G and the dipole field to amplitudes $\ge 10^{16}$ G. The dynamo's growth timescale depends on the initial large-scale magnetic field right after the merger. In the case of a long-lived hypermassive neutron star, an initial magnetic field of $\ge 10^{12}$ G would be enough for the magnetic field to be amplified in a few seconds. However, we show that the resolution of the current GRMHD simulations is insufficient to resolve the Tayler-Spruit dynamo due to high numerical dissipation at small scales. We find that the Tayler-Spruit dynamo could occur in hypermassive neutron stars and shorten their lifetime, which would have consequences on multi-messenger observations.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19328
GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks,['Computer Vision and Pattern Recognition'],"['Muhammad Sohail Danish', 'Muhammad Akhtar Munir', 'Syed Roshaan Ali Shah', 'Kartik Kuckreja', 'Fahad Shahbaz Khan', 'Paolo Fraccaro', 'Alexandre Lacoste', 'Salman Khan']","While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they fall short in addressing the unique demands of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, which is critical for applications such as environmental monitoring, urban planning, and disaster management. Some of the unique challenges in geospatial domain include temporal analysis for changes, counting objects in large quantities, detecting tiny objects, and understanding relationships between entities occurring in Remote Sensing imagery. To address this gap in the geospatial domain, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and covers a diverse set of variations in visual conditions, object type, and scale. We evaluate several state-of-the-art VLMs to assess their accuracy within the geospatial context. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific examples, highlighting the room for further improvements. Specifically, the best-performing GPT4o achieves only 40\% accuracy on MCQs, which is only double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19325
Learning Surrogate Rainfall-driven Inundation Models with Few Data,['Atmospheric and Oceanic Physics'],['Marzieh Alireza Mirhoseini'],"Flood hazard assessment demands fast and accurate predictions. Hydrodynamic models are detailed but computationally intensive, making them impractical for quantifying uncertainty or identifying extremes. In contrast, machine learning surrogates can be rapid, but training on scarce simulated or observed extreme data can also be ineffective. This work demonstrates the development of an effective surrogate model for flood hazard prediction by initializing deep learning (ResNet-18) with ensemble-approximated Conditional Gaussian Processes (EnsCGP) and finalizing it with a bias correction. The proposed methodology couples EnsCGP with a ResNet-18 architecture to estimate flood depth and uses ensemble optimal estimation for bias correction. The surrogate model was trained and evaluated using rainfall data from Daymet and hydrodynamic simulations from LISFLOOD-FP, spanning the period from 1981 to 2019. The training involved using data up to a certain year and testing on the subsequent year, iteratively progressing through the dataset. This process required approximately 100 training iterations and extensive data. Inundation depths are estimated rapidly at runtime (approximately 0.006 seconds per event). Results over multiple years in the current climate over Chicago demonstrate an average R-squared greater than 0.96, with median relative errors in flood depth estimates of about 1 percent.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19323
Deep learning interpretability for rough volatility,['Computational Finance'],"['Bo Yuan', 'Damiano Brigo', 'Antoine Jacquier', 'Nicola Pede']","Deep learning methods have become a widespread toolbox for pricing and calibration of financial models. While they often provide new directions and research results, their `black box' nature also results in a lack of interpretability. We provide a detailed interpretability analysis of these methods in the context of rough volatility - a new class of volatility models for Equity and FX markets. Our work sheds light on the neural network learned inverse map between the rough volatility model parameters, seen as mathematical model inputs and network outputs, and the resulting implied volatility across strikes and maturities, seen as mathematical model outputs and network inputs. This contributes to building a solid framework for a safer use of neural networks in this context and in quantitative finance more generally.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19317
On the characterization of Schmidt number breaking and annihilating channels,['Quantum Physics'],"['Bivas Mallick', 'Nirman Ganguly', 'A. S. Majumdar']","Transmission of high dimensional entanglement through quantum channels is a significant area of interest in quantum information science. The certification of high dimensional entanglement is usually done through Schmidt numbers. Schmidt numbers quantify the entanglement dimension of quantum states. States with high Schmidt numbers provide a larger advantage in various quantum information processing tasks compared to quantum states with low Schmidt numbers. However, some quantum channels can reduce the Schmidt number of states. Here we present a comprehensive analysis of Schmidt number breaking channels which reduce the Schmidt number of bipartite composite systems. From a resource theoretic perspective, it becomes imperative to identify channels that preserve the Schmidt number. Based on our characterization we lay down prescriptions to identify such channels which are non-resource breaking, i.e., preserve the Schmidt number. Additionally, we introduce a new class of quantum channels, termed Schmidt number annihilating channels which reduce the Schmidt number of a quantum state that is a part of a larger composite system. Finally, we study the connection between entanglement breaking, Schmidt number breaking, and Schmidt number annihilating channels.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19315
Smooth Perturbations to Rényi Entropy,['High Energy Physics - Theory'],['Andrew Buchanan'],"A method is presented for computing the Rényi entropy of a perturbed massless vacuum on the ball via a comparison with lattice field theory. If the perturbed state is Gaussian with smoothly varying correlation functions and the perturbation parameter has units of energy, I show the coefficients for Rényi entropy are analytically computable for all Rényi parameter $α$ in odd dimensions and for integer $α$ in even dimensions. I apply this procedure to compute coefficients for the large distant expansion for the Rényi mutual information of distant balls and the low temperature expansion for the entropy of a thermal field.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19312
Solving the Nonlinear Vlasov Equation on a Quantum Computer,['Quantum Physics'],"['Tamás Á Vaszary', 'Animesh Datta', 'Thomas Goffrey', 'Brian Appelbe']","We present a mapping of the nonlinear, electrostatic Vlasov equation with Krook type collision operators, discretized on a (1 + 1) dimensional grid, onto a recent Carleman linearization based quantum algorithm for solving ordinary differential equations (ODEs) with quadratic nonlinearities. We show that the quantum algorithm is guaranteed to converge only when the plasma parameters take unphysical values. This is due to the high level of dissipation in the ODE system required for convergence, that far exceeds the physical dissipation effect provided by the Krook operator. Additionally, we derive upper bounds for the query- and gate complexities of the quantum algorithm in the limit of large grid sizes. We conclude that these are polynomially larger than the time complexity of the corresponding classical algorithms. We find that this is mostly due to the dimension, sparsity and norm of the Carleman linearized evolution matrix.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19310
"Perspective of Software Engineering Researchers on Machine Learning Practices Regarding Research, Review, and Education",['Software Engineering'],"['Anamaria Mojica-Hanke', 'David Nader Palacio', 'Denys Poshyvanyk', 'Mario Linares-Vásquez', 'Steffen Herbold']","Context: Machine Learning (ML) significantly impacts Software Engineering (SE), but studies mainly focus on practitioners, neglecting researchers. This overlooks practices and challenges in teaching, researching, or reviewing ML applications in SE.
  Objective: This study aims to contribute to the knowledge, about the synergy between ML and SE from the perspective of SE researchers, by providing insights into the practices followed when researching, teaching, and reviewing SE studies that apply ML.
  Method: We analyzed SE researchers familiar with ML or who authored SE articles using ML, along with the articles themselves. We examined practices, SE tasks addressed with ML, challenges faced, and reviewers' and educators' perspectives using grounded theory coding and qualitative analysis.
  Results: We found diverse practices focusing on data collection, model training, and evaluation. Some recommended practices (e.g., hyperparameter tuning) appeared in less than 20\% of literature. Common challenges involve data handling, model evaluation (incl. non-functional properties), and involving human expertise in evaluation. Hands-on activities are common in education, though traditional methods persist.
  Conclusion: Despite accepted practices in applying ML to SE, significant gaps remain. By enhancing guidelines, adopting diverse teaching methods, and emphasizing underrepresented practices, the SE community can bridge these gaps and advance the field.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19304
Generalized Lindblad master equation for neutrino evolution,['High Energy Physics - Phenomenology'],"['Konstantin Stankevich', 'Alexander Studenikin', 'Maksim Vyalkov']","A new theoretical framework, based on the quantum field theory of open systems applied to neutrinos, has been developed. This framework aims to describe the neutrino evolution in external environment, taking into account the effect of neutrino quantum decoherence. We have applied this approach to investigate a novel mechanism for neutrino quantum decoherence, which arises due to the neutrino decay into a lighter neutrino state and a massless particle, as well as the inverse process of absorption of a massless particle by a neutrino. We derived the new generalized Lindblad master equation for the neutrino evolution that accounts for neutrino transitions between states with different momenta. We also demonstrate that studying of neutrino quantum decoherence through this master equation provides a unique possibility to determine or limit the neutrino decay width. On this basis we obtained the constraint on neutrino lifetime $\frac{τ_2}{m_2} > 1.83 \times 10^{-10} \frac{s}{eV}$ (for the case of the Dirac neutrino scalar decay and neutrino degenerate hierarchy).△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19303
Cosmic Topological Defects from Holography,['High Energy Physics - Theory'],"['Francesco Bigazzi', 'Aldo L. Cotrone', 'Andrea Olzi']","This work investigates cosmic topological defects in gauge theories, focusing on models with an $SU(N)$ gauge group coupled with a single flavor, explored through a holographic framework. At low energies, the effective theory is described by an axion-like particle resulting from the spontaneous breaking of the axial $U(1)_A$ flavor symmetry. As the Universe cools below a critical temperature, the chiral symmetry is broken, and non-trivial vacuum configurations form, resulting in the creation of cosmic strings and domain walls. We provide a UV description of these defects in a particular holographic theory, the Witten-Sakai-Sugimoto model, as probe D6-branes. We show the presence of a first-order phase transition separating string loop from domain wall solutions. String loops charged under the baryon symmetry and with angular momentum - vortons - can be understood as excitations of a topological phase of matter given by a Chern-Simons theory living on the D6-brane world volume. Finally, we provide an effective description of string loops and vortons in terms of degrees of freedom living on the flavor brane, i.e. mesonic modes.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19302
Structured Object Language Modeling (SoLM): Native Structured Objects Generation Conforming to Complex Schemas with Self-Supervised Denoising,['Software Engineering'],"['Amir Tavanaei', 'Kee Kiat Koo', 'Hayreddin Ceker', 'Shaobai Jiang', 'Qi Li', 'Julien Han', 'Karim Bouyarmane']","In this paper, we study the problem of generating structured objects that conform to a complex schema, with intricate dependencies between the different components (facets) of the object. The facets of the object (attributes, fields, columns, properties) can be a mix of short, structured, type-constrained facts, or long natural-language descriptions. The object has to be self-consistent between the different facets in the redundant information it carries (relative consistency), while being grounded with respect to world knowledge (absolute consistency). We frame the problem as a Language Modeling problem (Structured Object Language Modeling) and train an LLM to perform the task natively, without requiring instructions or prompt-engineering. We propose a self-supervised denoising method to train the model from an existing dataset of such objects. The input query can be the existing object itself, in which case the model acts as a regenerator, completing, correcting, normalizing the input, or any unstructured blurb to be structured. We show that the self-supervised denoising training provides a strong baseline, and that additional supervised fine-tuning with small amount of human demonstrations leads to further improvement. Experimental results show that the proposed method matches or outperforms prompt-engineered general-purpose state-of-the-art LLMs (Claude 3, Mixtral-8x7B), while being order-of-magnitude more cost-efficient.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19301
Fast Switching in Mixed-Integer Model Predictive Control,['Systems and Control'],"['Artemi Makarow', 'Christian Kirches']","We derive stability results for finite control set and mixed-integer model predictive control and propose a unified theoretical framework. The presentation rests upon the inherent robustness properties of common model predictive control with stabilizing terminal conditions and techniques for solving mixed-integer optimal control problems by continuous optimization. Partial outer convexification and binary relaxation transform mixed-integer problems into common optimal control problems. We derive nominal asymptotic stability for the resulting relaxed system formulation and implement sum-up rounding to restore efficiently integer feasibility. If fast control switching is technically possible and inexpensive, we can approximate the relaxed system behavior in the state space arbitrarily close. We integrate input perturbed model predictive control with practical asymptotic stability. Numerical experiments support our theoretical findings and illustrate practical relevance of fast and systematic control switching.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19300
Extracting Information in a Low-resource Setting: Case Study on Bioinformatics Workflows,['Computation and Language'],"['Clémence Sebe', 'Sarah Cohen-Boulakia', 'Olivier Ferret', 'Aurélie Névéol']","Bioinformatics workflows are essential for complex biological data analyses and are often described in scientific articles with source code in public repositories. Extracting detailed workflow information from articles can improve accessibility and reusability but is hindered by limited annotated corpora. To address this, we framed the problem as a low-resource extraction task and tested four strategies: 1) creating a tailored annotated corpus, 2) few-shot named-entity recognition (NER) with an autoregressive language model, 3) NER using masked language models with existing and new corpora, and 4) integrating workflow knowledge into NER models. Using BioToFlow, a new corpus of 52 articles annotated with 16 entities, a SciBERT-based NER model achieved a 70.4 F-measure, comparable to inter-annotator agreement. While knowledge integration improved performance for specific entities, it was less effective across the entire information schema. Our results demonstrate that high-performance information extraction for bioinformatics workflows is achievable.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19295
UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation,['Computer Vision and Pattern Recognition'],"['Yichong Lu', 'Yichi Cai', 'Shangzhan Zhang', 'Hongyu Zhou', 'Haoji Hu', 'Huimin Yu', 'Andreas Geiger', 'Yiyi Liao']","Photorealistic 3D vehicle models with high controllability are essential for autonomous driving simulation and data augmentation. While handcrafted CAD models provide flexible controllability, free CAD libraries often lack the high-quality materials necessary for photorealistic rendering. Conversely, reconstructed 3D models offer high-fidelity rendering but lack controllability. In this work, we introduce UrbanCAD, a framework that pushes the frontier of the photorealism-controllability trade-off by generating highly controllable and photorealistic 3D vehicle digital twins from a single urban image and a collection of free 3D CAD models and handcrafted materials. These digital twins enable realistic 360-degree rendering, vehicle insertion, material transfer, relighting, and component manipulation such as opening doors and rolling down windows, supporting the construction of long-tail scenarios. To achieve this, we propose a novel pipeline that operates in a retrieval-optimization manner, adapting to observational data while preserving flexible controllability and fine-grained handcrafted details. Furthermore, given multi-view background perspective and fisheye images, we approximate environment lighting using fisheye images and reconstruct the background with 3DGS, enabling the photorealistic insertion of optimized CAD models into rendered novel view backgrounds. Experimental results demonstrate that UrbanCAD outperforms baselines based on reconstruction and retrieval in terms of photorealism. Additionally, we show that various perception models maintain their accuracy when evaluated on UrbanCAD with in-distribution configurations but degrade when applied to realistic out-of-distribution data generated by our method. This suggests that UrbanCAD is a significant advancement in creating photorealistic, safety-critical driving scenarios for downstream applications.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19292
The Quaternary Gray Code and How It Can Be Used to Solve Ziggurat and Other Ziggu Puzzles,['Combinatorics'],"['Madeleine Goertz', 'Aaron Williams']","We investigate solutions to the new ""Ziggu"" family of exponential puzzles. These puzzles have $p$ pieces that form $m$ mazes. We encode the puzzle state as an quaternary number (base $4$) with $n=m+1$ digits, where each digit gives the horizontal or vertical position in one maze. We show that the number of states on a shortest solution is $6 \cdot 2^n - 3n - 5$ (OEIS A101946). This solution is unique, and it is generated from the start as follows: make the leftmost modification that doesn't undo the previous modification. Replacing ""leftmost"" with ""rightmost"" instead generates the unique longest solution that visits all $(3^{n+1} - 1)/2$ states (OEIS A003462). Thus, Ziggu puzzles can be viewed as $4$-ary, $3$-ary, or $2$-ary puzzles based on how the number of state encodings, valid states, or minimum states grow with each additional maze.
  Classic Gray code puzzles (e.g., Spin-Out) provide natural comparisons. Gray code puzzles with $p$ pieces have $2^p$ (OEIS A000079) or $\lfloor \frac{2}{3} \cdot 2^p \rfloor$ (OEIS A000975) states on their unique solution. The states visited in a Gray code puzzle solution follow the binary reflected Gray code. We show that Ziggu puzzles follow the quaternary reflected Gray code, as the shortest and longest solutions are both sublists of this order.
  These results show how to solve Ziggu puzzles from the start. We also provide $O(n)$-time ranking, comparison, and successor algorithms, which give the state's position along a solution, the relative order of two states, and the next state, respectively. While Gray code puzzles have simpler recursive descriptions and successor rules, the Ziggu puzzle has a simpler loopless algorithm to generate its shortest solution. The two families share a comparison function. Finally, we enrich the literature on OEIS A101946 by providing a bijection between Ziggu states and $2\times n$ Nurikabe grids.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19291
Construction and analysis of guiding center distributions for tokamak plasmas with ambient radial electric field,['Plasma Physics'],"['Andreas Bierwage', 'Philipp Lauber', 'Noriyoshi Nakajima', 'Kouji Shinohara', 'Guillaume Brochard', 'Young-chul Ghim', 'Wonjun Lee', 'Akinobu Matsuyama', 'Shuhei Sumida', 'Hao Yang', 'Masatoshi Yagi']","The contribution of a time-independent toroidally-symmetric radial electric field $E_r$ is implemented in VisualStart [Comp. Phys. Comm. 275 (2022) 108305; arXiv:2111.08224], a code whose purposes include the construction of guiding center (GC) drift orbit databases for the study of plasma instabilities in tokamaks. $E_r$ is important for the thermal part of the velocity distribution and for fast particle resonances in the kHz frequency range. KSTAR, JT-60U and ITER tokamak cases are used as working examples to test our methods and discuss practical issues connected with $E_r$. Two points are worth noting: First, the GC orbit space is sampled in the magnetic midplane as before, and we find that in the presence of $E_r$, midplane-based coordinates are not only equivalent but superior to conventional constants of motion, allowing to attain high numerical accuracy and efficiency with a relatively simple mesh. Second, the periodic parallel acceleration and deceleration of GCs via the mirror force is modulated by $E_r$. Although this parallel electric acceleration averages to zero during a poloidal transit (or bounce) period, it has important consequences, one being the known shift of the trapped-passing boundary. Another consequence is that electric frequency shifts depend on the chosen reference point, so that some care is required when evaluating the $E_r$-dependence of transit frequencies for resonance analyses.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19288
Short character values on large conjugacy classes,['Group Theory'],['Alexander R. Miller'],We provide an example of a finite group with a conjugacy class of average size on which fewer than half of the irreducible characters are either zero or a root of unity.△ Less,"28 November, 2024;",https://arxiv.org/pdf/2411.19283
Signal-based online acceleration and strain data fusion using B-splines and Kalman filter for full-field dynamic displacement estimation,['Signal Processing'],"['Aniruddha Das', 'Ashish Pal', 'Satish Nagarajaiah', 'Mohamed Sajeer M', 'Suparno Mukhopadhyay']","Displacement plays a crucial role in structural health monitoring (SHM) and damage detection of structural systems subjected to dynamic loads. However, due to the inconvenience associated with the direct measurement of displacement during dynamic loading and the high cost of displacement sensors, the use of displacement measurements often gets restricted. In recent years, indirect estimation of displacement from acceleration and strain data has gained popularity. Several researchers have developed data fusion techniques to estimate displacement from acceleration and strain data. However, existing data fusion techniques mostly rely on system properties like mode shapes or finite element models and require accurate knowledge about the system for successful implementation. Hence, they have the inherent limitation of their applicability being restricted to relatively simple structures where such information is easily available. In this article, B-spline basis functions have been used to formulate a Kalman filter-based algorithm for acceleration and strain data fusion using only elementary information about the system, such as the geometry and boundary conditions, which is the major advantage of this method. Also, the proposed algorithm enables us to monitor the full-field displacement of the system online with only a limited number of sensors. The method has been validated on a numerically generated dataset from the finite element model of a tapered beam subjected to dynamic excitation. Later, the proposed data fusion technique was applied to an experimental benchmark test of a wind turbine blade under dynamic load to estimate the displacement time history. In both cases, the reconstructed displacement from strain and acceleration was found to match well with the response from the FE model.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19282
The role of data-induced randomness in quantum machine learning classification tasks,['Quantum Physics'],"['Berta Casas', 'Xavier Bonet-Monroig', 'Adrián Pérez-Salinas']","Quantum machine learning (QML) has surged as a prominent area of research with the objective to go beyond the capabilities of classical machine learning models. A critical aspect of any learning task is the process of data embedding, which directly impacts model performance. Poorly designed data-embedding strategies can significantly impact the success of a learning task. Despite its importance, rigorous analyses of data-embedding effects are limited, leaving many cases without effective assessment methods. In this work, we introduce a metric for binary classification tasks, the class margin, by merging the concepts of average randomness and classification margin. This metric analytically connects data-induced randomness with classification accuracy for a given data-embedding map. We benchmark a range of data-embedding strategies through class margin, demonstrating that data-induced randomness imposes a limit on classification performance. We expect this work to provide a new approach to evaluate QML models by their data-embedding processes, addressing gaps left by existing analytical tools.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19281
Economic Dispatch and Power Flow Analysis for Microgrids,['Systems and Control'],"['Saskia A. Putri', 'Xiaoyu Ge', 'Javad Khazaei']","This study investigates the economic dispatch and optimal power flow (OPF) for microgrids, focusing on two configurations: a single-bus islanded microgrid and a three-bus grid-tied microgrid. The methodologies integrate renewable energy sources (solar PV and wind turbines), battery energy storage systems (BESS), and conventional generators (CHP, diesel, and natural gas), which are connected to the grid to ensure cost-efficient and reliable operation. The economic dispatch analysis evaluates the allocation of generation resources over daily and weekly horizons, highlighting the extensive utilization of renewable energy and the strategic use of BESS to balance system dynamics. The OPF analysis examines the distribution of active and reactive power across buses while ensuring voltage stability and compliance with operational constraints. Results show that the microgrid consistently satisfies load demand with minimal reliance on costly external grid power. Renewable energy sources are maximized for cost reduction, while BESS is employed strategically to address renewable intermittency. For the grid-tied microgrid, optimal power dispatch prioritizes cheaper sources, with Bus 1 contributing the largest share due to its favorable cost profile. Voltage variations remain within acceptable boundaries but indicate potential stability challenges under dynamic load changes, suggesting the need for secondary voltage control. These findings demonstrate the effectiveness of the proposed methodologies in achieving sustainable, cost-effective, and stable microgrid operations.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19279
"Examining the brightness variability, accretion disk, and evolutionary stage of the binary OGLE-LMC-ECL-14413",['Solar and Stellar Astrophysics'],"['R. E. Mennickent', 'G. Djurašević', 'J. A. Rosales', 'J. Garcés', 'J. Petrović', 'D. R. G. Schleicher', 'M. Jurkovic', 'I. Soszyński', 'J. G. Fernández-Trincado']","Our study aims to elucidate both short-term and long-term variations in the light curve of the eclipsing system OGLE-LMC-ECL-14413, with a particular focus on the unusual reversals in eclipse depth. We aim to clarify the role of the accretion disk in these fluctuations, especially in long-cycle changes spanning hundreds of days. Additionally, we seek to determine the evolutionary stage of the system and gain insights into the internal structure of its stellar components. We analyzed photometric time series from the Optical Gravitational Lensing Experiment (OGLE) project in the I and V bands, and from the MAssive Compact Halo Objects project in the BM and RM bands, covering a period of 30.85 years. Using light curve data from 27 epochs, we constructed models of the accretion disk. An optimized simplex algorithm was employed to solve the inverse problem, deriving the best-fit parameters for the stars, orbit, and disk. We also utilized the Modules for Experiments in Stellar Astrophysics software to assess the evolutionary stage of the binary system, investigating the progenitors and potential future developments. We found an orbital period of 38.15917(54) d and a long-term cycle of approximately 780 d. Temperature, mass, radius, and surface gravity values were determined for both stars. The photometric orbital cycle and the long-term cycle are consistent with a disk containing variable physical properties, including two shock regions. The disk encircles the more massive star and the system brightness variations align with the long-term cycle at orbital phase 0.25. Our mass transfer rate calculations correspond to these brightness changes. \texttt{MESA} simulations indicate weak magnetic fields in the donor star's subsurface, which are insufficient to influence mass transfer rates significantly.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19273
Online Policy Selection for Inventory Problems,['Optimization and Control'],"['Massil Hihat', 'Adeline Fermanian']","We tackle online inventory problems where at each time period the manager makes a replenishment decision based on partial historical information in order to meet demands and minimize costs. To solve such problems, we build upon recent works in online learning and control, use insights from inventory theory and propose a new algorithm called GAPSI. This algorithm follows a new feature-enhanced base-stock policy and deals with the troublesome question of non-differentiability which occurs in inventory problems. Our method is illustrated in the context of a complex and novel inventory system involving multiple products, lost sales, perishability, warehouse-capacity constraints and lead times. Extensive numerical simulations are conducted to demonstrate the good performances of our algorithm on real-world data.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19269
Twin-free $K_r$-saturated Graphs and Maximally Independent Sets in $K_3$-free Graphs,['Combinatorics'],['Asier Calbet'],"We say that two vertices are twins if they have the same neighbourhood and that a graph is $K_r$-saturated if it does not contain $K_r$ but adding any new edge to it creates a $K_r$. In 1964, Erdős, Hajnal and Moon showed that $sat(n,K_r)=(r-2)n+o(n)$ for $r \geq 3$, where $sat(n,K_r)$ is the minimum number of edges in a $K_r$-saturated graph on $n$ vertices, and determined the unique extremal graph. This graph has many twins, leading us to define $tsat(n,K_r)$ to be the minimum number of edges in a twin-free $K_r$-saturated graph on $n$ vertices. We show that $(5 +2/3)n + o(n) \leq tsat(n,K_3) \leq 6n+o(n)$ and that $\left(r+2\right)n + o(n) \leq tsat(n,K_r) \leq (r+3)n+o(n)$ for $r \geq 4$. We also consider a variant of this problem where we additionally require the graphs to have large minimum degree. Both of these problems turn out be intimately related to two other problems regarding maximally independent sets of a given size in $K_3$-free graphs and generalisations of these problems to $K_r$ with $r \geq 4$. The first problem is to maximise the number of maximally independent sets given the number of vertices and the second problem is to minimise the number of edges given the number of maximally independent sets. They are interesting in their own right.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19267
Monolithic piezoelectrically tunable hybrid integrated laser with sub-fiber laser coherence,['Optics'],"['Andrey Voloshin', 'Anat Siddharth', 'Simone Bianconi', 'Alaina Attanasio', 'Andrea Bancora', 'Vladimir Shadymov', 'Sebastien Leni', 'Rui Ning Wang', 'Johann Riemensberger', 'Sunil A. Bhave', 'Tobias J. Kippenberg']","Ultra-low noise lasers are essential tools in a wide variety of applications, including data communication, light detection and ranging (LiDAR), quantum computing and sensing, and optical metrology. Recent advances in integrated photonics, specifically the development of ultra-low loss silicon nitride (Si$_3$N$_4$) platform, have allowed attaining performance that exceeds conventional legacy laser systems, including the phase noise of fiber lasers. This platform can moreover be combined with monolithic integration of piezoelectrical materials, enabling frequency agile low noise lasers. However, this approach has to date not surpassed the trade-off between ultra-low frequency noise and frequency agility. Here we overcome this challenge and demonstrate a fully integrated laser based on the Si$_3$N$_4$ platform with frequency noise lower than that of a fiber laser, while maintaining the capability for high-speed modulation of the laser frequency. The laser achieves an output power of 30 mW with an integrated linewidth of 4.3 kHz and an intrinsic linewidth of 3 Hz, demonstrating phase noise performance that is on par with or lower than commercial fiber lasers. Frequency agility is accomplished via a monolithically integrated piezoelectric aluminum nitride (AlN) micro-electro-mechanical system (MEMS) actuator, which enables a flat frequency actuation bandwidth extending up to 400 kHz. This combination of ultra-low noise and frequency agility is a useful feature enabling tight laser locking for frequency metrology, fiber sensing, and coherent sensing applications. Our results demonstrate the ability of 'next generation' integrated photonic circuits (beyond silicon) to exceed the performance of legacy laser systems in terms of coherence and frequency actuation.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19264
"L4acados: Learning-based models for acados, applied to Gaussian process-based predictive control",['Systems and Control'],"['Amon Lahr', 'Joshua Näf', 'Kim P. Wabersich', 'Jonathan Frey', 'Pascal Siehl', 'Andrea Carron', 'Moritz Diehl', 'Melanie N. Zeilinger']","Incorporating learning-based models, such as Gaussian processes (GPs), into model predictive control (MPC) strategies can significantly improve control performance and online adaptation capabilities for real-world applications. Still, despite recent advances in numerical optimization and real-time GP inference, its widespread application is limited by the lack of an efficient and modular open-source implementation. This work aims at filling this gap by providing an efficient implementation of zero-order Gaussian process-based MPC in acados, as well as L4acados, a general framework for incorporating non-CasADi (learning-based) residual models in acados. By providing the required sensitivities via a user-defined Python module, L4acados enables the implementation of MPC controllers with learning-based residual models in acados, while supporting custom Jacobian approximations, as well as parallelization of sensitivity computations when preparing the quadratic subproblems. The computational efficiency of L4acados is benchmarked against available software using a neural network-based control example. Last, it is used demonstrate the performance of the zero-order GP-MPC method applied to two hardware examples: autonomous miniature racing, as well as motion control of a full-scale autonomous vehicle for an ISO lane change maneuver.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19258
Band-gap reduction and band alignments of dilute bismide III--V alloys,['Materials Science'],"['Abdul Saboor', 'Shoaib Khalid', 'Anderson Janotti']","Adding a few atomic percent of Bi to III--V semiconductors leads to significant changes in their electronic structure and optical properties. Bismuth substitution on the pnictogen site leads to a large increase in spin-orbit splitting $Δ_{\rm SO}$ at the top of the valence band ($Γ_{8v}-Γ_{7v}$) and a large reduction in the band gap, creating unique opportunities in semiconductor device applications. Quantifying these changes is key to the design and simulation of electronic and optoelectronic devices. Using hybrid functional calculations, we predict the band gap of III--Vs (III=Al, Ga, In and V=As, Sb) with low concentrations of Bi (3.125\% and 6.25\%), the effects of adding Bi on the valence- and conduction-band edges, and the band offset between these dilute alloys and their III--V parent compounds. As expected, adding Bi raises the valence-band maximum (VBM). However, contrary to previous assumptions, the conduction-band minimum (CBM) is also significantly lowered, and both effects contribute to the sizable band-gap reduction. Changes in band gap and $Δ_{\rm SO}$ are notably larger in the arsenides than in the antimonides. We also predict cases of band-gap inversion ($Γ_{6c}$ below $Γ_{8v}$) and $Δ_{\rm SO}$ larger than the band gap, which are key parameters for designing topological materials and for minimizing losses due to Auger recombination in infrared lasers.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19257
"Moderate, large and super large deviations principles for Poisson process with uniform catastrophes",['Probability'],"['A. Logachov', 'O. Logachova', 'A. Yambartsev']","In this paper, we expand and generalize the findings presented in our previous work on the law of large numbers and the large deviation principle for Poisson processes with uniform catastrophes. We study three distinct scalings: sublinear (moderate deviations), linear (large deviations), and superlinear (superlarge deviations). Across these scales, we establish different yet coherent rate functions.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19255
Non-parametric reconstructions of cosmic curvature: current constraints and forecasts,['Cosmology and Nongalactic Astrophysics'],"['Mariana L. S. Dias', 'Antônio F. B. da Cunha', 'Carlos A. P. Bengaly', 'Rodrigo S. Gonçalves', 'Jonathan Morais']","The assumption of a flat universe that follows the cosmological principle, i.e., that the universe is statistically homogeneous and isotropic at large scales, comprises one of the core foundations of the standard cosmological model -- namely, the $Λ$CDM paradigm. Nevertheless, it has been seldom tested in the literature. In this work, we assess the validity of this hypothesis by reconstructing the cosmic curvature with currently available observations, such as Type Ia Supernova and Cosmic Chronometers. We do so by means of null tests, given by consistency relations within the standard model scenario, using a non-parametric method -- which allows us to circumvent prior assumptions on the underlying cosmology. We find no statistically significant departure from the cosmological principle and null curvature in our analysis. In addition, we show that future cosmological observations, such as those expected from Hubble parameter measurements from redshift surveys, along with gravitational wave observations as standard sirens, will be able to significantly reduce the uncertainties of current reconstructions.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19252
Performance of a Superconducting Quantum Battery,['Quantum Physics'],"['Samira Elghaayda', 'Asad Ali', 'Saif Al-Kuwari', 'Artur Czerwinski', 'Mostafa Mansour', 'Saeed Haddadi']","Finding a quantum battery model that demonstrates a quantum advantage while remaining feasible for experimental production is a considerable challenge. In this paper, we introduce a superconducting quantum battery (SQB) model that exhibits such an advantage. The model consists of two coupled superconducting qubits that interact during the unitary charging process while remaining in equilibrium with a thermal reservoir. We first describe the model, provide evidence of the quantum advantage, and then discuss the fabrication process of the battery using superconducting qubits. Importantly, we derive analytical expressions for the ergotropy, instantaneous power, and capacity of the SQB, as well as their connection to quantum coherence. We demonstrate that leveraging the collective effects of Josephson energies and the coupling energy between qubits allows for optimization, resulting in improved energy redistribution and a significant enhancement in charging efficiency. This work highlights the complexities of tuning system parameters, which increase the potential for work extraction from the quantum battery, thereby providing a deeper understanding of the charging mechanisms involved. These findings can be applied to superconducting quantum circuit battery architectures, underscoring the feasibility of efficient energy storage in these systems. Our results pave the way for proposals of new superconducting devices capable of storing extractable work, emphasizing their potential for efficient energy storage.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19247
"Contrastive representations of high-dimensional, structured treatments",['Machine Learning'],"['Oriol Corcoll Andreu', 'Athanasios Vlontzos', ""Michael O'Riordan"", 'Ciaran M. Gilligan-Lee']","Estimating causal effects is vital for decision making. In standard causal effect estimation, treatments are usually binary- or continuous-valued. However, in many important real-world settings, treatments can be structured, high-dimensional objects, such as text, video, or audio. This provides a challenge to traditional causal effect estimation. While leveraging the shared structure across different treatments can help generalize to unseen treatments at test time, we show in this paper that using such structure blindly can lead to biased causal effect estimation. We address this challenge by devising a novel contrastive approach to learn a representation of the high-dimensional treatments, and prove that it identifies underlying causal factors and discards non-causally relevant factors. We prove that this treatment representation leads to unbiased estimates of the causal effect, and empirically validate and benchmark our results on synthetic and real-world datasets.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19245
How far can bias go? -- Tracing bias from pretraining data to alignment,['Computation and Language'],"['Marion Thaler', 'Abdullatif Köksal', 'Alina Leidinger', 'Anna Korhonen', 'Hinrich Schütze']","As LLMs are increasingly integrated into user-facing applications, addressing biases that perpetuate societal inequalities is crucial. While much work has gone into measuring or mitigating biases in these models, fewer studies have investigated their origins. Therefore, this study examines the correlation between gender-occupation bias in pre-training data and their manifestation in LLMs, focusing on the Dolma dataset and the OLMo model. Using zero-shot prompting and token co-occurrence analyses, we explore how biases in training data influence model outputs. Our findings reveal that biases present in pre-training data are amplified in model outputs. The study also examines the effects of prompt types, hyperparameters, and instruction-tuning on bias expression, finding instruction-tuning partially alleviating representational bias while still maintaining overall stereotypical gender associations, whereas hyperparameters and prompting variation have a lesser effect on bias expression. Our research traces bias throughout the LLM development pipeline and underscores the importance of mitigating bias at the pretraining stage.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19240
Hopf braces and semi-abelian categories,['Rings and Algebras'],"['Marino Gran', 'Andrea Sciandra']","Hopf braces have been introduced as a Hopf-theoretic generalization of skew braces. Under the assumption of cocommutativity, these algebraic structures are equivalent to matched pairs of actions on Hopf algebras, that can be used to produce solutions of the quantum Yang-Baxter equation. We prove that the category of cocommutative Hopf braces is semi-abelian and strongly protomodular. In particular, this implies that the main homological lemmas known for groups, Lie algebras and other classical algebraic structures also hold for cocommutative Hopf braces. Abelian objects are commutative and cocommutative Hopf algebras, that form an abelian Birkhoff subcategory of the category of cocommutative Hopf braces. Moreover, we show that the full subcategories of ""primitive Hopf braces"" and of ""skew braces"" form an hereditary torsion theory in the category of cocommutative Hopf braces, and that ""skew braces"" are also a Birkhoff subcategory and a localization of the latter category. Finally, we describe central extensions and commutators for cocommutative Hopf braces.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19238
"Strain-induced valley polarization, topological states, and piezomagnetism in two-dimensional altermagnetic V$_2$Te$_2$O, V$_2$STeO, V$_2$SSeO, and V$_2$S$_2$O",['Materials Science'],"['Jin-Yang Li', 'An-Dong Fan', 'Yong-Kun Wang', 'Ying Zhang', 'Si Li']","Altermagnets (AM) are a recently discovered third class of collinear magnets, and have been attracting significant interest in the field of condensed matter physics. Here, based on first-principles calculations and theoretical analysis, we propose four two-dimensional (2D) magnetic materials--monolayer V$_2$Te$_2$O, V$_2$STeO, V$_2$SSeO, and V$_2$S$_2$O--as candidates for altermagnetic materials. We show that these materials are semiconductors with spin-splitting in their nonrelativistic band structures. Furthermore, in the band structure, there are a pair of Dirac-type valleys located at the time-reversal invariant momenta (TRIM) X and Y points. These two valleys are connected by crystal symmetry instead of time-reversal symmetry. We investigate the strain effect on the band structure and find that uniaxial strain can induce valley polarization, topological states in these monolayer materials. Moreover, piezomagnetism can be realized upon finite doping. Our result reveals interesting valley physics in monolayer V$_2$Te$_2$O, V$_2$STeO, V$_2$SSeO, and V$_2$S$_2$O, suggesting their great potential for valleytronics, spintronics, and multifunctional nanoelectronics applications.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19237
Night-Side Relativistic Electron Precipitation Bursts in the Outer Radiation Belt: Insights from ELFIN and THEMIS,['Space Physics'],"['Xi Lu', 'Xiao-Jia Zhang', 'Anton V. Artemyev', 'Vassilis Angelopoulos', 'Jacob Bortnik']","Electromagnetic whistler-mode waves play a crucial role in the acceleration and precipitation of radiation belt electrons. Statistical surveys of wave characteristics suggest that these waves should preferentially scatter and precipitate relativistic electrons on the day side. However, the night-side region is expected to be primarily associated with electron acceleration. The recent low-altitude observations reveal relativistic electron precipitation in the night-side region. In this paper, we present statistical surveys of night-side relativistic electron losses due to intense precipitation bursts. We demonstrate that such bursts are associated with storm time substorm injections and are likely related to relativistic electron scattering by ducted whistler-mode waves. We also speculate on the role of injections in creating conditions favorable for relativistic electron precipitation.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19232
Habit Coach: Customising RAG-based chatbots to support behavior change,['Human-Computer Interaction'],"['Arian Fooroogh Mand Arabi', 'Cansu Koyuturk', ""Michael O'Mahony"", 'Raffaella Calati', 'Dimitri Ognibene']","This paper presents the iterative development of Habit Coach, a GPT-based chatbot designed to support users in habit change through personalized interaction. Employing a user-centered design approach, we developed the chatbot using a Retrieval-Augmented Generation (RAG) system, which enables behavior personalization without retraining the underlying language model (GPT-4). The system leverages document retrieval and specialized prompts to tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and narrative therapy techniques. A key challenge in the development process was the difficulty of translating declarative knowledge into effective interaction behaviors. In the initial phase, the chatbot was provided with declarative knowledge about CBT via reference textbooks and high-level conversational goals. However, this approach resulted in imprecise and inefficient behavior, as the GPT model struggled to convert static information into dynamic and contextually appropriate interactions. This highlighted the limitations of relying solely on declarative knowledge to guide chatbot behavior, particularly in nuanced, therapeutic conversations. Over four iterations, we addressed this issue by gradually transitioning towards procedural knowledge, refining the chatbot's interaction strategies, and improving its overall effectiveness. In the final evaluation, 5 participants engaged with the chatbot over five consecutive days, receiving individualized CBT interventions. The Self-Report Habit Index (SRHI) was used to measure habit strength before and after the intervention, revealing a reduction in habit strength post-intervention. These results underscore the importance of procedural knowledge in driving effective, personalized behavior change support in RAG-based systems.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19229
Designer gapped and tilted Dirac cones in lateral graphene superlattices,['Mesoscale and Nanoscale Physics'],"['A. Wild', 'R. R. Hartmann', 'E. Mariani', 'M. E. Portnoi']","We show that a planar array of bipolar waveguides in graphene can be used to engineer gapped and tilted two-dimensional Dirac cones within the electronic band structure. The presence of these gapped and tilted Dirac cones is demonstrated through a superlattice tight-binding model and verified using a transfer matrix calculation. By varying the applied gate voltages, the tilt parameter of these Dirac cones can be controlled and their gaps can be tuned to fall in the terahertz range. The possibility of gate-tunable gapped Dirac cones gives rise to terahertz applications via interband transitions and designer Landau level spectra both of which can be controlled via Dirac cone engineering. We anticipate that our paper will encourage Dirac cone tilt and gap engineering for gate-tunable device applications in lateral graphene superlattices.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19226
Stimulated down-conversion of single-photon emission in a quantum dot placed in a target-frequency microcavity,['Mesoscale and Nanoscale Physics'],"['I. V. Krainov', 'M. V. Rakhlin', 'A. I. Veretennikov', 'T. V. Shubina']","Currently, two optical processes are mainly used to realize single photon sources: deterministic transitions in a semiconductor quantum dot (QD) placed in a microcavity and spontaneous frequency down-conversion in materials with intrinsic nonlinearity. In this work, we consider another approach that combines the advantages of both, such as high power with on-demand generation from QDs and the possibility of frequency tuning from nonlinear sources. For this purpose, we use stimulated frequency down-conversion occurring directly in the QD inside a microcavity designed not to the exciton frequency in the QD but to the target single photon frequency, which is set by the difference between the exciton resonance and the stimulating laser energies. This down-conversion arises from the second-order nonlinear interaction of an exciton (bright heavy-hole or dark) and a light-hole exciton in the stimulating laser field. We present an analytical model for such a down-conversion process and evaluate its efficiency for a widely sought-after single photon source for the telecom C-band (1530-1565 nm). We show that the emission rate of down-converted single photons can approach MHz. At certain conditions, this process is comparable in efficiency to direct emission from an InAs/GaAs QD at 920 nm, which is outside the cavity mode.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19222
"Anisotropic spin-split states with canted persistent spin textures in two-dimensional Janus $1T^{'}$ $MXX'$ ($M$ = Mo, W; $X\neq X'$= S, Se, Te) controlled by surface alloying",['Mesoscale and Nanoscale Physics'],"['Moh. Adhib Ulil Absor', 'Muhammad Arifin', 'Iman Santoso', 'Harsojo']","Two-dimensional tungsten-based transition metal dichalcogenides (TMDCs), $MX_{2}$ ($M$: W, Mo; $X$: S, Se, Te) monolayers (MLs) with a $1T'$ structure, serve as significant-gap quantum spin Hall insulators. However, due to the centrosymmetric nature of these crystals, spin degeneracy persists throughout their electronic band structures, limiting their potential for spintronic applications. By modifying the chalcogen ($X$) atoms in the TMDCs ML surface to create a highly stable Janus $MXX'$ MLs structure, we demonstrate through density-functional theory calculations that substantial spin splitting of the electronic states can be achieved. Using the Janus $1T'$ WSTe ML as an example, we observe strongly anisotropic spin-splitting bands exhibiting canted persistent spin textures (PST) near the Fermi level, which differ significantly from those in commonly studied PST materials. We show that this intricate spin splitting and spin textures arise from the strong in-plane $p-d$ orbital coupling between the chalcogen atoms (Te and Se) and tungsten (W) atoms, driven by the reduced symmetry of the crystal's point group. The observed anisotropic spin splitting, along with canted PST, are further explained using a $\vec{k}\cdot\vec{p}$ model derived from symmetry analysis. Importantly, the spin-split states are highly sensitive to surface imperfections induced by the surface alloying effect, such as variations in the concentration of different chalcogen atoms on the ML surface. This highlights the potential of Janus $1T'$ $MXX'$ MLs as promising platforms for future spintronic devices.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19221
Revisiting the Symmetries of Galilean Electrodynamics,['High Energy Physics - Theory'],"['Andrea Fontanella', 'Juan Miguel Nieto García']","In this letter, we revisit the symmetries of Galilean Electrodynamics (GED) in a spacetime of generic dimension $d+1$. We show that these symmetries are infinitely many, and in $d=3$ they correspond to the conformal Milne algebra extended by $U(1)$. We discuss their application in the context of non-relativistic AdS$_5$/CFT$_4$ correspondence.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19217
ANDHRA Bandersnatch: Training Neural Networks to Predict Parallel Realities,['Computer Vision and Pattern Recognition'],['Venkata Satya Sai Ajay Daliparthi'],"Inspired by the Many-Worlds Interpretation (MWI), this work introduces a novel neural network architecture that splits the same input signal into parallel branches at each layer, utilizing a Hyper Rectified Activation, referred to as ANDHRA. The branched layers do not merge and form separate network paths, leading to multiple network heads for output prediction. For a network with a branching factor of 2 at three levels, the total number of heads is 2^3 = 8 . The individual heads are jointly trained by combining their respective loss values. However, the proposed architecture requires additional parameters and memory during training due to the additional branches. During inference, the experimental results on CIFAR-10/100 demonstrate that there exists one individual head that outperforms the baseline accuracy, achieving statistically significant improvement with equal parameters and computational cost.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19213
Review on spin-wave RF applications,['Applied Physics'],"['Khrystyna O. Levchenko', 'Kristýna Davídková', 'Jan Mikkelsen', 'Andrii V. Chumak']","This review explores the progress of spin-wave technology, emphasizing magnonics as a potential solution to modern challenges in radio frequency (RF) communication systems. To ensure clarity, the first two sections of the review reiterate premises of the magnonics, outline key milestones in spin-wave research, briefly review the state-of-the-art for technology and materials, as well as highlight potential directions for future developments. The third section is dedicated to spin-wave RF applications developed over the past 50 years, focusing on selected passive fundamental devices (filters, power limiters, delay lines, phase shifters, directional couplers, resonators, etc) and their operating principles. In the last section, the review further discusses the main advantages of spin-waves for RF applications while highlighting potential concerns, such as insertion losses, linearity and power capacity. Finally, potential solutions to address these challenges are proposed.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19212
Track Anything Behind Everything: Zero-Shot Amodal Video Object Segmentation,['Computer Vision and Pattern Recognition'],"['Finlay G. C. Hudson', 'William A. P. Smith']","We present Track Anything Behind Everything (TABE), a novel dataset, pipeline, and evaluation framework for zero-shot amodal completion from visible masks. Unlike existing methods that require pretrained class labels, our approach uses a single query mask from the first frame where the object is visible, enabling flexible, zero-shot inference. Our dataset, TABE-51 provides highly accurate ground truth amodal segmentation masks without the need for human estimation or 3D reconstruction. Our TABE pipeline is specifically designed to handle amodal completion, even in scenarios where objects are completely occluded. We also introduce a specialised evaluation framework that isolates amodal completion performance, free from the influence of traditional visual segmentation metrics.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19210
"A spiking photonic neural network of 40.000 neurons, trained with rank-order coding for leveraging sparsity",['Emerging Technologies'],"['Ria Talukder', 'Anas Skalli', 'Xavier Porte', 'Simon Thorpe', 'Daniel Brunner']","In recent years, the hardware implementation of neural networks, leveraging physical coupling and analog neurons has substantially increased in relevance. Such nonlinear and complex physical networks provide significant advantages in speed and energy efficiency, but are potentially susceptible to internal noise when compared to digital emulations of such networks. In this work, we consider how additive and multiplicative Gaussian white noise on the neuronal level can affect the accuracy of the network when applied for specific tasks and including a softmax function in the readout layer. We adapt several noise reduction techniques to the essential setting of classification tasks, which represent a large fraction of neural network computing. We find that these adjusted concepts are highly effective in mitigating the detrimental impact of noise.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19209
Optimal energy collection with rotational movements constraints in concentrated solar power plants,['Discrete Mathematics'],"['J. M. Díaz-Bañez', 'J. M. Higes-López', 'M. A. Pérez-Cutiño', 'J. Valverde']","In Concentrated Solar Power (CSP) plants based on Parabolic Trough Collectors (PTC), the Sun is tracked at discrete time intervals, with each interval representing a movement of the collector system. The act of moving heavy mechanical structures can lead to the development of cracks, bending, and/or displacements of components from their optimal optical positions. This, in turn, diminishes the overall performance of the entire system for energy capture. In this context, we introduce two combinatorial optimization problems to limit the number of tracking steps of the collector and hence the risk of failure incidents and contaminant leaks. On the one hand, the Minimum Tracking Motion (MTM)-Problem aims at detecting the minimum number of movements while maintaining the production within a given range. On the other hand, the Maximal Energy Collection (MEC)-Problem aims to achieve optimal energy production within a predetermined number of movements. Both problems are solved assuming scenarios where the energy collection function contains any number of local maximum/minimum due to optical errors of the elements in the PTCsystem. The MTM- and MEC-Problems are solved in O(n) time and O(n2mw*) time, respectively, being n the number of steps in the energy collection function, m the maximum number of movements of the solar structure, and w* the maximal amplitude angle that the structure can cover. The advantages of the solutions are shown in realistic experiments. While these problems can be solved in polynomial time, we establish the NP-hardness of a slightly modified version of the MEC-Problem. The proposed algorithms are generic and can be adapted to schedule solar tracking in other CSP systems.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19198
"Effective support, Dirac combs, and signal recovery",['Classical Analysis and ODEs'],"['G. Garza', 'K. Gurevich', 'A. Iosevich', 'A. Mayeli', 'K. Nguyen', 'N. Shaffer']","Let
  $f: {\mathbb Z}_N^d \to {\mathbb C}$ be a signal with the Fourier transform $\widehat{f}: \Bbb Z_N^d\to \Bbb C$. A classical result due to Matolcsi and Szucs (\cite{MS73}), and, independently, to Donoho and Stark (\cite{DS89}) states if a subset of frequencies ${\{\widehat{f}(m)\}}_{m \in S}$ of $f$ are unobserved due to noise or other interference, then $f$ can be recovered exactly and uniquely provided that $$ |E| \cdot |S|<\frac{N^d}{2},$$ where $E$ is the support of $f$, i.e., $E=\{x \in {\mathbb Z}_N^d: f(x) \not=0\}$. In this paper, we consider signals that are Dirac combs of complexity $γ$, meaning they have the form $f(x)=\sum_{i=1}^γ a_i 1_{A_i}(x)$, where the sets $A_i \subset {\mathbb Z}_N^d$ are disjoint, $a_i$ are complex numbers, and $γ\leq N^d$. We will define the concept of effective support of these signals and show that if $γ$ is not too large, a good recovery condition can be obtained by pigeonholing under additional reasonable assumptions on the distribution of values. Our approach produces a non-trivial uncertainty principle and a signal recovery condition in many situations when the support of the function is too large to apply the classical theory.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19195
Influencing Factors of the FLASH Effect: Unveiling the Importance of Free Radicals,['Medical Physics'],"['Yan Zhang', 'Chenyang Huang', 'Ankang Hu', 'Yucheng Wang', 'Wanyi Zhou', 'Jiaqi Qiu', 'Jian Wang', 'Qibin Fu', 'Tuchen Huang', 'Hao Zha', 'Wei Wang', 'Xiaowu Deng', 'Junli Li']","Purpose: Our aim was to elucidate the critical factors responsible for inducing the FLASH effect, focusing on the role of free radicals through simulation and experimental approaches. Methods and Materials: The whole abdomen of C57BL/6 mice was irradiated with 6 MeV electron beam. The endpoint was acute intestinal toxicity quantified by histological score. Total doses ranging from 6 to 15 Gy were evaluated. The impact of the mean dose rate (MDR) was assessed in the range of 40 to 900 Gy/s. Dose per pulse (DPP) of 0.5 Gy and 3 Gy were compared. The recombination of peroxyl radicals were simulated. Further comparisons were conducted by incorporating the antioxidant amifostine. Results: When varying total doses with a constant MDR of 900 Gy/s, the FLASH effect was not observed until the dose reached 15 Gy. For a total dose of 15 Gy and varying MDR, the FLASH effect was observed only when MDR reached 100 Gy/s. For a dose of 15 Gy and an MDR of 150 Gy/s, no significant difference in biological effect was observed between low DPP and high DPP. The simulation results indicated that the fraction of peroxyl radicals recombination remained nearly zero at conventional dose rates. For FLASH irradiation, the recombination fraction increased linearly with the dose. Notably, the dose delivery time corresponding to 50% change in the recombination fraction was approximately 300 ms. The addition of amifostine effectively eliminated the difference between FLASH group and CONV group. Conclusions: The critical requirement for observing the sparing effect at the biological endpoint is the administration of an adequate dose within the time window of the radical reaction. Additionally, the important role of free radical was verified after introducing antioxidants, suggesting that the generation and recombination of free radicals are pivotal factors influencing the FLASH sparing effect.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19194
Convex Regularization and Convergence of Policy Gradient Flows under Safety Constraints,['Machine Learning'],"['Pekka Malo', 'Lauri Viitasaari', 'Antti Suominen', 'Eeva Vilkkumaa', 'Olli Tahvonen']","This paper studies reinforcement learning (RL) in infinite-horizon dynamic decision processes with almost-sure safety constraints. Such safety-constrained decision processes are central to applications in autonomous systems, finance, and resource management, where policies must satisfy strict, state-dependent constraints. We consider a doubly-regularized RL framework that combines reward and parameter regularization to address these constraints within continuous state-action spaces. Specifically, we formulate the problem as a convex regularized objective with parametrized policies in the mean-field regime. Our approach leverages recent developments in mean-field theory and Wasserstein gradient flows to model policies as elements of an infinite-dimensional statistical manifold, with policy updates evolving via gradient flows on the space of parameter distributions. Our main contributions include establishing solvability conditions for safety-constrained problems, defining smooth and bounded approximations that facilitate gradient flows, and demonstrating exponential convergence towards global solutions under sufficient regularization. We provide general conditions on regularization functions, encompassing standard entropy regularization as a special case. The results also enable a particle method implementation for practical RL applications. The theoretical insights and convergence guarantees presented here offer a robust framework for safe RL in complex, high-dimensional decision-making problems.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19193
Per-event Uncertainty Quantification for Flow Cytometry using Calibration Beads,['Quantitative Methods'],"['Prajakta Bedekar', 'Megan A. Catterton', 'Matthew DiSalvo', 'Gregory A. Cooksey', 'Anthony J. Kearsley', 'Paul N. Patrone']","Flow cytometry measurements are widely used in diagnostics and medical decision making. Incomplete understanding of sources of measurement uncertainty can make it difficult to distinguish autofluorescence and background sources from signals of interest. Moreover, established methods for modeling uncertainty overlook the fact that the apparent distribution of measurements is a convolution of the inherent the population variability (e.g., associated with calibration beads or cells) and instrument induced-effects. Such issues make it difficult, for example, to identify signals from small objects such as extracellular vesicles. To overcome such limitations, we formulate an explicit probabilistic measurement model that accounts for volume and labeling variation, background signals and fluorescence shot noise. Using raw data from routine per-event calibration measurements, we use this model to separate the aforementioned sources of uncertainty and demonstrate how such information can be used to facilitate decision-making and instrument characterization.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19191
Sharkovskii theorem for infinite dimensional dynamical systems,['Dynamical Systems'],"['Anna Gierzkiewicz', 'Robert Szczelina']","We present adaptation of the relatively simple topological argument to show the existence of many periodic orbits in a system of Delay Differential Equations. Namely, we prove a Sharkovskii-type theorem: if the system has a periodic orbit of basic period $m$, then it must have all periodic orbits of periods $n \triangleright m$, for $n$ preceding $m$ in Sharkovskii ordering. The assumptions of the theorem can be verified with computer assistance. Moreover, the theory is general in a way that it can be applied to any dynamical system in infinite dimensions, provided the system is close to a one-dimensional map in a certain sense.
  As an exemplary application we show that the Rössler system perturbed by a delayed term retains periodic orbits of all natural periods for fixed values of parameters.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19190
Video Depth without Video Models,['Computer Vision and Pattern Recognition'],"['Bingxin Ke', 'Dominik Narnhofer', 'Shengyu Huang', 'Lei Ke', 'Torben Peters', 'Katerina Fragkiadaki', 'Anton Obukhov', 'Konrad Schindler']","Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19189
Characterization of Trees with Maximum Security,['Combinatorics'],"['Alex S. A. Alochukwu', 'Audace A. V. Dossou-Olory', 'Fadekemi J. Osaye', 'Valisoa R. M. Rakotonarivo', 'Shashank Ravichandran', 'Sarah J. Selkirk', 'Hua Wang', 'Hays Whitlatch']","The rank (also known as protection number or leaf-height) of a vertex in a rooted tree is the minimum distance between the vertex and any of its leaf descendants. We consider the sum of ranks over all vertices (known as the security) in binary trees, and produce a classification of families of binary trees for which the security is maximized. In addition, extremal results relating to maximum rank among all vertices in families of trees is discussed.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19188
Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs,['Computation and Language'],"['Anirudh Phukan', 'Divyansh', 'Harshit Kumar Morj', 'Vaishnavi', 'Apoorv Saxena', 'Koustava Goswami']","The rapid development of Large Multimodal Models (LMMs) has significantly advanced multimodal understanding by harnessing the language abilities of Large Language Models (LLMs) and integrating modality-specific encoders. However, LMMs are plagued by hallucinations that limit their reliability and adoption. While traditional methods to detect and mitigate these hallucinations often involve costly training or rely heavily on external models, recent approaches utilizing internal model features present a promising alternative. In this paper, we critically assess the limitations of the state-of-the-art training-free technique, the logit lens, in handling generalized visual hallucinations. We introduce a refined method that leverages contextual token embeddings from middle layers of LMMs. This approach significantly improves hallucination detection and grounding across diverse categories, including actions and OCR, while also excelling in tasks requiring contextual understanding, such as spatial relations and attribute comparison. Our novel grounding technique yields highly precise bounding boxes, facilitating a transition from Zero-Shot Object Segmentation to Grounded Visual Question Answering. Our contributions pave the way for more reliable and interpretable multimodal models.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19187
Inertial Dynamics of Run-and-Tumble Particle,['Statistical Mechanics'],"['Debraj Dutta', 'Anupam Kundu', 'Urna Basu']","We study the dynamics of a single inertial run-and-tumble particle on a straight line. The motion of this particle is characterized by two intrinsic time-scales, namely, an inertial and an active time-scale. We show that interplay of these two time-scales leads to the emergence of four distinct regimes, characterized by different dynamical behaviour of mean-squared displacement and survival probability. We analytically compute the position distributions in these regimes when the two time-scales are well separated. We show that in the large-time limit, the distribution has a large deviation form and compute the corresponding large deviation function analytically. We also find the persistence exponents in the different regimes theoretically. All our results are supported with numerical simulations.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19186
Classification and Ehrhart Theory of Denominator 2 Polygons,['Combinatorics'],"['Girtrude Hamm', 'Johannes Hofscheier', 'Alexander Kasprzyk']","We present an algorithm for growing the denominator $r$ polygons containing a fixed number of lattice points and enumerate such polygons containing few lattice points for small $r$. We describe the Ehrhart quasi-polynomial of a rational polygon in terms of boundary and interior point counts. Using this, we bound the coefficients of Ehrhart quasi-polynomials of denominator 2 polygons. In particular, we completely classify such polynomials in the case of zero interior points.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19183
A Unified Bayesian Framework for Mortality Model Selection,['Methodology'],"['Alex Diana', 'Jackie Wong Siaw Tze', 'Aniketh Pittea']","In recent years, a wide range of mortality models has been proposed to address the diverse factors influencing mortality rates, which has highlighted the need to perform model selection. Traditional mortality model selection methods, such as AIC and BIC, often require fitting multiple models independently and ranking them based on these criteria. This process can fail to account for uncertainties in model selection, which can lead to overly optimistic prediction interval, and it disregards the potential insights from combining models. To address these limitations, we propose a novel Bayesian model selection framework that integrates model selection and parameter estimation into the same process. This requires creating a model building framework that will give rise to different models by choosing different parametric forms for each term. Inference is performed using the reversible jump Markov chain Monte Carlo algorithm, which is devised to allow for transition between models of different dimensions, as is the case for the models considered here. We develop modelling frameworks for data stratified by age and period and for data stratified by age, period and product. Our results are presented in two case studies.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19176
"Long induced paths in $K_{s, s}$-free graphs",['Combinatorics'],"['Zach Hunter', 'Aleksa Milojević', 'Benny Sudakov', 'István Tomon']","More than 40 years ago, Galvin, Rival and Sands showed that every $K_{s, s}$-free graph containing an $n$-vertex path must contain an induced path of length $f(n)$, where $f(n)\to \infty$ as $n\to \infty$. Recently, it was shown by Duron, Esperet and Raymond that one can take $f(n)=(\log \log n)^{1/5-o(1)}$. In this note, we give a short self-contained proof that a $K_{s, s}$-free graphs with an $n$-vertex path contains an induced path of length at least $(\log \log n)^{1-o(1)}$, which comes closer to the best known upper bound $O((\log\log n)^2)$.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19173
Looking into the quantum entanglement in $H\to ZZ^\star$ at LHC within SMEFT framework,['High Energy Physics - Phenomenology'],"['Amir Subba', 'Ritesh K. Singh', 'Rohini M. Godbole']","We study $H\to ZZ^\star$ production process in final four lepton states at $13$ TeV LHC within SMEFT framework. The anomalous $HZZ$ couplings are parameterized with dimension-6 $SU(2)_L\times U(1)_Y$ gauge invariant operators. We compute the eight polarizations of each $Z$ boson and $64$ spin-correlations as asymmetries in angular functions of final decayed leptons in the rest frame of the $Z$ boson. We perform the sensitivity analysis of these asymmetries to anomalous couplings and one parameter limits on these couplings at $95\%$ confidence level are obtained. These asymmetries are further used to construct the joint density matrix (DM) for $ZZ^\star$ system. However, such DM suffers from negative probability and eigenvalues. To alleviate the negativity issues, we reconstruct the DM using asymmetries of symmetrized angular functions owing to the indistinguishability of two $Z$ bosons. The symmetrized DM is further employed to compute lower bound for concurrence as a witness of entanglement measurable at the collider experiments. The $ZZ^\star$ system is found to be in an entangled state for all values of the anomalous couplings. Notably, while the lower bound exhibits poorer sensitivity to anomalous couplings compared to asymmetries, it demonstrates distinct behavior for CP-even and odd couplings.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19171
Prospects for detecting the rare heavy Higgs decay $H\to hγγ$ at the LHC,['High Energy Physics - Phenomenology'],"['M. A. Arroyo-Ureña', 'Alejandro Ibarra', 'Pablo Roig', 'T. Valencia-Pérez']","We study the decay of a heavy CP-even neutral Higgs into an on-shell Standard Model-like Higgs boson and two photons, $H\to hγγ$, in the two-Higgs doublet model. We argue that the decay channel $H\to hγγ$, followed by the decay of the Standard Model Higgs $h\rightarrow b\bar b$, could be observed at the 5$σ$ level at the High-Luminosity LHC for masses of the heavy Higgs up to 900 GeV for the type-II, 500 GeV for the Lepton Specific and the Flipped 2HDMs, and at 3 sigmas for the type-I, for masses up to 600 GeV. We also discuss the possible role of the decay $H\to hγγ$ in discriminating among 2HDMs.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19170
Random polytopes in convex bodies: Bridging the gap between extremal containers,['Probability'],"['Florian Besau', 'Anna Gusakova', 'Christoph Thäle']","We investigate the asymptotic properties of random polytopes arising as convex hulls of $n$ independent random points sampled from a family of block-beta distributions. Notably, this family includes the uniform distribution on a product of Euclidean balls of varying dimensions as a key example. As $n\to\infty$, we establish explicit growth rates for the expected number of facets, which depend in a subtle way on the the underlying model parameters. For the case of the uniform distribution, we further examine the expected number of faces of arbitrary dimensions as well as the volume difference. Our findings reveal that the family of random polytopes we introduce exhibits novel interpolative properties, bridging the gap between the classical extremal cases observed in the behavior of random polytopes within smooth versus polytopal convex containers.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19163
Band Curvature Effects on Quantum Transport of Spin-1 Chiral Fermion Systems,['Mesoscale and Nanoscale Physics'],"['Risako Kikuchi', 'Ai Yamakage']",We theoretically investigate the quantum transport properties of three-dimensional spin-1 chiral fermion systems with a curved trivial band. In the multiband system with two distinct characters--a linear Dirac band and a quadratic trivial band--the hybridization induced by impurity effects leads to pronounced energy and temperature dependences in the electrical conductivity. We show that the conductivity is suppressed by the trivial band in the low-energy regime near the threefold degenerate point and enhanced in the band-crossing point of the Dirac and trivial bands. These results are derived using the self-consistent Born approximation within the framework of linear response theory.△ Less,"28 November, 2024;",https://arxiv.org/pdf/2411.19159
Bayesian Deconvolution of Astronomical Images with Diffusion Models: Quantifying Prior-Driven Features in Reconstructions,['Instrumentation and Methods for Astrophysics'],"['Alessio Spagnoletti', 'Alexandre Boucaud', 'Marc Huertas-Company', 'Wassim Kabalan', 'Biswajit Biswas']","Deconvolution of astronomical images is a key aspect of recovering the intrinsic properties of celestial objects, especially when considering ground-based observations. This paper explores the use of diffusion models (DMs) and the Diffusion Posterior Sampling (DPS) algorithm to solve this inverse problem task. We apply score-based DMs trained on high-resolution cosmological simulations, through a Bayesian setting to compute a posterior distribution given the observations available. By considering the redshift and the pixel scale as parameters of our inverse problem, the tool can be easily adapted to any dataset. We test our model on Hyper Supreme Camera (HSC) data and show that we reach resolutions comparable to those obtained by Hubble Space Telescope (HST) images. Most importantly, we quantify the uncertainty of reconstructions and propose a metric to identify prior-driven features in the reconstructed images, which is key in view of applying these methods for scientific purposes.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19158
Universal approximation of continuous functions with minimal quantum circuits,['Quantum Physics'],"['Adrián Pérez-Salinas', 'Mahtab Yaghubi Rad', 'Alice Barthe', 'Vedran Dunjko']","The conventional paradigm of quantum computing is discrete: it utilizes discrete sets of gates to realize bitstring-to-bitstring mappings, some of them arguably intractable for classical computers. In parameterized quantum approaches, widely used in quantum optimization and quantum machine learning, the input becomes continuous and the output represents real-valued functions. Various strategies exist to encode the input into a quantum circuit. While the bitstring-to-bitstring universality of quantum computers is quite well understood, basic questions remained open in the continuous case. For example, it was proven that full multivariate function universality requires either (i) a fixed encoding procedure with a number of qubits scaling as the dimension of the input or (ii) a tunable encoding procedure in single-qubit circuits. This reveals a trade-off between the complexity of the data encoding and the qubit requirements. The question of whether universality can be reached with a fixed encoding and constantly many qubits has been open for the last five years. In this paper, we answer this remaining fundamental question in the affirmative. We provide a constructive method to approximate arbitrary multivariate functions using just a single qubit and a fixed-generator parametrization, at the expense of increasing the depth. We also prove universality for a few of alternative fixed encoding strategies which may have independent interest. Our results rely on a combination of techniques from harmonic analysis and quantum signal processing.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19152
Counting Stacked Objects from Multi-View Images,['Computer Vision and Pattern Recognition'],"['Corentin Dumery', 'Noa Etté', 'Jingyi Xu', 'Aoxiang Fan', 'Ren Li', 'Hieu Le', 'Pascal Fua']","Visual object counting is a fundamental computer vision task underpinning numerous real-world applications, from cell counting in biomedicine to traffic and wildlife monitoring. However, existing methods struggle to handle the challenge of stacked 3D objects in which most objects are hidden by those above them. To address this important yet underexplored problem, we propose a novel 3D counting approach that decomposes the task into two complementary subproblems - estimating the 3D geometry of the object stack and the occupancy ratio from multi-view images. By combining geometric reconstruction and deep learning-based depth analysis, our method can accurately count identical objects within containers, even when they are irregularly stacked. We validate our 3D Counting pipeline on diverse real-world and large-scale synthetic datasets, which we will release publicly to facilitate further research.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19149
Puzzle: Distillation-Based NAS for Inference-Optimized LLMs,['Machine Learning'],"['Akhiad Bercovich', 'Tomer Ronen', 'Talor Abramovich', 'Nir Ailon', 'Nave Assaf', 'Mohammad Dabbah', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Netanel Haber', 'Ehud Karpas', 'Roi Koren', 'Itay Levy', 'Pavlo Molchanov', 'Shahar Mor', 'Zach Moshe', 'Najeeb Nabwani', 'Omri Puny', 'Ran Rubin', 'Itamar Schen', 'Ido Shahaf', 'Oren Tropp', 'Omer Ullman Argov', 'Ran Zilberstein']","Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.
  We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.△ Less",v1,https://arxiv.org/pdf/2411.19146
On the intersection of critical percolation clusters and other tree-like random graphs,['Probability'],"['Amine Asselah', 'Bruno Schapira']","We study intersection properties of two or more independent tree-like random graphs. Our setting encompasses critical, possibly long range, Bernoulli percolation clusters, incipient infinite clusters, as well as critical branching random walk ranges. We obtain sharp excess deviation bounds on the number of intersection points of two or more clusters, under minimal assumption on the two-point function. The proof are based on new bounds on the n-point function, in case of critical percolation, and on the joint moments of local times of branching random walks△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19145
Co-Learning: Towards Semi-Supervised Object Detection with Road-side Cameras,['Computer Vision and Pattern Recognition'],"['Jicheng Yuan', 'Anh Le-Tuan', 'Ali Ganbarov', 'Manfred Hauswirth', 'Danh Le-Phuoc']","Recently, deep learning has experienced rapid expansion, contributing significantly to the progress of supervised learning methodologies. However, acquiring labeled data in real-world settings can be costly, labor-intensive, and sometimes scarce. This challenge inhibits the extensive use of neural networks for practical tasks due to the impractical nature of labeling vast datasets for every individual application. To tackle this, semi-supervised learning (SSL) offers a promising solution by using both labeled and unlabeled data to train object detectors, potentially enhancing detection efficacy and reducing annotation costs. Nevertheless, SSL faces several challenges, including pseudo-target inconsistencies, disharmony between classification and regression tasks, and efficient use of abundant unlabeled data, especially on edge devices, such as roadside cameras. Thus, we developed a teacher-student-based SSL framework, Co-Learning, which employs mutual learning and annotation-alignment strategies to adeptly navigate these complexities and achieves comparable performance as fully-supervised solutions using 10\% labeled data.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19143
GDPR-Relevant Privacy Concerns in Mobile Apps Research: A Systematic Literature Review,['Software Engineering'],"['Orlando Amaral Cejas', 'Nicolas Sannier', 'Sallam Abualhaija', 'Marcello Ceci', 'Domenico Bianculli']","The General Data Protection Regulation (GDPR) is the benchmark in the European Union (EU) for privacy and data protection standards. Substantial research has been conducted in the requirements engineering (RE) literature investigating the elicitation, representation, and verification of privacy requirements in GDPR. Software systems including mobile apps must comply with the GDPR. With the growing pervasiveness of mobile apps and their increasing demand for personal data, privacy concerns have acquired further interest within the software engineering (SE) community at large. Despite the extensive literature on GDPR-relevant privacy concerns in mobile apps, there is no secondary study that describes, analyzes, and categorizes the current focus. Research gaps and persistent challenges are thus left unnoticed. In this article, we aim to systematically review existing primary studies highlighting various GDPR concepts and how these concepts are addressed in mobile apps research. The objective is to reconcile the existing work on GDPR in the RE literature with the research on GDPR-related privacy concepts in mobile apps in the SE literature. Our findings show that the current research landscape reflects a rather shallow understanding of GDPR requirements. Some GDPR concepts such as data subject rights (i.e., the rights of individuals over their personal data) are fundamental to GDPR, yet under-explored in the literature. In this article, we highlight future directions to be pursued by the SE community for supporting the development of GDPR-compliant mobile apps.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19142
One-loop electron self-energy with accelerated partial-wave expansion in Coulomb gauge,['Atomic Physics'],"['V. A. Yerokhin', 'Z. Harman', 'C. H. Keitel']","Numerical calculations of the electron self-energy without any expansion in the binding nuclear field are required in order to match the rapidly advancing precision of experimental spectroscopy. For the lightest elements, particularly hydrogen, these computations are complicated by large numerical cancellations and the slow convergence of the partial-wave expansion. Methods with accelerated convergence of the partial-wave expansion have been recently put forward [V. A. Yerokhin, K. Pachucki, V. M. Shabaev, Phys. Rev. A 72, 042502 (2005); J. Sapirstein and K. T. Cheng, Phys. Rev. A 108, 042804 (2023)]. In our work we extend the accelerated-convergence methods to the previously hardly accessible region of nuclear charges $Z < 5$ and higher excited states.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19135
TEA: Trajectory Encoding Augmentation for Robust and Transferable Policies in Offline Reinforcement Learning,['Machine Learning'],"['Batıkan Bora Ormancı', 'Phillip Swazinna', 'Steffen Udluft', 'Thomas A. Runkler']","In this paper, we investigate offline reinforcement learning (RL) with the goal of training a single robust policy that generalizes effectively across environments with unseen dynamics. We propose a novel approach, Trajectory Encoding Augmentation (TEA), which extends the state space by integrating latent representations of environmental dynamics obtained from sequence encoders, such as AutoEncoders. Our findings show that incorporating these encodings with TEA improves the transferability of a single policy to novel environments with new dynamics, surpassing methods that rely solely on unmodified states. These results indicate that TEA captures critical, environment-specific characteristics, enabling RL agents to generalize effectively across dynamic conditions.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19133
Wavelet Scattering Transform for Gravitational Waves Analysis. An Application to Glitch Characterization,['General Relativity and Quantum Cosmology'],"['Alessandro Licciardi', 'Davide Carbone', 'Lamberto Rondoni', 'Alessandro Nagar']","Gravitational waves, first predicted by Albert Einstein within the framework of general relativity, were confirmed in 2015 by the LIGO/Virgo collaboration, marking a pivotal breakthrough in astrophysics. Despite this achievement, a key challenge remains in distinguishing true gravitational wave signals from noise artifacts, or ""glitches,"" which can distort data and affect the quality of observations. Current state-of-the-art methods, such as the Q-transform, are widely used for signal processing, but face limitations when addressing certain types of signals. In this study, we investigate the Wavelet Scattering Transform (WST), a recent signal analysis method, as a complementary approach. Theoretical motivation for WST arises from its stability under signal deformations and its equivariance properties, which make it particularly suited for the complex nature of gravitational wave data. Our experiments on the LIGO O1a dataset show that WST simplifies classification tasks and enables the use of more efficient architectures compared to traditional methods. Furthermore, we explore the potential benefits of integrating WST with the Q-transform, demonstrating that ensemble methods exploiting both techniques can capture complementary features of the signal and improve overall performance. This work contributes to advancing machine learning applications in gravitational wave analysis, introducing refined preprocessing techniques that improve signal detection and classification.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19122
An adjunction theorem for Davydov-Yetter cohomology and infinitesimal braidings,['Quantum Algebra'],"['Matthieu Faitg', 'Azat M. Gainutdinov', 'Christoph Schweigert']","Davydov-Yetter cohomology $H_{\mathrm{DY}}^{\bullet}(F)$ is associated to a monoidal functor $F: \mathcal{C} \to \mathcal{D}$ between $\Bbbk$-linear monoidal categories where $\Bbbk$ is a field, and its second degree classifies the infinitesimal deformations of the monoidal structure of $F$. Our main result states that if $F$ admits a right adjoint $R$, then there is an object $Γ$ in the Drinfeld center $\mathcal{Z}(\mathcal{C})$ defined in terms of $R$ such that the Davydov-Yetter cohomology of $F$ can be expressed as the Davydov-Yetter cohomology of the identity functor on $\mathcal{C}$ with the coefficient $Γ$. We apply this result in the case when the product functor $\otimes: \mathcal{C} \boxtimes\mathcal{C} \to\mathcal{C}$ has a monoidal structure given by a braiding $c$ on $\mathcal{C}$ and determine explicitly the coefficient $Γ$ as a coend object in $\mathcal{Z}(\mathcal{C}) \boxtimes \mathcal{Z}(\mathcal{C})$. The motivation is that $H^{\bullet}_{\mathrm{DY}}(\otimes)$ contains a ``space of infinitesimal braidings tangent to $c$'' in a way that we describe precisely. For $\mathcal{C} = H\text{-}\mathrm{mod}$, where $H$ is a finite-dimensional Hopf algebra over a field $\Bbbk$, this is the Zariski tangent space to the affine variety of R-matrices for $H$. In the case of perfect $\Bbbk$, we give a dimension formula for this space as an explicit end involving only (low-degree) relative Ext's of the standard adjunction between $\mathcal{Z}(\mathcal{C})$ and $\mathcal{C}$. As a further application of the adjunction theorem, we describe deformations of the restriction functor associated to a Hopf subalgebra and a Drinfeld twist. Both applications are illustrated in the example of bosonization of exterior algebras.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19111
Inverse-design topology optimization of magnonic devices using level-set method,['Other Condensed Matter'],"['Andrey A. Voronov', 'Marcos Cuervo Santos', 'Florian Bruckner', 'Dieter Suess', 'Andrii V. Chumak', 'Claas Abert']","The inverse design approach in magnonics exploits the wave nature of magnons and machine learning to develop novel logical devices with unique functionalities that exceed the capabilities of analytical methods. Despite its potential in analog, Boolean, and neuromorphic computing, existing implementations are limited by memory usage, restricting computational depth and the design of complex devices. This study introduces a level-set parameterization approach for topology optimization, coupled with an adjoint-state method for memory-efficient solution of magnetization dynamics equations. The simulation platform employed is $\texttt{neuralmag}$, a GPU-accelerated micromagnetic software that features a unique nodal finite-difference discretization scheme integrated with automatic differentiation tools. To validate the proposed inverse design method, we first addressed a magnetic nanoparticle shape optimization task, demonstrating how additional constraints on the objective function can control the design solution space and govern the optimization process. Subsequently, the functionality of a magnonic demultiplexer was realized using a 300-nm-wide yttrium iron garnet conduit. This device achieves spatial frequency-selective separation of spin waves into distinct outputs. This task demonstrates the algorithm's efficiency in identifying local minima of the objective function across various initial topologies, establishing its effectiveness as a versatile inverse design tool for creating magnonic logic device designs.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19109
Mapping the Milky Way with Gaia XP spectra I: Systematic flux corrections and atmospheric parameters for 68 million stars,['Astrophysics of Galaxies'],"['Xianhao Ye', 'Wenbo Wu', 'Carlos Allende Prieto', 'David S. Aguado', 'Jingkun Zhao', 'Jonay I. González Hernández', 'Rafael Rebolo', 'Gang Zhao', 'Zhuohan Li', 'Carlos del Burgo', 'Yuqin Chen']","Gaia XP spectra for over two hundred million stars have great potential for mapping metallicity across the Milky Way. Several recent studies have analyzed this data set to derive parameters and characterize systematics in the fluxes. We aim to construct an alternative catalog of atmospheric parameters from Gaia XP spectra by fitting them with synthetic spectra based on model atmospheres, and provide corrections to the XP fluxes according to stellar colors, magnitudes, and extinction. We use GaiaXPy to obtain calibrated spectra and apply FERRE to match the corrected XP spectra with models and infer atmospheric parameters. We train a neural network using stars in APOGEE to predict flux corrections as a function of wavelength for each target. Based on the comparison with APOGEE parameters, we conclude that our estimated parameters have systematic errors and uncertainties in $T_{\mathrm{eff}}$, $\log g$, and [M/H] about $-38 \pm 167$ K, $0.05 \pm 0.40$ dex, and $-0.12 \pm 0.19$ dex, respectively, for stars in the range $4000 \le T_{\mathrm{eff}} \le 7000$ K. The corrected XP spectra show better agreement with both models and Hubble Space Telescope CALSPEC data. Our correction increases the precision of the relative spectrophotometry of the XP data from $3.2\% - 3.7\%$ to $1.2\% - 2.4\%$. Finally, we have built a catalog of atmospheric parameters for stars within $4000 \le T_{\mathrm{eff}} \le 7000$ K, comprising $68,394,431$ sources, along with a subset of $124,188$ stars with $\mathrm{[M/H]} \le -2.5$. Our results confirm that the Gaia XP flux calibrated spectra show systematic patterns as a function of wavelength that are tightly related to colors, magnitudes, and extinction. Our optimization algorithm can give us accurate atmospheric parameters of stars with a clear and direct link to models of stellar atmospheres, and can be used to efficiently search for extremely metal-poor stars.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19105
"Algorithmic modelling of a complex redundant multi-state system subject to multiple events, preventive maintenance, loss of units and a multiple vacation policy through a MMAP",['Methodology'],"['Juan Eloy Ruiz-Castro', 'Hugo Alaín Zapata-Ceballos']","A complex multi-state redundant system undergoing preventive maintenance and experiencing multiple events is being considered in a continuous time frame. The online unit is susceptible to various types of failures, both internal and external in nature, with multiple degradation levels present, both internally and externally. Random inspections are continuously monitoring these degradation levels, and if they reach a critical state, the unit is directed to a repair facility for preventive maintenance. The repair facility is managed by a single repairperson, who follows a multiple vacation policy dependent on the operational status of the units. The repairperson is responsible for two primary tasks: corrective repairs and preventive maintenance. The time durations within the system follow phase-type distributions, and the model is constructed using Markovian Arrival Processes with marked arrivals. A variety of performance measures, including transient and stationary distributions, are calculated using matrix-analytic methods. This approach enables the expression of key results and overall system behaviour in a matrix-algorithmic format. In order to optimize the model, costs and rewards are integrated into the analysis. A numerical example is presented to showcase the model's flexibility and effectiveness in real-world applications.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19104
Nonequilibrium steady-state dynamics of Markov processes on graphs,['Statistical Mechanics'],"['Stefano Crotti', 'Thomas Barthel', 'Alfredo Braunstein']","We propose an analytic approach for the steady-state dynamics of Markov processes on locally tree-like graphs. It is based on the definition of a probability distribution for infinite edge trajectories in terms of infinite matrix products. For homogeneous ensembles on regular graphs, the distribution is parametrized by a single $d\times d\times r^2$ tensor, where $r$ is the number of states per variable and $d$ is the matrix-product bond dimension. While the approach becomes exact in the large-$d$ limit, it usually produces extremely accurate results even for small $d$. The $d^2r^2$ parameters are found by solving a fixed point equation, for which we provide an efficient belief-propagation procedure. We apply it to a variety of models, including Ising-Glauber dynamics with symmetric and asymmetric couplings and the SIS model. Even for small $d$, the results are compatible with Monte Carlo estimates and accurately reproduce known exact solutions. The method gives access to accurate temporal correlations which, in some regimes, may be virtually impossible to estimate by sampling.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19100
Pralekha: An Indic Document Alignment Evaluation Benchmark,['Computation and Language'],"['Sanjay Suryanarayanan', 'Haiyue Song', 'Mohammed Safi Ur Rahman Khan', 'Anoop Kunchukuttan', 'Mitesh M. Khapra', 'Raj Dabre']","Mining parallel document pairs poses a significant challenge because existing sentence embedding models often have limited context windows, preventing them from effectively capturing document-level information. Another overlooked issue is the lack of concrete evaluation benchmarks comprising high-quality parallel document pairs for assessing document-level mining approaches, particularly for Indic languages. In this study, we introduce Pralekha, a large-scale benchmark for document-level alignment evaluation. Pralekha includes over 2 million documents, with a 1:2 ratio of unaligned to aligned pairs, covering 11 Indic languages and English. Using Pralekha, we evaluate various document-level mining approaches across three dimensions: the embedding models, the granularity levels, and the alignment algorithm. To address the challenge of aligning documents using sentence and chunk-level alignments, we propose a novel scoring method, Document Alignment Coefficient (DAC). DAC demonstrates substantial improvements over baseline pooling approaches, particularly in noisy scenarios, achieving average gains of 20-30% in precision and 15-20% in F1 score. These results highlight DAC's effectiveness in parallel document mining for Indic languages.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19096
Tracking Progress Towards Sustainable Development Goal 6 Using Satellite Imagery,['Computer Vision and Pattern Recognition'],"['Othmane Echchabi', 'Nizar Talty', 'Josh Manto', 'Aya Lahlou', 'Ka Leung Lam']","Clean water and sanitation are essential for health, well-being, and sustainable development, yet significant global disparities remain. Although the United Nations' Sustainable Development Goal 6 has clear targets for universal access to clean water and sanitation, data coverage and openness remain obstacles for tracking progress in many countries. Nontraditional data sources are needed to fill this gap. This study incorporated Afrobarometer survey data, satellite imagery (Landsat 8 and Sentinel-2), and deep learning techniques (Meta's DINO model) to develop a modelling framework for evaluating access to piped water and sewage systems across diverse African regions. The modelling framework demonstrated high accuracy, achieving over 96% and 97% accuracy in identifying areas with piped water access and sewage system access respectively using satellite imagery. It can serve as a screening tool for policymakers and stakeholders to potentially identify regions for more targeted and prioritized efforts to improve water and sanitation infrastructure. When coupled with spatial population data, the modelling framework can also estimate and track the national-level percentages of the population with access to piped water and sewage systems. In the future, this approach could potentially be extended to evaluate other SDGs, particularly those related to critical infrastructure.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19093
An updated determination of light-cone distribution amplitudes of octet baryons in lattice QCD,['High Energy Physics - Lattice'],"['G. S. Bali', 'V. M. Braun', 'S. Bürger', 'M. Göckeler', 'M. Gruber', 'F. Kaiser', 'B. A. Kniehl', 'O. L. Veretin', 'P. Wein']","We present updated results on the wave function normalization constants and the first moments of the light cone distribution amplitudes for the lowest-lying baryon octet. The analysis is carried out on a large number of $n_f=2+1$ lattice gauge ensembles, including ensembles at physical pion (and kaon) masses. These are spread across five different lattice spacings, enabling a controlled continuum limit. The main differences with respect to our earlier work are the use of two-loop conversion factors to an $\overline{\mathrm{MS}}$-like scheme and of an updated set of low-energy constants in the parametrization of the quark mass dependence. As a byproduct, for the first time, the anomalous dimensions for local three-quark operators with one derivative are obtained.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19091
Numerical analysis of a constrained strain energy minimization problem,['Numerical Analysis'],"['Tilman Aleman', 'Arnold Reusken']","We consider a setting in which an evolving surface is implicitly characterized as the zero level of a level set function. Such an implicit surface does not encode any information about the path of a single point on the evolving surface. In the literature different approaches for determining a velocity that induces corresponding paths of points on the surface have been proposed. One of these is based on minimization of the strain energy functional. This then leads to a constrained minimization problem, which has a corresponding equivalent formulation as a saddle point problem. The main topic of this paper is a detailed analysis of this saddle point problem and of a finite element discretization of this problem. We derive well-posedness results for the continuous and discrete problems and optimal error estimates for a finite element discretization that uses standard $H^1$-conforming finite element spaces.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19089
On the Goppa morphism,['Algebraic Geometry'],['Ángel Luis Muñoz Castañeda'],"We investigate the geometric foundations of the space of geometric Goppa codes using the Tsfasman-Vladut H-construction. These codes are constructed from level structures, which extend the classical Goppa framework by incorporating invertible sheaves and trivializations. A key contribution is the definition of the Goppa morphism, a map from the moduli space of level structures, denoted $LS_{g,n,d}$, to the Grassmannian $\mathrm{Gr}(k,n)$. This morphism allows problems related to distinguishing attacks and key recovery in the context of geometric Goppa codes to be translated into a geometric language, addressing questions about the equations defining the image of the Goppa morphism and its fibers. Furthermore, we identify the ranges of the degree parameter $d$ that should be avoided to maintain security against distinguishers. Our results, valid over arbitrary base fields, also apply to convolutional Goppa codes.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19088
On the origin of the Hercules group: I. chemical signatures indicating the outer bar origin,['Astrophysics of Galaxies'],"['Yusen Li', 'Kenneth Freeman', 'Helmut Jerjen', 'Sven Buder', 'Michael Hayden', 'Ankita Mondal']","The Hercules kinematic group is a kinematic anomaly of stars observed in the solar neighbourhood (SNd). In this series of papers, we present a comprehensive study of this structure. This paper focuses on its chemical signatures over several groups of elements. The next paper discusses its kinematical properties. While studies suggested a non-native origin of Hercules stars due to the distinct chemical and kinematic features, previous studies focussed mainly on the Fe abundances. We adopt chemical data with abundances of elements from APOGEE and GALAH to seek further chemical evidence of the origin. Our analysis reveals that the low alpha population of the low angular momentum Hercules group is significantly enhanced in iron-peak (Fe, Ni, Mn) and odd-Z (Na, Al) elements, and slightly deficient in alpha elements (O, Ca, Ti) compared to kinematically local stars. The super enhancement in iron-peak elements and deficiency in alpha elements support their origin from the outer thin bar in the inner Galaxy. Moreover, the enhancement in Na and Al indicates these stars as the youngest stars in the old sequence from the inner thick disc. Hence, the origin of these stars can be related to the outer bar region. These chemical signatures require the underlying dynamical mechanism that forms the Hercules group to be capable of transporting stars in the inner Galaxy out to the SNd. The next paper will consider the Trojan orbits as the favoured mechanism.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19085
Search for non-standard neutrino interactions with the first six detection units of KM3NeT/ORCA,['High Energy Physics - Experiment'],"['S. Aiello', 'A. Albert', 'A. R. Alhebsi', 'M. Alshamsi', 'S. Alves Garre', 'A. Ambrosone', 'F. Ameli', 'M. Andre', 'L. Aphecetche', 'M. Ardid', 'S. Ardid', 'J. Aublin', 'F. Badaracco', 'L. Bailly-Salins', 'Z. Bardačová', 'B. Baret', 'A. Bariego-Quintana', 'Y. Becherini', 'M. Bendahman', 'F. Benfenati', 'M. Benhassi', 'M. Bennani', 'D. M. Benoit', 'E. Berbee', 'V. Bertin']","KM3NeT/ORCA is an underwater neutrino telescope under construction in the Mediterranean Sea. Its primary scientific goal is to measure the atmospheric neutrino oscillation parameters and to determine the neutrino mass ordering. ORCA can constrain the oscillation parameters $Δm^{2}_{31}$ and $θ_{23}$ by reconstructing the arrival direction and energy of multi-GeV neutrinos crossing the Earth. Searches for deviations from the Standard Model of particle physics in the forward scattering of neutrinos inside Earth matter, produced by Non-Standard Interactions, can be conducted by investigating distortions of the standard oscillation pattern of neutrinos of all flavours. This work reports on the results of the search for non-standard neutrino interactions using the first six detection units of ORCA and 433 kton-years of exposure. No significant deviation from standard interactions was found in a sample of 5828 events reconstructed in the 1 GeV$-$1 TeV energy range. The flavour structure of the non-standard coupling was constrained at 90\% confidence level to be $|\varepsilon_{μτ} | \leq 5.4 \times 10^{-3}$, $|\varepsilon_{eτ} | \leq 7.4 \times 10^{-2}$, $|\varepsilon_{eμ} | \leq 5.6 \times 10^{-2}$ and $-0.015 \leq \varepsilon_{ττ} - \varepsilon_{μμ} \leq 0.017$. The results are comparable to the current most stringent limits placed on the parameters by other experiments.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19078
Improving sub-seasonal wind-speed forecasts in Europe with a non-linear model,['Machine Learning'],"['Ganglin Tian', 'Camille Le Coz', 'Anastase Alexandre Charantonis', 'Alexis Tantet', 'Naveen Goutham', 'Riwal Plougonven']","Sub-seasonal wind speed forecasts provide valuable guidance for wind power system planning and operations, yet the forecasting skills of surface winds decrease sharply after two weeks. However, large-scale variables exhibit greater predictability on this time scale. This study explores the potential of leveraging non-linear relationships between 500 hPa geopotential height (Z500) and surface wind speed to improve subs-seasonal wind speed forecasting skills in Europe. Our proposed framework uses a Multiple Linear Regression (MLR) or a Convolutional Neural Network (CNN) to regress surface wind speed from Z500. Evaluations on ERA5 reanalysis indicate that the CNN performs better due to their non-linearity. Applying these models to sub-seasonal forecasts from the European Centre for Medium-Range Weather Forecasts, various verification metrics demonstrate the advantages of non-linearity. Yet, this is partly explained by the fact that these statistical models are under-dispersive since they explain only a fraction of the target variable variance. Introducing stochastic perturbations to represent the stochasticity of the unexplained part from the signal helps compensate for this issue. Results show that the perturbed CNN performs better than the perturbed MLR only in the first weeks, while the perturbed MLR's performance converges towards that of the perturbed CNN after two weeks. The study finds that introducing stochastic perturbations can address the issue of insufficient spread in these statistical models, with improvements from the non-linearity varying with the lead time of the forecasts.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19077
Shallow Quantum Scalar Products with Phase Information,['Quantum Physics'],"['Lila Cadi Tazi', 'David Muñoz Ramo', 'Alex J. W. Thom']","The measurement of scalar products between two vectors is a common task in scientific computing and, by extension, in quantum computing. In this work, we introduce two alternative quantum circuits for computing scalar products with phase information, combining the structure of the swap test, the vacuum test, and the Hadamard test. These novel frameworks, called the zero-control and one-control tests, present different trade-offs between circuit depth and qubit count for accessing the scalar product between two quantum states. We demonstrate that our approach significantly reduces the gate count for large numbers of qubits and decreases the scaling of quantum requirements compared to the Hadamard test.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19072
Towards an Implementation of the Knowledge-Based Control Plane for Intelligent Swarm Networks,['Networking and Internet Architecture'],"['Xuanchi Guo', 'Anh Le-Tuan', 'Danh Le-Phuoc']","This paper proposes the possibility of integrating Dynamic Knowledge Graph (DKG) with Software-Defined Networking (SDN). This new approach aims to assist the management and control capabilities of the swarm network. The DKG works as a unified network data view, capturing network information such as topology, flow rules, host information, switch information, link status, and in-band network telemetry (INT) data. Benefited from the deep programmability of SDN, the network information can be converted into RDF format constantly, and the DKG will be dynamically updated. This approach helps the network operators to control their network infrastructure, such as allocating resource effectively and decision making at the application layer. Potential use cases demonstrate the applicability and advantages of the proposed approach. Examples include access control in swarm network scenarios and applying adaptive routing strategies, etc. These use cases illustrate how DKG-based SDN can address swarm network management challenges effectively, optimizing performance and resource utilization.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19068
Distributed matrix multiplication with straggler tolerance over very small field,['Information Theory'],"['Adrián Fidalgo-Díaz', 'Umberto Martínez-Peñas']","The problem of distributed matrix multiplication with straggler tolerance over finite fields is considered, focusing on field sizes for which previous solutions were not applicable (for instance, the field of two elements). We employ Reed-Muller-type codes for explicitly constructing the desired algorithms and study their parameters by translating the problem into a combinatorial problem involving sums of discrete convex sets. We generalize polynomial codes and matdot codes, discussing the impossibility of the latter being applicable for very small field sizes, while providing optimal solutions for some regimes of parameters in both cases.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19065
A theorem on vacuum stability,['High Energy Physics - Phenomenology'],"['André Milagre', 'Luís Lavoura']","We consider an extension of the Standard Model with one or more scalar multiplets beyond the Higgs doublet $Φ$. The additional scalar multiplets are supposed to carry arbitrary hypercharges. We prove that, in such a model, if the field configuration where only $Φ$ has a nonzero vacuum expectation value (VEV) is a local minimum of the potential, then it has a lower value of the potential than any field configuration where both $Φ$ and other scalar multiplets have nonzero VEVs. We also consider the field configurations where only one multiplet $Ξ$ (different from $Φ$) with isospin $J \le 7/2$ has nonzero VEV; we compute the values of the potential at those possible vacua.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19063
"A study of particle acceleration, heating, power deposition, and the damping length of kinetic Alfvén waves in non-Maxwellian coronal plasma",['Solar and Stellar Astrophysics'],"['S. Ayaz', 'Gary P. Zank', 'Imran A. Khan', 'G. Li', 'Yeimy J. Rivera']","The heating of the solar corona and solar wind, through suprathermal particles and kinetic Alfvén waves within the 0 - 10 $R_{\rm Sun}$ range, has been a subject of great interest for many decades. This study investigates the acceleration and heating of charged particles and the role of KAWs in the solar corona. We investigate how KAWs transport energy and accelerate/heat the charged particles, focusing on the behavior of perturbed EM fields, Poynting flux vectors, net power transfer, resonant particle speed, group speed, and the damping length of KAWs. The study examines how these elements are influenced by suprathermal particles κand the electron-to-ion temperature $T_e/T_i$. We use kinetic plasma theory coupled with the Vlasov-Maxwell model to investigate the dynamics of KAWs and particles. We assume a collisionless, homogeneous, and low-beta electron-ion plasma in which Alfvén waves travel in the kinetic limits. The results show the perturbed EM fields are significantly influenced by $κ$ and $T_e/T_i$. We evaluate both the parallel and perpendicular Poynting fluxes and find that the parallel Poynting flux dissipates gradually for lower κvalues. The perpendicular flux dissipates quickly over shorter distances. Power deposition in solar flux tubes is significantly influenced by κand Te/Ti. We find that particles can heat the solar corona over long distances in the parallel direction and short distances in the perpendicular direction. The group velocity of KAWs increases for lower κvalues, and the damping length is enhanced under lower κ, suggesting longer energy transport distances. These findings offer a comprehensive understanding of particle-wave interactions in the solar corona and wind, with potential applications for missions such as the Parker Solar Probe (PSP), and can also apply to other environments.△ Less",v1,https://arxiv.org/pdf/2411.19061
Ghost projection via focal-field diffraction catastrophes,['Optics'],"['James A. Monro', 'Andrew M. Kingston', 'David M. Paganin']","Ghost projection is the reversed process of computational classical ghost imaging that allows any desired image to be synthesized using a linear combination of illuminating patterns. Typically, physical attenuating masks are used to produce these illuminating patterns. A mask-free alternative form of ghost projection is explored here, where the illuminations are a set of caustic-laden diffraction patterns known as diffraction catastrophes. These are generated by focusing a coherent beam with spatially modulated phase having random Zernike-polynomial aberrations. We demonstrate, via simulation, that a suitable linear combination of such random focal-field intensity patterns can be used as a basis to synthesize arbitrary images. In our proof-of-concept ghost-projection synthesis, the positive weighting coefficients in the decomposition are proportional to exposure times for each focal-field diffraction catastrophe. Potential applications include dynamic on-demand beam shaping of focused fields, aberration correction and lithography.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19053
A dichotomy theorem on the complexity of 3-uniform hypergraphic degree sequence graphicality,['Combinatorics'],"['Sara Logsdon', 'Arya Maheshwari', 'István Miklós', 'Angelina Zhang']","We present a dichotomy theorem on the parameterized complexity of the 3-uniform hypergraphicality problem. Given $0<c_1\le c_2 < 1$, the parameterized 3-uniform Hypergraphic Degree Sequence problem, $3uni-HDS_{c_1,c_2}$, considers degree sequences $D$ of length $n$ such that all degrees are between $c_1 {n-1 \choose 2}$ and $c_2 {n-1\choose 2}$ and it asks if there is a 3-uniform hypergraph with degree sequence $D$. We prove that for any $0<c_2< 1$, there exists a unique, polynomial-time computable $c_1^*$ with the following properties. For any $ c_1\in (c_1^*,c_2]$, $3uni-HDS_{c_1,c_2}$ can be solved in linear time. In fact, for any $c_1\in (c_1^*,c_2]$ there exists an easy-to-compute $n_0$ such that any degree sequence $D$ of length $n\ge n_0$ and all degrees between $c_1 {n-1\choose 2}$ and $c_2 {n-1\choose 2}$ has a 3-uniform hypergraph realization if and only if the sum of the degrees can be divided by $3$. Further, $n_0$ grows polynomially with the inverse of $c_1-c_1^*$. On the other hand, we prove that for all $c_1<c_1^*$, $3uni-HDS_{c_1,c_2}$ is NP-complete. Finally, we briefly consider an extension of the hypergraphicality problem to arbitrary $t$-uniformity. We show that the interval where degree sequences (satisfying divisibility conditions) always have $t$-uniform hypergraph realizations must become increasingly narrow, with interval width tending to $0$ as $t \rightarrow \infty$.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19049
Stability of heat kernel bounds under pointed Gromov-Hausdorff convergence,['Metric Geometry'],['Aobo Chen'],"We present a method for constructing a heat kernel on the pointed Gromov-Hausdorff limit space and demonstrate the stability of heat kernel estimates under this convergence. Furthermore, we establish the Mosco convergence of the associated energy forms along a subsequence.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19047
Aggregating Data for Optimal and Private Learning,['Machine Learning'],"['Sushant Agarwal', 'Yukti Makhija', 'Rishi Saket', 'Aravindan Raghuveer']","Multiple Instance Regression (MIR) and Learning from Label Proportions (LLP) are learning frameworks arising in many applications, where the training data is partitioned into disjoint sets or bags, and only an aggregate label i.e., bag-label for each bag is available to the learner. In the case of MIR, the bag-label is the label of an undisclosed instance from the bag, while in LLP, the bag-label is the mean of the bag's labels. In this paper, we study for various loss functions in MIR and LLP, what is the optimal way to partition the dataset into bags such that the utility for downstream tasks like linear regression is maximized. We theoretically provide utility guarantees, and show that in each case, the optimal bagging strategy (approximately) reduces to finding an optimal clustering of the feature vectors or the labels with respect to natural objectives such as $k$-means. We also show that our bagging mechanisms can be made label-differentially private, incurring an additional utility error. We then generalize our results to the setting of Generalized Linear Models (GLMs). Finally, we experimentally validate our theoretical results.△ Less","28 November, 2024;",https://arxiv.org/pdf/2411.19045
